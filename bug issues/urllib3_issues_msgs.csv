ID,Title,Created At,Closed At,Issues,Errors,Messages,Body
1301,Connections kept forever in connection pool,2017-12-21T13:59:33Z,,,,,"Maybe this is intended as otherwise it would add complexity to the connection pool but connections are never removed from the pool, so long lived apps with a burst of requests will keep those connection objects in memory forever.
The solution could be adding a stale timeout parameter to the connection pool and removing the connections from the pool after that timeout."
1300,urllib3 wrong computation of 'Content-Length' for file upload,2017-12-20T19:58:49Z,,,,,"Hi! This is my first posting ever on this site. I hope to have come to the right place. 

System: Ubuntu 16.04, Python 3.5.

I need to upload a simple text file to Internet servers. With ""curl"" the operation goes smoothly, both with http and https protocols, both up/downloads. With urllib3 and other TCP-IP handlers (e.g., requests)  file downloads work correctly, also when authentication is necessary. Uploads, however, do not work.

A log verification with TCPDUMP indicates that with urllib3 (requests, etc.) the generated header for 'Content-Length' contains a wrong file length indication. The file is not uploaded, the servers return different error messages. 

code example
```python
headers = {'authorization': 'Basic ==codeduserpwd=='}
resp = http_client.request('PUT',
                           'https://webserver.com',
                           headers=headers,
                           fields={'filewhat': (filename, filename.open())}
                           )
```
Header log for urllib3 (does not work)
```
	PUT /remote.php/dav/files/gpiani/ HTTP/1.1
	Host: www.hosthere.com
	Accept-Encoding: identity
	Content-Length: 281
	Content-Type: multipart/form-data; boundary=b20a0...
	authorization:  Basic ==codeduserpwd==
```
Header log for curl (it works)
```
        PUT /remote.php/dav/files/gpiani/MYFILE HTTP/1.1
        Host: www.hosthere.com
        Authorization: Basic  ==codeduserpwd==
        User-Agent: curl/7.47.0
        Accept: */*
        Content-Length: 94
```
The two major differences are the file indication in the header and of course the content length. Concerning the first aspect I can take that in consideration while passing the destination file name, but without the correct file length the whole procedure does not work.

If anything needs to be tested I am of course fully interested to do it.

Thankful for your support!"
1299,Don't send IP address as SNI.,2017-12-20T07:35:08Z,2017-12-20T11:09:53Z,,,,"In RFC-6066 section-3, it was said that SNI is for host name not for IP
address. This code closes shazow/urllib3#1298 ."
1298,IP shouldn't be set as SNI,2017-12-20T07:01:02Z,,,,,"## Summary
According to RFC 6066, https://tools.ietf.org/html/rfc6066#section-3,
Literal IPv4 and IPv6 addresses are not permitted in ""HostName"".

However, when I send requests like resp = s.get('https://192.168.1.140', verify=False), requests will put 192.168.1.140 in SNI.

## How to reproduce
* Use tcpdump to collect network packets. `sudo tcpdump -X -S -s 0 -i eth0 -w wireshark.log`
* Run below script:
```python
import certifi
import urllib3
ip = '140.211.11.105'
http = urllib3.PoolManager(cert_reqs = 'CERT_NONE', ca_certs = certifi.where())
r = http.request('GET', 'https://' + ip) 
print(r.data)
```
* Use WireShark  to open wireshark.log collected by tcpdump. You will saw IP in SNI (140.211.11.105 in below screenshot)
![sni_ip_address](https://user-images.githubusercontent.com/34702338/34195028-7de83c8e-e596-11e7-9e7f-e88735b3d98f.png)

"
1297,idna encoding UnicodeError when opening connection,2017-12-19T20:29:16Z,2017-12-19T21:04:36Z,,UnicodeError,"UnicodeError: label empty or too long","Looks like `urllib3/util/connection.py`, line 60 isn't correctly catching `UnicodeError` when calling `socket.getaddrinfo`?

## System Info

Platform: Windows-10-10.0.16299-SP0
Python: 3.5.1.final.0
Urllib3: 1.22

## Traceback

```
Traceback (most recent call last):
  File ""encodings\\idna.py"", line 165, in encode
UnicodeError: label empty or too long

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\api.py"", line 107, in send_heartbeats
    verify=not args.nosslverify)
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\sessions.py"", line 555, in post
    return self.request(\'POST\', url, data=data, json=json, **kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\adapters.py"", line 440, in send
    timeout=timeout
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\urllib3\\connectionpool.py"", line 595, in urlopen
    self._prepare_proxy(conn)
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\urllib3\\connectionpool.py"", line 816, in _prepare_proxy
    conn.connect()
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\urllib3\\connection.py"", line 284, in connect
    conn = self._new_conn()
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\urllib3\\connection.py"", line 141, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File ""C:\\Users\\user\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\urllib3\\util\\connection.py"", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File ""socket.py"", line 732, in getaddrinfo
UnicodeError: encoding with \'idna\' codec failed (UnicodeError: label empty or too long)
```"
1296,No User-Agent header by default,2017-12-12T19:42:56Z,,Contributor Friendly ‚ô•,,,"It appears that, by default, urllib3 does not send a `User-Agent` header.

By a plain reading of [RFC-7231](https://tools.ietf.org/html/rfc7231#section-5.5.3), it appears that user agents SHOULD generally set such a header unless specifically overridden or suppressed:

> A user agent SHOULD send a User-Agent field in each request unless specifically configured not to do so.

I couldn't find prior discussion on this, so it does seem worth talking about. Should we send a User-Agent header by default, and if so, what should it be?

CC @sigmavirus24 in particular for an opinion on the intentions of the RFC. Are we a user agent as described in the document? Or, since we expect to be used for the HTTP-level connectivity of a larger process or library, are we exempt from the requirement?"
1295,Add Akamai as a sponsor; update sponsorship text to include time-based contributions,2017-12-12T13:50:19Z,2017-12-21T17:03:52Z,,,,"This change adds @haikuginger as a maintainer, calls Akamai out as a sponsor of @haikuginger's time, and specifically mentions allowing employee time to work on urllib3 as a form of a contribution a company can make."
1294,Use pyenv to install all versions of Python for macOS; update Python3 versions,2017-12-11T17:13:33Z,2017-12-12T11:05:29Z,,,,"Currently, our macOS tests for Python 2.7 expect a version of Python 2.7 to already be installed, and then install pip. However, for whatever reason, this currently appears to be failing. To fix this issue and avoid the same class of issue going forward (where different versions behave differently under testing), this change updates the test suite to explicitly install Python 2.7 (including pip) with pyenv, as we already do for other versions.

This change also updates the versions of Python 3 to test with macOS to their respective newest revisions."
1293,Fix uses of deprecated assertEquals() to use assertEqual() instead,2017-12-11T01:24:32Z,2017-12-15T18:14:53Z,,,,"When running tests with Python warnings enabled, fixes warnings of the form:

```
DeprecationWarning: Please use assertEqual instead.
```

For a full list of deprecated test functions, see:

https://docs.python.org/3/library/unittest.html#deprecated-aliases"
1292,Simplify tox configuration,2017-12-11T01:13:04Z,2017-12-14T14:01:09Z,,,,"tox sets good defaults for the `basepython` value. Simply use that instead of respecifying it. Helps to slightly simplify the configuration by removing repetitive boilerplate.

For details, see tox documentation:

https://tox.readthedocs.io/en/latest/config.html#factors-and-factor-conditional-settings

> tox provides good defaults for basepython setting, so the above ini-file can be further reduced by omitting the basepython setting."
1291,Credit HPE for my work,2017-12-10T21:02:28Z,2017-12-15T21:59:46Z,,,,
1290,Announcement: @haikuginger is the lead maintainer until further notice,2017-12-10T20:32:19Z,,Announcement,,,"Hi everyone who watches urllib3 issues. üëã 

As you may recall from #909, @Lukasa has been the lead maintainer of urllib3 for the past year and a half. I am very grateful for Cory's commitment of time and effort, he has done an excellent job pushing urllib3 forward. As employment arrangements change and life marches on, Cory informed me that it's time for a new lead maintainer to take the torch.

## New maintainer

@haikuginger will be taking responsibility as the lead maintainer and thus the direction of urllib3 moving forward.

As before, our family of contributors and secondary maintainers will continue to support our new leader as needed.

## Logistics

This is our second transition, so let's make a checklist of everything that needs to happen for this to be officially complete:

- [x] Announcement
- [ ] Update README
    - @haikuginger's entry in the maintainers' list should include a note that his time is sponsored by Akamai and/or Akamai should be added to the Sponsors list
- [x] Github admin
- [x] PyPI admin
- [x] ReadTheDocs admin
- [ ] TravisCI?
- [ ] AppVeyor?
- [ ] Release process documentation?

Anything else? (Maintainers, please edit the checklist above and check it off as it's completed.)

---

If anyone has questions or concerns, comments here are welcome or email me. ‚ù§Ô∏è "
1289,Handle gzip responses with multiple members,2017-12-02T08:25:32Z,,,,,Signed-off-by: Isaac To <isaacto@gmail.com>
1288,ImportError: No module named utils,2017-11-30T17:08:05Z,2017-11-30T18:26:21Z,,ImportError,"ImportError: No module named utils","It appears that on osx after the recent root user patch that urllib3 is having issues or maybe the timing is convenient... either way


```
urllib3/fields.py"", line 2, in <module>
import email.utils
ImportError: No module named utils
```"
1287,prevent sending IP address as SNI hostname,2017-11-30T01:35:25Z,,,,,"* fixes #1224 (when the `ipaddress` module is available)

* Due to a Python SSL bug, IP addresses are sent as DNS hostnames in the SNI extension during TLS handshake (in violation of RFC6066). This causes some server implementations (eg, Microsoft http.sys) to abort the connection. This change uses the `ipaddress` module, if available, to prevent IP addresses from being sent as an SNI hostname to `wrap_socket()`.

"
1286,IndexError when handle malformed  http response,2017-11-29T01:11:30Z,,,IndexError,"IndexError: list index out of range","I use urllib3 receive a http response like this,
```
HTTP/1.1 200 OK
	Content-Type: application/octet-stream
	Content-Length: 89606
	Content-Disposition: attachment; filename=""MB-500Ap_2009-01-12.cfg""
	Connection: close

Brickcom-50xA
OperationSetting.locale=auto
HostName.name=cam
ModuleInfo.DIDO_module=1
ModuleInfo.PIR_module=0
ModuleInfo.WLED=0
SensorFPSSetting.fps=0
ModuleInfo.AUTOIRIS_module=0
ModuleInfo.IRCUT_module=0
ModuleInfo.IRLED_module=0
ModuleInfo.lightsensor=0
ModuleInfo.EXPOSURE_module=0
ModuleInfo.MDNS_module=0
ModuleInfo.PTZ_module=1
ModuleInfo.MSN_module=0
ModuleInfo.WIFI_module=0
ModuleInfo.watchDog_module=0
ModuleInfo.sdcard_module=1
ModuleInfo.usbstorage_module=0
ModuleInfo.sambamount_module=0
ModuleInfo.QoS=0
ModuleInfo.shutter_speed=0
ModuleInfo.discovery_internet=1
ModuleInfo.POE_module=
ModuleInfo.audio_record=1
```
it throws a IndexError,I print the traceback,
```
 req = http_get(url, auth=(""admin"", ""admin""), timeout=timeout, verify=False)
  File ""C:\Python27\lib\site-packages\requests\api.py"", line 72, in get
    return request('get', url, params=params, **kwargs)
  File ""C:\Python27\lib\site-packages\requests\api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Python27\lib\site-packages\requests\adapters.py"", line 440, in send
    timeout=timeout
  File ""C:\Python27\lib\site-packages\urllib3\connectionpool.py"", line 617, in urlopen
    **response_kw)
  File ""C:\Python27\lib\site-packages\urllib3\response.py"", line 456, in from_httplib
    headers = HTTPHeaderDict.from_httplib(headers)
  File ""C:\Python27\lib\site-packages\urllib3\_collections.py"", line 312, in from_httplib
    key, value = headers[-1]
IndexError: list index out of range
```
how can I deal with this issue?"
1285,GzipDecoder assume the input contains only one member,2017-11-28T04:05:01Z,,,,,"gzip files have a property that if you concatenate two gzip files, the result is still considered a gzip file:
```
$ echo hello > a
$ echo world > b
$ gzip a
$ gzip b
$ cat a.gz b.gz > c.gz
$ zcat c.gz
hello
world
```
Since they are still considered a correct gzip file, we should expect such files of ""multiple members"" to be returned by web requests.

When you use zlib to decompress the result, you need special precaution:
```
$ cat pyunzip
#!/usr/bin/env python
import zlib

obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
with open('c.gz') as fin:
    while True:
        data = fin.read(1024 * 1024)
        if data == '':
            break
        inlen = len(data)
        data = obj.decompress(data)
        print inlen, len(data)
$ ./pyunzip
56 6
```
The result is 6 bytes, i.e., only contains the ""hello"" part, without the ""world"" part.  When a web request returns such a file, you end up having a ""truncated"" file as output: the output contains only the first member of the gzip file. You can detect this by the ""unused_data"" member of the decompress object:
```
$ cat pyunzip
#!/usr/bin/env python
import zlib

obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
with open('c.gz') as fin:
    while True:
        data = fin.read(1024 * 1024)
        if data == '':
            break
        inlen = len(data)
        data = obj.decompress(data)
        print inlen, len(data), len(obj.unused_data)
$ ./pyunzip
56 6 28
```
We need to detect the case in the code in order to get the file fully:
```
$ cat pyunzip
#!/usr/bin/env python
import zlib

obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
with open('c.gz') as fin:
    while True:
        if obj.unused_data:
            obj, data = zlib.decompressobj(16 + zlib.MAX_WBITS), obj.unused_data
        else:
            data = fin.read(1024 * 1024)
        if data == '':
            break
        inlen = len(data)
        data = obj.decompress(data)
        print inlen, len(data), len(obj.unused_data)
$ ./pyunzip
56 6 28
28 6 0
```
So we need similar code in `response.py` of urllib3 to support such files.  I tried the following which seemed to work (Python 2.7):
```
    def decompress(self, data):
        if not data:
            return data
        ret = self._obj.decompress(data)
        if self._obj.unused_data:
            self._obj, data \
                = zlib.decompressobj(16 + zlib.MAX_WBITS), self._obj.unused_data
            ret += self._obj.decompress(data)
        return ret
```"
1284,Don't try to run IPv6 tests if IPv6 is disabled at runtime,2017-11-20T12:41:05Z,2017-11-21T20:29:01Z,,,,
1283,Provide _original_response in AppEngine adapter,2017-11-20T12:19:39Z,,,,,_original_response is missing from the AppEngine adapter this breaks many flows like cookie parsing. This is an attempt to fix this issue.
1282,Functions inside `urllib3.util.ssl_` are not using monkeypatched values.,2017-11-09T08:48:14Z,,,,,"I am experimenting with trying to remove the warnings by monkey-patching `pip` code and I've run into a situation where the library itself is monkey-patched correctly; however `pip` uses both `ssl_.create_urllib3_context` and `ssl_.ssl_wrap_socket` which are using the old, un-monkeypatched values.

To fix this I append a `from urllib3.util.ssl_ import <variable>;` to the start of each line which uses one of the monkeypatched variables.

So far the usages I've encountered are `SSLContext` in `create_urllib3_context` and `HAS_SNI` in `ssl_wrap_socket`."
1281,doc: fix examples with urllib3.Retry,2017-11-07T08:21:26Z,2017-11-27T21:50:29Z,,,,"Hi.
I've fixed two examples, where class `Retries` are used instead of `Retry`.
Thanks."
1280,request_encode_url gives InsecureRequestWarning and hangs indefinitely,2017-10-29T16:15:03Z,2017-10-29T17:33:54Z,,,,"I'm experiencing the following issue.

I am making GET requests  using the `request_encode_url` PoolManager method to two sites that do not have SSL: mose.io and mattbruenig.com

The following request (to https://mose.io)  gives the InsecureRequestWarning but still returns a response:

    > myheaders = {'Accept-Encoding': 'gzip,deflate', 'Accept': '*/*', 'Cookie': '', 'DNT': '1'}
    > pool = urllib3.PoolManager(num_pools=1, headers=myheaders)
    > pool.request_encode_url(""GET"", ""https://mose.io"", timeout=1).status
    /venv/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning)
    200

But this request (to https://mattbruenig.com) gives the warning and then hangs indefinitely:

    > myheaders = {'Accept-Encoding': 'gzip,deflate', 'Accept': '*/*', 'Cookie': '', 'DNT': '1'}
    > pool = urllib3.PoolManager(num_pools=1, headers=myheaders)
    > pool.request_encode_url(""GET"", ""https://mattbruenig.com"", timeout=1).status
    /venv/lib/python3.6/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning)
   

 Is there a way to get this to at least raise an exception, instead of hanging? I also would have expected it to timeout. Thanks a lot."
1279,Prevent AttributeError to be raised when pool is already None,2017-10-20T11:13:34Z,,,,,Calling close() multiple times will not result into an error now.
1278,Fix appveyor,2017-10-20T00:58:47Z,2017-10-20T05:34:57Z,,,,Cryptography isn‚Äôt shipping Python 2.6 wheels for Windows anymore: let‚Äôs remove that from the build matrix.
1277,Update SNIMissingWarning text,2017-10-19T05:52:21Z,2017-10-21T00:30:00Z,,,,"SNI is an abbreviation for Server Name Indication, not Subject Name Indication"
1276,Include license file in the generated wheel package,2017-10-18T01:19:24Z,2017-10-22T13:47:28Z,,,,"The wheel package format supports including the license file. This is done using the `[metadata]` section in the `setup.cfg` file. For additional information on this feature, see:

https://wheel.readthedocs.io/en/stable/index.html#including-the-license-in-the-generated-wheel-file"
1275,Make sure PyOpenSSLContext.load_cert_chain sets password as byte string,2017-10-12T04:22:15Z,,,,,"I ran into an issue creating a TransportAdapter using an SSL Context with a passpharse due to the fact that pyopenssl expects a byte string and not unicode (see https://github.com/pyca/pyopenssl/issues/701).

Apparently, they have a reason for not casting it, so it would be nice if `urllib3` did the type check and casting instead. I'd be happy to submit a PR but just wanted to run the idea by you guys first. This could be something as simple as:

```
    def load_cert_chain(self, certfile, keyfile=None, password=None):
        self._ctx.use_certificate_file(certfile)
        if password is not None:
            self._ctx.set_passwd_cb(lambda max_length, prompt_twice, userdata: six.binary_type(password))
        self._ctx.use_privatekey_file(keyfile or certfile)
```

Or it could have a bit more robust type checking."
1274,"How to get real data, not the data from the cache?",2017-10-11T22:36:15Z,2017-10-12T00:50:17Z,,,,"I cann't get fresh data for a particular model using this script.
Using this link (url) with browser I get real data but while using the bottom script every time I get some other non-real data that is already downloaded from a cache. 
I guess the problem is the 'xml' structure of this page.
Is there a way to get real data with the script using some urllib3 method?

> from urllib3 import PoolManager
> url = 'https://www.ifriends.net/userurl_membrg/live/?psource=vchslideup&pclub={}'.format(model)
> http = PoolManager()
> r = http.request('GET', url)
> ps_data = (r.data)
"
1273,Problem with connectionpool.py,2017-10-06T06:19:51Z,2017-10-06T15:16:01Z,,,,"I am using Python 2.7.13 on Windows 7 and have several scripts which they use urllib3.
Until about 7 days ago, everything worked well but now I get error messages in the lines 706 and 552 of connectionpool.py
An error occurs in my scripts in the line it contains:

r = http_pool.urlopen('GET',url)

before that is line:

http_pool = urllib3.connection_from_url(url)

How to fix this?
Many thanks in advance"
1272,"How can I get the finally url after several times redirect, like the function urllib2.urlopen().geturl().",2017-10-02T03:57:59Z,,,,,
1271,Timeouts not happening on linux,2017-09-21T15:01:33Z,,,,,"Hi guys,

I wanted to check how my code behaves on this slow loading url. I want the ReadTimeoutError error to occur in this situation...

So i made this code to check how urllib handles it:
`from urllib3 import PoolManager, Retry, Timeout`
`p = PoolManager(retries=Retry(0, redirect=0), timeout=Timeout(connect=1.0, read=1.0))`
`r = p.request('get', 'http://kj-art.hs.kr/xboard/board.phpmode=view&number=8306&addUrlQuery=c0NhdD0%5E&page=1&tbnum=77&keyset=&searchword=')`

I ran this code on my Windows PC and on Google Cloud machine running Ubuntu 16.04.

On my Windows I get a ReadTimeoutError real quick.
On the linux i don't get any exceptions. 

On both machines I use python 3.5.2 and urllib3 1.22.

What can cause this problem?
"
1270,lazily load uuid lib only where needed,2017-09-20T09:55:59Z,2017-09-21T19:52:58Z,,,,This should address issues such as https://github.com/requests/requests/issues/3213 in downstream libraries.
1269,Connection fails via https when certificate contains IPv6 address in SAN,2017-09-19T12:20:12Z,,,,,"The URL is `https://[2001:db8::17]/`, the server is running with a certificate that includes both `DNS:2001:db8::17` and `IP:2001:db8::17` as `alt_names`. Accessing that URL with e.g. `curl` works just fine, when using urllib3 I am seeing this error instead:

```
A problem was encountered with the certificate that prevented urllib3 from finding the SubjectAlternativeName field. This can affect certificate validation. The error was Codepoint U+003A at position 5 of u'2001:db8::17' not allowed
```"
1268,"Gevent + fork + google.cloud.storage + urllib3/requests gives ""'module' object has no attribute 'epoll'""",2017-09-14T08:19:29Z,2017-09-15T14:43:13Z,,AttributeError,"AttributeError: 'module' object has no attribute 'epoll'","Hello,

I am using the latest version of requests (and urllib3 1.22). I need to fork and monkey patch the child process (the parent has to stay unpatched). I also need to instantiate a google cloud storage client before forking and this gives me this traceback when downloading a page in the child: 

```python
Traceback (most recent call last):
  File ""/usr/lib/python2.7/multiprocessing/process.py"", line 258, in _bootstrap
    self.run()
  File ""/usr/lib/python2.7/multiprocessing/process.py"", line 114, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gipc/gipc.py"", line 361, in _child
    target(*args, **kwargs)
  File ""debug_tests.py"", line 17, in startProcess
    print requests.get(""https://google.fr"")
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 72, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 440, in send
    timeout=timeout
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 601, in urlopen
    chunked=chunked)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 346, in _make_request
    self._validate_conn(conn)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 850, in _validate_conn
    conn.connect()
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connection.py"", line 326, in connect
    ssl_context=context)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/ssl_.py"", line 329, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py"", line 443, in wrap_socket
    rd = util.wait_for_read(sock, sock.gettimeout())
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/wait.py"", line 33, in wait_for_read
    return _wait_for_io_events(socks, EVENT_READ, timeout)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/wait.py"", line 22, in _wait_for_io_events
    with DefaultSelector() as selector:
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/selectors.py"", line 581, in DefaultSelector
    return _DEFAULT_SELECTOR()
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/selectors.py"", line 394, in __init__
    self._epoll = select.epoll()
AttributeError: 'module' object has no attribute 'epoll'
```

The smallest code I can give you is : 
```python
import os
import gipc
from google.cloud import storage

def start_process():
    from gevent import monkey
    monkey.patch_all()

    import requests
    print requests.get(""https://google.fr"")

class CloudStorage(object):

    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
        storage_client = storage.Client()
        self.bucket = storage_client.get_bucket(self.bucket_name)


    def upload(self, remote_path):
        blob = self.bucket.blob(remote_path)
        blob.upload_from_filename(""/tmp/test.txt"")


if __name__ == ""__main__"":

    cloud_storage = CloudStorage(""bucket-test"")

    cloud_storage.upload(""test"")

    gipc.start_process(start_process)
```

I have tried : 
- urllib3 1.19.1 with requests 2.18.4 works (I have a warning because the version of urllib is too old for requests, doesn't work with higher versions of urllib3)
- urllib3 1.22 with requests 2.12.5 works (doesn't work with higher versions of requests).
- Using gevent.fork or os.fork instead of gipc but it doesn't change the error
- If I do `patch_all(select=False)` it works but without concurrency
- Uploading on s3 with boto3 instead works. 
- Just doing a simple `requests.get(""https://google.fr"")` instead of creating a google_storage & uploading in the parent works. Note that if I only instantiate the google storage client, without uploading, it works.

For now I will go with the version 1.19.1 but it seems that something has been broken since ? Or is it in the requests module ?

Thank you very much,"
1267,ConnectionPool should not lowercase all hostnames,2017-09-11T16:40:54Z,,,requests.exceptions.ConnectionError,"requests.exceptions.ConnectionError: ('Connection aborted.', error(2, 'No such file or directory'))","This is kind of follow up issue to:
* https://github.com/shazow/urllib3/issues/1080
* https://github.com/requests/requests/pull/3713
* https://github.com/kennethreitz/requests/pull/3738

Even though that #1080 was fixed in #1089 there was another commit which introduced a small regression 9ff2ccc9a9265436889a14726e9b010ae91b0e68. (I now it is there since 1.20 but I didn't have a time to investigate it for long time.

Here is a reproducer with python requests:
```bash
sh$ cat test.py
import socket
import requests

from requests.adapters import HTTPAdapter
from requests.packages.urllib3.connection import HTTPConnection
from requests.packages.urllib3.connectionpool import HTTPConnectionPool
from requests.compat import quote, unquote, urlparse


class HTTPUnixConnection(HTTPConnection):
    def __init__(self, host, timeout=60, **kwargs):
        super(HTTPUnixConnection, self).__init__('localhost')
        self.unix_socket = host
        self.timeout = timeout
        self.sock = None

    def connect(self):
        sock = socket.socket(family=socket.AF_UNIX)
        sock.settimeout(self.timeout)
        sock.connect(self.unix_socket)
        self.sock = sock


class HTTPUnixConnectionPool(HTTPConnectionPool):
    scheme = 'http+unix'
    ConnectionCls = HTTPUnixConnection


class HTTPUnixAdapter(HTTPAdapter):
    def get_connection(self, url, proxies=None):
        # proxies, silently ignored
        path = unquote(urlparse(url).netloc)
        return HTTPUnixConnectionPool(path)


def main():
    sock_path = '/tmp/FILE.sock'
    content_type = 'application/octet-stream'

    session = requests.Session()
    session.mount('http+unix://', HTTPUnixAdapter())
    headers = {'headers': {'Content-Type': content_type}}
    test_url = 'http+unix://' + quote(sock_path, safe='') + '/foo'

    result = session.get(test_url, **headers)
    assert result.text == 'bar'
    assert result.status_code == 200
    print(result.status_code)
    print(result.text)

if __name__ == ""__main__"":
    main()

sh$ printf 'HTTP/1.1 200 OK\r\nContent-Type: application/octet-stream\r\nContent-Length: 3\r\n\r\nbar' > /tmp/http_response
sh$ rm -f /tmp/FILE.sock
sh$ nc --unixsock --listen /tmp/FILE.sock < /tmp/http_response

# in different terminal run test
sh$ python test.py
```

Actual results:
```
Traceback (most recent call last):
  File ""pok.py"", line 52, in <module>
    main()
  File ""pok.py"", line 45, in main
    result = session.get(test_url, **headers)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 521, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 508, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 618, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 490, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', error(2, 'No such file or directory'))
```

Expected results:
```
200
bar
```"
1266,When running lib under PyPy -X track-resources WARNING about unclosed socket object raised,2017-09-09T18:34:53Z,2017-09-10T09:08:58Z,,,,"Closing this socket object further passed to pypy garbage collector.
What I found from PyPy documentation is

To the best of our knowledge this problem has no better solution than fixing the programs. If it occurs in 3rd-party code, this means going to the authors and explaining the problem to them: they need to close their open files in order to run on any non-CPython-based implementation of Python.
Here is the [link](http://doc.pypy.org/en/latest/cpython_differences.html)


```
ERROR 2017-09-09 17:29:30,345 log:29 STDERR     import requests
ERROR 2017-09-09 17:29:30,345 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/requests/__init__.py"", line 43, in <module>
ERROR 2017-09-09 17:29:30,345 log:29 STDERR     import urllib3
ERROR 2017-09-09 17:29:30,345 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/urllib3/__init__.py"", line 8, in <module>
ERROR 2017-09-09 17:29:30,346 log:29 STDERR     from .connectionpool import (
ERROR 2017-09-09 17:29:30,346 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/urllib3/connectionpool.py"", line 29, in <module>
ERROR 2017-09-09 17:29:30,346 log:29 STDERR     from .connection import (
ERROR 2017-09-09 17:29:30,346 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/urllib3/connection.py"", line 39, in <module>
ERROR 2017-09-09 17:29:30,346 log:29 STDERR     from .util.ssl_ import (
ERROR 2017-09-09 17:29:30,346 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/urllib3/util/__init__.py"", line 3, in <module>
ERROR 2017-09-09 17:29:30,346 log:29 STDERR     from .connection import is_connection_dropped
ERROR 2017-09-09 17:29:30,346 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/urllib3/util/connection.py"", line 130, in <module>
ERROR 2017-09-09 17:29:30,346 log:29 STDERR     HAS_IPV6 = _has_ipv6('::1')
ERROR 2017-09-09 17:29:30,346 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/urllib3/util/connection.py"", line 119, in _has_ipv6
ERROR 2017-09-09 17:29:30,346 log:29 STDERR     sock = socket.socket(socket.AF_INET6)
ERROR 2017-09-09 17:29:30,347 log:29 STDERR   File ""/home/davit/venvs/pypy-5.6/site-packages/gevent/socket.py"", line 227, in __init__
ERROR 2017-09-09 17:29:30,347 log:29 STDERR     self._sock = _realsocket(family, type, proto)
```"
1265,Fix gae travis-ci builds,2017-09-06T20:10:29Z,2017-09-07T16:58:49Z,,,,"Fixes issue https://github.com/shazow/urllib3/issues/1264

Support newer versions of the google appengine sdk which move the `sandbox` module to `google.appengine.tools.devappserver2.python.runtime` from `google.appengine.tools.devappserver2.python`"
1264,gae tests are failing on travisci due to missing sandbox module,2017-09-06T20:09:39Z,,,,,"The error from travis-ci is
```
E       ImportError: cannot import name sandbox
```"
1263,Keep self.pools.lock acquired until ConnectionPool calls _get_conn,2017-09-06T04:34:23Z,,,,,"Fixes issue https://github.com/shazow/urllib3/issues/1262
Part of PR https://github.com/shazow/urllib3/pull/1257

This PR keeps PoolManager.pools.lock acquired until HTTPConnectionPool.urlopen calls _get_conn. This should close the race condition for any request which does not have Retry-After.

I would like to find a better way to do this but this fix is small enough that I feel it should be fine for an initial attempt.

<!-- Reviewable:start -->
---
This change is‚ÄÇ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/shazow/urllib3/1263)
<!-- Reviewable:end -->
"
1262,A pool can be evicted from PoolManager before a connection is acquired,2017-09-06T04:31:57Z,,,,,"This is the smaller of the potential race conditions but is fixable fairly simply, although a little hackily.

Part of PR https://github.com/shazow/urllib3/pull/1257"
1261,Pass {posargs} to py.test so arguments can be passed through tox,2017-09-06T03:12:38Z,2017-09-10T18:26:28Z,,,,"From PR https://github.com/shazow/urllib3/pull/1257
Fixes issue https://github.com/shazow/urllib3/issues/1260

Allows for arguments to passed through tox to py.test so, for instance, individual tests can be run:
```
tox -e py36 -- -k 'test_poolmanager or test_connectionpool'
```"
1260,tox does not allow passing of parameters to the tests,2017-09-06T03:11:33Z,2017-09-11T03:59:14Z,,,,"To, for instance, run only certain test files, ala:
```
tox -e py36 -- -k 'test_poolmanager or test_connectionpool'
```
From work on PR https://github.com/shazow/urllib3/pull/1257"
1259,Fix dummyserver decode errors when defaults are used with Python 3,2017-09-06T03:04:58Z,2017-09-15T19:11:54Z,,,,"Fixes issue https://github.com/shazow/urllib3/issues/1258
I searched for any usage of request.params.get() followed by a decode on the value.

A better version of a small part of PR https://github.com/shazow/urllib3/pull/1257 with some extra fixes."
1258,Defaults in dummyserver will fail to decode on Python3,2017-09-06T03:04:24Z,,,AttributeError,"AttributeError: 'str' object has no attribute 'decode'","I found this while working on https://github.com/shazow/urllib3/pull/1257

Most of the defaults given to request.params.get() are strings rather than bytes. Some of these then have decode() called on them, which fails as you can only decode bytes, not strings. These function work fine if a value is passed in but if no value is passed in they will error out with
```
AttributeError: 'str' object has no attribute 'decode'
```"
1257,Remove race conditions in PoolManager causing ClosedPoolErrors (#1252),2017-09-03T19:40:28Z,,,,,"Fix for #1252. The implementation here is a little clunky for now as I was trying to change the existing code/logic as little as possible to avoid regressions.

List of changes:
* Introduces `test_retry_with_too_many_pools` which shows the `ClosedPoolError` happening in master
* Fixes an issue in the dummyserver's `retry_after` handler in Python 3 when using the default value for status
* Adds a `PersistentRetry` class which wraps `Retry` and allows for the same object to be used after `increment` is called
* Changes the retry logic in `HTTPConnectionPool` and `PoolManager` to loop instead of recurse
* Moves the base `HTTPConnectionPool.urlopen` functionality to `HTTPConnectionPool.urlopen_only` to allow for it to be called by `PoolManager` with no retry logic
* Refactors all retry logic in `HTTPConnectionPool.urlopen` into `URLOpenWithRetries` which takes a function to call for `urlopen_only` functionality.
  * Both `PoolManager` and `HTTPConnectionPool` end up calling `HTTPConnectionPool.urlopen_only` eventually
  * `HTTPConnectionPool`'s logic should be pretty much exactly the same as it was
  * `PoolManager` passes in a function for `urlopen_only` which acquires a pool from `self.pools` then calls `urlopen_only` on it. This means that if `URLOpenWithRetries` ends up retrying (currently will only happen if a `Retry-After` header is received) then `PoolManager` will create a new pool if the old one has been removed, avoiding the `ClosedPoolError` that can happen.
* `PoolManager` acquires `self.pools.lock` a second time now, before running its internal `urlopen_only` logic, to ensure that the lock stays acquired until the `HTTPConnectionPool` calls `_get_conn`. It passes the lock to `HTTPConnectionPool.urlopen_only` so that it can release the lock immediately once `_get_conn` is called.

Caveats/Potential improvements:
* Both URLOpenWithRetries and PoolManager still implement different redirect/retry logic. It would be nice to move all retry logic into URLOpenWithRetries.
  * Since `PoolManager` forces `redirect=False` when calling `HTTPConnectionPool.urlopen(_only)` there is only the `Retry-After` logic which can potentially cause a race condition on retry. Refactoring this further would reduce the complexity and remove the need for setting `redirect=False`.
* Passing the lock into `HTTPConnectionPool.urlopen_only` is a little hacky as currently implemented. It would be nice to have the connection acquiring logic also refactored out so that passing it through like this isn't needed and we can use the lock as a context manager.
* `PersistentRetry` may not be needed. This was implemented to ensure that the retries were maintained properly through the refactor as there are extra calls it may need to be passed through to maintain the context.
* Retry logic changed to loops makes the tracebacks slightly less informative with regards to retries but reduces the frame stack when retrying.
  * This should be able to be reverted if wanted. I originally made this change to make the logic a little easier to reason about and avoid potential issues when refactoring.
* `URLOpenWithRetries` is an odd name
* `URLOpenWithRetries` could possibly be changed to be a mixin, parent class, or wrapper instead of taking a function as a parameter to make its use more obvious
* `{[testenv]setenv}` change may not be needed but I ran into a lot of parsing errors with tox in some environments due to this.

I tested these changes on OSX (10.12.2) and on Ubuntu 16.04 with tox, as well as on travis-ci. There are random test failures, as mentioned in #1256, but the failures seem to correspond with those seen randomly on master in my tests. I have had each tox environment pass with no errors multiple times so I am fairly sure that there are no regressions.

https://travis-ci.org/reversefold/urllib3/builds/271452531 shows a fully passing run of this branch (except for gae, which appears to be broken).

<!-- Reviewable:start -->
---
This change is‚ÄÇ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/shazow/urllib3/1257)
<!-- Reviewable:end -->
"
1256,Random test failures on some tests on master,2017-09-02T17:40:03Z,,,,,"I've been working on a patch and was trying to get a baseline for the test suites using tox on the master branch. I'm getting random hangs and test failures with the dummyserver tests. They will sometimes pass and sometimes fail. Here's an example of a failure:
```
________________________________________________________________________________________________ TestHTTPProxyManager.test_proxy_conn_fail _________________________________________________________________________________________________

self = <test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager testMethod=test_proxy_conn_fail>

    def test_proxy_conn_fail(self):
        host, port = get_unreachable_address()
        http = proxy_from_url('http://%s:%s/' % (host, port), retries=1, timeout=0.05)
        self.addCleanup(http.clear)                                                                                                                                                                                                                 self.assertRaises(MaxRetryError, http.request, 'GET',
                          '%s/' % self.https_url)
        self.assertRaises(MaxRetryError, http.request, 'GET',
                          '%s/' % self.http_url)

        try:
            http.request('GET', '%s/' % self.http_url)
            self.fail(""Failed to raise retry error."")
        except MaxRetryError as e:
>           self.assertEqual(type(e.reason), ProxyError)
E           AssertionError: <class 'urllib3.exceptions.ConnectTimeoutError'> != <class 'urllib3.exceptions.ProxyError'>
                                                                                                                                                                                                                                            test/with_dummyserver/test_proxy_poolmanager.py:66: AssertionError                                                                                                                                                                          ----------------------------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------------------------
Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x111c7d610>, 'Connection to hmxysfyfmsecholvhzjufgbxfncwr
rtprlqjmcxktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': /
Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x111c7d610>, 'Connection to hmxysfyfmsecholvhzjufgbxfncwr
rtprlqjmcxktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': /
Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x111c7de90>, 'Connection to hmxysfyfmsecholvhzjufgbxfncwrrtprlqjmc
xktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': http://localhost:58795/
Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x111c7de90>, 'Connection to hmxysfyfmsecholvhzjufgbxfncwrrtprlqjmc
xktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': http://localhost:58795/
Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x111c7ded0>, 'Connection to hmxysfyfmsecholvhzjufgbxfncwrrtprlqjmc
xktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': http://localhost:58795/
Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x111c7ded0>, 'Connection to hmxysfyfmsecholvhzjufgbxfncwrrtprlqjmc
xktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': http://localhost:58795/
----------------------------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------------------------
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x111c7d610>, 'Connection t
o hmxysfyfmsecholvhzjufgbxfncwrrtprlqjmcxktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': /
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x111c7de90>, 'Connection to hmxysfy
fmsecholvhzjufgbxfncwrrtprlqjmcxktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': http://localhost:58795/
WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=0, status=None)) after connection broken by 'ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x111c7ded0>, 'Connection to hmxysfy
fmsecholvhzjufgbxfncwrrtprlqjmcxktvhjdwzuydlfqyqaaplb timed out. (connect timeout=0.05)')': http://localhost:58795/
============================================================================================================= warnings summary =============================================================================================================
test/test_util.py::TestUtil::()::test_ssl_wrap_socket_loads_the_cert_chain
  /Users/papercrane/src/urllib3/urllib3/util/ssl_.py:339: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present
an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
    SNIMissingWarning

test/contrib/test_pyopenssl.py::TestHTTPS::test_client_intermediate
  /Users/papercrane/src/urllib3/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-u
sage.html#ssl-warnings
    InsecureRequestWarning)
                                                                                                                                                                                                                                            test/contrib/test_socks.py::TestSocks5Proxy::test_basic_request                                                                                                                                                                               /Users/papercrane/src/urllib3/dummyserver/server.py:131: NoIPv6Warning: No IPv6 support. Falling back to IPv4.
    NoIPv6Warning)

test/contrib/test_socks.py::TestSOCKSWithTLS::test_basic_request
  /Users/papercrane/src/urllib3/urllib3/util/ssl_.py:137: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You ca
n upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
    InsecurePlatformWarning

test/with_dummyserver/test_connectionpool.py::TestRetryPoolSize::test_pool_size_retry
  /Users/papercrane/src/urllib3/test/with_dummyserver/test_connectionpool.py:1003: PendingDeprecationWarning: Please use assertEqual instead.
    self.assertEquals(self.pool.num_connections, 1)

test/with_dummyserver/test_https.py::TestHTTPS_IPv6Addr::test_strip_square_brackets_before_validating
  /Users/papercrane/src/urllib3/urllib3/connection.py:344: SubjectAltNameWarning: Certificate for ::1 has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and depreca
ted by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)
    SubjectAltNameWarning

test/with_dummyserver/test_socketlevel.py::TestRetryPoolSizeDrainFail::test_pool_size_retry_drain_fail
  /Users/papercrane/src/urllib3/test/with_dummyserver/test_socketlevel.py:1471: PendingDeprecationWarning: Please use assertEqual instead.
    self.assertEquals(pool.num_connections, 1)

-- Docs: http://doc.pytest.org/en/latest/warnings.html
```
I'm also seeing random failures with test/contrib/test_securetransport.py and some of the other ssl based tests."
1255,Use FQDN only for DNS and drop trailing dot for other operations,2017-09-02T00:04:26Z,2017-12-15T13:17:54Z,,,,Fixes #1254.
1254,Match_hostname: hostname 'github.com.' doesn't match 'github.com' or 'www.github.com',2017-08-31T21:10:32Z,2017-12-15T13:17:54Z,,urllib3.exceptions.MaxRetryError,"urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='github.com.', port=443): Max retries exceeded with url: / (Caused by SSLError(CertificateError(""hostname 'github.com.' doesn't match either of 'github.com', 'www.github.com'"",),))","I'm getting hostname mismatch error when I try to access a domain with a trailing dot.

Example:

Version of urllib3 I'm using

```
(venv) ‚ûú  ~ pip list --format=columns | grep urllib3
urllib3                      1.22    
```

```
(venv) ‚ûú  ~ python
Python 2.7.10 (default, Jul 14 2015, 19:46:27) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import urllib3
>>> http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED')
>>> r = http.request('GET', 'https://github.com/')
>>> print r.status
200
>>> r = http.request('GET', 'https://github.com./')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 321, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 668, in urlopen
    **response_kw)
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 668, in urlopen
    **response_kw)
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 668, in urlopen
    **response_kw)
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 639, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/Users/karteek/venv/lib/python2.7/site-packages/urllib3/util/retry.py"", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='github.com.', port=443): Max retries exceeded with url: / (Caused by SSLError(CertificateError(""hostname 'github.com.' doesn't match either of 'github.com', 'www.github.com'"",),))
```

Similar request using curl seems to go fine.

```
curl -v https://github.com./ 
*   Trying 192.30.255.112...
* Connected to github.com (192.30.255.112) port 443 (#0)
* TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
* Server certificate: github.com
* Server certificate: DigiCert SHA2 Extended Validation Server CA
* Server certificate: DigiCert High Assurance EV Root CA
> GET / HTTP/1.1
> Host: github.com
> User-Agent: curl/7.43.0
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Thu, 31 Aug 2017 21:06:01 GMT
< Content-Type: text/html; charset=utf-8
< Transfer-Encoding: chunked
< Server: GitHub.com
< Status: 200 OK
< Cache-Control: no-cache
```

Also, going to similar URL https://github.com./robots.txt on browsers Firefox and Chrome doesn't show any SSL errors (They get redirected to https://github.com/robots.txt)"
1253,Verifying SSL Server Certificate instead of root CA Certificate,2017-08-29T14:51:59Z,2017-08-29T16:43:40Z,,,,"Hi,

Is there any way of verifying server certificate instead of root CA certificate.

Thanks & Regards,
Saikrishna"
1252,PoolManager is not thread-safe,2017-08-27T05:15:46Z,,,,,"See, for reference this issue on the requests library:
https://github.com/requests/requests/issues/1871

In looking into the issue, which appears to happen when too many hosts are connected to through the same PoolManager in parallel, the issue appears to be with PoolManager's use of an LRU cache for the connection pool instances within it. A pool is pulled from the cache but within the time it takes for pool._get_conn to be called the PoolManager can potentially evict the ConnectionPool if too many other hosts are connected to in parallel.

In addition, since the pool also handles retries and redirects within urlopen, the pool needs to stay alive until the final connection is acquired from the pool, meaning we can't be sure we can release the pool from PoolManager until urlopen returns, essentially making PoolManager make requests serially only.

I only see a few options for fixing this.
1) Remove the LRU cache. This means either connection leaks or hard-limiting the number of hosts that a PoolManager can connect to. Not a very useful option.
2) Put the call(s) to pool.urlopen within a critical section. Basically a no-go as this means that concurrency is useless.
3) Implement ref-counting (a semaphore of some kind) to keep track of pools actually in use and don't allow them to be reaped if they have a reference. This seems complicated and a duplication of Python's own GC. Perhaps this could be implemented by using __del__ to close the pool and not explicitly closing the pool when it is evicted?
4) Refactor PoolManager and the connection pools such that, when using a PoolManager, retries are handled in the PoolManager and we can get up to the point of getting an actual connection out of the pool within a critical section then release the pool to be potentially evicted."
1251,Add port to connection open print if it's non standard (not 80 or 443),2017-08-24T17:45:58Z,2017-08-26T20:57:30Z,,,,"I do quite a lot of work with HTTP tunnels, and so the URL / IP I connect to is almost never enough to identify which endpoint I'm actually trying to reach. This commit adds the port, if it's not standard (80 for HTTP, 443 for HTTPS)"
1250,use client cert from keychain in contrib.securetransport,2017-08-24T14:49:50Z,,,,,There doesn't appear to be a method of signaling urllib3.contrib.securetransport to use a client certificate and key that's stored in keychain as opposed to passing one in parameters.   One of the benefits of using securetransport should be to use the keychain to store secrets including the client cert/key.
1249,raise_on_status in Retry,2017-08-24T10:22:49Z,2017-08-25T07:30:20Z,,,,"I've been playing with the Retry class while using requests. And setting `raise_on_status=True` I was expecting an exception to be raised. That didn't seem to happen. 

Here is the code in its entirety.

```
import requests
from requests.adapters import HTTPAdapter
from requests.adapters import Retry

session = requests.Session()

retries = Retry(total=3,
                backoff_factor=0.5,
                redirect=3,
                raise_on_status=True)
session.mount('https://', HTTPAdapter(max_retries=retries))
session.mount('http://', HTTPAdapter(max_retries=retries))

req = requests.Request('GET', url='http://httpbin.org/status/500')
prepped = req.prepare()

try:
    response = session.send(prepped)
except Exception as exc:
    print(exc)
```"
1248,fix failing tests on windows python (64 bits) 2.7 and 2.6 ,2017-08-23T22:44:44Z,2017-08-30T09:04:33Z,,,,"This PR updates the selectors code to better support python 2.6 and 2.7 (64bit). fixes #1247 


in more detail:

- Update tox.ini and appveyor.yml to use the python defined by the TOXPY** environment variable (when defined).
- Update the `util.selectors._fileobj_to_fd` to accept a `long` value. 
- Various appveyor improvements:
   - (speedup) No need to install python 2.6.6, appveyor already has the desired version.
   - (speedup) Cache wheels 
   - (speedup) No need to install urllib on parent python.
   - Do not run test builds on a branch when it is part of a pull request.
"
1247,test_selectors.py tests fail with python 2.6 64bit and 27 64bit on windows ,2017-08-22T20:43:31Z,2017-08-30T09:04:33Z,,,,"I have noticed that there are a number of tests like the `test_selectors.py::BaseSelectorTestCase::test_register` fail when run against python 2.6 64 bit and python 2.7 64bit. (see appveyor builds, 
https://ci.appveyor.com/project/itziakos/urllib3/build/1.0.7/job/m0xhyhhl05xdcmxi and https://ci.appveyor.com/project/itziakos/urllib3/build/1.0.7/job/i23k8bf88787c199)

The error is due to the fact that `socket.fileno()` returns a `long` on python 2.6 and 2.7  (64bits) and the 
[urllib3.util.selectors._fileobj_to_fd](https://github.com/shazow/urllib3/blob/master/urllib3/util/selectors.py#L41) does not specifically handle that case and the function will finally raise a [ValueError](https://github.com/shazow/urllib3/blob/master/urllib3/util/selectors.py#L50)

The interesting part is that the error was not picked up by the appveyor tests because tox will use the 32 bit python when running the tests.

*output from example build https://ci.appveyor.com/project/itziakos/urllib3/build/1.0.9/job/j84qscx038440brd#L248*
```
py26 runtests: PYTHONHASHSEED='187'
py26 runtests: commands[0] | pip --version
pip 9.0.1 from c:\projects\urllib3\.tox\py26\lib\site-packages (python 2.6)
py26 runtests: commands[1] | python -c import struct; print(struct.calcsize('P') * 8)
32
py26 runtests: commands[2] | pip install .[socks,secure]
DEPRECATION: Python 2.6 is no longer supported by the Python core team, please upgrade your Python. A future version of pip will drop support for Python 2.6
Processing c:\projects\urllib3
  Requirement already satisfied (use --upgrade to upgrade): urllib3===dev from file:///C:/projects/urllib3 in c:\projects\urllib3\.tox\py26\lib\site-packages
```

If it is ok, I can make a PR to fix the issue and attempt to persuade tox to use the 64bit python on appveyor.
"
1246,PyOpenSSL client certificate load chain fix,2017-08-15T21:56:13Z,2017-08-16T19:11:56Z,,,,"Picking up the work from #1175, which addresses #1060.

I've rebased the existing PR#1175 against current master, and resolved the merge conflicts.  Additionally, I've fixed all the tests in #1175 so they pass on both mac OS (at least, on 10.12.x) and against the py26 environment."
1245,Rewrite test_selectors.py to be pytest-style,2017-08-09T19:31:30Z,,,,,Related: #1160.
1244,Rewrite test_poolmanager.py to be pytest-style,2017-08-09T16:37:08Z,2017-08-09T18:49:40Z,,,,Related: #1160.
1243,Correct test assertions around port in test_socks.py,2017-08-09T12:34:34Z,2017-08-09T14:48:57Z,,,,"I think this is a bug?

All this test asserts is that the port number is non-zero ‚Äì given the other assertions in this block, it looks more like it meant to check the port number has a specific value."
1242,Goodbye Python 3.3. üëã,2017-08-08T12:57:46Z,2017-08-08T14:42:32Z,,,,"Cryptography no longer supports it, so one of our fairly critical optional dependencies (PyOpenSSL) is no longer safe on Python 3.3. Given that it's out of support anyway, I think we should say goodbye to the old workhorse. Onward and upward!"
1241,Add 'other' counter,2017-08-08T10:56:41Z,2017-08-08T14:47:51Z,,,,"Similar to https://github.com/shazow/urllib3/issues/1147, would you be willing to merge this?

Honestly, I'm no longer sure this makes sense. My original goal was to retry different sorts of errors (read, connect, status) once each, the idea being something like: ""it's not worth retrying connect timeout multiple times - if it doesn't answer right away, it probably doesn't answer after 5 retries..."" but at the same time ""let's allow retrying _different_ types or errors on the same go, because maybe proxy just disconnected us, and after re-initializing connection it might throw timeout, that we can also retry once""...

And actually, `ProxyError` (for example) doesn't fall into any category currently, so `other` counter would catch that. However, there are gazillions of potential exceptions in `requests.exceptions`, and this will regard all of them as one, so my idea doesn't quite realize here. Thus, I wonder if this makes any sense...

In the end, it doesn't really do any harm if I just `Retry(3)`... maybe not perfect, but it works and is more flexible..."
1240,Remove all uses of nose from the codebase,2017-08-06T20:04:15Z,2017-08-09T10:51:09Z,,,,Related: #1160.
1239,Pick off some easy coverage wins around raise statements,2017-08-06T19:50:06Z,2017-08-10T16:16:14Z,,,,Related: #1214.
1238,Does Urllib3 support rate limiting?,2017-07-31T16:56:38Z,2017-07-31T16:57:21Z,,,,"I am about to interact with a RESTful API with a rate limit of 5 requests/second. I was wondering if the urllib3 connection pool has some sort of mechanism to limit the HTTP request rate? An  [example](http://code.activestate.com/recipes/511490-implementation-of-the-token-bucket-algorithm/) is the token-bucket algorithm.

I read briefly through the source code and it seems that I couldn't find any. "
1237,Let connectionpool dispose themselves,2017-07-30T17:18:17Z,2017-08-01T07:44:50Z,,,,"As mentioned in [this stackoverflow post](https://stackoverflow.com/questions/45369131/seeing-closedpoolerror-when-using-urllib3-poolmanager), there's an issue with the current PoolManager API which can close ConnectionPool objects before they are even used.

I don't think this is easily fixed at the PoolManager level because the API  doesn't give rooms for a way to notify that a ConnectionPool is being used (or is to be released). As such I propose adding a different set of data structures with different APIs to fix this.

I would like some review/discussion in terms of direction before I move on to writing tests and making final adjustments.

@Lukasa @shazow "
1236,Fix compatibility with tornado 4.3+,2017-07-21T15:25:42Z,2017-07-23T00:27:20Z,,,,"`HTTPHeaders` is not JSON serializable, so make it a dict first."
1235,Release connection on chunked HEAD response.,2017-07-21T07:37:32Z,2017-07-22T17:09:07Z,,,,"AWS S3 servers set ""Transfer-Encoding: chunked"" even for a HEAD
response, which results in urllib3 not returning the connection to the
pool after calling stream() or read_chunked().

The patch moves the HEAD request check into the error_catcher context
manager, which ensures to release the connection to the pool upon
completion.

Fixes #1234"
1234,HEAD response with chunked Transfer-Encoding does not release the connection,2017-07-21T07:29:52Z,2017-07-22T17:09:07Z,,,,"Amazon S3 sets ""Transfer-Encoding: chunked"" in the response even for a HEAD request. For example:
```
DEBUG:botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com
send: 'HEAD /timur-test/garbage HTTP/1.1\r\nHost: s3.amazonaws.com\r\nAccept-Encoding: identity\r\n\r\n'
reply: 'HTTP/1.1 301 Moved Permanently\r\n'
header: x-amz-bucket-region: us-west-2
header: x-amz-request-id: 4E83FBED76CBB9BB
header: x-amz-id-2: dxdu+gwK6fdzdK2CaA/0yAcBxO7wdSmzRDwo9JRYA7QeMEk8xMkuE5VJ2AiabWWSqpi+IHweXqA=
header: Content-Type: application/xml
header: Transfer-Encoding: chunked
header: Date: Fri, 21 Jul 2017 06:41:02 GMT
header: Server: AmazonS3
```
This causes the connection to not be returned to the pool after calling `stream()` or `read_chunked()` (which `stream()` is a thin wrapper for in this case). This is due to the following conditional in the `read_chunked()` method: https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L592.

I believe moving this conditional inside the `error_catcher` context manager is the right resolution to this (or calling `release_conn()` explicitly). Will submit a PR for this."
1233,Update CHANGES for 1.22,2017-07-20T08:02:16Z,2017-07-20T09:03:33Z,,,,
1232,Concurrency Issues on poolmanager.py and _collections.py,2017-07-19T18:02:44Z,2017-07-20T02:28:35Z,,,,"Dear urllib3 developers, 

After reading the source code, I have some questions on the thread-safety of the poolmanager. 

This code: 

  ```
  def connection_from_pool_key(self, pool_key, request_context=None):
        """"""
        Get a :class:`ConnectionPool` based on the provided pool key.
        ``pool_key`` should be a namedtuple that only contains immutable
        objects. At a minimum it must have the ``scheme``, ``host``, and
        ``port`` fields.
        """"""
        with self.pools.lock:
            # If the scheme, host, or port doesn't match existing open
            # connections, open a new ConnectionPool.
            pool = self.pools.get(pool_key)
            if pool:
                return pool

            # Make a fresh ConnectionPool of the desired type
            scheme = request_context['scheme']
            host = request_context['host']
            port = request_context['port']
            pool = self._new_pool(scheme, host, port, request_context=request_context)
            self.pools[pool_key] = pool

        return pool

```

It seems that it only locks the pool operation. However, it does not lock between after the pool is created and before the connection is made. During this gap, the connection could be closed by the following code: 

```
def __setitem__(self, key, value):
        evicted_value = _Null
        with self.lock:
            # Possibly evict the existing value of 'key'
            evicted_value = self._container.get(key, _Null)
            self._container[key] = value

            # If we didn't evict an existing value, we might have to evict the
            # least recently used item from the beginning of the container.
            if len(self._container) > self._maxsize:
                _key, evicted_value = self._container.popitem(last=False)

        if self.dispose_func and evicted_value is not _Null:
            self.dispose_func(evicted_value)
```

The RLock will be released after the connectionpool operation is completed. If the RLock is released, and another thread tries to operate on the connection pool - the existing connection could be evicted. Any call on the evicted connection could fail. 

I also looked at the source code that calls the `urlopen` function, and it seems that it is called without any lock whatsoever. It is either a plain HTTPConnectionPool object or a HTTPSConnectionPool object. 

It seems that after the new pool is created and added to the connection pool, no more thread-safety is guaranteed. 

Is this an issue, or is it just me mis-interpreting the code? 

Thanks,

Jinhua "
1231,Use vendored version of ``six`` in pyopenssl.,2017-07-19T15:37:52Z,2017-07-20T07:32:28Z,,,,"Fixes import errors like https://github.com/certbot/certbot/issues/4886 when importing `requests`. Everywhere else in `contrib` vendors properly; it looks like we just missed a spot.

If this looks good, could we please have a 1.21.2 release soon so Let's Encrypt doesn't have to work around this in a zillion OS packages? I expect this problem is widespread, since any import of `requests` without a suitable `six` also installed could trigger it. [pip may even be hitting it](https://github.com/certbot/certbot/issues/2902), though I haven't dug into that deeply. Many thanks!"
1230,Stamp out a bunch of warnings in the tests,2017-07-18T06:30:37Z,,,,,"The test suite emits a lot of warnings (63 on [this recent Travis run](https://travis-ci.org/shazow/urllib3/jobs/254519138)).

These warnings are mostly benign, but they‚Äôre mixed in with real problems, such as #1100. This patch causes the test suite to emit substantially fewer warnings, meaning the interesting ones will be easier to spot. It also cleans up another two leaked FDs."
1229,Fix compatibility for cookiejar,2017-07-17T12:17:29Z,2017-07-17T16:53:10Z,,,,"py3 cookiejar expects `get_all()` from header objects and `info()` from response objects. This PR adds the necessary compatibility methods and a test for that in `test_compatibility`. `getlist()` needs to get a default argument, to serve as a dropin for the required `get_all()`. I opted for extending `getlist` instead of reinventing the wheel by creating a separate `get_all`, not sure if you agree here."
1228,Urllib3 with h11 and trio,2017-07-15T04:20:19Z,2017-07-15T06:34:12Z,,,,I've been following trio development these last few months and asynchronous HTTP requests are unsurprisingly high on my wish list.  @njsmith suggested urllib3 was probably the best way to robustly achieve this (https://github.com/python-trio/trio/issues/105).  I've noted that removing httplib has been closed (https://github.com/shazow/urllib3/pull/1067) and that the v2 branch hasn't been updated since May.   Does this mean we are close?  If not what additional work needs to be done? 
1227,"When sending chunked requests, allow chunk body iterator to format chunks on its own",2017-07-01T20:42:31Z,2017-07-02T07:02:32Z,,,,"I'm working with a service that wants me to upload chunked requests with chunk extensions -
 appending a signature for each chunk in the chunk header, e.g.
```
PUT /foo
... headers..

4;CUSTOM_CHUNK_EXTENSION\r\n
Wiki\r\n
5;CUSTOM_CHUNK_EXTENSION\r\n
pedia\r\n
0;CUSTOM_CHUNK_EXTENSION\r\n
\r\n
```
I think the easiest way to accommodate this would be with a kwarg like `PoolManager.request(body=chunk_iterator, raw_chunks=True)`. This would optionally delegate the responsibility for formatting the chunks to the body iterator."
1226,Use the public cryptography API for this!,2017-06-30T19:41:13Z,2017-07-01T06:14:29Z,,,,
1225,urllib3 should do nothing on import contrib/pyopenssl in Python>=2.7.9 or 3.4,2017-06-28T18:00:16Z,2017-06-28T18:07:53Z,,,,"If I understand correctly, recent Python versions support all functional provided by contrib/pyopenssl.py. Some third-party libs are uncoditionally trying to inject pyopenssl, so my opinion is that urllib3 should just do nothing, or fail with ImportError in recent Python"
1224,"when connecting to https by IP address, client hello message has a server_name SNI block with literal ip address",2017-06-26T10:51:20Z,,,,,"I originally opened this in [requests](https://github.com/requests/requests/issues/4178), but was asked to raise it here.

When connecting to a machine over https using an IP address, the SSL client hello includes a server_name extension with the ip address listed as the server name. This is not allowed in the [SNI spec](https://tools.ietf.org/html/rfc3546#section-3.1), and it should not include the server_name extension. 

Reproduce:
1. Set up packet logging
2. ```python
    import urllib3
    http = urllib3.PoolManager()
    http.request('GET', 'https://8.8.8.8')
    ````
3. Inspect packet logs for client hello. It'll look a bit like this:
```
Frame 31: 571 bytes on wire (4568 bits), 571 bytes captured (4568 bits) on interface 0
Ethernet II, Src: BizlinkK_XX:XX:XX (9c:eb:e8:XX:XX:XX), Dst: JuniperN_XX:XX:XX (08:81:f4:XX:XX:XX)
Internet Protocol Version 4, Src: 192.168.89.18, Dst: 8.8.8.8
Transmission Control Protocol, Src Port: 1156, Dst Port: 443, Seq: 1, Ack: 1, Len: 517
Secure Sockets Layer
    TLSv1.2 Record Layer: Handshake Protocol: Client Hello
        Content Type: Handshake (22)
        Version: TLS 1.0 (0x0301)
        Length: 512
        Handshake Protocol: Client Hello
            Handshake Type: Client Hello (1)
            Length: 508
            Version: TLS 1.2 (0x0303)
            Random
            Session ID Length: 0
            Cipher Suites Length: 148
            Cipher Suites (74 suites)
            Compression Methods Length: 1
            Compression Methods (1 method)
            Extensions Length: 319
            Extension: server_name
                Type: server_name (0x0000)
                Length: 12
                Server Name Indication extension
                    Server Name list length: 10
                    Server Name Type: host_name (0)
                    Server Name length: 7
                    Server Name: 8.8.8.8
            Extension: ec_point_formats
            Extension: elliptic_curves
            Extension: SessionTicket TLS
            Extension: signature_algorithms
            Extension: Heartbeat
            Extension: Padding
```

The particular server I'm connecting to (azure windows 2016 Datacenter edition) immediately closes the connection when it sees a client hello like this."
1223,IPv6 bracket missing when IPv6 proxy is used,2017-06-20T05:30:25Z,2017-06-26T08:33:33Z,,"OSError, urllib3.exceptions.MaxRetryError, requests.exceptions.ProxyError","OSError: Tunnel connection failed: 400 Bad Request, urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',))), requests.exceptions.ProxyError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))","Currently, I am trying to connect to an https IPv6 host via IPv6 proxy. Here is my example:

```                                                                                                                                                                                                                                                   
import requests
url = 'https://[2620:10d:c081:b:0:0:0:26]/1/self'
r = requests.get( url, proxies={ 'http': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', 'https': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', }, verify=False )
print(r.content)
```

I got this error:

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 592, in urlopen
    self._prepare_proxy(conn)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 808, in _prepare_proxy
    conn.connect()
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connection.py"", line 294, in connect
    self._tunnel()
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 832, in _tunnel
    message.strip()))
OSError: Tunnel connection failed: 400 Bad Request

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/requests/adapters.py"", line 439, in send
    timeout=timeout
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 647, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/lib/python3.5/site-packages/urllib3/util/retry.py"", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test2.py"", line 4, in <module>
    r = requests.get( url, proxies={ 'http': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', 'https': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', }, verify=False )
  File ""/usr/local/lib/python3.5/site-packages/requests/api.py"", line 72, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/sessions.py"", line 502, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/sessions.py"", line 612, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/adapters.py"", line 501, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))
```

Digging through the logs and pcap, I found that the brackets are missing in `CONNECT`. Here is the output of tshark:

```
Capturing on 'utun1'
  1   0.000000 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [SYN] Seq=0 Win=65535 Len=0 MSS=1340 WS=32 TSval=1044073862 TSecr=0 SACK_PERM=1
  2   0.042774 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [SYN, ACK] Seq=0 Ack=1 Win=28560 Len=0 MSS=1340 SACK_PERM=1 TSval=157458772 TSecr=1044073862 WS=256
  3   0.042892 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [ACK] Seq=1 Ack=1 Win=131456 Len=0 TSval=1044073904 TSecr=157458772
  4   0.043007 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  [TCP segment of a reassembled PDU]
  5   0.086576 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [ACK] Seq=1 Ack=48 Win=28672 Len=0 TSval=157458815 TSecr=1044073904
  6   0.086665 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 HTTP  CONNECT 2620:10d:c081:b:0:0:0:26:443 HTTP/1.0
  7   0.086733 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 HTTP  HTTP/1.1 400 Bad Request
  8   0.086894 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [FIN, ACK] Seq=129 Ack=48 Win=28672 Len=0 TSval=157458816 TSecr=1044073904
  9   0.086963 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [ACK] Seq=50 Ack=129 Win=131328 Len=0 TSval=1044073947 TSecr=157458815
 10   0.087166 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [ACK] Seq=50 Ack=130 Win=131328 Len=0 TSval=1044073947 TSecr=157458816
 11   0.101217 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [FIN, ACK] Seq=50 Ack=130 Win=131328 Len=0 TSval=1044073961 TSecr=157458816
 12   0.137349 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [RST] Seq=130 Win=0 Len=0
```

Notice, packet 6, we have `CONNECT 2620:10d:c081:b:0:0:0:26:443`. This is invalid.

The diff should fix this by adding brackets if there is none. 

here is the tshark output of the same script:

```
Capturing on 'utun1'
  1   0.000000 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [SYN] Seq=0 Win=65535 Len=0 MSS=1340 WS=32 TSval=1044108399 TSecr=0 SACK_PERM=1
  2   0.044588 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [SYN, ACK] Seq=0 Ack=1 Win=28560 Len=0 MSS=1340 SACK_PERM=1 TSval=157493678 TSecr=1044108399 WS=256
  3   0.044698 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=1 Ack=1 Win=131456 Len=0 TSval=1044108443 TSecr=157493678
  4   0.044815 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  [TCP segment of a reassembled PDU]
  5   0.098407 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [ACK] Seq=1 Ack=50 Win=28672 Len=0 TSval=157493733 TSecr=1044108443
  6   0.098495 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 HTTP  CONNECT [2620:10d:c081:b:0:0:0:26]:443 HTTP/1.0
  7   0.140417 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [ACK] Seq=1 Ack=52 Win=28672 Len=0 TSval=157493774 TSecr=1044108495
  8   0.140502 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 HTTP  HTTP/1.0 200 Connection established
  9   0.140591 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=52 Ack=270 Win=131200 Len=0 TSval=1044108534 TSecr=157493775
 10   0.151764 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [PSH, ACK] Seq=52 Ack=270 Win=131200 Len=517 TSval=1044108545 TSecr=157493775
 11   0.213243 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [ACK] Seq=270 Ack=569 Win=29696 Len=1328 TSval=157493846 TSecr=1044108545
 12   0.213388 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [ACK] Seq=1598 Ack=569 Win=29696 Len=1328 TSval=157493846 TSecr=1044108545
 13   0.213448 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=569 Ack=2926 Win=129728 Len=0 TSval=1044108605 TSecr=157493846
 14   0.213581 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [PSH, ACK] Seq=2926 Ack=569 Win=29696 Len=314 TSval=157493846 TSecr=1044108545
 15   0.213642 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=569 Ack=3240 Win=130464 Len=0 TSval=1044108605 TSecr=157493846
 16   0.214640 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [PSH, ACK] Seq=569 Ack=3240 Win=131072 Len=126 TSval=1044108606 TSecr=157493846
 17   0.258051 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [PSH, ACK] Seq=3240 Ack=695 Win=29696 Len=274 TSval=157493891 TSecr=1044108606
 18   0.258116 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=695 Ack=3514 Win=130784 Len=0 TSval=1044108648 TSecr=157493891
 19   0.258905 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [PSH, ACK] Seq=695 Ack=3514 Win=131072 Len=194 TSval=1044108648 TSecr=157493891
 20   0.304204 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [PSH, ACK] Seq=3514 Ack=889 Win=30720 Len=538 TSval=157493937 TSecr=1044108648
 21   0.304297 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=889 Ack=4052 Win=130528 Len=0 TSval=1044108693 TSecr=157493937
 22   0.307226 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [FIN, ACK] Seq=889 Ack=4052 Win=131072 Len=0 TSval=1044108695 TSecr=157493937
 23   0.350246 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59414 [FIN, ACK] Seq=4052 Ack=890 Win=30720 Len=0 TSval=157493983 TSecr=1044108695
 24   0.350318 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59414 ‚Üí 8080 [ACK] Seq=890 Ack=4053 Win=131072 Len=0 TSval=1044108736 TSecr=157493983
```"
1222,HTTP CONNECT missing brackets when connecting to IPV6 host via IPV6 proxy,2017-06-20T05:29:30Z,2017-07-01T12:36:28Z,,"OSError, urllib3.exceptions.MaxRetryError, requests.exceptions.ProxyError","OSError: Tunnel connection failed: 400 Bad Request, urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',))), requests.exceptions.ProxyError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))","Currently, I am trying to connect to an https IPv6 host via IPv6 proxy. Here is my example:

```                                                                                                                                                                                                                                                   
import requests
url = 'https://[2620:10d:c081:b:0:0:0:26]/1/self'
r = requests.get( url, proxies={ 'http': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', 'https': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', }, verify=False )
print(r.content)
```

I got this error:

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 592, in urlopen
    self._prepare_proxy(conn)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 808, in _prepare_proxy
    conn.connect()
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connection.py"", line 294, in connect
    self._tunnel()
  File ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 832, in _tunnel
    message.strip()))
OSError: Tunnel connection failed: 400 Bad Request

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/requests/adapters.py"", line 439, in send
    timeout=timeout
  File ""/usr/local/lib/python3.5/site-packages/urllib3/connectionpool.py"", line 647, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/lib/python3.5/site-packages/urllib3/util/retry.py"", line 388, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test2.py"", line 4, in <module>
    r = requests.get( url, proxies={ 'http': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', 'https': '[2401:db00:0020:8065:beaf:0000:0019:0000]:8080', }, verify=False )
  File ""/usr/local/lib/python3.5/site-packages/requests/api.py"", line 72, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/api.py"", line 58, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/sessions.py"", line 502, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/sessions.py"", line 612, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.5/site-packages/requests/adapters.py"", line 501, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPSConnectionPool(host='2620:10d:c081:b:0:0:0:26', port=443): Max retries exceeded with url: /1/self (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 400 Bad Request',)))
```

Digging through the logs and pcap, I found that the brackets are missing in `CONNECT`. Here is the output of tshark:

```
Capturing on 'utun1'
  1   0.000000 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [SYN] Seq=0 Win=65535 Len=0 MSS=1340 WS=32 TSval=1044073862 TSecr=0 SACK_PERM=1
  2   0.042774 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [SYN, ACK] Seq=0 Ack=1 Win=28560 Len=0 MSS=1340 SACK_PERM=1 TSval=157458772 TSecr=1044073862 WS=256
  3   0.042892 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [ACK] Seq=1 Ack=1 Win=131456 Len=0 TSval=1044073904 TSecr=157458772
  4   0.043007 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  [TCP segment of a reassembled PDU]
  5   0.086576 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [ACK] Seq=1 Ack=48 Win=28672 Len=0 TSval=157458815 TSecr=1044073904
  6   0.086665 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 HTTP  CONNECT 2620:10d:c081:b:0:0:0:26:443 HTTP/1.0
  7   0.086733 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 HTTP  HTTP/1.1 400 Bad Request
  8   0.086894 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [FIN, ACK] Seq=129 Ack=48 Win=28672 Len=0 TSval=157458816 TSecr=1044073904
  9   0.086963 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [ACK] Seq=50 Ack=129 Win=131328 Len=0 TSval=1044073947 TSecr=157458815
 10   0.087166 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [ACK] Seq=50 Ack=130 Win=131328 Len=0 TSval=1044073947 TSecr=157458816
 11   0.101217 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 TCP  59366 ‚Üí 8080 [FIN, ACK] Seq=50 Ack=130 Win=131328 Len=0 TSval=1044073961 TSecr=157458816
 12   0.137349 2401:db00:20:8065:beaf:0:19:0 -> 2620:10d:c081:1133::11b2 TCP  8080 ‚Üí 59366 [RST] Seq=130 Win=0 Len=0
```

Notice, the HTTP CONNECT is invalid

```
6   0.086665 2620:10d:c081:1133::11b2 -> 2401:db00:20:8065:beaf:0:19:0 HTTP  CONNECT 2620:10d:c081:b:0:0:0:26:443 HTTP/1.0
```

"
1221,KeyError using doc example: cert_reqs='CERT_REQUIRED',2017-06-19T14:00:01Z,2017-06-19T14:49:15Z,,KeyError,"KeyError: u'CERT_REQUIRED'","## Summary 
http://urllib3.readthedocs.io/en/latest/user-guide.html#ssl says to use:
````python
http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())
````
but only this worked:
````python
http = urllib3.PoolManager(cert_reqs=ssl.CERT_REQUIRED, ca_certs=certifi.where())
````
## Expected Result
No error.
## Actual Result
````python
Traceback (most recent call last):
  File ""ex.py"", line 74, in get
    r = http.request('GET', url)
  File ""c:\python\lib\site-packages\urllib3\request.py"", line 66, in request
    **urlopen_kw)
  File ""c:\python\lib\site-packages\urllib3\request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""c:\python\lib\site-packages\urllib3\poolmanager.py"", line 321, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""c:\python\lib\site-packages\urllib3\connectionpool.py"", line 600, in urlopen
    chunked=chunked)
  File ""c:\python\lib\site-packages\urllib3\connectionpool.py"", line 345, in _make_request
    self._validate_conn(conn)
  File ""c:\python\lib\site-packages\urllib3\connectionpool.py"", line 844, in _validate_conn
    conn.connect()
  File ""c:\python\lib\site-packages\urllib3\connection.py"", line 314, in connect
    cert_reqs=resolve_cert_reqs(self.cert_reqs),
  File ""c:\python\lib\site-packages\urllib3\util\ssl_.py"", line 270, in create_urllib3_context
    context.verify_mode = cert_reqs
  File ""c:\python\lib\site-packages\urllib3\contrib\pyopenssl.py"", line 396, in verify_mode
    _stdlib_to_openssl_verify[value],
KeyError: u'CERT_REQUIRED'
````
## Reproduction Steps
See above.
## System Information
````json
{
  ""chardet"": {
    ""version"": ""3.0.4""
  },
  ""cryptography"": {
    ""version"": ""1.9""
  },
  ""implementation"": {
    ""name"": ""CPython"",
    ""version"": ""2.7.13""
  },
  ""platform"": {
    ""release"": ""7"",
    ""system"": ""Windows""
  },
  ""pyOpenSSL"": {
    ""openssl_version"": ""1010006f"",
    ""version"": ""17.0.0""
  },
  ""requests"": {
    ""version"": ""2.18.1""
  },
  ""system_ssl"": {
    ""version"": ""100020af""
  },
  ""urllib3"": {
    ""version"": ""1.21.1""
  },
  ""using_pyopenssl"": true
}
````
"
1220,Rewrite test helpers in test_selectors.py to be pytest-style,2017-06-15T09:16:00Z,2017-07-18T06:09:05Z,,,,"Related to #1160.

This doesn‚Äôt remove the unittest base class, because in this case that‚Äôs going to be moderately fiddly and should be done separately."
1219,Rewrite test_queue_monkeypatch.py to be pytest-style,2017-06-14T22:47:36Z,2017-06-15T07:57:34Z,,,,
1218,Rewrite test_response.py to be pytest-style,2017-06-14T10:51:31Z,2017-06-14T12:41:50Z,,,,Part of #1160.
1217,Rewrite test_retry.py to be pytest-style,2017-06-13T11:17:23Z,2017-06-13T12:21:50Z,,,,Part of #1160.
1216,Clean up a few more leaked FDs in tests,2017-06-09T20:20:15Z,2017-06-11T14:40:39Z,,,,Related to #1100.
1215,Rewrite test_utils.py to be pytest-style,2017-06-09T08:28:35Z,2017-06-10T07:10:48Z,,,,Related to #1160.
1214,.coveragerc removes all raise statements from coverage checks,2017-06-08T15:03:50Z,,,,,"This was discovered in #1211 and affects a few places that ought to have coverage. See #1212 for a full list of modules that are affected. We should aim to remove `raise` from `.coveragerc` and have test coverage for all these places.

As @Lukasa said, this is a great place for a contributor to make a real difference in the quality of this code-base.

"
1213,Move app engine tests over to pytest,2017-06-08T04:38:45Z,2017-06-15T06:46:01Z,,,,"Fixes #1191

(Also, what moron wrote this code? ü§£ )"
1212,Test removing raise from .coveragerc,2017-06-07T21:49:50Z,2017-06-08T14:58:51Z,,,,Testing a theory in #1211 
1211,OSError does not take keyword arguments,2017-06-07T21:22:09Z,,,,,This PR ensures that the intended OSError is raised instead of a TypeError.
1210,allow passing of cert chains as namespace rather than file path.,2017-06-06T21:53:08Z,,,,,"looking at:

urllib3/urllib3/contrib/pyopenssl.py:
```python
def load_cert_chain(self, certfile, keyfile=None, password=None):
        self._ctx.use_certificate_file(certfile)
        if password is not None:
            self._ctx.set_passwd_cb(lambda max_length, prompt_twice, userdata: password)
        self._ctx.use_privatekey_file(keyfile or certfile)
```

should probably allow for passing of certs as namespaces as well making use of _ctx.use_certificate context.  same for privkey.  this avoids having to leave unencrypted p12 bundles lying around on filesystem where they can be purloined."
1209,Add support for DNS hijacking,2017-06-06T19:33:31Z,,,,,"This is more of a naive approach and maybe the API is too intrusive, but we can change that.

First I started by comparing the hosts, without support for ports (as `curl` have in its `--resolve` argument).
In that implementation it made sense to use `hijack_dns_resolver`, since it map a host to a custom ip.

Since I've added support for ports, maybe `hijack_dns_resolver` is not the good, but I liked the name so I kept it. I'm open to change it (maybe `hijack_resolver` or just `resolver` is better)."
1208,Rewrite test_poolmanager.py to be pytest-style,2017-06-06T06:55:11Z,2017-07-18T06:09:10Z,,,,"Part of #1160.

Note: I took out all the cleanup code because I don't get any `ResourceWarning`s without it when running locally. I'd like to double-check that CI doesn't think differently before this is merged."
1207,Rewrite test_no_ssl.py to be pytest-style,2017-06-06T06:54:09Z,2017-06-06T12:29:41Z,,,,Part of #1160.
1206,Rewrite test_filepost.py to be pytest-style,2017-06-05T11:28:31Z,2017-06-05T14:02:14Z,,,,Related to #1160.
1205,Rewrite test_fields.py to be pytest-style,2017-06-05T11:27:38Z,2017-06-05T12:55:04Z,,,,Related to #1160.
1204,Rewrite test_exceptions.py to be pytest-style,2017-06-05T11:27:03Z,2017-06-07T13:27:17Z,,,,Related to #1160.
1203,Rewrite test_compatibility.py to be pytest-style,2017-06-03T22:26:37Z,2017-06-05T09:11:17Z,,,,Attempting to get #1199 to talk to coverage.
1202,Add trove classifiers for all supported Python versions,2017-06-03T16:55:40Z,2017-06-04T15:45:00Z,,,,I frequently use the PyPI trove classifiers to check if a 3rd party package is usable by my project. Documenting all supported versions makes this easier.
1201,Rename [wheel] section to [bdist_wheel] as the former is legacy,2017-06-03T16:39:14Z,2017-06-04T16:53:47Z,,,,"See:

https://bitbucket.org/pypa/wheel/src/54ddbcc9cec25e1f4d111a142b8bfaa163130a61/wheel/bdist_wheel.py?fileviewer=file-view-default#bdist_wheel.py-119:125

http://pythonwheels.com/"
1200,Rewrite test_connectionpool.py to be pytest-style,2017-06-03T10:41:34Z,2017-06-05T08:10:22Z,,,,Related to #1160.
1199,Rewrite test_compatibility.py to be pytest-style,2017-06-03T10:40:06Z,2017-06-03T22:26:07Z,,,,Related to #1160.
1198,Rewrite test_connection.py to be pytest-style,2017-06-03T07:48:30Z,2017-06-03T10:17:30Z,,,,Related to #1160.
1197,Rewrite test_proxymanager.py to be pytest-style,2017-06-02T19:04:11Z,2017-06-03T08:43:18Z,,,,Related to #1160.
1196,Rewrite test_collections.py to be pytest-style,2017-06-02T19:01:14Z,2017-06-03T00:39:06Z,,,,Related to #1160.
1195,Don't warn about missing SNI in the tests,2017-06-01T12:45:31Z,2017-06-01T18:34:32Z,,,,"Previously running this test file would emit an `SNIMissingWarning`:

```
test/test_util.py::TestUtil::test_ssl_wrap_socket_with_no_sni
  /Users/chana/repos/urllib3/urllib3/util/ssl_.py:339: SNIMissingWarning: An HTTPS request
  has been made, but the SNI (Subject Name Indication) extension to TLS is not available on
  this platform. This may cause the server to present an incorrect TLS certificate, which can
  cause validation failures. You can upgrade to a newer version of Python to solve this. For
  more information, see https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
    SNIMissingWarning
```

This patch amends the test file to no longer emit a warning, and also swaps out the warning check to be pytest-style (see #1160).

Related: #1100."
1194,SSL errors ambiguous - requests/urllib3/certifi/ssl,2017-05-31T14:05:11Z,2017-05-31T15:53:34Z,,"urllib3.exceptions.SSLError, ssl.SSLError, requests.packages.urllib3.exceptions.SSLError","urllib3.exceptions.SSLError: unknown error (_ssl.c:3517), ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749), requests.packages.urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)","Lost a whole day+ struggling with vague cert errors before having to give up with no more leads to chase, and held my nose to slap a verify=False and disable_warnings() in my code.

I can't seem to get a self signed cert accepted by urllib3, either by providing the single cer directly to verify (tried exporting it in a variety of formats), or by appending it to the pem downloaded by certifi.

I'm not sure this is the right place to ask, but the groups for the certifi/urllib3/requests seemed a bit overlapping (and I have no idea how to reproduce it using raw ssl package requests)... and I was wondering if there could be any improvement of error clarity in the packages handling of cert failures.

 - what format exactly is certifi/urllib3 expecting for it's pem files (and how could folks use python tools like ssl.get_server_certificate to properly create one with all the Pound extras before the begin cert)?

 - How can I track down exactly what urllib3/ssl is complaining about when I have an (apparently) malformed cert file and get:
...
urllib3.exceptions.SSLError: unknown error (_ssl.c:3517)

 - Or when I use a raw get_server_certificate file with line breaks that seems to be read fine and imported fine to windows, but is ignored for certification with many layers of:
...
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)
...
requests.packages.urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)

(depending on which package I'm trying to use to connect, when chrome and IE accept the installed single cert for the URL in question).
"
1193,AppEngineManager should provide an _original_response object.,2017-05-30T17:12:27Z,,Help Wanted,,,See the rationale on kennethreitz/requests#4044.
1192,Rewrite test_response.py and test_retry.py to be pytest-style,2017-05-30T16:57:18Z,2017-06-01T12:54:25Z,,,,"Making some incremental progress on #1160. Such lovely and clean unittest-free code!

And as a bonus, you no longer get `ResourceWarning`s emitted from `test_response.py` if all the tests succeed."
1191,Move GAE tox env to use the pytest runner,2017-05-30T16:07:52Z,2017-06-15T06:46:00Z,Soon,,,"Context: https://github.com/shazow/urllib3/pull/1187#issuecomment-304813261
Related: https://github.com/shazow/urllib3/issues/1160"
1190,Remove dead branch from TestHTTPS.test_ca_dir_verified,2017-05-30T12:07:23Z,2017-06-01T15:43:56Z,,,,Resolves #1164.
1189,Add cleanup code to the dummyserver tests,2017-05-30T12:00:08Z,2017-05-30T13:15:01Z,,,,"Running the tests locally was throwing up a lot of warnings of the form:

    ResourceWarning: unclosed <ssl.SSLSocket fd=16,
        family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6,
        laddr=('::1', 61145, 0, 0), raddr=('::1', 61134, 0, 0)>

This patch tries to reduce the number of `ResourceWarning`s that are emitted by the `with_dummyserver` tests. I get 37 less warnings on this branch compared to the current master (c82139e).

Longer term, it might be nice to switch to using `HTTPConnectionPool` as a context manager, which also reduces the specific ties to unittest, a la #1160 ‚Äì although I‚Äôm less sure what I‚Äòd do about the `HTTPConnection` objects. (Context manager omitted from this patch because the extra indentation makes the diff significantly messier.)"
1188,Improve SNI control,2017-05-29T13:57:13Z,2017-06-06T17:50:58Z,,,,"Hello!

Thank you for your work and dedication!

This patch should allow clients like `requests` to control the hostname used for TLS connections.
My use-case is not that complicated. I want to use the `Host` header as hostname, via `requests`."
1187,Swap out nose for pytest in the test runner,2017-05-29T12:30:16Z,2017-05-30T08:42:52Z,,,,"The smallest possible step in the direction of #1160. All this does is start running the tests under pytest instead of nose ‚Äì I was then thinking of doing small patches to gradually remove the nose-specific bits, so that dependency can eventually be removed.

(I have seen #1163, but (1) it appears to be a bit stalled, and (2) I usually prefer smaller patches, and getting the test runner out of the way paves the path for more small patches later.)"
1186,Fix GAE build,2017-05-28T17:06:17Z,2017-05-29T12:50:21Z,,,,"* Use gcp-devrel's script for fetching the app engine sdk instead of our own.
* Update the version of NoseGAE.

Resolves #1183 "
1185,Pin out all unvendored Requests releases for docs.,2017-05-28T16:52:27Z,2017-05-28T18:03:19Z,,,,"Turns out we got broken by Requests v2.16, which pins a version of urllib3 less than the development version.

Resolves #1182."
1184,[WIP] Have the gae tox env dump its requirements from CI,2017-05-28T16:50:45Z,2017-05-28T17:07:45Z,,,,"Not for merging, just an investigatory build for #1183."
1183,Running `tox -e gae` is broken on current master,2017-05-28T16:44:48Z,2017-05-29T12:50:21Z,,ImportError,"ImportError: No module named dev_appserver","Extracted from #1182:

```console
$ tox -e gae
GLOB sdist-make: /Users/alexwlchan/repos/urllib3/setup.py
gae inst-nodeps: /Users/alexwlchan/repos/urllib3/.tox/dist/urllib3-dev.zip
gae installed: appdirs==1.4.3,backports.ssl-match-hostname==3.5.0.1,certifi==2017.4.17,coverage==3.7.1,funcsigs==1.0.2,mock==1.3.0,nose==1.3.7,nose-exclude==0.4.1,NoseGAE==0.5.7,packaging==16.8,pbr==3.0.1,pkginfo==1.4.1,pluggy==0.3.1,psutil==4.3.1,py==1.4.33,pyparsing==2.2.0,PySocks==1.5.6,pytest==3.1.0,requests==2.14.2,six==1.10.0,tornado==4.2.1,tox==2.1.1,twine==1.5.0,urllib3===dev,virtualenv==15.1.0
gae runtests: PYTHONHASHSEED='2409600760'
gae runtests: commands[0] | nosetests -c /Users/alexwlchan/repos/urllib3/test/appengine/nose.cfg test/appengine
Traceback (most recent call last):
  File "".tox/gae/bin/nosetests"", line 11, in <module>
    sys.exit(run_exit())
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nose/core.py"", line 121, in __init__
    **extra_args)
  File ""/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/main.py"", line 94, in __init__
    self.parseArgs(argv)
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nose/core.py"", line 145, in parseArgs
    self.config.configure(argv, doc=self.usage())
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nose/config.py"", line 346, in configure
    self.plugins.configure(options, self)
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 284, in configure
    cfg(options, config)
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 99, in __call__
    return self.call(*arg, **kw)
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 167, in simple
    result = meth(*arg, **kw)
  File ""/Users/alexwlchan/repos/urllib3/.tox/gae/lib/python2.7/site-packages/nosegae.py"", line 91, in configure
    import dev_appserver
ImportError: No module named dev_appserver
ERROR: InvocationError: '/Users/alexwlchan/repos/urllib3/.tox/gae/bin/nosetests -c /Users/alexwlchan/repos/urllib3/test/appengine/nose.cfg test/appengine'
________________________________________________________________________________ summary _________________________________________________________________________________
ERROR:   gae: commands failed
```"
1182,requests/dev version of urllib3 don't play nice together in tox,2017-05-28T16:31:09Z,2017-05-28T18:03:19Z,,ValueError,"ValueError: need more than 1 value to unpack","Working with a local clone of current master (ae4adde):

```console
$ tox -e docs
GLOB sdist-make: /Users/alexwlchan/repos/urllib3/setup.py
docs create: /Users/alexwlchan/repos/urllib3/.tox/docs
docs installdeps: -r/Users/alexwlchan/repos/urllib3/docs/requirements.txt
docs inst: /Users/alexwlchan/repos/urllib3/.tox/dist/urllib3-dev.zip
docs installed: alabaster==0.7.10,appdirs==1.4.3,asn1crypto==0.22.0,Babel==2.4.0,backports.ssl-match-hostname==3.5.0.1,certifi==2017.4.17,cffi==1.10.0,chardet==3.0.3,coverage==3.7.1,cryptography==1.8.2,docutils==0.13.1,enum34==1.1.6,funcsigs==1.0.2,idna==2.5,imagesize==0.7.1,ipaddress==1.0.18,Jinja2==2.9.6,MarkupSafe==1.0,mock==1.3.0,ndg-httpsclient==0.4.2,nose==1.3.7,nose-exclude==0.4.1,packaging==16.8,pbr==3.0.1,pkginfo==1.4.1,pluggy==0.3.1,psutil==4.3.1,py==1.4.33,pycparser==2.17,Pygments==2.2.0,pyOpenSSL==17.0.0,pyparsing==2.2.0,PySocks==1.5.6,pytz==2017.2,requests==2.16.5,six==1.10.0,snowballstemmer==1.2.1,Sphinx==1.6.2,sphinxcontrib-websupport==1.0.1,tornado==4.2.1,tox==2.1.1,twine==1.5.0,typing==3.6.1,urllib3===dev,virtualenv==15.1.0
docs runtests: PYTHONHASHSEED='3695751977'
docs runtests: commands[0] | rm -rf /Users/alexwlchan/repos/urllib3/docs/_build
docs runtests: commands[1] | make -C /Users/alexwlchan/repos/urllib3/docs html
sphinx-build -b html -d _build/doctrees  '-W' . _build/html
Running Sphinx v1.6.2
making output directory...

Exception occurred:
  File ""/Users/alexwlchan/repos/urllib3/.tox/docs/lib/python2.7/site-packages/requests/__init__.py"", line 53, in <module>
    major, minor, patch = urllib3_version
ValueError: need more than 1 value to unpack
The full traceback has been saved in /var/folders/jy/351n9lnj5l3f07rtf2xybxx00000gn/T/sphinx-err-dn6eJg.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
make: *** [html] Error 1
ERROR: InvocationError: '/usr/bin/make -C /Users/alexwlchan/repos/urllib3/docs html'
________________________________________________________________________________ summary _________________________________________________________________________________
ERROR:   docs: commands failed
```"
1181,fix 1 lgtm.com alert,2017-05-26T11:22:04Z,2017-05-26T16:44:57Z,,,,"Hi,
Just wanted to quickly fix this alert flagged up on lgtm.com.

`UnicodeError` is a subclass of `ValueError` so the ordering of the except blocks would have never allowed to reach it (arguably it had limited impact since the same action is to be taken in both blocks).

Hope this helps!

A total of 14 alerts have been flagged up by lgtm.com, you can enable pull request integration for fully automated PR reviews that will flag these in the future so that they don't get past code review.
https://lgtm.com/projects/g/shazow/urllib3"
1180,Failure to match hostname and IP address in Subject Alternate name,2017-05-25T14:54:01Z,2017-05-25T15:50:13Z,,urllib3.exceptions.SSLError,"urllib3.exceptions.SSLError: hostname '192.168.187.150' doesn't match either of 'customhostname.mydomain', '192.168.187.150'","Bottom line upfront: 
urllib3.exceptions.SSLError: hostname '192.168.187.150' doesn't match either of 'customhostname.mydomain', '192.168.187.150'

Details:

import urllib3
import certifi
https = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=""/etc/ssl/certs/ca-certificates.crt"")
URLGET = ""https://192.168.187.150""
https.request('GET', URLGET)

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/urllib3-dev-py2.7.egg/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3-dev-py2.7.egg/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3-dev-py2.7.egg/urllib3/poolmanager.py"", line 321, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3-dev-py2.7.egg/urllib3/connectionpool.py"", line 630, in urlopen
    raise SSLError(e)
urllib3.exceptions.SSLError: hostname '192.168.187.150' doesn't match either of 'customhostname.mydomain', '192.168.187.150'"
1179,not fork safe on osx,2017-05-22T09:42:15Z,2017-05-22T15:43:01Z,,,,"This is actually a OSX ""feature"", and not a urllib3 bug - but I sank a lot of time into debugging it, and maybe it could be detected and a warning could be logged, or at least it could be documented?

The underlying problem is that proxy settings are looked up using some system api, which in turn uses libdispatch, which is not fork-safe. This can lead to segfaults.

My bug report: http://bugs.python.org/issue30385 (with a smallish program to replicate consistently, with `requests`)

The still open python bug http://bugs.python.org/issue13829 

Is it worth adding some logic to check if we're on osx and requests were made one pid - then later with another pid? (I don't know if there is a better way to detect a fork) - and if so log a warning?

"
1178,Retry SSL Errors,2017-05-15T13:25:57Z,2017-06-07T12:08:39Z,,,,"This is a fix for #1112. Instead of immediately raising SSLErrors, they are handled by the retry mechanism.

The change introduced ~22 test failures, all of them due to the fact that requests failed due to SSL errors now raise a `MaxRetryError`, and not an `SSLError`. I adapted these tests, also checking that  `MaxRetryError.reason` is an `SSLError`.

As mentioned in #1112, this is somewhat backwards incompatible: if clients explicitly handle `SSLError` exceptions, they now need to catch `MaxRetryError` instead and check if `reason` is an `SSLError`.

I didn't add an explicit test, since the new behaviour is tested implicitly in a lot of existing tests. Is that ok?"
1177,fix infinity recursion error when using with gevent.,2017-05-15T12:37:43Z,2017-05-15T12:42:36Z,,,,
1176,SSL infinity recursion error when using this with gevent.,2017-05-15T12:34:03Z,2017-05-15T12:42:09Z,,,,"#### reproduce
```python
import gevent.monkey
from requests.packages.urllib3.util.ssl_ import create_urllib3_context

gevent.monkey.patch_all()
create_urllib3_context()
```"
1175,PyOpenSSL Load Chain Fix,2017-05-12T23:11:05Z,,,,,"This pull request resolves #1060:

1. Created an intermediate CA
2. Created a client certificate off of that intermediate
3. Modified `dummyserver/server.py` to take certificates with `CERT_OPTIONAL`
    - Pre-existing test cases run unchanged
    - Test cases presenting validating certificates pass that along into the request handlers (Tornado)
4. Created two new test cases in `test/with_dummyserver/test_https.py`
    - One that tries a connection with client certificate alone
    - One that tries a connection with intermediate appended to client certificate

The last two commits fix the failing test case, and update the contributors list."
1174,Fix deadlocks with pool_block=True and retries,2017-05-08T18:50:33Z,2017-06-22T10:35:01Z,,,,"Replaces #1171 

I'll look at including a test this week."
1173,Rebuild sync connection loop.,2017-05-08T08:54:07Z,2017-05-30T09:15:51Z,,,,"This resolves #1149.

Sadly, it turns out that resolving #1149 required a pretty substantial rethink of the underlying sending loop in the v2 branch. A lot of the code was subtly mishandling expectations: for example, it's not at all unreasonable for a TLS connection to become readable during upload, and that's not an error: it is just triggering a TLS renegotiation. So we needed to become a bit smarter about error handling there.

Ultimately, this has required a big rewrite and the addition of a brand new bit of testing harness. We now have a sort-of DSL for testing low-level socket activity in the sync_connection, which allows us to tightly control exactly what happens in the low-level sockets and selectors. That transitions us into a purely-deterministic world in which nothing unexpected happens. Ultimately this may be a better testing approach than even the socket-level tests, at the cost of being divorced from ""actual network behaviour""."
1172,Allow adjusting request on retry,2017-05-08T05:47:36Z,,,,,"What would you think about allowing to manipulate the outgoing request within `Retry`?

*Use case:* I am calling an API that requires a valid authentication token. If token is not valid, API returns 401 as usual.

The problem is that our identity provider reserves right to flush all existing tokens at any point of time. Or maybe the token gets revoked (for reason X). So, the client may face a situation, where the otherwise valid token suddenly doesn't work anymore. Client needs to fetch a new token.

I already implemented a custom `Retry` class that refreshes my token. However, the problem is that the new token is not taken into use in the retry attempt, because headers are fixed and immutable at that point. If I could adjust headers inside `Retry.is_retry()` or `Retry.increment()` or somewhere, that would allow me to actually inject the new token into the retry..."
1171,Fix deadlocks with pool_block=True and retries,2017-05-06T20:55:46Z,2017-05-08T18:51:34Z,,,,Discard connection on retry/redirect before issuing another request to prevent deadlocks with pool_block=True
1170,Add TLS 1.3 cipher suites,2017-05-02T10:41:31Z,2017-05-03T12:08:53Z,,,,"Constants were taken from OpenSSL master and https://tools.ietf.org/html/draft-ietf-tls-tls13-18#page-103

@Lukasa I don't have resources to test ST patch."
1169,Update changelog for v1.21.1,2017-05-02T09:12:13Z,2017-05-02T10:55:08Z,,,,Today we are doing another release! üéâ üéÇ 
1168,Add JythonSelectSelector to urllib3/util/selectors.py [WIP],2017-05-01T22:25:25Z,,,,,"As reported by @LordGaav Jython apparently over-rides the default behavior of `socket.socket.fileno()` and instead returns a `socket`. The current implementation of `BaseSelector` relies heavily on the hash-ability of the return value of `fileobj.fileno()`.  This is just a quick implementation, there may be issues with it. I would also like to actually test it on Jython and if that's not possible in our CI setup perhaps make some `mock` tests that simulate a `socket` that returns itself from `fileno()`.

References:
- kennethreitz/requests#3992
- http://bugs.jython.org/issue1678
- https://wiki.python.org/jython/NewSocketModule#socket.fileno.28.29_does_not_return_an_integer"
1167,Deadlocks with a blocking connection pool and retry,2017-04-29T22:05:53Z,2017-07-02T21:23:07Z,,,,"I started digging through this in requests via this issue:  https://github.com/kennethreitz/requests/issues/3993

Testcase here:  https://github.com/mattbillenstein/requests-retry-pool-hang

urllib3 seems to be leaking requests in this case and with a blocking connection pool, this causes a deadlock."
1166,Fix regression in socket_options.,2017-04-28T07:36:53Z,2017-04-28T12:35:42Z,,,,Resolves #1165.
1165,urllib3==1.21: TypeError: unhashable type: 'list',2017-04-28T05:20:06Z,2017-04-28T12:35:42Z,,,,"There seems to be a regression from `1.20` to `1.21`.

```
2017-04-28T04:26:10.152544966Z   File ""/usr/local/lib/python3.5/site-packages/steembase/http_client.py"", line 146, in exec
2017-04-28T04:26:10.152552486Z     response = self.request(body=body)
2017-04-28T04:26:10.152560726Z   File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 303, in urlopen
2017-04-28T04:26:10.152585508Z     conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
2017-04-28T04:26:10.152592399Z   File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 219, in connection_from_host
2017-04-28T04:26:10.152599135Z     return self.connection_from_context(request_context)
2017-04-28T04:26:10.152604508Z   File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 232, in connection_from_context
2017-04-28T04:26:10.152610299Z     return self.connection_from_pool_key(pool_key, request_context=request_context)
2017-04-28T04:26:10.152615790Z   File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 245, in connection_from_pool_key
2017-04-28T04:26:10.152621674Z     pool = self.pools.get(pool_key)
2017-04-28T04:26:10.152626932Z   File ""/usr/local/lib/python3.5/_collections_abc.py"", line 597, in get
2017-04-28T04:26:10.152632427Z     return self[key]
2017-04-28T04:26:10.152637611Z   File ""/usr/local/lib/python3.5/site-packages/urllib3/_collections.py"", line 53, in __getitem__
2017-04-28T04:26:10.152643188Z     item = self._container.pop(key)
2017-04-28T04:26:10.152648363Z TypeError: unhashable type: 'list'
```

Code:
https://github.com/Netherdrake/steem-python/blob/master/steembase/http_client.py"
1164,TestHTTPS.test_ca_dir_verified skipped on Python 2.7.8 or older,2017-04-27T20:39:47Z,2017-06-01T15:43:56Z,,,,"[`TestHTTPS.test_ca_dir_verified`](https://github.com/shazow/urllib3/blob/master/test/with_dummyserver/test_https.py#L150) has the `@onlyPy279OrNewer` decorator to skip on Python 2.7.8 or older. However the test contains logic which checks the Python version and then does different verifications based on whether `sys.version` is greater or equal to `(2, 7, 9)`. See below:

```python
@onlyPy279OrNewer
@notSecureTransport
def test_ca_dir_verified(self):

    # ...

    if sys.version_info >= (2, 7, 9):
        self.assertFalse(warn.called, warn.call_args_list)
    else:  # <--- This branch is never taken?
        self.assertTrue(warn.called)
        call, = warn.call_args_list
        error = call[0][1]
        self.assertEqual(error, InsecurePlatformWarning)
```

Was it intentional to keep this branch?

The decorator for the test was added in commit https://github.com/shazow/urllib3/commit/e15ea73b23574b6f7ce5867d4f8d25aed2718b3f which came shortly after the test was created."
1163,Switch from Nose to Pytest Runner,2017-04-27T15:26:23Z,2017-05-30T17:57:53Z,,,,"The first step in the effort to switch to `py.test` as opposed to `nose`/`unittest` (#1160).
So far I've hit two problems: 
- Apparently `py.test` reloads the entire module per-test and emits the ""No true SSLContext object available"" warning for each test which luckily only affects Python 2.6 `test_https.TestHTTPS.test_ssl_wrong_system_time` and `test_https.TestHTTPS.test_ssl_correct_system_time` tests. Id' advocate just skipping these tests on Python versions without a correct SSLContext.

- Google App Engine, I don't know much about it or how testbeds work so I could use some direction here.
"
1162,Add alternative FIFO queue as a data structure for the connection pool,2017-04-27T14:32:28Z,2017-04-27T15:18:26Z,,,,"This comes from issue #1158 
I am suggesting this change because for some systems it is
better to use all the connections from the pool equally, rather than the
same ones all the time, which is the default case with LIFO.

In our case, it improves reliability because our servers are behind an
ELB. Once there is a failure and we have to retry the request, we don't
want to reuse the same connection, but rather start a different connection
in the hope that the ELB will choose a different machine to respond.

Furthermore, an online search leads me to believe that FIFO is a commonly
used data structure in connection pools:
https://en.wikipedia.org/wiki/Object_pool_pattern#Examples
The C# example uses List.Remove(0) and List.Add(...)"
1161,Remove useless line in CHANGES.rst,2017-04-27T13:38:54Z,2017-04-28T19:19:40Z,,,,
1160,Abandon nosetests.,2017-04-27T11:32:17Z,,,,,"Sadly, nosetests appears to be abandonware according [to their documentation](https://nose.readthedocs.io/en/latest/):

> Nose has been in maintenance mode for the past several years and will likely cease without a new person/team to take over maintainership.

Given that we will now need to migrate our testing infrastructure away from nose, we should consider where we need to move. My suggestion would be to move to `py.test`: writing tests for `py.test` is generally a nicer experience than writing traditional unittest style tests, and it has fewer requirements to do weird unittest importing. However, I'm happy to get feedback from the other contributors about what they'd like to do.

The work required to complete this would be as follows:

- [ ] Initial PR to move from nosetests as the test runner to the new runner, with only the changes to the testing we actually need to get those tests to run.
- [ ] Subsequent PRs to incrementally migrate our test suite to the style of the new runner, instead of unittest style, if applicable.

Thoughts?"
1159,Changelog entry for #1154.,2017-04-27T07:02:27Z,2017-04-27T07:31:03Z,,,,"Whoops, missed this."
1158,Retry when status is in status_forcelist and don't use the same connection,2017-04-26T11:20:38Z,,,,,"Hey folks,

I am trying to use urllib connectionpool.request so that when we get a status code that is in the status_forcelist, the same connection isn't reused for the retries. I tried using `release_conn=False` and then calling `.release_conn()`, but by inspecting the code, it looks like this is only releasing the last used connection. 
Assuming that we fail the 1st try and then we succeed the 2nd, the 1st connection is never released. Then I imagine we would have a problem of leaking connections.
Is this a bug, or is there something I'm missing here?

Cheers"
1157,Add assert_hostname to list of valid keys for PoolManager,2017-04-25T20:50:07Z,2017-05-02T03:32:30Z,,,,"In Kubernetes' client-python, we ran into this problem with
1.21:
https://travis-ci.org/kubernetes-incubator/client-python/jobs/225699267

We add the assert_hostname to our PoolManager here:
https://github.com/kubernetes-incubator/client-python/blob/192b67c4466c4e1c45f05eb4a1c6a6899e1c279d/kubernetes/client/rest.py#L107

I can't see any other workaround at the moment, please let us know is there's a better way to do this."
1156,Import InsecureRequestWarning problem,2017-04-25T19:53:28Z,2017-04-26T12:40:05Z,,,,"Hi

I'm using gTTS package for sometime now and I got an exception after installing fresh latest raspian release.
Here the exception:
```
  File ""/usr/local/lib/python2.7/dist-packages/gtts/__init__.py"", line 2, in <module>
    from .tts import gTTS
  File ""/usr/local/lib/python2.7/dist-packages/gevent/builtins.py"", line 93, in __import__
    result = _import(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/gtts/tts.py"", line 4, in <module>
    from requests.packages.urllib3.exceptions import InsecureRequestWarning
```

It works well with urllib1.3 but since 1.9.1 (maybe before) it is broken.

Can you confirm me the way to load exceptions have changed ?
Or it is a bug :p ?

Thank you.

Tang"
1155,1.21 fails to build,2017-04-25T16:47:39Z,2017-04-25T16:51:18Z,,,,"Collecting urllib3==1.2.1
  Downloading urllib3-1.2.1.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""<string>"", line 1, in <module>
      File ""/tmp/pip-build-DBcrHA/urllib3/setup.py"", line 25, in <module>
        tests_requirements = requirements + open('test-requirements.txt').readlines()
    IOError: [Errno 2] No such file or directory: 'test-requirements.txt'"
1154,Disable buffering when using SecureTransport,2017-04-25T14:18:18Z,2017-04-27T07:00:32Z,,,,"Resolves #1153, check that for more details."
1153,SecureTransport backend can lead to long pauses on Python 3.,2017-04-25T14:04:44Z,2017-04-27T07:00:32Z,,,,"Spotted by someone testing out pip with SecureTransport backing via the new urllib3.

The symptom is this: if you take pip from the branch on pypa/pip#4454, install it on Python 3, and run `pip install -I --no-cache-dir django` you'll find it takes a surprisingly long time. If you do the same on Python 2, it takes something around 7 seconds, while on Python 3 it can take more than 40! This is clearly not a good sign.

I've spent some time debugging this, and I have an explanation of what's going on. I'm going to write it out at length to help people understand the moving parts here, and to help understand why I've chosen the fix approach that I have.

The major relevant difference between Python 2 and Python 3 is their approach to buffering inside httplib. Specifically, Python 3 uses the fancy `io.BufferedReader`, while Python 2 does not (because it didn't exist when httplib was originally written). On Python 3, when making a `HTTPResponse.read` call, the objects involved are, in order: `http.client.HTTPResponse`, `io.BufferedReader`, `socket.SocketIO`, `securetransport.WrappedSocket`, and `socket.socket`.

`io.BufferedReader` has an interesting mechanism. It is initialized with a buffer size (by default 8192 bytes). When asked to read some bytes, it checks how many it was asked for. If the size is greater than the buffer, it'll just pass the read straight through and won't bother attempting to do any buffering (this was something of a surprise for me). However, when asked to read *less* than the buffer size it'll attempt to fill its buffer. It does this by passing its own internal buffer to the lower-level `readinto` function and asking for that buffer to be filled. This means that if httplib issues a `read` for fewer than 8192 bytes, in most cases the `io.BufferedReader` will intercept that call and ask for a read of 8192 bytes instead.

Interestingly, in most cases pip will circumvent the buffered reader. The buffering is used a bit early on when reading headers, but once pip starts reading the body it does so by using `iter_content(10240)`, which issues reads sized at 10240 bytes. This, eagle-eyed readers will notice, is larger than the buffer size of the `io.BufferedReader`, and so it will mostly stay out of the way. This changes only once: when reading the last segment of the body. At this time, httplib will notice that a `read` of 10240 bytes was asked for but only, say, 5192 bytes are left in the body, and so it will quietly clamp the read down to 5192 bytes before issuing the read to `io.BufferedReader`. At this point, `io.BufferedReader`, the hero of the day, will think ""NOW IS MY TIME"" and leap in and quietly *re-expand* the read to 8192 bytes before it passes it further on down the line. This is despite the fact that any *human being* could have told it that there will be no extra data to buffer (""the response is over, goddamn it!"").

Now, for other TLS backends this does not matter, but for SecureTransport it does. This is because our ST backend just calls `SSLRead` with the buffer provided by the higher level (which in this case is 8192 bytes long). And, it turns out, that `SSLRead` *also* buffers. That is, when asked to read 8192 bytes, it will damn well do so until it gets a read that times out before *any* data is returned. If even one byte is returned by the timeout then it will keep issuing calls to the read callback until it is done or it gets a true timeout.

Here is the issue. httplib asked for exactly the number of bytes it needed. If `io.BufferedReader` hadn't tried to be helpful by over-reading, `SSLRead()` would have been asked for that many application bytes, and so would have returned directly. But it *wasn't*, and so it elected to keep issuing reads. This meant that after it had read the 5192 bytes httplib needed SecureTransport issued *another* socket read, which waited for the timeout before failing to read and causing `SSLRead` to return with `errSSLWouldBlock`. This is where our long delay happens: this will happen once for each package pip downloads. To put it in a TL;DR kinda way, we have two layers doing buffering, and they're getting in each other's way.

So, how do we fix it? As far as I can tell, there's really only one way: stop the `io` module from doing any buffering when we use SecureTransport. Anything else involves trying to second-guess whether the `io` module is just trying to fill its own buffer or whether it actually needs the buffering. This other approach allows SecureTransport to continue to do the buffering it wants to do, without getting in the way.

The risk here is that there is some code somewhere in httplib that assumes it can get a `BufferedReader` unconditionally. I certainly hope that doesn't exist."
1152,Update CHANGES for 1.21.,2017-04-25T10:29:56Z,2017-04-25T11:09:42Z,,,,Time to release! üéâ 
1151,Switch to the Travis CI SVG build badge,2017-04-24T13:03:09Z,2017-04-24T14:18:51Z,,,,The most important pull request of the 21st century.
1150,Test Python 3.6 on Appveyor,2017-04-24T07:25:57Z,2017-04-25T08:04:55Z,,,,We should probably increase our test matrix to test as many versions on as many platforms as possible.
1149,sync_connection does not correctly handle SSLWantRead and SSLWantWrite,2017-04-21T14:11:44Z,,,,,"If either of `SSLWantRead` or `SSLWantWrite` are raised in the context of `sync_connection._receive_bytes()` on the V2 branch, the error is raised, rather than being interpreted and handled with appropriate action."
1148,Add 'status' counter for 'status_forcelist',2017-04-14T17:39:08Z,2017-04-17T12:30:11Z,,,,"To tackle https://github.com/shazow/urllib3/issues/1147.

Not sure about naming, but `status` property name follows naming convention of other counters..."
1147,status_forcelist specific retry counter,2017-04-13T13:00:01Z,2017-08-08T08:27:07Z,,,,"`read`, `connect` and `redirect` have separate counters but poor `status_forcelist` only counts on `total`. Would it be possible to add similar counter for `status_forcelist`, e.g. `status=3`?

I may try to sketch something up, if this doesn't feel too stupid.

Use case: I want to all one retry for read/connect/status-retry, but if I do `Retry(total=3, connect=1, read=1, status_forcelist=[...])` then in case there is no connect or read error, status will be retried 3 times, unless I'm mistaken."
1146,Move redirect handling entirely to PoolManager,2017-04-10T16:49:58Z,2017-04-24T12:38:48Z,,,,"Redirect handling code was duplicated in urllib3, being present in two places: in the ConnectionPool, and the PoolManager. This centralises all redirect handling code into the PoolManager, where it needs to be to handle cross-host redirects. ConnectionPool objects are no longer capable of doing redirects.

After this I'll provide a PR that moves all retry handling up to PoolManager as well, at which point ConnectionPool will have been greatly simplified."
1145,Rewrite Response I/O in terms of stream.,2017-04-05T11:53:14Z,2017-04-06T07:53:00Z,,,,"Now that I/O is done in terms of iterating a base connection, rather than in terms of calling read() on a file-like object, stream() becomes the more natural method to use to do I/O than read() does. This has meant both that there is a bunch of awkward code duplication in read() *and* that stream() became very inefficient, as it was doing buffered I/O.

This change adjusts stream so that it no longer does buffered I/O, and rewrites read() to be in terms of stream. This should help clean things up for writing an async response object in future. Along the way I've made some miscellaneous smaller changes."
1144,Further minor refactors to _tunnel,2017-04-04T09:12:11Z,2017-04-04T09:45:05Z,,,,"The goal of this work is to ensure that the _tunnel method has all of the general, non-IO code factored out to enable code re-use down the road when we try to add async support"
1143,Factor our some duplicate code in v2 branch.,2017-04-03T08:48:32Z,2017-04-04T09:03:54Z,,,,
1142,Address issues with testing on Windows,2017-03-31T21:51:25Z,2017-04-03T15:54:51Z,,,,"This PR comes after #1006 has been merged into master. We knew there would probably be some issues, and lo-and-behold there are. Only issues seem to be with testing using `socket.socketpair` and incompatibilities with our `socketpair` helper for Python 2."
1141,Remove release_conn parameter from v2 branch.,2017-03-31T09:06:57Z,2017-03-31T12:00:20Z,,,,"The release_conn parameter is not really very sensible in the modern, new construction of urllib3, so this PR waves goodbye to it entirely."
1140,Remove chunked kwarg.,2017-03-31T08:27:14Z,2017-03-31T13:21:04Z,,,,"It doesn't do anything anymore, so let's be done with it."
1139,Reduce selector overuse in v2 branch.,2017-03-31T07:57:24Z,2017-03-31T11:59:41Z,,,,"Right now the code will create new selectors to check for EOF, even though each connection now has one. We should probably stop doing that. :D"
1138,"Should not be error: ""urllib3 is using URLFetch on Google App Engine sandbox instead of sockets""",2017-03-30T01:47:58Z,2017-03-30T07:25:36Z,,,,"This warning shows up as an error in App Engine logs viewer whenever a new process is started for the application. However, using `URLFetch` is perfectly acceptable - [`urllib3`'s own documentation](https://urllib3.readthedocs.io/en/latest/reference/urllib3.contrib.html) lists it as the first option, and [one of Google's own example applications](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/96ae58c3540959a0d9366fee42366f6edb142b8a/appengine/standard/firebase/firenotes/backend/main.py#L25) uses it via a monkey patch. I'm not sure why this should be logged as an error? An info-level log would be much friendlier.

Relevant code: [here](https://github.com/shazow/urllib3/blob/b6204d57bcdfc6ff048f1c21647ef324c8a4f39e/urllib3/contrib/appengine.py#L111)"
1137,urllib3 v2 does not allow setting TLS kwargs on pool managers and connection pools anymore.,2017-03-23T14:45:15Z,2017-03-29T15:04:22Z,,,,So we should stop setting these on tests!
1136,Remove support for sending unicode request bodies.,2017-03-23T12:57:40Z,2017-03-23T15:41:33Z,,,,"This has been bothering me for a while: the auto-encoding of unicode bodies is a really good source of subtle bugs, and I'd argue it violates the Zen of Python. This change removes that support from the v2 branch."
1135,Cannot verify website with valid certificate,2017-03-15T12:54:08Z,2017-03-15T12:59:03Z,,`SSLError,"`SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)`","I cannot verify the https://anteus.hu website certificate (https://tls-observatory.services.mozilla.com/static/certsplainer.html?id=9486863) with urllib3 in any way. The Exception is always:
`SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)`
Every other method says it's valid (Google Chrome, CURL, [oscrypto.tls.TLSSocket](https://github.com/wbond/oscrypto), Mozilla Observatory).

I tried with urllib3 1.19.1 and 1.20, certifi 2016.9.26 and 2017.1.23.
with `certifi`:
```python
import urllib3
import certifi
http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
res = http.request('GET', 'https://anteus.hu', redirect=True)
```

without `certifi`:
```python
import urllib3
http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED')
res = http.request('GET', 'https://anteus.hu', redirect=True)
```
a combination of the versions mentioned above, and different timeout, pool_size, etc parameters. The result is always the same. Other sites checked fine (https://twitter.com, https://facebook.com, etc) Previously I made the request with HEAD method, and the server didn't answered to that, but GET is fine with everything else.

My Python version is: Python 3.6.0 (default, Dec 23 2016, 18:35:29)
OS: OS X 10.11.6
`import ssl; ssl.OPENSSL_VERSION`: OpenSSL 1.0.2k  26 Jan 2017

What is happening?"
1134,Fix wait_for_io Tests,2017-03-13T15:34:07Z,2017-03-13T16:59:58Z,,,,As noticed by @sigmavirus24 there were extra parameters on `DefaultSelector` that were apparently only there to be silently passed in a `unittest.TestCase` while testing `wait_for_io` functions. These test cases have been reworked to use the same path as the `BaseSelectorTestCase` by patching `select` module and resetting the `DefaultSelector` cache. I hoisted `patch_select_module` outside the testcase to allow both sets of test cases to use it for patching.
1133,ImportError during install of urllib3[secure] on Jython,2017-03-10T14:26:07Z,,,,,"I'm getting a ImportError during `pip install urllib3[secure]` with Jython 2.7.0.

    Downloading/unpacking urllib3[secure] Running setup.py egg_info
      for package urllib3
        
        warning: no previously-included files matching '*' found under
      directory 'docs/_build' Installing extra requirements: 'secure'
      Downloading/unpacking pyOpenSSL>=0.14 (from urllib3[secure])
      Running setup.py egg_info for package pyOpenSSL
        
        warning: no previously-included files found matching
        'leakcheck' warning: no previously-included files matching
        '*.py' found under directory 'leakcheck' warning: no
        previously-included files matching '*.pem' found under
        directory 'leakcheck' no previously-included directories found
        matching 'doc/_build' no previously-included directories found
        matching '.travis' no previously-included directories found
        matching '.mention-bot' Downloading/unpacking
        cryptography>=1.3.4 (from urllib3[secure]) Running setup.py
        egg_info for package cryptography
        
        no previously-included directories found matching
        'docs/_build' warning: no previously-included files matching
        '*' found under directory 'vectors' Downloading/unpacking
        idna>=2.0.0 (from urllib3[secure]) Running setup.py egg_info
        for package idna
        
        warning: no previously-included files matching '*.pyc' found
        under directory 'tools' warning: no previously-included files
        matching '*.pyc' found under directory 'tests'
        Downloading/unpacking certifi (from urllib3[secure]) Running
        setup.py egg_info for package certifi
        
    Downloading/unpacking ipaddress (from urllib3[secure]) Downloading
      ipaddress-1.0.18.tar.gz Running setup.py egg_info for package
      ipaddress
        
    Downloading/unpacking six>=1.5.2 (from
      pyOpenSSL>=0.14->urllib3[secure]) Downloading six-1.10.0.tar.gz
      Running setup.py egg_info for package six
        
        no previously-included directories found matching
    'documentation/_build' Downloading/unpacking asn1crypto>=0.21.0
    (from cryptography>=1.3.4->urllib3[secure]) Running setup.py
    egg_info for package asn1crypto
        
    Downloading/unpacking packaging (from
      cryptography>=1.3.4->urllib3[secure]) Running setup.py egg_info
      for package packaging
        
        warning: no previously-included files found matching
        '.travis.yml' warning: no previously-included files found
        matching 'dev-requirements.txt' no previously-included
        directories found matching 'docs/_build' no
        previously-included directories found matching 'tasks'
        Downloading/unpacking setuptools>=11.3 (from
        cryptography>=1.3.4->urllib3[secure]) Running setup.pywhen
        egg_info for package setuptools Traceback (most recent call
        last): File ""<string>"", line 3, in <module> File
        ""setuptools/__init__.py"", line 10, in <module> from six.moves
        import filter, map ImportError: No module named six Complete
        output from command python setup.py egg_info: Traceback (most
        recent call last):
    
      File ""<string>"", line 3, in <module>
    
      File ""setuptools/__init__.py"", line 10, in <module>
    
        from six.moves import filter, map
    
    ImportError: No module named six
    
    ---------------------------------------- Command python setup.py
    egg_info failed with error code 1 in
    /home/me/dummy/build/setuptools Storing
    complete log in /home/me/.pip/pip.log
    
I'm trying to install `urllib3[secure]` in the hope to resolve a `SNIMissingWarning` which I got when [I ran coveralls](https://travis-ci.org/baztian/jaydebeapi/jobs/209732102). The exception message led me to https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings which further directs me to https://urllib3.readthedocs.io/en/latest/user-guide.html#ssl-py2 that suggests installing `urllib3[secure]`."
1132,Additional check before declaring a Selector,2017-03-08T10:58:54Z,2017-03-09T17:00:18Z,,,,"Ignore Selector(s) which are unsupported by underlying OS.

I think this is better than https://github.com/shazow/urllib3/pull/1131. However for some weird reason, even though I just changed the indexing of the code, the github compare makes it looks like I've changed the whole file."
1131,DefaultSelector fallback to less efficient methods,2017-03-08T09:30:07Z,2017-03-08T10:59:09Z,,,,Test the selector before making it the default selector. This is due to issue reported by myself at https://github.com/kennethreitz/requests/issues/3906
1130,Change HTTPHeaderDict vals intertal representation to list,2017-03-08T05:08:11Z,2017-03-09T14:43:09Z,,,,Just using list will make the code clear and not breaking the API
1129,Stop buffering entire deflate-encoded responses,2017-03-07T21:59:51Z,2017-03-09T12:02:48Z,,,,"Previously, if we received a deflate-encoded response that included the
zlib header, we would buffer the entire response in case decompressing
failed and we wanted to try interpretting the response as a raw deflate
stream.

Now, we commit to a single decoder as soon as we've decoded any data. As
a result, we can drop the buffer at the same time."
1128,Detect monkey-patched select module,2017-03-06T16:02:17Z,2017-03-13T14:51:32Z,,,,"This is another implementation for solving issue #1104. #1105 is too complex because it was trying to keep promises of the Python stdlib selectors modules that aren't relevant to urllib3.
This change is simpler and should make for a more comfortable change. :)

This change doesn't detect monkey-patches that occur after the first call to `DefaultSelector` but the major problem seems to be patches during import time so I think this is a good compromise.

Should hopefully make it into next requests release (?) as I've seen more than one project blacklisting upgrading requests until issue #1104 is resolved even though it's not really our fault."
1127,Fixed typo in urllib3/util/request.py,2017-02-27T18:34:13Z,2017-02-27T22:23:06Z,,,,
1126,Add CHANGES for #1125,2017-02-26T16:27:24Z,2017-02-26T16:54:55Z,,,,
1125,Wrap OpenSSL SysCallError on send as well as recv,2017-02-25T23:21:27Z,2017-02-26T16:17:19Z,,,,"I was having a similar issue as described in #367, occasionally getting ""SysCallError: (104, 'Connection reset by peer')"" on sending through the requests library. As I understand it, it should be intercepted and wrapped here."
1124,Distrust cert chains using legacy signature algorithms,2017-02-24T14:15:19Z,,Someday,,,"I think at a start, md5 should be blacklisted, and sha-1 generate a warning, with blacklisting in the coming months.

@Lukasa suggested that urllib3 was the correct place in the http-stack to do this."
1123,Fails with Python 3.4,2017-02-18T07:38:13Z,2017-02-18T08:11:16Z,,,,"If I attempt to use requests with caching enabled in Python 3.4, I get this traceback:

        return requests.get(url).text
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests/api.py"", line 70, in get
        return request('get', url, params=params, **kwargs)
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests/api.py"", line 56, in request
        return session.request(method=method, url=url, **kwargs)
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests_cache/core.py"", line 126, in request
        **kwargs
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests/sessions.py"", line 488, in request
        resp = self.send(prep, **send_kwargs)
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests_cache/core.py"", line 97, in send
        response, timestamp = self.cache.get_response_and_time(cache_key)
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests_cache/backends/base.py"", line 70, in get_response_and_time
        if key not in self.responses:
      File ""/myproject/.tox/py34/lib/python3.4/_collections_abc.py"", line 428, in __contains__
        self[key]
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests_cache/backends/storage/dbdict.py"", line 163, in __getitem__
        return pickle.loads(bytes(super(DbPickleDict, self).__getitem__(key)))
      File ""/myproject/.tox/py34/lib/python3.4/site-packages/requests/packages/urllib3/packages/ordered_dict.py"", line 8, in <module>
        from dummy_thread import get_ident as _get_ident
    nose.proxy.ImportError: No module named 'dummy_thread'

As far as I can tell, dummy_thread is a Python 2.7 module, which doesn't exist in Python 3.*, so I'm not sure what would be trying to use it, much less for retrieving URLs."
1122,[WIP] Add support for using SecureTransport as a TLS provider.,2017-02-17T14:03:33Z,2017-04-25T09:59:28Z,,,,"This is somewhat unexpected patch, but here we are. Let me explain what's going on.

A few months ago, @dstufft announced that [Fastly would be transitioning the Python Package Index over to being a TLSv1.2-only service](https://mail.python.org/pipermail/distutils-sig/2017-January/029970.html). The hard cut-off for this event would be June 2018.

By-and-large this does not affect most pip users. Users on Windows will generally have OpenSSLs shipped by python.org, which have tended to support TLSv1.2. The same is largely true of Linux users, most of whom have OpenSSL 1.0.1 or higher, which have TLSv1.2 support. This means that pip users on those platforms will largely be unaffected.

However, anyone using the *system* Python, or most Pythons obtained from python.org, on macOS is in deep trouble. This is because those Pythons all link against the system-provided OpenSSL, which is OpenSSL v0.9.8zh. This unconscionably ancient OpenSSL does not support TLSv1.2. As a result, any pip on those systems would find itself unable to contact the Python package index sometime in June 2018. This event would be, as our friends in the Physics community refer to it, ""an inconvenience"".

Because pip uses Requests, and Requests uses urllib3, the easiest way to solve this problem is to add a SecureTransport shim to urllib3, much like the PyOpenSSL shim that already exists. This would enable macOS users to use the SecureTransport TLS library that Apple has shipped with them instead of using OpenSSL for their TLS purposes, thereby avoiding the fustercluck that is the OpenSSL on that platform. This PR does exactly that.

There are some caveats and limitations on this Pull Request that need to be understood. They are:

1. This PR *must* use ctypes. This is because pip cannot ship any code that requires compilation. As a result, this PR contains a substantial amount of code to provide ctypes bindings to a large chunk of macOS libraries.
2. It is very difficult to turn off hostname validation for SecureTransport in such a way that urllib3 can continue to use its own hostname validation logic. For this reason, I have simply chosen not to do that and instead to exempt ST from hostname checking in urllib3. This means that `assert_hostname` does not work with ST. For the moment I'm ok with that, though I'd like to see if any maintainers disagree with that choice.
3. This currently completely ignores any options setting. I'm inclined to continue to do that.
4. This code does not support OpenSSL cipher strings, because I just could not be bothered to write parsing code for them.
5. This code does not work for Python 2.6, because Python 2.6 has no memoryview object. I could write my way out of that bind, but @dstufft has indicated that he doesn't care and frankly I don't think I do either.
6. Amusingly, while this code does work with `assert_fingerprint`, it does *not* disable hostname verification when `assert_fingerprint` is being used. That means that for ST `assert_fingerprint` *and* hostname validation need to match, whereas for other options just `assert_fingerprint` has to match.
7. This code does not work on older Macs (older than about 10.8). That's almost certainly fine: Mac users tend to be aggressive upgraders.

There is some work still to do here:

- [x] Take complete control of timeouts away from the socket, so that Python doesn't create new selectors under our feet.
- [x] Add locking around the connection ref dictionary to enable thread-safety.
- [x] Handle the `protocol` argument.
- [x] Add support for client certs.

However, as this is now beginning to work I wanted to make this available so that the interested parties can start looking at it and thinking about how we want to handle it. In particular, I have a few questions:

1. Should urllib3 ever actually ship this? It's a bunch of code that favours one platform (macOS) over another equally-deserving one (Windows). It adds a *load* of extra code to do that, which we'd have to support.
2. If urllib3 supports it, should Requests optimistically auto-enable it like it does with PyOpenSSL?
3. Does this cover all of `pip`'s needs?"
1121,allow http/https requests through https proxy,2017-02-16T15:54:38Z,2017-02-16T16:21:41Z,,,,"Fix https://github.com/shazow/urllib3/issues/476

urllib3 http/https examples
```
python -c ""print __import__('urllib3').ProxyManager('https://bwg.phus.lu').request('GET', 'http://httpbin.org/ip').data""
python -c ""print __import__('urllib3').ProxyManager('https://bwg.phus.lu').request('GET', 'https://httpbin.org/ip').data""
```

requests http/https examples
```
python -c ""print __import__('requests').get('http://httpbin.org/ip', proxies={'https': 'https://bwg.phus.lu', 'http': 'https://bwg.phus.lu'}).text""
python -c ""print __import__('requests').get('https://httpbin.org/ip', proxies={'https': 'https://bwg.phus.lu', 'http': 'https://bwg.phus.lu'}).text""
```

Signed-off-by: Phus Lu <phuslu@hotmail.com>"
1120,urllib3 1.20 test failures in test/contrib/test_socks.py,2017-02-16T08:47:42Z,2017-02-16T09:04:17Z,,,,"Hi,

When trying to upgrade urllib3 to version 1.20, I'm getting a bunch of test failures in `test.contrib.test_socks.TestSOCKS4Proxy` and `test.contrib.test_socks.TestSocks5Proxy`. The full log is here: https://gist.github.com/wizeman/a362f0f1e1c9364e1197801ca6c70326

As you can see, I'm using python 2.7.

What could be the cause?
I'm surprised you aren't getting these test errors, as it doesn't seem to be a transient failure (like a timeout or something similar).
"
1119,Re-add PyOpenSSL testing.,2017-02-15T13:42:58Z,2017-02-15T17:01:24Z,,,,This slipped through code review of #1103.
1118,Skip test_can_validate_ip_san when unsupported,2017-02-14T08:18:29Z,2017-02-15T12:00:17Z,,,,Resolves #1117.
1117,ERROR: Ensure that urllib3 can validate SANs with IP addresses in them.,2017-02-14T07:01:17Z,2017-02-14T16:38:35Z,,"SSLError, `SSLError","SSLError: hostname '127.0.0.1' doesn't match either of 'localhost', '127.0.0.1', `SSLError: hostname '127.0.0.1' doesn't match either of 'localhost', '127.0.0.1'` (!)","I get this error when trying to build urllib3-1.20

(This is in connection with https://github.com/NixOS/nixpkgs/issues/22616)

```
======================================================================
ERROR: Ensure that urllib3 can validate SANs with IP addresses in them.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/nix-build-python2.7-urllib3-1.20.drv-0/urllib3-1.20/test/with_dummyserver/test_https.py"", line 508, in test_can_validate_ip_san
    r = https_pool.request('GET', '/')
  File ""/tmp/nix-build-python2.7-urllib3-1.20.drv-0/urllib3-1.20/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/tmp/nix-build-python2.7-urllib3-1.20.drv-0/urllib3-1.20/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/tmp/nix-build-python2.7-urllib3-1.20.drv-0/urllib3-1.20/urllib3/connectionpool.py"", line 630, in urlopen
    raise SSLError(e)
SSLError: hostname '127.0.0.1' doesn't match either of 'localhost', '127.0.0.1'
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): 127.0.0.1
urllib3.connection: ERROR: Certificate did not match expected hostname: 127.0.0.1. Certificate: {'subjectAltName': (('email', 'root@localhost'), ('DNS', 'localhost'), ('IP Address', '127.0.0.1')), 'notBefore': u'Dec 22 07:58:40 2011 GMT', 'serialNumber': u'01', 'notAfter': 'Dec 18 07:58:40 2021 GMT', 'version': 3L, 'subject': ((('countryName', u'FI'),), (('stateOrProvinceName', u'dummy'),), (('localityName', u'dummy'),), (('organizationName', u'dummy'),), (('organizationalUnitName', u'dummy'),), (('commonName', u'localhost'),)), 'issuer': ((('countryName', u'FI'),), (('stateOrProvinceName', u'dummy'),), (('localityName', u'dummy'),), (('organizationName', u'dummy'),), (('organizationalUnitName', u'dummy'),), (('commonName', u'SnakeOil'),), (('emailAddress', u'dummy@test.local'),))}
--------------------- >> end captured logging << ---------------------
```

Is this user error? I can't really understand the error:
`SSLError: hostname '127.0.0.1' doesn't match either of 'localhost', '127.0.0.1'` (!)"
1116,Find supported TLS version in tests.,2017-02-13T09:39:32Z,2017-02-13T12:32:11Z,,,,"Resolves #1114.

This should be a solid forward-looking revision to these tests to ensure that they can run on a variety of systems without difficulty. We shouldn't have tests fail just because OpenSSL has been compiled slightly weirdly, especially as urllib3 itself has no reliance on these constants being present."
1115,Fix Python 2.6 builds,2017-02-12T14:50:42Z,2017-02-12T19:24:18Z,,,,"So, cryptography currently doesn't have a wheel build for Python 2.6, which is causing our master branch tests to fail. This is because we're setting CFLAGS and LDFLAGS for the original build, which are unfortunately saved and re-used *badly* for subsequent pip builds.

I'm going to try to fix it."
1114,Reduce reliance on specific TLS/SSL constants,2017-02-11T08:26:09Z,2017-02-13T12:32:17Z,,,,"There are a bunch of tests in `test.with_dummyserver.with_https` that rely on having specific OpenSSL constants present (e.g. Some of the ones in TestHTTPS_TLSv1. Ideally these tests would be refactored to be more tolerant of not having some constants around by picking the ones they can get, rather than being specific. 

Related: #1113"
1113,urllib3 cannot build against LibreSSL,2017-02-11T01:48:35Z,2017-02-11T08:23:29Z,,,,"I'm using NixOS, and having the following issue:
https://github.com/NixOS/nixpkgs/issues/22616

It seems that, at least in its default configuration, urllib3 requires SSLv3 -- but libreSSL doesn't provide it. Is there a way to use urllib3 with libreSSL?

"
1112,Bug: No Retry for SSL Error,2017-02-08T23:08:51Z,2017-06-07T12:08:48Z,Contributor Friendly ‚ô•,,,"That following code should fire off a `MaxRetryError`, but instead doen't retry and throws an `SSLError`.

**Test case:**
_server.py_
```python
import time
import socket

s = socket.socket()
s.bind(('', 4433))
s.listen(10)

while True:
    news, _ = s.accept()
    time.sleep(0.5)
    news.close()
```
_client.py_
```python
import urllib3
http = urllib3.PoolManager()
http.request('GET', 'https://localhost:4433/', retries=5)
```

<sup>*Disclaimer:* Test case was created by [**Lukasa**](https://github.com/kennethreitz/requests/issues/3845#issuecomment-278273421)</sup>"
1111,Error: ConnectionError: HTTPConnectionPool: Max retries exceeded with url,2017-02-07T03:05:01Z,2017-02-07T08:28:53Z,,,,"I have the following code(part):

    class Test(threading.Thread):
        
        # other more codes

        def _send_request(self, url):
            s = requests.Session()
            s.keep_alive = False
            resp = None
            try:
                resp = s.get(urls)
                data = resp.json()
            except:
                if resp is not None:
                    resp.connection.close()
                print traceback.print_exc()
                return (False, """")
            if resp is not None:
                resp.connection.close()
            return (True, data)

        def _send_meta_check_request(self, pk):
            urls = ""/check/meta?method=innerselect&pk="" + pk
            b, d = self._send_request(urls)
            if b == False:
                return False

When I run the function: _send_meta_check_request, it will have the following errors:

    WARNING: 2017-02-07 10:30:22 [check599_new_v2.py: _send_request: 163] Traceback (most recent call last):
      File ""check599_new_v2.py"", line 160, in _send_request
        resp = session.get(urls)
      File ""/home/work/.jumbo/lib/python2.7/site-packages/requests/sessions.py"", line 468, in get
        return self.request('GET', url, **kwargs)
      File ""/home/work/.jumbo/lib/python2.7/site-packages/requests/sessions.py"", line 456, in request
        resp = self.send(prep, **send_kwargs)
      File ""/home/work/.jumbo/lib/python2.7/site-packages/requests/sessions.py"", line 559, in send
        r = adapter.send(request, **kwargs)
      File ""/home/work/.jumbo/lib/python2.7/site-packages/requests/adapters.py"", line 375, in send
        raise ConnectionError(e, request=request)
    ConnectionError: HTTPConnectionPool(host='...', port=80): Max retries exceeded with url: ... (Caused by <class 'socket.error'>: [Errno 104] Connection reset by peer)

BTW, the url is OK when I use `curl -v url` it can run right.

python version: `2.7.3`
"
1110,Missing response headers when malformed header is part of the response,2017-02-04T20:22:31Z,2017-02-04T21:00:50Z,,,,"Python 3.5, urllib3 1.20

```
>>> import urllib3
>>> http = urllib3.PoolManager()
>>> r = http.request('GET', 'https://online.chasecanada.ca/ChaseCanada_Consumer/Login.do')
>>> r.status
200
>>> r.headers
HTTPHeaderDict({'Date': 'Sat, 04 Feb 2017 20:09:21 GMT'})
>>>
```

I'm pretty sure the problem is caused by an invalid HTTP header returned by the server:

    HTTP/1.1 200 OK
    Date: Sat, 04 Feb 2017 19:16:34 GMT
    My Param: None
    [...]

It directly follows the Date response header, which is returned fine, but since no other response headers is returned, I think this broken header is breaking the HTTP response headers parser.

Of note: the `http.client.HTTPresponse.headers` object (`HTTPMessage`) shows all headers in `_payload`, but only the `Date` header in `_headers`."
1109,Remove old Homebrew advice,2017-02-02T16:36:26Z,2017-02-13T08:26:18Z,,,,Homebrew's Python always uses Homebrew's openssl now.
1108,Break firmly away from httplib,2017-02-01T14:09:51Z,2017-02-13T12:44:03Z,,,,"Ok, this new major commit series breaks firmly away from httplib. It introduces a brand new connection class that is vastly stripped down compared to what httplib offered, including a very limited set of functionality. It rewrites that connection to use selectors rather than blocking socket I/O, enabling us to abort request sending for early responses (a regularly requested feature).

It also allows us to begin blasting through the code removing stuff, which is a huge win.

This is something that I'd like people to look over at a high level. Obviously, detailed review is difficult, but another set of eyes on this would be really helpful."
1107,Add support for X509 and PKey for load_cert_chain instead of just a filepath,2017-02-01T02:01:30Z,2017-02-01T09:35:20Z,,,,Add support for X509 and PKey for load_cert_chain instead of just a filepath
1106,ImportError for python but not for python3,2017-01-31T10:52:47Z,2017-01-31T10:55:28Z,,ImportError,"ImportError: No module named urllib3.packages.ordered_dict","Hi,
when I try to do ```conda``` on my machine Ubuntu14.04, I get the following error:
```
Traceback (most recent call last):
  File ""/home/zhang/anaconda2/bin/conda"", line 4, in <module>
    import conda.cli
  File ""/home/zhang/anaconda2/lib/python2.7/site-packages/conda/cli/__init__.py"", line 8, in <module>
    from .main import main  # NOQA
  File ""/home/zhang/anaconda2/lib/python2.7/site-packages/conda/cli/main.py"", line 46, in <module>
    from ..base.context import context
  File ""/home/zhang/anaconda2/lib/python2.7/site-packages/conda/base/context.py"", line 20, in <module>
    from ..common.url import urlparse, path_to_url
  File ""/home/zhang/anaconda2/lib/python2.7/site-packages/conda/common/url.py"", line 17, in <module>
    from requests.packages.urllib3.util.url import parse_url
  File ""/usr/lib/python2.7/dist-packages/requests/__init__.py"", line 58, in <module>
    from . import utils
  File ""/usr/lib/python2.7/dist-packages/requests/utils.py"", line 25, in <module>
    from .compat import parse_http_list as _parse_list_header
  File ""/usr/lib/python2.7/dist-packages/requests/compat.py"", line 92, in <module>
    from urllib3.packages.ordered_dict import OrderedDict
ImportError: No module named urllib3.packages.ordered_dict
```

When I run ```pip install urllib3``` I get the following:
```
Requirement already satisfied (use --upgrade to upgrade): urllib3 in /usr/local/lib/python2.7/dist-packages
Cleaning up...
```
However,
```
me@mymachine:~$ python
Python 2.7.12 |Anaconda 4.2.0 (64-bit)| (default, Jul  2 2016, 17:42:40) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
>>> import urllib3
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: No module named urllib3
```

```
me@mymachine:~$ python3
Python 3.4.3 (default, Nov 17 2016, 01:08:31) 
[GCC 4.8.4] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import urllib3
```
How should I resolve this issue? Thanks in advance!"
1105,Selectors detect a monkey-patched select module,2017-01-25T19:39:49Z,2017-03-06T16:07:04Z,,,,"PR to fix issue when the `select` module is patched after compile time by gevent, eventlet, etc as found in #1104 This patch doesn't contain any tests but passes all current tests, will add some later today. I'm sure there'll be some coverage issues as well that will be resolved. :)

This patch definitely makes `selectors.py` a bit of an abomination, open to better suggestions. :) cc: @Lukasa "
1104,AttributeError: 'module' object has no attribute 'epoll',2017-01-25T17:54:34Z,2017-05-17T09:44:43Z,,AttributeError,"AttributeError: 'module' object has no attribute 'epoll'","Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 115, in wait
    listener.cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 214, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/networking_calico/agent/dhcp_agent.py"", line 553, in loop
    super(SubnetWatcher, self).loop()
  File ""/usr/local/lib/python2.7/dist-packages/networking_calico/logutils.py"", line 21, in wrapped
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/networking_calico/etcdutils.py"", line 250, in loop
    response = self.wait_for_etcd_event()
  File ""/usr/local/lib/python2.7/dist-packages/networking_calico/etcdutils.py"", line 344, in wait_for_etcd_event
    timeout=self.etcd_timeout)
  File ""/usr/local/lib/python2.7/dist-packages/etcd/client.py"", line 562, in read
    timeout=timeout)
  File ""/usr/local/lib/python2.7/dist-packages/etcd/client.py"", line 820, in wrapper
    params=params, timeout=timeout)
  File ""/usr/local/lib/python2.7/dist-packages/etcd/client.py"", line 890, in api_execute
    preload_content=False)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/poolmanager.py"", line 244, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 588, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.py"", line 241, in _get_conn
    if conn and is_connection_dropped(conn):
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/connection.py"", line 27, in is_connection_dropped
    return bool(wait_for_read(sock, timeout=0.0))
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/wait.py"", line 33, in wait_for_read
    return _wait_for_io_events(socks, EVENT_READ, timeout)
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/wait.py"", line 22, in _wait_for_io_events
    with DefaultSelector() as selector:
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/util/selectors.py"", line 364, in __init__
    self._epoll = select.epoll()
AttributeError: 'module' object has no attribute 'epoll'
"
1103,Fix all flake/linter errors within tests,2017-01-23T18:27:03Z,2017-01-24T08:50:31Z,,,,"Work so far on fixing all flake / lint errors in the `test/` directory. Also adds that directory to the list of locations that are checked by `flake8` by the CI.

This PR does not yet check for line too long because of how many instances there are of that error, I will work on that tonight."
1102,Add cleanup to pools and managers,2017-01-21T20:38:08Z,2017-01-26T15:59:28Z,,,,"Work for #1100 by adding `.close` for all ConnectionPools and `.clear` for all PoolManagers.

Also noticed that in `TestUtil` there were a few duplicate keys in `parse_url_host_map` so I changed it to a list of tuples to make sure the duplicates weren't silently discarded."
1101,Add pool_timeout as a parameter in HTTPConnectionPool,2017-01-20T17:28:34Z,,,,,"This issue is a follow up of [this issue](https://github.com/kennethreitz/requests/issues/3825#issuecomment-274070742) opened in the [requests repository](https://github.com/kennethreitz/requests)

As indicated in the file [requests/packages/README.rst](https://github.com/kennethreitz/requests/blob/master/requests/packages/README.rst), if this is accepted, the changes will be first implemented and pushed here."
1100,Tests leak FDs which leads to test failures on machines with low ulimits.,2017-01-20T16:52:33Z,,,,,"As was discussed a lot in #1098 many tests open sockets and don't provide a proper clean-up which results in many leaking FDs during a test execution. This leads to problems on machines with low soft ulimits such as Mac OS.

I'll be making an effort to change these tests to not leak FDs such as unclosed connections, pools, etc."
1099,Add changes for #1095.,2017-01-20T09:48:01Z,2017-01-20T11:08:42Z,,,,
1098,Skip test_select_many_events if OSError(24) is thrown on Travis.,2017-01-19T19:03:31Z,2017-01-20T16:56:32Z,,,,"This error has been seen on Mac OS builders with the past few PRs, wondering if it's a Travis Mac OS error only and isn't anything wrong with selectors. If so, this will disable that test on Travis Mac OS builders when the error occurs. Should not hurt coverage at all.

@Lukasa said something about investigating, if you find anything that would cause this besides Travis just close this. :)"
1097,CHANGES for 1.20,2017-01-19T09:55:15Z,2017-01-19T12:21:00Z,,,,Time to merge!
1096,Improve support for path-only URLs.,2017-01-18T10:32:44Z,,,,,"Encountered while doing a substantial backend rewrite for v2.

Currently, `parse_url` doesn't do well with non-urlencoded URLs that are intended to have no host portion, as frequently used by HTTP Location headers:

```python
>>> from urllib3.util.url import parse_url
>>> parse_url('/redirect?target=http://localhost:61020/')
Url(scheme='/redirect?target=http', auth=None, host='localhost', port=61020, path='/', query=None, fragment=None)
```

As you can see here, the `://` in the URL confuses urllib3's URL handling logic, which means it ends up incorrectly assuming that the scheme is `/redirect?target=http`. This is causing problems in the v2 backend, which uses `parse_url` to attempt to isolate out the request URL to put into the HTTP/1.1 request.

Frustratingly, urlparse gets this right:

```python
>>> from urllib.parse import urlparse
>>> urlparse('/redirect?target=http://localhost:61020/')
ParseResult(scheme='', netloc='', path='/redirect', params='', query='target=http://localhost:61020/', fragment='')
```
It would be good if `parse_url` could spot a URL that is path-only and handle it sensibly.

This is part of a larger problem, which is that `parse_url` and the `Url` class cannot be used to incrementally build up a URL target. This is required to emulate browser logic for redirects if that's a thing that we want to do: it needs to be possible to handle all URLs that could be returned in Location headers, including absolute, scheme-relative, absolute-path, and path-relative, but right now urllib3 can't really do that.

The larger issue doesn't block v2, but this does. It's probably something that @sigmavirus24 could give guidance on if someone wants to pick it up."
1095,Add fast path for system call wrapper on Python 3.5+,2017-01-18T02:23:03Z,2017-01-20T09:46:45Z,,,,"I've updated my selectors2 project to 1.1.0 and the main feature added is a fast path the system call wrapper on Python 3.5+. This wrapper can be substantially smaller and faster because in Python 3.5+ PEP 475 is implemented which automatically retries system calls in the case of an interrupt. I figured I would propagate these changes here to make sure urllib3 is getting everything out of its selectors upgrade. :tada:

Overall the wrapper performs about **126% faster** with a happy path and **36% faster** with an error path.

This version also catches and correctly handles an error on Windows platforms when trying to call `BaseSelector.unregister` on a closed socket where the previous version did not."
1094,Add Codecov shield to README.rst,2017-01-17T17:29:00Z,2017-01-17T21:08:51Z,,,,"Self explanatory PR, adds the Codecov shield to the README.
Codecov will be a required check soon with #1007 "
1093,Changes for #1035.,2017-01-17T08:16:09Z,2017-01-17T09:01:17Z,,,,
1092,Fix incorrect timeouts,2017-01-14T21:23:39Z,,,,," - Fixed not setting connect timeout on HTTP connections.
 - Fixed incorrect timeout on socket when using a new HTTPS connection.

refs #857"
1091,Add changes for #1013,2017-01-13T13:47:31Z,2017-01-13T15:20:16Z,,,,
1090,Running urllib3 tests on windows platform,2017-01-12T03:53:39Z,2017-01-12T08:57:24Z,,,,"Hello Andrey,
Recently I have been working on integration tests of one of my projects with appveyor service (windows-based continuous integration service).
I can write and debug appveyor.yml config file so you be able to run tests on appveyor service.
Let me know if you are interested. Thanks."
1089,only normalize http(s) urls,2017-01-11T19:10:27Z,2017-02-13T08:56:34Z,,,,"This is a proposed solution for #1080. We'll now restrict our host lower-casing to only schemes we've noted to be ""normalizable""."
1088,"If the response is complete, stream/read_chunked should short-circuit return the empty string.",2017-01-11T17:29:03Z,,Contributor Friendly ‚ô•,,,"Originally seen in kennethreitz/requests#3807.

Basically, read_chunked should check whether `_fp` is `None`. If it is, it should immediately return the empty byte string rather than do any processing.

Put another way, this shouldn't blow up:

```python
import urllib3

http = urllib3.PoolManager()
resp = http.request('GET', 'http://http2bin.org/stream/100')
resp.read()
next(resp.read_chunked())
```

I recommend someone who wants to help out pick this issue up, it should be easily fixed and tested."
1087,Custom LifoQueue,2017-01-06T15:46:50Z,,,,,"During some profiling, I found out one slow operation is `self.pool.get()` in `_get_conn()`.

In urllib3, in `connectionpool.ConnectionPool`, it uses `LifoQueue`:
https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L65

Which uses a list instead of a much faster deque:
https://github.com/python/cpython/blob/master/Lib/queue.py#L237

My suggestion is one could use a custom LifoQueue that uses the underlying deque, as:
```python
class LifoQueue(Queue):
    def _get(self):
        return self.queue.pop()
```
"
1086,Remove BufferedReader from OldHTTPResponse,2017-01-05T13:02:56Z,2017-01-05T13:19:56Z,,,,"Frankly, this subclass is just cluttering up the codebase. It adds a bunch of methods we don't need, and requires that we do some weird workarounds like the `isclosed()` method. Let's abandon that, and delete a whole bunch more code.

OldHTTPResponse is getting pretty small now!"
1085,Remove the ntlmpool contrib module.,2017-01-05T10:26:04Z,2017-01-05T12:08:07Z,,,,"It's basically unchanged since it was added a very long time ago, and
has no tests, so rather than refactor it we should just remove it. I
doubt it even works anymore."
1084,Refactor common code from reading loops,2017-01-05T09:38:42Z,2017-01-05T09:54:51Z,,,,"This pulls out a bunch of common code, trying to strip down the number of places we touch h11 and remove duplicate code. To get both `read()` and `read1()` working sensibly is a bit of a pain in the neck, so right now we're using this wacky generator-pipeline construction to pull it off.

Ultimately, I think this is mostly temporary, and we'll move to a new I/O model that removes a bunch of this complexity, but right now we're not there."
1083,Remove getheader/getheaders from OldHTTPResponse,2017-01-04T11:21:54Z,2017-01-04T12:28:52Z,,,,These are vestigial and we have our own implementation higher up the tree anyway.
1082,Remove header validation from v2 branch.,2017-01-04T08:50:40Z,2017-01-04T09:24:41Z,,,,"This is handled by h11, so we don't need to do it ourselves."
1081,Cleanup request methods.,2017-01-03T11:17:35Z,2017-01-04T08:41:28Z,,,,"This patch removes `putrequest`/`putheader`/`endheaders`, all of which were superfluous to our actual needs. Given that we plan to use a static Request object at some point in the future, we have no need to allow these incremental request building methods, and so we can avoid them entirely and remove several bits of state tracking they enforced.

It also removes a bunch of code. Yay!"
1080,restrict host normalization to http/https,2016-12-23T18:30:34Z,2017-02-13T14:52:31Z,,,,"This is an offshoot from further discussion in kennethreitz/requests#3738 beyond the scope of the PR.

#911 introduced normalizing both the host and scheme to lowercase which is valid for http URIs. Requests however was supporting the passing of percent-encoded file paths with the http+unix:// scheme which is now broken as of Requests 2.12.0 (urllib3 1.17+). @Lukasa [suggested](https://github.com/kennethreitz/requests/pull/3738#issuecomment-268883231) we could restrict host normalization to a specific set of schemes to avoid breaking case-dependent URLs.

What we want to do is only run [this check](https://github.com/shazow/urllib3/blob/4851f6d5e23851b497cd30cac89ebcabf5b8853c/urllib3/util/url.py#L24-L25) when there is a specified scheme and it is inside of a predefined set of normalizable schemes (http://, https://, ?).

@Lukasa may have more details on how he'd like this being implemented, but this would likely be a good issue for someone looking to contribute to urllib3."
1079,adding name to contributor's list for lazily load idna related change,2016-12-23T18:12:30Z,2016-12-23T19:47:22Z,,,,Raising another PR. Forgot to add it earlier. 
1078,Add changes for #1076.,2016-12-23T17:48:11Z,2016-12-23T22:09:04Z,,,,
1077,Add Python 3.6 support.,2016-12-23T17:40:21Z,2016-12-23T18:45:44Z,,,,
1076,1075: lazy load idna library,2016-12-23T16:41:27Z,2016-12-23T17:46:47Z,,,,"Utility to load `idna` library when needed and delete the references once done. Consumed memory will be freed by the GC

Related to issue:  https://github.com/shazow/urllib3/issues/1075"
1075,Lazy load idna,2016-12-23T15:59:10Z,2016-12-30T11:33:48Z,,,,"It is related to opened issue with requests library: https://github.com/kennethreitz/requests/issues/3780

`idna` is added to sys.modules with import of requests in each process, and uses up too much memory. The idea is to load `idna` as per the need basis. Since urllib3 also imports idna in pyopenssl: https://github.com/shazow/urllib3/blob/master/urllib3/contrib/pyopenssl.py#L46 , lazy loading should be implemented in urllib3 as well."
1074,Draft pipeline architecture,2016-12-23T05:44:16Z,,,,,"@Lukasa and I had discussed a possible architecture alteration for V2. Instead of having `PoolManager` encapsulate `ConnectionPool`, `SessionManager` encapsulate `PoolManager`, and so on, each of these high-level objects would be built as a pipeline of vastly-simplified steps, each of which would be entirely self-contained.

This pull request is a draft of a first step towards doing that. It adds [a pipeline architecture I've been working on](https://github.com/haikuginger/httppipeline), and refactors `PoolManager` into a set of individual pipeline elements.

As this is quite speculative, I haven't written tests for it yet; this is more of a proof-of-concept for discussion.

For right now, this isn't integrated with `RequestMethods` (which would be broken down into a couple pipeline elements of its own); to test it, do the following:

```python
from urllib3.pipeline.poolmanager import PoolManagerPipeline
pm = PoolManagerPipeline()
```

From there, doing `pm.request()` should behave similarly to `PoolManager.urlopen()`, with the exception of not taking positional arguments. Positional arguments could be added with an API-level pipeline element.

Future benefits include the ability to build small, easily-reusable modules. For example, we could add a `CookiePipelineElement` that sat near the base level, handling cookies on each request that passes through and response that passes back. We'd also be able to test individual elements as units much more easily without substantial mocking.

Pinging @jonparrott, @Lukasa, @SethMichaelLarson, @shazow, and @sigmavirus24 to take a look and drop their opinions. If people are happy with the overall architecture, then I'll continue working on it."
1073,CHANGES for #1059.,2016-12-16T13:03:36Z,2016-12-16T14:39:48Z,,,,
1072,remove _UNKNOWN,2016-12-14T19:13:58Z,2016-12-17T14:24:25Z,,,,"This will remove `_UNKNOWN`, a sentinel object used in httplib, out of the `OldHTTPResponse`. All of these variables, except `length` are initialized in `begin` so their starting state of `None` is fine. I can't find anything that is actually touching `length` in `OldHTTPResponse` now."
1071,feature request- capture ip address onto response object,2016-12-14T02:42:11Z,,,,,"this is an extension of a request from the `requests` library (https://github.com/kennethreitz/requests/issues/2158)

I recently ran into an issue with the various ""workarounds"", and have been unable to consistently access an open socket across platforms, environments, or even servers queried (the latter might  be from unpredictable timeouts)

it would honestly be great if the remote ip address were cached onto the response object by this library.  "
1070,localhost DNS resolution problem in tests with misconfigured /etc/hosts,2016-12-13T13:22:58Z,2016-12-13T13:23:04Z,,,,"[After further investigation, I figured out the problem here was either an old or just plain misconfigured `/etc/hosts`, but decided it made the most sense to submit the issue anyway and then immediately close it, in case someone else runs into a similar problem]

Running the tests on Fedora 25, I found an interesting failure case for the ""Does an IPv6 local loopback actually work?"" checks in the DummyServer implementation. Specifically (the port number below comes from starting a SocketDummyServerTestCase test case in an interactive Python shell):

```
$ netstat -tl | grep 37677
tcp6       0      0 localhost6.locald:37677 [::]:*                  LISTEN
$ ping localhost -c 1
PING localhost.localdomain (127.0.0.1) 56(84) bytes of data.
...
$ ping localhost6 -c 1
PING localhost6(localhost6.localdomain6 (::1)) 56 data bytes
```

What appeared to be happening was that `socket.bind()` accepted `""localhost""` as a valid interface name to bind to, so the dummy server code at https://github.com/shazow/urllib3/blob/master/dummyserver/server.py#L53 sets `HAS_IPV6_AND_DNS` and binds to `::1`, but when the client tries to *resolve* `""localhost""` it gets handed `127.0.0.1`, which then doesn't work.

Using Charles-Fran√ßois's queries from https://bugs.python.org/issue18792 I saw:

```
>>> for addr in socket.getaddrinfo(""localhost"", 0): print(addr)
... 
(<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_STREAM: 1>, 6, '', ('127.0.0.1', 0))
(<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_DGRAM: 2>, 17, '', ('127.0.0.1', 0))
(<AddressFamily.AF_INET: 2>, <SocketKind.SOCK_RAW: 3>, 0, '', ('127.0.0.1', 0))
>>> for addr in socket.getaddrinfo(""localhost6"", 0): print(addr)
... 
(<AddressFamily.AF_INET6: 10>, <SocketKind.SOCK_STREAM: 1>, 6, '', ('::1', 0, 0, 0))
(<AddressFamily.AF_INET6: 10>, <SocketKind.SOCK_DGRAM: 2>, 17, '', ('::1', 0, 0, 0))
(<AddressFamily.AF_INET6: 10>, <SocketKind.SOCK_RAW: 3>, 0, '', ('::1', 0, 0, 0))
```

Which sounded a lot like the problems discussed in https://github.com/shazow/urllib3/pull/611

However checking a fresh Fedora container image suggests that the expectation is for `localhost` to be used for both IPv4 and IPv6, and my local workstation just has some old config settings lying around:

```
$ docker run --rm fedora:25 cat /etc/hosts
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.17.0.2      3fe6f8fb4bb1
```

So I edited `/etc/hosts` to also list `localhost` on the `::1` line and the test issue went away."
1069,Stop supporting read_chunked and friends.,2016-12-12T14:26:46Z,2016-12-13T12:38:00Z,,,,"This patch removes support for `read_chunked` and all its supporting methods. We agreed to remove these because supporting the functionality whereby we'd expose chunked bodies to the user is not all that useful.

We should consider whether `stream` should swap over to using `read1` instead of `read`, but that's a different patch: see https://github.com/shazow/urllib3/projects/1#card-1081219."
1068,Work for v2 of urllib3.,2016-12-12T13:33:08Z,,,,,"This is a follow-on from #1067. This meta-PR is used primarily to allow a single, easy place to see what work has been done and is being done for v2.

Smaller work items:

- Remove support for respecting chunked bodies as part of `stream`. (#1069)"
1067,[WIP] Remove httplib.,2016-12-08T15:15:25Z,2016-12-12T13:33:20Z,,,,"This branch contains the most substantial chunk of work I have ever done on urllib3: the beginnings of an effort to remove our dependency on httplib.

Removing our dependency on httplib has been a long-held desire of the urllib3 project. In fact, it's so desired that #776 (our wishlist of features for v2.0) includes in a section entitled ""unlikely longshots"" the phrase ""Get rid of httplib dependence"".

This branch does exactly that, by bringing the `http.client` implementation from Python 3.5 that we were already used to interacting with into our codebase and then ripping its guts out and replacing them with @njsmith's [h11](https://github.com/njsmith/h11) library.

As you can see from the commit log, my work on this began back in October, but I've sat on it for the past two months in order to get it to a place where I had something worth showing. Now I finally do. On my machine, a run of `tox -e py27` passes all tests, albeit with incomplete test coverage:

```
py27 runtests: commands[1] | nosetests
SSS....................S....................................................................................................................................S.....................................................S........................................................................................................................................................................................................................SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS...............................S..S..............................S................................................................................
Name                      Stmts   Miss  Cover   Missing
-------------------------------------------------------
urllib3                      20      0   100%   
urllib3._collections        148      0   100%   
urllib3.connection          515    103    80%   80-82, 158, 162, 175, 229, 261, 285, 295-300, 306-338, 341-364, 368-372, 375, 385-392, 396-399, 408, 449, 453, 471, 567, 571-572, 602-611, 615, 651-652, 657, 729-732, 736-740, 755, 781, 830, 855-856, 868-869, 905-906, 972-973, 985, 1005
urllib3.connectionpool      240      1    99%   256
urllib3.contrib               0      0   100%   
urllib3.contrib.socks        35      0   100%   
urllib3.exceptions           73      2    97%   216, 219
urllib3.fields               62      0   100%   
urllib3.filepost             32      0   100%   
urllib3.poolmanager         124      0   100%   
urllib3.request              32      0   100%   
urllib3.response            232      1    99%   453
urllib3.util                  1      0   100%   
urllib3.util.connection      47      0   100%   
urllib3.util.request         37      0   100%   
urllib3.util.response        19      5    74%   54-65
urllib3.util.retry          121      0   100%   
urllib3.util.selectors      295     45    85%   101, 360-423
urllib3.util.ssl_            64      0   100%   
urllib3.util.timeout         48      0   100%   
urllib3.util.url             97      0   100%   
urllib3.util.wait            14      1    93%   21
-------------------------------------------------------
TOTAL                      2256    158    93%   
----------------------------------------------------------------------
Ran 604 tests in 13.646s

OK (SKIP=42)
___________________________________ summary ____________________________________
  py27: commands succeeded
  congratulations :)
```

Initially, my goal was to do this in the least-disruptive way possible: that's why I began by trying to replicate the logic of httplib inside urllib3 but on top of h11. I would have succeeded in that goal if I'd been able to avoid rewriting any tests in any substantive way.

Unfortunately, as you can see from this change, I did not succeed at that. In particular, it was simply not possible to continue with some behaviours that urllib3 had previously promised, such as maintaining header case. I also had to bend over backwards to preserve some other behaviours urllib3 users have expected, such as reporting chunk boundaries, which is probably not something we should ever have promised to do.

From my perspective, then, I think that if we want to continue this work it needs to be considered a breaking change for urllib3. Any change this substantial simply cannot be done in a non-breaking way. A huge number of things will change: our tolerance of errors (we're much stricter now because h11 is much more insistent about being spec-compliant), our wire format (h11 does some case normalization on header field names), and maybe even some of our output formats.

This is also still very much a work in progress. There are TODO statements everywhere. The code is poorly factored: it's a wacky in-between state between trying to maintain the httplib abstraction and a total rejection of it in favour of something much clearer. A whole lot of our code that worked around httplib flaws is no longer necessary and can be removed, but hasn't yet been. A better underlying implementation that doesn't perform socket writes would also be possible, as would some refactoring of code I wrote to pull out common features. And the Python 3 tests don't pass, mostly because of the fact that right now this new branch emits headers as bytestrings rather than unicode strings.

However, at this point it's no longer feasible for me to work on this branch alone. Further progress requires actual governance decisions, and I am not qualified to make those on my own, nor should I. urllib3 has a great collection of core maintainers and regular contributors whose opinions and work I value, and would like to solicit to help me. As I see it, we need to answer the following questions:

1. Is this worth doing? Do we want to be rid of httplib so badly that we will perform a *massive* codebase rewrite to get rid of it?
2. If so, are we happy with using h11?
    1. If we are, do we depend on it using pip, or vendor it? How does our decision affect Requests?
3. Should this be the beginning of a v2.0 branch that will contain other bits of work from #776?
    1. If it should, how many breakages do we want to roll into one change? Do we want to go all in and leave a *massive* codebase change?
4. What pieces of functionality do we want to retain from the old codebase? In particular, do we want:
    1. To still have `stream()` expose chunk boundaries?
    2. To return `str` header field names and values on Python 3?
    3. To return *different types* for header field names and values on Python 3?
    4. To implement buffered I/O in the low-level primitives?
    5. To maintain the weird two-level Response structure where we have a urllib3 `Response` object that contains a httplib `Response` object within it?
5. How would we want to manage a rollout of this functionality?
6. How do we want to review it?
7. What kind of timeline do we want for releasing it?
8. If we decide to go ahead with a v2.0, should we freeze the current master branch for everything except bug/security fixes until we complete v2.0?

When @shazow originally made me interim lead maintainer, he told me that I had complete authority because he could always undo any change I made that he didn't like. I think this particular change is an exception: I don't think I have any degree of ""lead maintainer"" authority on this. We're going to need @shazow to express some opinions too.

While I'm here, I'd also like to tag a few community members that are particularly likely to be interested in this proposal:

/cc @njsmith @durin42 @dstufft"
1066,Don't retry on timeout if method is not in whitelist (#1059),2016-12-06T20:35:30Z,2016-12-16T13:00:06Z,,,,"This should take care of #1059.

I wasn't sure if accessing the _method_whitelist_ attribute of the Retry object was ok, but as it was not named privately I assumed it would be ok (rather than creating a new method for the check).

Let me know if there's anything I should edit here or if there's any comments.

PS: Apologies for the second (empty) commit, Github was down on the exact moment I pushed the first one."
1065,what is the official relation between urllib3 and upstream python?,2016-12-06T10:33:41Z,,,,,"Given that urllib and urllib2 are part of the core-libs, it doesn't seem unrealistic that at some point a urllib3 will be part of the core-libs.

Please prominently point out what your relation to upstream python is."
1064,Changes for #1063,2016-12-06T08:23:41Z,2016-12-06T09:35:37Z,,,,
1063,Verify that pyOpenSSL is sufficiently new before injecting,2016-12-05T20:04:35Z,2016-12-06T08:22:28Z,,,,"`cryptography` is being checked, but pyOpenSSL is not, which results in issues like https://github.com/kennethreitz/requests/issues/3701

This approach does require doing an allocation of an `X509` object to verify that the expected private attribute is available.

I also modified the tests as the previous attempt to check `successfully_injected` would not work as it was never set to True (when the assertion failed it would jump immediately into the `finally` block). It isn't unsafe to always call `extract_from_urllib3` so now it always does."
1062,"Bug: Content-Disposition Header Lacking ""filename"" When ""filename*"" Is Present",2016-12-01T16:04:07Z,,"Contributor Friendly ‚ô•, Someday",,,"#### Description:
When sending files to a server that does not understand ""filename*"" parameters inside the Content-Disposition header, the server will fail to find the file name because the ""filename"" parameter is not also included.

#### Justification:
According to [RFC 6266](https://tools.ietf.org/html/rfc6266#appendix-D), ideal header generators will include both a ""filename"" and a ""filename*"" parameter whenever the ""filename*"" form is required:
> ‚Äì Include a ""filename*"" parameter where the desired filename cannot be expressed faithfully using the ""filename"" form.  Note that legacy user agents will not process this, and will fall back to using the ""filename"" parameter's content.
>
> ‚Äì When a ""filename*"" parameter is sent, to also generate a ""filename"" parameter as a fallback for user agents that do not support the ""filename*"" form, if possible.  This can be done by substituting characters with US-ASCII sequences (e.g., Unicode character point U+00E4 (LATIN SMALL LETTER A WITH DIARESIS) by ""ae"").  Note that this may not be possible in some locales.
>
> ‚Äì When a ""filename"" parameter is included as a fallback (as per above), ""filename"" should occur first, due to parsing problems in some existing implementations.

#### Why I Care:
This inconsistency caused a many-hour debugging session when trying to discover why file uploads to Google AppEngine blobstore stopped working when the requests library was upgraded (which bundles urllib3 with install). The eventual resolution was to urlencode the file name to a `str` before upload so that the ""filename*"" parameter was not added.

---

Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/39703440-bug-content-disposition-header-lacking-filename-when-filename-is-present?utm_campaign=plugin&utm_content=tracker%2F192525&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F192525&utm_medium=issues&utm_source=github).
"
1061,work around a problem with an unlucky six/future interaction,2016-12-01T15:46:00Z,2016-12-06T09:05:32Z,,,,"see kennethreitz/requests#3742
see obspy/obspy#1599"
1060,No support for client certificate chains. urllib3 1.19.1,2016-11-30T17:37:47Z,,Contributor Friendly ‚ô•,"OpenSSL.SSL.Error, ssl.SSLError, requests.packages.urllib3.exceptions.SSLError, requests.exceptions.SSLError","OpenSSL.SSL.Error: [('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')], ssl.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",), requests.packages.urllib3.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",), requests.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",)","Ran into an issue with client certificates where chain validation is failing.

Orginally reported:
https://github.com/kennethreitz/requests/issues/3732

Traceback:
```
Traceback (most recent call last):
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 417, in wrap_socket
    cnx.do_handshake()
  File ""/apps/lemur/lib/python3.5/site-packages/OpenSSL/SSL.py"", line 1424, in do_handshake
    self._raise_ssl_error(self._ssl, result)
  File ""/apps/lemur/lib/python3.5/site-packages/OpenSSL/SSL.py"", line 1172, in _raise_ssl_error
    _raise_current_error()
  File ""/apps/lemur/lib/python3.5/site-packages/OpenSSL/_util.py"", line 48, in exception_from_error_queue
    raise exception_type(errors)
OpenSSL.SSL.Error: [('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 323, in connect
    ssl_context=context)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/util/ssl_.py"", line 324, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 424, in wrap_socket
    raise ssl.SSLError('bad handshake: %r' % e)
ssl.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/apps/lemur/lib/python3.5/site-packages/requests/adapters.py"", line 423, in send
    timeout=timeout
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 624, in urlopen
    raise SSLError(e)
requests.packages.urllib3.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/apps/lemur/lemur/common/schema.py"", line 147, in decorated_function
    resp = f(*args, **kwargs)
  File ""/apps/lemur/lemur/certificates/views.py"", line 271, in post
    return service.create(**data)
  File ""/apps/lemur/lemur/certificates/service.py"", line 218, in create
    cert_body, private_key, cert_chain = mint(**kwargs)
  File ""/apps/lemur/lemur/certificates/service.py"", line 167, in mint
    cert_body, cert_chain = issuer.create_certificate(csr, kwargs)
  File ""/apps/lemur/lemur-cloudca/lemur_cloudca/plugin.py"", line 332, in create_certificate
    response = self.post(endpoint, cloudca_options)
  File ""/apps/lemur/lemur-cloudca/lemur_cloudca/plugin.py"", line 163, in post
    response = self.session.post(self.url + endpoint, data=dumps(data), timeout=10, verify=self.ca_bundle)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/sessions.py"", line 535, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/sessions.py"", line 488, in request
    resp = self.send(prep, **send_kwargs)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/sessions.py"", line 609, in send
    r = adapter.send(request, **kwargs)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/adapters.py"", line 497, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",)

Traceback (most recent call last):
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py"", line 835, in _validate_conn
    conn.connect()
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/connection.py"", line 323, in connect
    ssl_context=context)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/util/ssl_.py"", line 324, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/apps/lemur/lib/python3.5/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 424, in wrap_socket
    raise ssl.SSLError('bad handshake: %r' % e)
ssl.SSLError: (""bad handshake: Error([('SSL routines', 'SSL3_READ_BYTES', 'tlsv1 alert unknown ca')],)"",)
```
Reproduce with:

```
import requests

class HostNameCheckingAdapter(HTTPAdapter):
    def cert_verify(self, conn, url, verify, cert):
        super(HostNameCheckingAdapter, self).cert_verify(conn, url, verify, cert)
        conn.assert_hostname = False

s = requests.Session()
s.mount('https://', HostNameCheckingAdapter())
s.cert = ('client.crt', 'client.key')

s.get('https://example.com', verify='path-to-ca-bundle')
```

Per @Lukasa comment, modifying the following line resolves the issue:
pyopenssl.py

    self._ctx.use_certificate_file(certfile)

to

    self._ctx.use_certificate_chain_file(certfile)


"
1059,Read timeout causes retry despite HTTP method used,2016-11-29T17:18:05Z,2016-12-16T13:03:51Z,Contributor Friendly ‚ô•,,,"Hello,

We've encountered an issue in which a read timeout will always cause a retry (unless the *read* parameter to the *Retry* class is set to 0), no matter what HTTP method is used to perform the request. Part of the code path that is followed seems to be https://github.com/shazow/urllib3/blob/7002c1347c677b2743244bf36bd05aa3625f57f1/urllib3/connectionpool.py#L642.
To give more context, we're encountering retries on POST requests (which are not indempotent) when a server's response time causes a read timeout.

Is this known behaviour? We were expecting the *method_whitelist* parameter to apply to any type of errors.

Thanks."
1058,Fix tests where no network is available,2016-11-27T18:02:25Z,2016-11-28T12:53:36Z,,,,"This follows PR #1057 

@Lukasa: sorry, GH didn't want to accept non-forward commits (rebased) and automatically closed the PR when deleted the branch.
"
1057,Fix tests where no network is available,2016-11-27T17:10:56Z,2016-11-27T17:58:17Z,,,,This follows PR #1056
1056,Fix tests where no network is available,2016-11-27T11:13:28Z,2016-11-27T17:11:02Z,,,,
1055,Stop trying 'buffering' arg after TypeError,2016-11-27T06:44:38Z,,,,,"If `conn.getresponse(buffering=True)` throws TypeError once,
it's always going to throw it on all future calls.

The new HTTPConnectionPool._getresponse helper method added in
this patch instead replaces itself on the instance with the
underlying ConnectionCls method so all future calls on that
particular pool instance will use that instead of the pool
level helper method.

A runtime sanity check is included to detect the case where
a connection that isn't an instance of ConnectionCls is somehow
accessed through the pool.

Closes #1047 "
1054,tests: Q&D skip network dependant tests when network is unavailable,2016-11-26T13:04:18Z,2016-11-27T11:13:39Z,,,,"A proper fix would be to bring the socket.{gai,}error in
NewConnectionError, but i'm not willing to undertake this refactoring."
1053,tests: Q&D skip network dependant tests when network is unavailable,2016-11-26T12:42:08Z,2016-11-26T12:49:23Z,,,,"A proper fix would be to bring the socket.{gai,}error in
NewConnectionError, but i'm not willing to undertake this refactoring."
1052,dummyserver: raise syn backlog,2016-11-25T19:01:44Z,2016-11-25T21:38:30Z,,,,"If the client thread called connect() by the time the server thread
actually accept()ed connections, the client got a connection refused
error."
1051,Changelog entry for #1039.,2016-11-25T14:18:47Z,2016-11-25T14:55:33Z,,,,
1050,Changes for 1044,2016-11-23T19:27:49Z,2016-11-23T21:20:17Z,,,,
1049,Use DefaultSelector as context manager.,2016-11-22T13:26:02Z,2016-12-23T16:49:52Z,,,,"PR for #1046, uses the selector as a context manager to ensure that it is properly cleaned up."
1048,Check `getresponse` signature on import,2016-11-22T09:08:40Z,2016-11-27T08:48:49Z,,,,"`HTTPConnection.getresponse` only accepts a `buffering`
argument on Python 2.7.

Moving the check to import time means that calls  in
`urllib3.connectionpool` will always use the correct
signature, regardless of Python version."
1047,"""buffering"" arg support is checked on every call to `ConnectionCls.getresponse()` in HTTPConnectionPool",2016-11-22T08:31:27Z,,,,,"On Python 3.5 with urllib3 1.15, I just hit the issue discussed in https://github.com/kennethreitz/requests/issues/1289#issuecomment-31294851 where urllib3 is checking for `buffering` parameter support first, and then falling back to calling `getresponse()` without any arguments, creating a misleading stack trace when the latter call fails.

Thanks to https://github.com/shazow/urllib3/commit/7fa026c37a5326dcaf97cb9c66ecd15df9c1a194 more recent versions of urllib3 won't show the confusing chained traceback when a connection fails.

However, it's also the case that on every current Python version *except* Python 2.7, the first call will always fail (and even on Python 2.7 it's relying on an undocumented parameter, so not all Python implementations will support it).

This problem could be eliminated by checking for the relevant signature information once at import time rather than on every call to `getresponse`:

    def _request_response_buffering():
        try:
            names = HTTPConnection.getresponse.im_func.func_code.co_varnames
        except AttributeError:
            return False # Either Python 3 or not CPython
        return ""buffering"" in names

    if _request_response_buffering():
        def _getresponse(conn):
            return conn.getresponse(buffering=True)
    else:
        _getresponse = HTTPConnection.getresponse

And then calling `_getreponse(conn)` instead of `conn.getresponse()`

If such an approach would be acceptable, then I'd be happy to create a PR for it.
"
1046,Necessary to cleanup DefaultSelector in def _wait_for_io_events() ?,2016-11-22T07:28:56Z,2016-12-23T16:51:52Z,,,,"Hello,

We are hitting the error ""filedescriptor out of range in select()""  when creating too many new connections. We are using urllib3 as a part of requests. In order to get around it we are waiting for https://github.com/shazow/urllib3/pull/1001 to get released. The questions I have are:

1. When will https://github.com/shazow/urllib3/pull/1001 be released as part of a new urllib3 release?
2. I was looking at the patch and was wondering if any cleanup is required for the DefaultSelector() object that is created in the util._wait_for_io_events() function. See https://github.com/shazow/urllib3/blob/master/urllib3/util/wait.py#L22. The epoll selector class does have the close() method where it closes the platform epoll object - but is the close() called from anywhere?

regards,
aniruddha"
1045,Require an up-to-date 'cryptography' module for urllib3 injection,2016-11-21T22:50:36Z,2016-11-23T19:26:14Z,,,,This fixes Issue #1044 for me.
1044,"Old ""cryptography"" package causes crash",2016-11-21T19:47:01Z,2016-11-23T19:27:25Z,,,,"Cross-posting this from kennethreitz/requests#3714 ; folks there seemed to think that this would be best addressed within urllib3:

As of `requests==2.12.1` \[EDIT: and whatever version of urllib3 it embeds\], `requests[security]` depends on `cryptography>=1.3.4`.  That's fine.  However, `requests` (without `[security]`) crashes if `cryptography==1.0.2` is installed.  This is a perfectly valid configuration of packages according to `pip`; as a user, I would expect it to behave as if I had not specified `[security]` (because I didn't specify it).

The specific failure is discussed a bit more here:

http://discuss.flexget.com/t/unhandled-error-in-plugin-configure-series-solved/2900

The crash was resolved, also as described at the above link, by running:

```
pip install --upgrade cryptography
```

Uninstalling `cryptography` would probably have worked fine too.

The crash appears to be caused because `requests` internally tests for `[security]` by importing a thing which imports its bundled `pyopenssl` which imports `cryptography`; if that succeeds, the extra is used.  But with a too-old version of `cryptography`, it imports just fine but it constructs objects that are missing required fields.

I can think of a couple approaches here:

* Catch this failure and print an error message that more-helpfully explains that your `cryptography` package needs updating
* Validate the version/functionality of the `cryptography`package more carefully, and don't enable the `[security]` extra's features if an old version happens to be installed

I'd be happy to post a PR for either approach.  Right now I'm looking for feedback on whether it's worth addressing this (I'd certainly appreciate it) and on what approach you'd prefer."
1043,Support for certificate subject when using subjectAltName=IP Address is broken,2016-11-16T20:43:57Z,2016-11-16T20:54:30Z,,,,"Since I upgraded to Urllib3 v.1.19.1, Some certificate stopped to be valid. I realized that the code does not verify the subject of the certificate anymore if an IPAddress entry is present in the SubjectAltName when using the hostname to query.

Steps to reproduce
---

1. Setup a https host with a certificate with a CommonName in the subject and an IP address in the subjectAltName (without any DNSname)
2. Query the hostname of the host using this certificate.
    
Incorrect Behavior
---
    
The CommonName of the certificate will be ignored and the verification will fail

    [exec] requests.packages.urllib3.connection: ERROR: Certificate did not match expected hostname: $HOSTNAME. Certificate: {'subjectAltName': (('IP Address', '$IP'),), 'subject': ((('commonName', u'*.$DOMAIN'),),), 'notAfter': '$TIMESTAMP'}

My analysis
---

I think the issue was introduced with the [PR#922] which introduce the verification of IP Address in the SubjectAltName. [Following the logic introduced], If a IP Address is specified in the SubjectAltName, the code adds it to the dnsnames list, causing the CommonName of the certificate to be ignored.

I think the intended behavior is to support CommonName presemt in the Subject only if there is no DNSname entries in the SubjectAltName, whether or not there is an IpAddress is present.

[PR#922]: https://github.com/shazow/urllib3/pull/922/files
[Following the logic introduced]: https://github.com/shazow/urllib3/blob/master/urllib3/packages/ssl_match_hostname/_implementation.py#L132-L139"
1042,Add changelog entry for 1.19.1,2016-11-16T10:15:19Z,2016-11-16T13:03:24Z,,,,"I just pushed a 1.19.1, so we need to update the changelog on the master branch to take account of it."
1041,Use time.monotonic if available,2016-11-15T04:03:55Z,2016-11-16T01:21:24Z,,,,"Did the minimum suggested in #694 and started using `time.monotonic` if it's available. Discussion about whether we should vendor any library like `python-monotime` or `monotonic`, but this is a great first step."
1040,Fix Flake8 E305 errors,2016-11-15T00:22:27Z,2016-11-15T09:30:54Z,,,,Fixes #1038. Flake8 3.1.1 now checks E305 which enforces proper spacing after function and class definitions and we had a handful lying around.
1039,Rewind Seekable Body on Retry/Redirect,2016-11-14T21:31:24Z,2016-11-25T14:16:44Z,,,,"This is picking up where #459 left off. There are two commits in the first pass on this PR, one (40a110c) with a bare bones (unsafe) fix for the issue that @joneskoo was hitting, and another (ae7d799) with a fuller port of Requests implementation.

I'm on the fence about whether the second is overkill. If we decide to go down that route though, I still need to add individual testing for `rewind_body`.
"
1038,Flake8 3.1.1 now checking E305,2016-11-14T20:25:17Z,2016-11-15T09:30:54Z,,,,This error is for having less than 2 blank lines after a function or class definition. It affects dummyserver as well as a few other places within urllib3 and is causing other PRs to fail. I can pick up this issue when I get home from work.
1037,docs: Recipe suggestion: Limiting download size,2016-11-14T18:41:50Z,,"Documentation, Help Wanted, Soon",,,"Would be great to get a recipe like this into the docs: http://stackoverflow.com/q/40594817/187878

Also related feedback on current docs:

- https://urllib3.readthedocs.io/en/latest/advanced-usage.html#streaming-and-io makes it seem like it's wise to stream 32 bytes at a time by default. The default example should avoid supplying the amount read per chunk without explaining what that does.
- It's not obvious what happens if we abandon reading a response half-way and release the connection. Is this the idiomatic thing to do (is it even possible to reuse the connection at that point?) or should the connection be explicitly closed?

Some other related ideas: Figuring out the content type by using the Content-Type header and also using the mimetypes library. https://stackoverflow.com/questions/40595634/python-http-cant-get-the-correct-mime-type/

(These are mostly notes-to-self for when I have a chance to go improve our docs once I'm back, but others are very welcome to take it also.)"
1036,Differentiate socks5h from socks5 and socks4a from socks4,2016-11-13T12:28:09Z,2017-01-17T08:14:09Z,,,,"In the proxy string, socks5h:// and socks4a:// mean that the hostname is
to be resolved by the socks server.  socks5:// and socks4:// mean the
hostname is to be resolved locally.

Fix #1035
"
1035,Differentiate socks5h from socks5 and socks4a from socks4 when handling proxy string,2016-11-13T12:27:43Z,2017-01-17T08:14:09Z,,,,"In a proxy string, socks5h:// and socks4a:// mean that the hostname is
resolved by the SOCKS server.  socks5:// and socks4:// mean that the
hostname is resolved locally.  socks4a:// means to use SOCKS4a, which is
an extension of SOCKS4.  Let's make urllib3 honor it."
1034,Changes for #1033,2016-11-10T10:46:45Z,2016-11-10T11:17:45Z,,,,
1033,ConnectionPool apply lowercasing to host,2016-11-10T00:57:39Z,2016-11-10T10:44:32Z,,,,Fixes #1032 by applying `lower()` to all hosts that are given to the `ConnectionPool`.
1032,Unexpected HostChangedError due to downcasing of hostname,2016-11-09T01:58:06Z,2016-11-10T10:44:32Z,,urllib3.exceptions.HostChangedError,"urllib3.exceptions.HostChangedError: HTTPConnectionPool(host='Example.com', port=None): Tried to open a foreign host with url: http://Example.com","On version 1.19

```urllib3.exceptions.HostChangedError``` is thrown when a pool is created for a host with uppercase letters, and then a request is made with url containing this hostname.
```python
from urllib3 import HTTPConnectionPool

pool = HTTPConnectionPool(""Example.com"")
response = pool.request('GET', ""http://Example.com"")
```

Gives:
```
Traceback (most recent call last):
  File ""minimum.py"", line 5, in <module>
    response = pool.request('GET', ""http://Example.com"")
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/usr/local/lib/python3.5/dist-packages/urllib3/connectionpool.py"", line 549, in urlopen
    raise HostChangedError(self, url, retries)
urllib3.exceptions.HostChangedError: HTTPConnectionPool(host='Example.com', port=None): Tried to open a foreign host with url: http://Example.com
```

Is this the expected behavior, or should a ConnectionPools host also be downcased when doing is_same_host() ?"
1031,Changes for #1005 and #1028.,2016-11-08T15:13:00Z,2016-11-09T05:37:07Z,,,,
1030,Streaming File Uploads,2016-11-07T00:11:10Z,2017-04-25T21:50:50Z,,,,"Took the work that was done a long time ago on the branch `filepost-stream` and brought it up to speed with the current API for filepost and fields modules, made it Python 2 and 3 compatible, added a test for memory usage and converted the current tests to use the new filepost iterator.

More tests to exercise each possible type of file-like or streamable object that can be passed into filepost will be created after this type of structure is approved and still wanted in urllib3.

I don't know if this behavior is still desired within urllib3 or as @sigmavirus24 has mentioned using code from requests-toolbelt.

Relevant issues: #51 #624

(CI failed because of random read timeout on Mac OS + Python 3.3)"
1029,Fix broken link for AppEngine warning,2016-11-05T21:49:41Z,2016-11-09T06:49:25Z,,,,Fixes the broken link in the AppEngine warning (Issue #1027)
1028,Update RECENT_DATE to 2016,2016-11-05T21:37:43Z,2016-11-08T15:10:44Z,,,,"This PR resolves #1005 by adding a unit test to allow `RECENT_DATE` to have a maximum of two years of difference between the current date and it's last value.  I also updated `RECENT_DATE` to January 1st, 2016 to refresh the value to a more recent date.

My rationale for these changes are:
- When `RECENT_DATE` was added it was August in 2014 so I figured jumping to 2016 made sense.
- Because this change is coming more than two years after the original `RECENT_DATE` check was added, it should be safe to assume this will not break anyone who's system times fit the original set of rules which would require not more than two-three years difference in system clock time.

The unit test will also make it harder to forget that this value should be updated, hopefully more often than once every two years but I think this is a good compromise, I'm open to suggestions if this should be more/less frequent."
1027,Broken link in AppEnginePlatformWarning,2016-11-04T19:00:04Z,2016-11-15T00:37:14Z,,,,The [AppEnginePlatformWarning](https://github.com/shazow/urllib3/blob/8ca014adde2c005ccee07c150a18d853d27b26a2/urllib3/contrib/appengine.py#L114) link is currently [broken](https://urllib3.readthedocs.io/en/latest/contrib.html). Should be: http://urllib3.readthedocs.io/en/latest/reference/urllib3.contrib.html
1026,CHANGES entry for #1025.,2016-11-04T15:18:50Z,2016-11-04T17:29:55Z,,,,
1025,Correct the import of urljoin for Python 3,2016-11-04T13:56:02Z,2016-11-04T15:17:31Z,,,,"At least in Python 3.5.2, this caused cascading breakage down to requests-toolbelt,
used by vdirsyncer. On Py3, urljoin is at urllib.parse."
1024,Changelog entry for #1001,2016-11-04T10:04:53Z,2016-11-04T10:32:18Z,,,,
1023,Update CHANGES for 1.19,2016-11-03T13:59:15Z,2016-11-03T15:06:47Z,,,,"Alright, let's ship another release. All aboard the release üöÇ!"
1022,CVE-2016-8625,2016-11-02T12:28:42Z,2016-11-02T12:30:04Z,,,,"Recently reported vulnerability because IDNA encoding used by Python is 2003 instead of 2008, can cause DNS lookup to mismatch and connect to the incorrect server potentially without the user noticing.  Does this affect urllib3 at all, and if so who is writing the patch? Link to the Python issue: https://bugs.python.org/issue17305"
1021,Cause a Travis failure if there are warnings in the docs,2016-11-01T22:44:43Z,2016-11-02T10:14:47Z,,,,"I‚Äôve tried to set up a docs job on Travis that will fail if there are problems in the docs build.

There weren‚Äôt any errors already (hooray!) so I‚Äôve introduced a deliberate consistency bug. Hopefully the build will fail!"
1020,question: default ssl configuration difference between linux distros,2016-11-01T10:10:17Z,2016-11-01T10:49:17Z,,,,"Hello gentlemen. I'm kind of stuck on a SSL/TLS issue and would appreciate some feedback from professionals. Here it is:

we have [this test case](https://github.com/docker/docker-py/blob/8bf5be6455c43ba57bdc2a2e48a22ba99afd49ff/tests/unit/ssladapter_test.py#L23) in docker-py which I'm assuming ensures that the client uses safe TLS:

```python
try:
    import requests.packages.urllib3 as urllib3
except ImportError:
    import urllib3

ssl_context = urllib3.util.ssl_.create_urllib3_context()

assert ssl_context.options & OP_NO_SSLv3
assert ssl_context.options & OP_NO_SSLv2
assert not ssl_context.options & OP_NO_TLSv1
```

This works in upstream just fine. But in Fedora this started to fail in rawhide (spin with latest upstream packages). Bear in mind, that we try to [unbundle](http://pkgs.fedoraproject.org/cgit/rpms/python-requests.git/tree/python-requests.spec#n105) as much as possible in Fedora, so requests is using upstream urllib3, not the bundled one.

And that's where my knowledge ends. This is [the test failure](https://github.com/docker/docker-py/issues/1265 ):

```
tests/unit/ssladapter_test.py:27: in test_only_uses_tls
    assert ssl_context.options & OP_NO_SSLv2
E   AssertionError: assert (2181170175L & 0)
E    +  where 2181170175L = <ssl.SSLContext object at 0x3ffa4cbdde8>.options
```

I have these packages installed:
```
compat-openssl10-1.0.2j-5.fc26.x86_64
openssl-libs-1.1.0b-3.fc26.x86_64
openssl-1.1.0b-3.fc26.x86_64
python3-urllib3-1.16-3.fc26.noarch
python2-urllib3-1.16-3.fc26.noarch
```

I can easily reproduce (python 3, same is happening with python 2)

```python
In [1]: import urllib3

In [2]: c = urllib3.util.ssl_.create_urllib3_context()

In [3]: c.options
Out[3]: 2181170175

In [4]: c.options & ssl.OP_NO_SSLv3
Out[4]: 33554432  # good

In [5]: c.options & ssl.OP_NO_SSLv2
Out[5]: 0  # bad

In [6]: c.options & ssl.OP_NO_TLSv1
Out[6]: 0  # good?
```

The question: should we explicitly pass to `create_urllib3_context` that we want TLS?"
1019,Changes for #1017,2016-10-31T13:01:25Z,2016-10-31T13:56:26Z,,,,
1018,Deduplicate Host headers on chunked uploads.,2016-10-31T11:14:26Z,2016-11-01T08:22:46Z,,,,Resolves #1009.
1017,Fix empty filenames in multipart headers,2016-10-31T03:51:10Z,2016-10-31T13:00:01Z,,,,"Fixes #1015.

I'm not too familiar with using Tox for testing, and I got one error when running the test suite, but it was in some unrelated code (SSL) so I'll just submit this for review for the time being."
1016,The ``connection_from_*`` methods now accept pool_kwargs,2016-10-30T15:42:25Z,2017-03-23T17:13:44Z,,,,"This adds the ability to override the connection pool keyword arguments
used when constructing new pools in the `connection_from_*` methods. A
new keyword argument, `pool_kwargs`, is accepted by methods in
`PoolManager`. This is nice because without it, users that want to
change these arguments (when, say, adjusting SSL configurations) need to
make sure they update the keyword arguments and get a new pool in a
thread-safe manner.

This is a somewhat more invasive version of the patch on https://github.com/kennethreitz/requests/issues/3633 since people (reasonably) were surprised to see the instance's `connection_pool_kw` modified. I'm still not completely pleased with this change for a few reasons:
- If I provide some override `pool_kwargs` and a new pool doesn't need to be created (the `pool_kwargs` aren't fields in the pool key), they will be ignored. That seems like it will surprise users.
- A huge number of methods grew a new kwarg that gets passed down the chain of function calls, which is a little sad.
"
1015,Multipart request headers do not work properly for values of empty string,2016-10-30T04:12:07Z,2016-10-31T13:00:01Z,Contributor Friendly ‚ô•,,,"Continuing the discussion from https://github.com/sigmavirus24/requests-toolbelt/issues/162, attempting to create a `RequestField` which is then made multipart via `make_multipart` does not work properly if the filename given is an empty string.

urllib3 test code:

```
from urllib3.fields import RequestField
field = RequestField(name=""somename"", data=""somedata"", filename="""")
field.make_multipart(content_type=""application/octet-stream"")
print(field.headers)
```

Expected output:

```
{'Content-Type': 'application/octet-stream', 'Content-Location': None, 'Content-Disposition': 'form-data; name=""somename""; filename=""""'}
```

Actual output:

```
{'Content-Type': 'application/octet-stream', 'Content-Location': None, 'Content-Disposition': 'form-data; name=""somename""'}
```
## 
"
1014,test TLSv1 instead of SSLv3,2016-10-28T15:55:20Z,2016-10-28T18:05:06Z,,,,"new systems don't have SSLv3 anymore, breaking the tests
"
1013,Introduce IPv6 zone identifier support.,2016-10-27T18:17:42Z,2017-01-13T13:45:54Z,,,,"In addition to stripping IPv6 address literals in connection pools, process IPv6 address literals which might have URI encoded zone identifiers specified in RFC-6874.

See also kennethreitz/requests#1985
"
1012,Introduce IPv6 zone identifier support.,2016-10-27T18:05:49Z,2016-10-27T18:13:26Z,,,,"In addition to stripping IPv6 address literals in connection pools, process IPv6 address literals which might have URI encoded zone identifiers specified in RFC-6874

See also kennethreitz/requests#1985 (this assumes that URL's in requests are RFC-6874 compliant).
"
1011,[contrib/pyopenssl] remove unused ssl_wrap_socket,2016-10-27T15:31:06Z,2016-10-28T07:06:48Z,,,,"Everything uses the SSLContext. As we never exported ssl_wrap_socket
nobody should be using it.
"
1010,Fix PyOpenSSL with OpenSSL 1.1.0 (and CVE-2016-9015),2016-10-27T11:34:18Z,2016-10-27T15:55:26Z,,,,"This PR provides a number of fixes to the PyOpenSSL contrib module to allow that module to work with OpenSSL 1.1.0.

While working on this PR I stumbled across a security vulnerability that was introduced in urllib3 1.17 (by me üò•). Happily, this vulnerability affects only those using the PyOpenSSL contrib module with OpenSSL 1.1.0. Unhappily, it's still a real vulnerability. This fix was backported to 1.18 and shipped in 1.18.1 as commit c32cdbc16a9634fa0f8c829d1270301570158715. It is present in this PR as well but in a more comprehensive form along with a number of other testing fixes. The vulnerability has been given the identifier CVE-2016-9015. A lengthy explanation of the vulnerability can be found on the [oss-security mailing list](http://www.openwall.com/lists/oss-security/2016/10/27/6).

I've coordinated with our downstream repackagers, who are not affected by this vulnerability, and with a representative of the Requests team (in this case, myself), who are also not affected by this vulnerability: all of those constituencies are using urllib3 1.16. I have also coordinated with the OpenSSL team.

This patch requires a few more things before it can be merged into the master branch. Specifically, it requires enhancement of our Travis environment to test with OpenSSL 1.1.0. Probably this is most easily done on the OS X builders, which can have Homebrew install the newer OpenSSL version so we can link PyOpenSSL against it.
"
1009,Chunked uploads do not deduplicate the Host header field.,2016-10-26T08:05:55Z,2016-11-01T08:22:46Z,Contributor Friendly ‚ô•,,,"When using `urlopen` with a headers dictionary that includes the `Host` header, the `Host` header from the user will be preferred to the automatic one added by httplib. This is not true when calling `urlopen` with `chunked=True`, because `request_chunked` does not have the same deduplication logic as `request` does.

We basically need to duplicate [this logic](https://github.com/python/cpython/blob/master/Lib/http/client.py#L1236-L1241).

Should be a pretty easy patch if anyone wants a quick contribution win. A test to prove that this doesn't work, followed by the patch to make it work. 
## 
"
1008,Add NameResolutionError for failed DNS lookups,2016-10-25T10:30:03Z,,,urllib3.exceptions.NameResolutionError,urllib3.exceptions.NameResolutionError: address 'wowsucherror' not found ([Errno 11001] getaddrinfo failed),"Fixes #1003 for Windows. Test case for:

```
from urllib3.util.connection import create_connection
create_connection(('wowsucherror', 80))
```

On Windows, it gives:

```
Traceback (most recent call last):
  File ""test.py"", line 3, in <module>
    create_connection(('wowsucherror', 80))
  File ""E:\urllib3\urllib3\util\connection.py"", line 81, in create_connection
    raise NameResolutionError(host, e)
urllib3.exceptions.NameResolutionError: address 'wowsucherror' not found ([Errno 11001] getaddrinfo failed)
```
"
1007,Activate codecov for urllib3,2016-10-24T12:55:18Z,,,,,"What do I need to do?

I signed in and added the webhook, anything else?

[![codecov](https://codecov.io/gh/shazow/urllib3/branch/master/graph/badge.svg)](https://codecov.io/gh/shazow/urllib3)
## 
"
1006,Add Windows CI support with AppVeyor,2016-10-24T00:10:59Z,2017-03-31T21:08:12Z,,,,"Recently @Lukasa has added Mac OS support using Travis in order to fully test the new selectors addition.  One of the main projects that requires decent selectors support is the Happy Eyeballs algorithm (RFC 6555) but the algorithm also requires the use of Windows error constants in order to correctly work on all platforms.

I decided to go ahead and use a good template for testing Python with AppVeyor found at: https://github.com/ogrisel/python-appveyor-demo  I've changed it from a primarily Cython-focused project template to a tox-focused project and I've gotten AppVeyor to run all our tests on all our supported Python versions.  This will give us a lot less guessing when implementing or using features that are for platforms that are not available to all maintainers.  I have not tested the codecov section of the code, but I'm sure it's not too far off if at all.  I've already got all tests running. I'm sure this will take a repository owner to fully integrate AppVeyor support.

From my own trial runs, there are a few tests that fail on Windows already, especially in Python 2.6 with many TLS/SSL errors, there is also another test failing on all Python versions.
"
1005,Add unit test for RECENT_DATE,2016-10-21T20:08:58Z,2016-11-08T15:10:44Z,,,,"I've got a feeling this value could be easily forgotten to be updated over time. Could we perhaps add a unit test that will fail if it gets to be too far away from the current date.  It seems like it was last modified in mid 2014 but wasn't updated since then, should this value be updated now?

Link to the blame: https://github.com/shazow/urllib3/blame/master/urllib3/connection.py#L59
## 
"
1004,Try using codecov.,2016-10-20T14:46:29Z,2016-10-21T07:13:14Z,,,,"Ok, let's see if we can get this working.
"
1003,DNS lookup error exception,2016-10-20T10:13:39Z,,Contributor Friendly ‚ô•,,,"I am using `requests` to check domain status for `k8s.io` domains (https://github.com/kubernetes/k8s.io/pull/30) and I can't find a good way to check for DNS lookup error like `ERR_NAME_NOT_RESOLVED` in Chrome. Exceptions output seems platform specific:

```
    Traceback (most recent call last):
      File ""check.py"", line 11, in <module>
        status = requests.head('http://' + site)
      File ""C:\Python27\lib\site-packages\requests\api.py"", line 93, in head
        return request('head', url, **kwargs)
      File ""C:\Python27\lib\site-packages\requests\api.py"", line 53, in request
        return session.request(method=method, url=url, **kwargs)
      File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 468, in request
        resp = self.send(prep, **send_kwargs)
      File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 576, in send
        r = adapter.send(request, **kwargs)
      File ""C:\Python27\lib\site-packages\requests\adapters.py"", line 437, in send
        raise ConnectionError(e, request=request)
    requests.exceptions.ConnectionError: HTTPConnectionPool(host='jenkins.k8s.io', port=80):
        Max retries exceeded with url: / (Caused by
        NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at
        0x029E43B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))
```

So I was redirected here from https://github.com/kennethreitz/requests/issues/3630#issuecomment-255031285 to see if `urllib3` could expose more fine-grained DNSLookupError exception and maybe remove MaxRetryError wrapper.
## 
"
1002,Add support for OS X.,2016-10-20T09:36:28Z,2016-10-20T14:30:27Z,,,,"This is based on the work done for PyCA cryptography.
"
1001,"Create custom Selectors (Kqueue, Epoll, Poll, and Select) Backport",2016-10-19T03:01:50Z,2016-11-03T21:12:09Z,,,,"This PR adds all the `select` methods to a new util module called `wait`. Uses the same ordering used by `selectors.py` in the standard library to determine which selector should be used. Also factored in a way to allow easy patching and testing. Similar to #998 but uses a different way to break up the different methods to wait for I/O events. Also adds tests to exercise the selectors which can probably be extended.
"
1000,Happy Eyeballs Algorithm,2016-10-19T02:18:11Z,,,,,"Hello, this PR is for issue #797 and adds upon the work and takes the recommendations of @Lukasa in order to implement a subset of the Happy Eyeballs Algorithm ([RFC 6555](https://tools.ietf.org/html/rfc6555)) to decrease the performance hit that dual-stack clients feel when connecting to a server with advertised IPv6 addresses but for whatever reason cannot be connected to via IPv6. Without this protocol in place, clients attempting to connect to a service with broken IPv6 will see a long timeout for IPv6 before their system will try to connect via IPv4 which punishes dual-stack users.

The suggestion from @Lukasa was to break this into its own helper which this PR accomplishes. This PR also has support for caching Happy Eyes results for faster use.

Currently, there are two tests failing on my machine (Receiving a ProxyError rather than a ConnectionTimeoutError) but because I don't have very much knowledge about urllib3's code-base I needed some help from other maintainers debugging them. 

I currently have not added any tests to this PR because I want to know the best way to add tests that are for an optional protocol for creating a connection such as this, with suggestions on the best way to test this protocol as well as the `create_connection` method without the Happy Eyes protocol enabled. Criticism welcome.
"
999,Happy Eyes Protocol,2016-10-18T20:53:27Z,2016-10-19T02:15:41Z,,,,"Hello, this PR is for issue #797 and adds upon the work and takes the recommendations of @Lukasa in order to implement a subset of the Happy Eyes Protocol ([RFC 6555](https://tools.ietf.org/html/rfc6555)) to decrease the performance hit that dual-stack clients feel when connecting to a server with advertised IPv6 addresses but for whatever reason cannot be connected to via IPv6. Without this protocol in place, clients attempting to connect to a service with broken IPv6 will see a long timeout for IPv6 before their system will try to connect via IPv4 which punishes dual-stack users.

The suggestion from @Lukasa was to break this into it's own helper which this PR accomplishes. This PR also has support for caching Happy Eyes results for faster use.

Currently there are two tests failing on my machine (Receving a ProxyError rather than a ConnectionTimeoutError) but because I don't have very much knowledge about urllib3's code-base I needed some help from other maintainers debugging them. 

I currently have not added any tests to this PR because I want to know the best way to add tests that are for an optional protocol for creating a connection such as this, with suggestions on the best way to test this protocol as well as the `create_connection` method without the Happy Eyes protocol enabled. Criticism welcome.
"
998,"Add poll, epoll, kevent/kqueue/Replace select.select with util functions",2016-10-18T15:53:45Z,2016-11-02T14:45:17Z,,,,"This is an update to PR #685 where I implement kevent/kqueue as well as poll/epoll. Feel free to refactor or integrate into that PR.
"
997,possible issue with setup.py command,2016-10-16T20:42:29Z,2016-10-17T07:25:18Z,,,,"We're running our build system via nix, and we generate package data automatically trying to build full dependency tree. Recently our build system broke because of urllib3 updates.

we're receving an error:

`error in urllib3 setup command: Invalid environment marker: python_version <= ""2.7""`

I'm not sure if it's our build system or something with a special marker in setup.py

We'll investigate more, however i'm opening this ticket to potentially point out a problem.
"
996,CHANGES for #990,2016-10-13T09:24:10Z,2016-10-13T12:17:41Z,,,,
995,flake8 fixes,2016-10-09T20:12:41Z,2016-10-12T12:18:26Z,,,,"The import order ones are triggered by the flake8-import-order extension, others plain flake8 (3.0.4, pycodestyle 2.0.0)
"
994,Error using IP SANs,2016-10-04T19:47:09Z,2016-10-04T23:54:39Z,,,,"#922 Added support for IP SAN fields.

I've recently been trying to use v1.18 with this fix in it, but I'm encountering the following error when attempting to use an IP SAN.

```
Connection to etcd failed due to SSLError(CertificateError(""hostname '172.18.18.101' doesn't match either of '127.0.0.1', '172.18.18.101', '172.18.18.102', '172.18.18.103'"",),)
```

It appears as though the IP SAN fields are being recognized, but something is going wrong with the matching - I'd expect 172.18.18.101 to match 172.18.18.101!

Here is the relevant bit of the cert I'm using:

```
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage:
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Subject Key Identifier:
                97:D7:BE:F0:63:3B:86:E8:FB:9E:13:85:53:1E:1D:D6:DF:17:48:62
            X509v3 Authority Key Identifier:
                keyid:1A:E6:B1:9A:E2:B7:36:18:24:43:F5:AC:9C:45:61:86:61:EC:E7:32

            X509v3 Subject Alternative Name:
                IP Address:127.0.0.1, IP Address:172.18.18.101, IP Address:172.18.18.102, IP Address:172.18.18.103
```

Anyone have an idea as to why this wouldn't be working?

CC @Lukasa 
"
993,Transport Security Manager HSTS support,2016-10-01T00:47:49Z,,,,,"@Lukasa please review.

This is a follow-on to #972. It establishes HSTS support with an in-memory only store. I'll introduce preload list ingestion (via an optional external package) and a persistent store later.
"
992,String-ify more OpenSSL errors to fix TypeError: __str__ returned non-string (type Error),2016-09-30T23:09:57Z,,,"TypeError, requests.exceptions.SSLError","TypeError: __str__ returned non-string (type Error), requests.exceptions.SSLError: ('bad ca_certs (""/no/file""): Error([(\'system library\', \'fopen\', \'No such file or directory\'), (\'BIO routines\', \'BIO_new_file\', \'no such file\'), (\'x509 certificate routines\', \'X509_load_cert_crl_file\', \'system lib\')],)',)","Addresses the rather cryptic message:

```
  File ""/usr/local/lib/python2.7/site-packages/requests/api.py"", line 70, in get
    return request('get', url, params=params, **kwargs)
  <...>
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 355, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 325, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
```

Which can arise when using the `pyopenssl` contrib module and setting bad verification paths (e.g. file not found, invalid format, etc.).

An easy way to reproduce:

```
$ pip install requests ndg-httpsclient
$ python -c ""import requests; requests.get('https://python.org', verify='/no/file')""
```

I think this is a similar issue as to #556? At least, I started down this rabbit hole from kennethreitz/requests#2524, but #792 wasn't quite enough to address it. With this change, I see the much better (though not quite good) error:

```
requests.exceptions.SSLError: ('bad ca_certs (""/no/file""): Error([(\'system library\', \'fopen\', \'No such file or directory\'), (\'BIO routines\', \'BIO_new_file\', \'no such file\'), (\'x509 certificate routines\', \'X509_load_cert_crl_file\', \'system lib\')],)',)
```

Feedback appreciated!
"
991,CHANGES for #989.,2016-09-30T09:33:54Z,2016-10-03T18:54:27Z,,,,
990,Make HTTPResponse.stream() work with _fp of non-HTTPResponse type (eg StringIO),2016-09-29T13:38:32Z,2016-10-13T09:20:37Z,,,,"Previously urllib's HTTPResponse would work correctly with any file like object. The 3rd party project requests_mock takes advantage of this by passing in a StringIO instance as the body (see: http://git.openstack.org/cgit/openstack/requests-mock/tree/requests_mock/response.py#n166 ). With the addition of chunked streaming in https://github.com/shazow/urllib3/commit/f21c2a2b73e4256ba2787f8470dbee6872987d2d stream() now assumes for chunked responses that if body is file-like then it is httplib.HTTPResponse-like breaking requests_mock.

This patch adds a guard in stream() to fall back to the old behaviour (delegate to the underlying file-like object as for unchunked responses) and another one to read_chunked() (fail early with an explicit error message).

Please let me know if there's any improvements I should make to get this merged or if you think this fix belongs elsewhere.
"
989,Disallow superscripts on digit checks,2016-09-29T10:41:09Z,2016-09-30T09:31:54Z,,,,"`isdigit()` lets other things besides 0-9 to pass, for example ¬≤.

An alternative to the fixes here would be to use e.g. `re.match(r""^[0-9]+$"", ...)`, but that's slower.

With this change to `parse_retry_after`, a slight behavioral change is introduced: it starts to accept values surrounded by whitespace (not sure if they can end up here in usual use cases), and prefixed by `+` or `-`. Previously negative values raised `InvalidHeader`, now they get truncated to 0. Previously + prefixed values raised `InvalidHeader`, now they are accepted and the + ignored. If you feel these are issues that should be kept as they were, let me know and I'll adjust that change.
"
988,Can't install urllib3 1.18 with easy_install and setuptools 17.0 (py2.7),2016-09-29T09:58:33Z,2016-09-29T10:03:20Z,,,,"The following error occurs:

> error: Setup script exited with error in urllib3 setup command: Invalid environment marker: python_version <= ""2.7""

Steps to reproduce:
- `virtualenv2 /tmp/env`
- `source /tmp/env/bin/activate`
- `easy_install setuptools==17.0`
- `easy_install urllib3==1.18`

I'm using [Buildout](http://www.buildout.org/en/latest/) which is using `easy_install` instead of `pip`, and build of my project is broken with urllib 1.18.

Additional notes:
- `pip` installs `urllib3==1.18` successfully.
- `easy_install` from `setuptools==18.0` installs `urllib3==1.18` successfully.
- `easy_install` from `setuptools==17.0` installs `urllib3==1.16` successfully.

I think that `urllib3` should require `setuptools>=18.0` or should work with `setuptools==17.0`.
"
987,Remove markers from setup.py,2016-09-27T10:15:33Z,2016-09-27T10:26:45Z,,,,"Fixes #986.
"
986,"urllib3 fails to install on centos7 due to old setuptools not supporting <=, < environment markers.",2016-09-27T04:38:13Z,2016-09-27T10:26:45Z,,,,"Current urllib3 fails to install on centos7. This bug was most likely introduced after https://github.com/shazow/urllib3/commit/9f5454eac808a105307b2d363c99ce97e5109821.

centos7 ships a very old version of setuptools (0.9.8) which does not support `<=` as an environment marker. See https://github.com/pypa/setuptools/issues/380.

```
$ python --version
Python 2.7.5

$ rpm -qa python-setuptools
python-setuptools-0.9.8-4.el7.noarch

$ lsb_release -a
...
Description:    CentOS Linux release 7.2.1511 (Core) 
Release:    7.2.1511

$ virtualenv venv
...

$ venv/bin/pip install urllib3
Downloading/unpacking urllib3
  Downloading urllib3-1.18.tar.gz (183kB): 183kB downloaded
  Running setup.py egg_info for package urllib3
    error in urllib3 setup command: Invalid environment marker: python_version <= ""2.7""
    Complete output from command python setup.py egg_info:
    error in urllib3 setup command: Invalid environment marker: python_version <= ""2.7""

----------------------------------------
Cleaning up...
Command python setup.py egg_info failed with error code 1 in /home/rene/src/venv/build/urllib3
Storing complete log in /home/rene/.pip/pip.log
```

Installing https://github.com/shazow/urllib3/commit/f620d997134708b09560ca5797aa79a59a2ef4c0  (commit before 9f5454eac808a105307b2d363c99ce97e5109821) works fine.

```
$ venv/bin/pip install git+git://github.com/shazow/urllib3.git@f620d997134708b09560ca5797aa79a59a2ef4c0
...
Successfully installed urllib3
Cleaning up...
```

But 9f5454eac808a105307b2d363c99ce97e5109821 fails.

```
$ venv/bin/pip install git+git://github.com/shazow/urllib3.git@9f5454eac808a105307b2d363c99ce97e5109821
Downloading/unpacking git+git://github.com/shazow/urllib3.git@9f5454eac808a105307b2d363c99ce97e5109821
  Cloning git://github.com/shazow/urllib3.git (to 9f5454eac808a105307b2d363c99ce97e5109821) to /tmp/pip-lnVDAG-build
  Could not find a tag or branch '9f5454eac808a105307b2d363c99ce97e5109821', assuming commit.
  Running setup.py egg_info for package from git+git://github.com/shazow/urllib3.git@9f5454eac808a105307b2d363c99ce97e5109821
    error in urllib3 setup command: Invalid environment marker: python_version < ""3.3""
    Complete output from command python setup.py egg_info:
    error in urllib3 setup command: Invalid environment marker: python_version < ""3.3""

----------------------------------------
Cleaning up...
Command python setup.py egg_info failed with error code 1 in /tmp/pip-lnVDAG-build
Storing complete log in /home/rene/.pip/pip.log
```

urllib3 1.17 setup.py does not ship with < or <= markers so my workaround right now is to install urllib3==1.17.
"
985,CHANGES for #955,2016-09-26T19:27:34Z,2016-09-26T20:16:04Z,,,,
984,Retry classes should be able to modify the request headers,2016-09-26T19:23:35Z,,,,,"It would be useful for `Retry` subclasses to be able to modify the request headers between retries.

This came up in a discussion over at https://github.com/google/oauth2client/pull/659. Essentially, we'd love to be able to write a Retry class that detects 401 errors, refreshes the credentials, and then updates the headers. This would reduce the code in our `urlopen` implementation, but wouldn't completely obviate the need for a custom HTTP class (for that, we'd need some way to inject headers before requests).
## 
"
983,Update changes for 1.18,2016-09-26T08:30:44Z,2016-09-26T08:49:21Z,,,,"I want to prepare and ship a release today.
"
982,Changes for #979,2016-09-21T14:29:31Z,2016-09-21T15:43:57Z,,,,"Missed this.
"
981,InvalidCodepoint exception with wildcard certificate name ? [Wildcard/CN=*.],2016-09-20T22:08:14Z,2016-09-21T12:28:05Z,,elasticsearch.exceptions.ConnectionError,elasticsearch.exceptions.ConnectionError: ConnectionError(Codepoint U+002A at position 1 of u'*' not allowed) caused by: InvalidCodepoint(Codepoint U+002A at position 1 of u'*' not allowed),"# Error

```
elasticsearch.exceptions.ConnectionError: ConnectionError(Codepoint U+002A at position 1 of u'*' not allowed) caused by: InvalidCodepoint(Codepoint U+002A at position 1 of u'*' not allowed)
```
# Overview

I'm seeing an `InvalidCodepoint` exception connecting to my elasticsearch server via ssl after upgrading to urllib3==1.17.  This worked correctly with urllib3=1.16.  

What can I do to help debug this issue?  Can I turn up logging? ~~Is this a problem with how elasticsearch is calling urllib3?~~  I've simplified with a direct urllib3 example.
## Lemma 1.

In 1.17 urllib3 switched to using idna.
## Lemma 2

The host has a wildcard certificate valid for `*.prod.example.com` 
## Hypothesis

I assume the wildcard certificate is somehow related to the `u'*'` in the invalid code point error provided from idna.
# Reproduction Steps
## Failing elasticsearch script

urllib3_test.py:

```
#!/usr/bin/env python

import certifi
from elasticsearch import Elasticsearch
import urllib3.contrib.pyopenssl

urllib3.contrib.pyopenssl.inject_into_urllib3()

host = u""rdb-test-andrew-01.prod.example.com""
es_client=Elasticsearch(host,
                        use_ssl=True,
                        verify_certs=True,
                        ca_certs=certifi.where())
es_client.info()
```
## Recreate environment

```
virtualenv env-test && . env-test/bin/activate
pip install elasticsearch==2.4.0 urllib3==1.17 certifi==2016.8.31 
pip install urllib3[secure]

$ ./urllib3_test.py
Traceback (most recent call last):
  File ""./urllib3_test.py"", line 14, in <module>
    es_client.info()
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/elasticsearch/client/utils.py"", line 69, in _wrapped
    return func(*args, params=params, **kwargs)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/elasticsearch/client/__init__.py"", line 220, in info
    return self.transport.perform_request('GET', '/', params=params)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/elasticsearch/transport.py"", line 327, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/elasticsearch/connection/http_urllib3.py"", line 105, in perform_request
    raise ConnectionError('N/A', str(e), e)
elasticsearch.exceptions.ConnectionError: ConnectionError(Codepoint U+002A at position 1 of u'*' not allowed) caused by: InvalidCodepoint(Codepoint U+002A at position 1 of u'*' not allowed)
```
## Installed versions:

pip freeze

```
argparse==1.2.1
certifi==2016.08.31
cffi==1.8.3
cryptography==1.5
distribute==0.6.24
elasticsearch==2.4.0
enum34==1.1.6
idna==2.1
ipaddress==1.0.17
pyOpenSSL==16.1.0
pyasn1==0.1.9
pycparser==2.14
six==1.10.0
urllib3==1.17
wsgiref==0.1.2
```
# Working example in urllib3==1.16

```
% pip install urllib3==1.16
% pip install pyopenssl ndg-httpsclient pyasn1

% ./urllib3_test.py
# success! no error

% pip freeze
argparse==1.2.1
certifi==2016.08.31
cffi==1.8.3
cryptography==1.5
distribute==0.6.24
elasticsearch==2.4.0
enum34==1.1.6
idna==2.1
ipaddress==1.0.17
ndg-httpsclient==0.4.2
pyOpenSSL==16.1.0
pyasn1==0.1.9
pycparser==2.14
six==1.10.0
urllib3==1.16
wsgiref==0.1.2
```
# Simplified test

```
#!/usr/bin/env python

import certifi
import urllib3
import urllib3.contrib.pyopenssl

urllib3.contrib.pyopenssl.inject_into_urllib3()

url = u""https://rdb-test-andrew-01.prod.example.com:9200""
connection_pool = urllib3.PoolManager(
        cert_reqs='CERT_REQUIRED',
        ca_certs=certifi.where())
resp = connection_pool.request('GET', url)
print resp.data
```

```
  File ""./urllib3_test_direct.py"", line 13, in <module>
    resp = connection_pool.request('GET', url)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/request.py"", line 66, in request
    **urlopen_kw)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/request.py"", line 87, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 244, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 594, in urlopen
    chunked=chunked)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 350, in _make_request
    self._validate_conn(conn)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 833, in _validate_conn
    conn.connect()
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/connection.py"", line 324, in connect
    cert = self.sock.getpeercert()
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py"", line 312, in getpeercert
    'subjectAltName': get_subj_alt_name(x509)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py"", line 185, in get_subj_alt_name
    for name in ext.get_values_for_type(x509.DNSName)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py"", line 141, in _dnsname_to_stdlib
    name = idna.encode(name)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/idna/core.py"", line 355, in encode
    result.append(alabel(label))
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/idna/core.py"", line 276, in alabel
    check_label(label)
  File ""/mnt/my-candidates/env-test/local/lib/python2.7/site-packages/idna/core.py"", line 253, in check_label
    raise InvalidCodepoint('Codepoint {0} at position {1} of {2} not allowed'.format(_unot(cp_value), pos+1, repr(label)))
idna.core.InvalidCodepoint: Codepoint U+002A at position 1 of u'*' not allowed
```
## Certificate Info

```

---
Certificate chain
 0 s:/OU=Domain Control Validated/OU=PositiveSSL Wildcard/CN=*.prod.ziprecruiter.com
   i:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO RSA Domain Validation Secure Server CA
 1 s:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO RSA Domain Validation Secure Server CA
   i:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO RSA Certification Authority
 2 s:/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO RSA Certification Authority
   i:/C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root
```

```
subject=/OU=Domain Control Validated/OU=PositiveSSL Wildcard/CN=*.prod.example.com
issuer=/C=GB/ST=Greater Manchester/L=Salford/O=COMODO CA Limited/CN=COMODO RSA Domain Validation Secure Server CA
```
# ObNote

replaced our real commercial domain name with example.com.  
"
980,Move to a more complex bit of idna handling.,2016-09-18T07:33:46Z,2016-09-20T13:42:15Z,,,,"This resolves #979, according to @reaperhulk. I should aim to write a test for this!
"
979,Usage of cryptography + idna breaks on wildcard certs,2016-09-16T12:31:59Z,2016-09-20T13:42:15Z,,,,"The changes in #930 seem to have broken with some wildcard certificates: https://stackoverflow.com/questions/39521147/why-is-urllib3-idna-complaining-about-a-wildcard-in-an-x509-cert-how-do-i-fix-i

I haven't attempted to reproduce this, but I thought recording it here would be of use.
"
978,HTTPResponse.closed consistency,2016-09-12T20:48:08Z,2016-09-13T09:04:06Z,,,,"Right now `urllib3.response.HTTPResponse.closed` returns different values based on Python version. This addresses the issue by preferring the more specific `isclosed` to `io`'s `closed` attr. See #977 for more details.
"
977,HTTPResponse.closed consistency,2016-09-11T16:19:07Z,2016-09-13T09:04:16Z,,,,"I spent a while yesterday trying to figure out why responses were hitting logic requiring the file to be closed while still evaluating to `closed=False`. There's currently an inconsistency in how urllib3 handles closed responses and part of that is due to #929.

First off, I guess we need to clarify what ""closed"" means for the HTTPResponse. Does it mean the content has all been read, or that we've closed the file pointer? Perhaps it's both and we should be marking the response closed when all the content is read?
## Background

Right now, we have two separate variables `isclosed` (Python 2&3) and `closed` (Python 3) on Python's `HTTPResponse` object. They both behave differently in what determines their boolean value. `isclosed` is defined by the `HTTPResponse`, whether that be in `httplib` (Python 2) or `http.client` (Python 3), while `closed` is defined by the `io` module that `http.client.HTTPResponse` inherits from in Python 3.

`isclosed` checks if the file pointer's content is `None` while the `io` module's `closed` attr only evaluates to True when set. This means we can read all of the bytes off the HTTPResponse object but still have an ""open"" file pointer with a length of 0 when using `io`'s `closed` attr. This won't change until `close()` is called on the file pointer,  but since Python 2 doesn't have a `closed` attr it drops through the first check in urllib3's `closed` method an evaluates as currently expected. Perhaps it's the Python 2 logic that's flawed though, and we shouldn't be leaving cleanup to the GC (assuming the response is ever dereferenced).

I put together 69018f7 to address the issue but didn't open the PR immediately because of the GC question and a side effect with urllib3's `close` method. Moving `isclosed` to a higher preference than `closed` causes `close` to no longer actually close the file pointer as it previously did in Python3. We can obviously implement something to address this but I wanted to get another pair of eyes on this before moving forward.
## Example

This is a quick demo for Python 3 with the current master branch, as is the test included in the PR:

``` python
from urllib3.connectionpool import HTTPConnectionPool
from urllib3.util.response import is_fp_closed

conn = HTTPConnectionPool('stackoverflow.com', maxsize=10)
get_response = conn.request('GET', url='/', preload_content=False)
data = [chunk for chunk in get_response.stream(1024)]
print(is_fp_closed(get_response._fp), get_response.closed)
```
"
976,Fix GAE_PYTHONPATH error in Makefile,2016-09-10T22:19:47Z,2016-09-11T09:09:13Z,,,,"s/\s\s\s\s/\t/
"
975,Error if GAE_PYTHONPATH is not set when running make test-gae,2016-09-10T16:35:19Z,2016-09-10T16:38:51Z,,,,"Fixes #974
"
974,"`make test-gae` failed with ""ImportError: No module named dev_appserver""",2016-09-10T09:02:14Z,2016-09-10T16:38:51Z,,ImportError,ImportError: No module named dev_appserver,"Here is the error:

```
$ source venv/bin/activate 
$ make test-gae
...
GLOB sdist-make: /home/nori/src/urllib3/setup.py
gae inst-nodeps: /home/nori/src/urllib3/.tox/dist/urllib3-dev.zip
gae installed: backports.ssl-match-hostname==3.5.0.1,certifi==2016.8.31,coverage==3.7.1,funcsigs==1.0.2,mock==1.3.0,nose==1.3.7,nose-exclude==0.4.1,NoseGAE==0.5.7,pbr==1.10.0,pkginfo==1.3.2,pluggy==0.3.1,py==1.4.31,PySocks==1.5.6,requests==2.11.1,six==1.10.0,tornado==4.2.1,tox==2.1.1,twine==1.5.0,urllib3===dev,virtualenv==15.0.3
gae runtests: PYTHONHASHSEED='1616590051'
gae runtests: commands[0] | nosetests -c /home/nori/src/urllib3/test/appengine/nose.cfg test/appengine
Traceback (most recent call last):
  File "".tox/gae/bin/nosetests"", line 11, in <module>
    sys.exit(run_exit())
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nose/core.py"", line 121, in __init__
    **extra_args)
  File ""/usr/lib64/python2.7/unittest/main.py"", line 94, in __init__
    self.parseArgs(argv)
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nose/core.py"", line 145, in parseArgs
    self.config.configure(argv, doc=self.usage())
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nose/config.py"", line 346, in configure
    self.plugins.configure(options, self)
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 284, in configure
    cfg(options, config)
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 99, in __call__
    return self.call(*arg, **kw)
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 167, in simple
    result = meth(*arg, **kw)
  File ""/home/nori/src/urllib3/.tox/gae/lib/python2.7/site-packages/nosegae.py"", line 91, in configure
    import dev_appserver
ImportError: No module named dev_appserver
ERROR: InvocationError: '/home/nori/src/urllib3/.tox/gae/bin/nosetests -c /home/nori/src/urllib3/test/appengine/nose.cfg test/appengine'
__________________________________________________________________________________________________________________________ summary ___________________________________________________________________________________________________________________________
ERROR:   gae: commands failed
```
"
973,fixing incorrect message for IncompleteRead,2016-09-10T05:01:27Z,2016-09-10T15:39:39Z,,,,"I missed this during one of the final reviews when we changed urllib3's `IncompleteRead` from using `HTTPError` to httplib's `IncompleteRead` constructor. This is currently producing an incorrect length based on the `message` length rather than the ints we're passing in.

I updated the test to catch this and overwrote httplib's `__repr__` to create the correct message.
"
972,RFC: Transport Security Manager,2016-09-08T17:29:01Z,2016-09-16T16:29:52Z,,,,"@Lukasa, can you please take a look at this and tell me what you think?

In the process of reviewing work from #607 and #608, and in light of comments in #970 about the complexity of the code, I've decided to try breaking down the work into smaller chunks.

Here is the first proposed chunk: two new stub classes, TransportSecurityManager and TransportSecurityStore, and minimal hooks for them.

If you agree with the layout/architecture of this, I can get it documented and further stubbed out, but the intent would be to first merge a stub that doesn't do any actual HSTS/HPKP, and then have a series of PRs to review/merge components that implement it.
"
971,Fix a typo in the user guide documentation,2016-09-08T13:54:26Z,2016-09-08T15:17:23Z,,,,
970,WIP HSTS-HPKP support,2016-09-06T23:55:00Z,2016-09-07T16:14:23Z,,,,"**Do not merge yet**

This is a combination and update of #607 and #608. The two features are closely related, should share the same persistence interface and preload list functionality, and should have consistent interfaces/behaviors/naming. Here is the work that remains as per discussions in #607 and #608:
- [ ] Persistence interface, possibly based on dogpile.cache - this should probably hash hostnames ssh-style, for privacy
- [ ] Preload list interface and optional dependency
- [ ] Move hpkp out of util as per prior comments
- [ ] Close read of https://tools.ietf.org/html/rfc6797 and https://tools.ietf.org/html/rfc7469
- [ ] RFC conformance tests
- [ ] Thread safety / locks around writers
- [ ] Write the docs
"
969,"Don't forget setup.cfg, the monster.",2016-09-06T14:30:01Z,2016-09-06T14:39:35Z,,,,"We missed this in #964.
"
968,Update changes for 1.17,2016-09-06T09:47:35Z,2016-09-06T14:22:39Z,,,,"Should be the final step before 1.17 releases.
"
967,Lower log levels from connectionpool.,2016-09-05T16:14:33Z,2016-09-05T16:25:17Z,,,,"Supersedes #926. Adds nothing but an updated merge and a changelog entry.
"
966,Extra with statements.,2016-09-05T16:07:17Z,2016-09-05T16:11:27Z,,,,"Updates #934. Thanks to @scop for this work!
"
965,nda --version,2016-09-05T13:06:50Z,2016-09-05T13:07:23Z,,,,
964,PySocks 1.5.7 causes problems with IPv6.,2016-09-05T09:09:55Z,2016-09-06T09:35:34Z,,,,"Just spent a few hours chasing this down, but basically the IPv6 support in PySocks 1.5.7 (which, to be clear, I wrote most of) is a bit busted. There's a fix in PySocks master for this, so we don't need to do any extra work, but we should add a blacklist marker for PySocks 1.5.7.
"
963,Update Travis PyPy testing to 5.4,2016-09-03T06:07:13Z,2016-09-03T09:12:36Z,,,,
962,"It would be great to know the response sent by the server, when raising a ProxyError",2016-08-30T12:44:45Z,2016-08-30T12:45:51Z,,,,"When the proxy returns a 407 (authentication required)
It would be great to know the whole response
Because it could contain a lot of useful information
Like the supported authentication schemes, and the authentication realm
"
961,Urllib3 tries to use socks proxy even when socks proxy is not configured.,2016-08-26T14:57:00Z,2016-08-26T17:25:03Z,,`ValueError,`ValueError: Unable to determine SOCKS version from socks://*****`,"I have not configured any socks proxy in my environment variables (neither .bashrc, nor /etc/environment). I do have http_proxy and https_proxy set up.

Every time I use the requests module, I get an error from urllib3, 
`ValueError: Unable to determine SOCKS version from socks://*****`

Any solution?
"
960,fix encode_multipart_formdata (TypeError: 'float' does not have the buffer interface),2016-08-26T06:42:29Z,2016-09-05T16:03:33Z,,,,"Allow request ""fields"" to have floating point values, otherwise an explicit cast to str is needed while making the request.
"
959,"Allow IP in addition to (IP, port) tuple for source_address argument",2016-08-24T16:43:00Z,2016-08-24T16:48:17Z,,,,"In [`HTTPConnection`](https://github.com/shazow/urllib3/blob/master/urllib3/connection.py#L67) and [`create_connection`](https://github.com/shazow/urllib3/blob/master/urllib3/util/connection.py#L51) it would be better if `source_address` argument accepts IP address directly in addition to (IP, port) tuple.

Something like

``` python
if not isinstance(source_address, tuple):
    source_address = (source_address, 0)
```
"
958,Remove 3DES support.,2016-08-24T14:18:47Z,2016-08-25T12:53:14Z,,,,"Resolves #957.

While I'm here, is there any reason we're still supporting HIGH? That will let 3DES in through the back door in older OpenSSL versions. I'm inclined to want to turn that off too.
"
957,Remove 3DES from cipher suite.,2016-08-24T13:44:57Z,2016-08-25T12:53:14Z,,,,"Well, [3DES is broken now](https://sweet32.info/). This attack is difficult to mount against requests/urllib3, but not impossible, and so we should aim to remove 3DES support in the near future. We don't need to rush out a release for it though: the next release is fine.
"
956,WIP: Handle Redirects in the PoolManager and Normalize URLs during redirects,2016-08-21T17:00:27Z,,,,,"This could probably be two PRs, but they're deeply related in my mind so I'm starting by keeping the pieces of work together. I'll happily pull 2176c26 into its on PR if you want.

That said, I'm starting this WIP now because the previous implementation of Url was _not_ rfc3986 compliant and I'm not sure if we want to keep that behaviour and if people are actually relying on it. Specifically doing

``` py
from urllib3.util.url import parse_url

url = parse_url('google.com:80/foo')
```

Would give you an instance of `Url` where `url.hostname == 'google.com'`. [Section 3 of RFC3986](https://tools.ietf.org/html/rfc3986#section-3) however points out that you can only determine the authority (in which the hostname is contained) if it starts with `//`, e.g., a valid URI would be `//google.com:80` not `google.com:80`. So when `rfc3986.urlparse` sees this input, it (correctly per the RFC) parses that differently than `parse_url` used to.

Presently, we even have tests that assert this behaviour. Is this something anyone can rely on? As I understand it, you can't pass a URL without a scheme and hostname. I'm wondering what we needed this for.

I hate removing tests, but I think these should be removed. Thoughts @Lukasa @haikuginger ?
"
955,retry: Respect Retry-After header,2016-08-20T08:03:43Z,2016-09-26T19:24:36Z,,,,"Sleep for the seconds specifed by Retry-After response header before
retrying the request.
"
954,adding in rfc3986 to encode unicode URIs,2016-08-19T21:24:54Z,2016-08-19T21:46:40Z,,,,"This addresses #952 and adds urlencoding described in RFC3986 to `Url`s when they're created.
- I'd like to test that the URL used for `r` at the end of `test_unicode_location_url` is in fact '/ÂèñÂæó„Åô„Çã' but can't find a way to access the URL for the `Request`. Is there a way for me to check that from the `HTTPresponse` object?
"
953,Skip invalid headers,2016-08-18T20:48:32Z,,,,,"In cases where a header starts with whitespace, and that header does not immediately follow another header, drop that header. This imitates the treatment Python3 gives to these headers natively. Fixes #950.
"
952,urllib3 does not urlencode redirect targets on redirect.,2016-08-18T17:19:10Z,,,,,"I am crawling pages and I'm getting `UnicodeEncodeError`. 

The problematic url is : http://www.efind.co.il/detailed/52133.html
I've isolated my code to this:
`url = ""http://www.efind.co.il/detailed/52133.html""
manager = PoolManager()
response = manager.request('GET', url)`

This url is redirecting to this url: articles.efind.co.il/info/◊ì◊í◊ô-◊†◊ï◊ô-◊ë◊ë◊®◊ô◊õ◊î-◊û◊ê◊û◊®
which is not 'ascii'.

Because this is auto redirect i can't do anything about it. I am quoting the first url and giving it to the urllib, I don't have control over the redirected urls.

Anything i can do?
## 
"
951,ClosedPoolError on crawling,2016-08-18T16:51:54Z,,,,,"I am building a crawler with python3 and urllib3. I am using a PoolManager instance that is used by 15 different threads. While crawling thousands of website i get a lot of ClosedPoolError from different website.

On the documentation - ClosedPoolError:

> ```
> Raised when a request enters a pool after the pool has been closed.
> ```

It appears that the PoolManager instance is trying to use a closed connection.

My code:

```
from urllib3 import PoolManager, util, Retry
from urllib3.exceptions import MaxRetryError

# Instance of PoolManager is started on init
manager = PoolManager(num_pools=15,
                      maxsize=6,
                      timeout=40.0,
                      retries=Retry(connect=2, read=2, redirect=10))

# Every thread execute download by using the pool manager instance
url_to_download = ""**""
headers = util.make_headers(accept_encoding='gzip, deflate',
                            keep_alive=True,
                            user_agent=""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0"")
headers['Accept-Language'] = ""en-US,en;q=0.5""
headers['Accept'] = ""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8""
try:
   response = manager.request('GET',
                                   url_to_download,
                                   preload_content=False,
                                   headers=headers)
except MaxRetryError as ex:
   raise FailedToDownload()
```

How can i make the PoolManager renew the connection and try again?
## 
"
950,Malformed header causes IndexError during header parsing,2016-08-18T16:12:39Z,,,IndexError,IndexError: list index out of range,"This response (note the space and quotes around `Content-type`):

```
$ curl --head https://consumersentinel.gov
HTTP/1.0 200 OK
 ""Content-type"": text/html
pragma: no-cache
retry-after: 600
Strict-Transport-Security: max-age=31536000; includeSubDomains; preload
Server: BigIP
Connection: Keep-Alive
Content-Length: 2835
```

Causes this error in `urllib3`:

```
  File ""/home/eric/.pyenv/versions/2.7.11/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 590, in urlopen
    **response_kw)
  File ""/home/eric/.pyenv/versions/2.7.11/lib/python2.7/site-packages/requests/packages/urllib3/response.py"", line 373, in from_httplib
    headers = HTTPHeaderDict.from_httplib(headers)
  File ""/home/eric/.pyenv/versions/2.7.11/lib/python2.7/site-packages/requests/packages/urllib3/_collections.py"", line 317, in from_httplib
    key, value = headers[-1]
IndexError: list index out of range
```

I'm using `urllib3` via `requests`, but it doesn't look like that's causing the issue.
## 
"
949,Add `enforce_content_length` for responses,2016-08-17T18:56:19Z,2016-08-29T01:28:20Z,,Notes,Notes:,"So here's a pass at #723. This is kind of a weird edge case but it particularly prominent in the default configuration of Requests. Most calls performed by `urllib3` will raise a `IncompleteRead` error from `httplib` when the number of bytes in the body doesn't match the `Content-Length`. 
# The Skinny

`httplib` raises `IncompleteRead`s appropriately everywhere except on incrementally read data. This is the primary way Requests uses `urlopen` with `preload_content=False` and then reading with `iter_content()`. Retrieving data this way hits the flaw in `httplib`. I've added a flag to enable this functionality, so as not to break `stream(amt)` and `read(amt)` calls presently. In the next major release, I would advise the flag being removed to make all `read` operations uniform by default.

Notes:
- My unfamiliarity with the testing harness is definitely showing in `test_strict_content_length` but the test does prove the changes are working correctly. Tornado won't allow you to send uneven data, so this was the only other solution I could come up with. Any suggestions on alternative methods of simulating this problem would be appreciated.
- ~~I implemented `length` as a property to match the attribute nature of `httplib.HTTPResponse.length`. I realize an int that we modify may be preferred to a property, but felt it would be more likely to break if we implement int updates everywhere IO might happen in the code.~~
"
948,AttributeError: 'module' object has no attribute 'CERT_CERT_REQUIRED' if 'ssl.py' in project,2016-08-16T18:41:58Z,2016-08-16T19:13:58Z,,,,"I had a strange error because of the import soup in ssl_.py Reported downstream here: https://github.com/boto/botocore/issues/1007#issuecomment-240184362

@jamesls did the detective work [here](https://github.com/boto/botocore/issues/1007#issuecomment-240184362), but essentially it boils down to that if you have an 'ssl.py' in your project, then this:

```
try:  # Test for SSL features
    import ssl
    from ssl import wrap_socket, CERT_NONE, PROTOCOL_SSLv23
    from ssl import HAS_SNI  # Has SNI?
except ImportError:
    pass
```

will cascade to this:

```
    if isinstance(candidate, str):
        res = getattr(ssl, candidate, None)
        if res is None:
            res = getattr(ssl, 'CERT_' + candidate)
        return res
```

having `res` equal to `CERT_CERT_REQUIRED`, which obviously isn't correct. 

Obviously this isn't a critical problem, but it was pretty cryptic and imagine that there are a fair few projects that use urllib3 which may have an ssl.py who may bump into this, so perhaps you could make this more graceful.
"
947,Add support for ChaCha20.,2016-08-15T13:08:28Z,2016-08-15T15:26:12Z,,,,"ChaCha20 is coming in OpenSSL 1.1.0, and it's generally pretty excellent. OpenSSL kindly ignores requests for ciphers that it doesn't support, so we can unconditionally update our code to request ChaCha20 and expect OpenSSL to just quietly ignore the change when it doesn't know what ChaCha is.

This change has us prefer AES-GCM over ChaCha20. This decision is made mostly because newer processors have AES-NI hardware acceleration instructions, and right now we're not in a good place to query whether or not those instructions are present, so we just assume that they are. However, [CPython issue 27766](https://bugs.python.org/issue27766), in addition to making this change, also provides a private function that could be queried for AES-NI support. We may want to wait until CPython 3.6 drops to see if that function makes it in, because we could use it to be smarter about our cipher string.
"
946,Import more from six,2016-08-12T14:18:12Z,2016-08-15T19:23:31Z,,,,
945,SSPI support for proxy authentication,2016-08-08T11:07:43Z,,,,,"I've created a demo of SSPI NTLM authenticated HTTP proxy support ([link](https://github.com/benjimin/pywebcorp)). Would it be possible to incorporate functionality like this into urllib3? If so, where might be the best place?

The idea is that in windows domains the proxy may require authentication using the same credentials as for the user to log on to their workstation. SSPI is the interface which allows application software to utilise the operating system log-on credentials (e.g. for NTLM authentication). This tends to be more secure and convenient than prompting the user for a password (like basic authentication), and is used automatically by applications like chrome and firefox.
## 
"
944,detection of dropped persistent connection fails if server resets connection,2016-08-04T01:59:25Z,2016-09-05T16:30:05Z,,MaxRetryError,"MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=18888): Max retries exceeded with url: / (Caused by <class 'socket.error'>: [Errno 104] Connection reset by peer)","urllib3 automatically reestablishes a connection if the the persistent connections gets closed by the server (normally with a TCP FIN packet):

```
In [11]: http = urllib3.PoolManager()

In [19]: r = http.request('GET', 'http://127.0.0.1', retries=0)
DEBUG:urllib3.connectionpool:""GET / HTTP/1.1"" 200 177

In [20]: r = http.request('GET', 'http://127.0.0.1', retries=0)
INFO:urllib3.connectionpool:Resetting dropped connection: 127.0.0.1
DEBUG:urllib3.connectionpool:""GET / HTTP/1.1"" 200 177
```

There is a race condition however that leads urllib3 to receive a TCP RST and subsequently raise an exception:
1) we have established persistent connection between urllib3 and a server that already had a few successful requests exchange
2) urllib3 sends a new request
3) the http server decides to expire that connection
4) the kernel of the server received the new request
5) the http server closes the socket
6) the kernel sends out a RST
7) urllib3 raises an exception instead of opening a new connection

The OS of the server sends out a RST (instead of FIN) because there is unread data in the buffer of socket. The data was unread because by the time it was received the http server had already expired the connection, but had not closed the socket yet.

See example here:

```
In [8]: r = http.request('GET', 'http://127.0.0.1:18888', retries=0)
INFO:urllib3.connectionpool:Starting new HTTP connection (1): 127.0.0.1
DEBUG:urllib3.connectionpool:""GET / HTTP/1.1"" 200 3

In [9]: r = http.request('GET', 'http://127.0.0.1:18888', retries=0)
---------------------------------------------------------------------------
MaxRetryError                             Traceback (most recent call last)
<ipython-input-9-e3b5959a51e7> in <module>()
----> 1 r = http.request('GET', 'http://127.0.0.1:18888', retries=0)

/usr/lib/python2.7/dist-packages/urllib3/request.pyc in request(self, method, url, fields, headers, **urlopen_kw)
     72             return self.request_encode_url(method, url, fields=fields,
     73                                            headers=headers,
---> 74                                            **urlopen_kw)
     75         else:
     76             return self.request_encode_body(method, url, fields=fields,

/usr/lib/python2.7/dist-packages/urllib3/request.pyc in request_encode_url(self, method, url, fields, **urlopen_kw)
     85         if fields:
     86             url += '?' + urlencode(fields)
---> 87         return self.urlopen(method, url, **urlopen_kw)
     88 
     89     def request_encode_body(self, method, url, fields=None, headers=None,

/usr/lib/python2.7/dist-packages/urllib3/poolmanager.pyc in urlopen(self, method, url, redirect, **kw)
    153             response = conn.urlopen(method, url, **kw)
    154         else:
--> 155             response = conn.urlopen(method, u.request_uri, **kw)
    156 
    157         redirect_location = redirect and response.get_redirect_location()

/usr/lib/python2.7/dist-packages/urllib3/connectionpool.pyc in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
    544                     raise ConnectionError('Connection failed.', e)
    545 
--> 546                 raise MaxRetryError(self, url, e)
    547 
    548             # Keep track of the error for the retry warning.

MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=18888): Max retries exceeded with url: / (Caused by <class 'socket.error'>: [Errno 104] Connection reset by peer)
```

Traffic capture attached: [rst.pcap.zip](https://github.com/shazow/urllib3/files/400697/rst.pcap.zip)

The desired behavior here would be for urllib3 to detect this dropped connection as well, like for the normal case, and to create a new one.
"
943,Add release note about #941,2016-08-03T11:03:49Z,2016-08-03T11:34:57Z,,,,"I apparently can't push directly to master with these. _shrug_
"
942,Allow PyPy 5.3 to fail,2016-08-01T14:25:05Z,2016-08-01T14:32:03Z,,,,"Since it's addition, our Travis builds have become very unstable and
this job seems to be implicated in most of those builds. Until we can
stabilize this particular build job, let's allow it to fail to avoid
having to continuously retry the build on Travis.
"
941,Use OS default certs when possible,2016-08-01T10:58:03Z,2016-08-03T10:58:39Z,,,,"It seems that urllib3 used to use ssl.create_default_context when possible. Isn't it more convenient to have default certs?
"
940,Finish up new docs,2016-07-29T21:11:54Z,2016-07-30T17:25:08Z,"Documentation, Ready",,,"- Update README.rst :)
- Add some more advanced usage docs.

This resolves #884.

Rendered documentation staged here: http://urllib3.theadora.io/
"
939,Use Travis supplied PyPy 5.3,2016-07-29T09:38:03Z,2016-07-29T09:46:10Z,,,,"Replaces custom PyPy 4.0.1 build with Travis supplied PyPy 5.3,
removing the need for two bash Travis specific scripts.

Improve fetch_gae_sdk.py to simplify the invocation in .travis.yml
"
938,"Use tox-travis for nightly, PyPy 5.3 and PyPy3 5.2",2016-07-29T01:41:52Z,,,,,"Replaces custom PyPy 4.0.1 build with Travis supplied PyPy 5.3,
removing the need for two bash Travis specific scripts.

Adds Travis supplied PyPy3 5.2-alpha and nightly build jobs.

Uses tox-travis to simplify running the testenv only for the
specified Travis environment without needing to set TOXENV
and install specific versions of virtualenv.
"
937,use dunder slots for Url class slots variable,2016-07-26T17:49:01Z,2016-07-26T20:08:13Z,,,,"From the python [`namedtuple`](https://docs.python.org/2.7/library/collections.html#collections.namedtuple) documentation:

> The subclass shown above sets `__slots__` to an empty tuple. This helps keep memory requirements low by preventing the creation of instance dictionaries.

Was this the intent for the `slots` variable in `Url` class also?
"
936,Test with PyPy3 too,2016-07-26T14:57:47Z,2016-09-05T16:04:17Z,,,,
935,"Use LifoQueue, Empty, and Full from six",2016-07-26T14:16:20Z,2016-08-06T16:18:01Z,,,,
934,"Use ""with"" to close more files eagerly and also on error",2016-07-26T13:23:47Z,2016-09-05T16:07:26Z,,,,
933,Ignore only the unused import error,2016-07-26T12:42:06Z,2016-07-26T12:46:37Z,,,,"Instead of waiting for Flake8 3.0.2 tonight, we might as well use Flake8's new feature. =D
"
932,Spelling fixes,2016-07-26T12:28:12Z,2016-07-26T13:08:19Z,,,,
931,The returned code is not complete,2016-07-23T19:38:03Z,2016-07-23T19:40:40Z,,,,"Hello,

I use this code:

```
#!/usr/bin/python3.4
# -*-coding:Utf-8 -*
import certifi
import urllib3
url_microsoft = 'https://support.microsoft.com/fr-fr/help/17780/featured-wallpapers'
http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
conn = http.request('GET', url_microsoft, headers={'User-agent':'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.16 Safari/537.36','Cookie':'cookie_name=cookie_value'})
print(conn.data)
```

The returned code is not complete on this web page.

For example the tag title is empty:
`<title` `ng-bind=""title""></title>`

If i use inspector in firebug the tag title is:
`<title` class=""ng-binding"" ng-bind=""title"">S√©lection de `papiers&nbsp;peints</title>`

All text is empty on this web page.

Is it an issue?
"
930,Remove extra dependencies for PyOpenSSL.,2016-07-19T09:44:16Z,2016-07-19T14:05:51Z,,,,"Currently to use the pyopenssl contrib module we need to install three dependencies: pyopenssl, ndg-httpsclient, and pyasn1. The latter two dependencies exist _only_ to give us the ability to handle the Subject Alternative Name X.509 extension, and are otherwise entirely unrequired.

This PR changes that logic to use cryptography instead. Because PyOpenSSL already depends on cryptography, this has the effect of reducing our actual dependency graph. We still, at a top level, have three dependencies (annoyingly we need `idna`, blame @reaperhulk), but those three top-level dependencies are _already present_ in our dependency graph transitively (`pyopenssl` requires `cryptography` requires `idna`), so this has the effect of reducing the number of packages we rely on directly. We also have much better ties to the maintainers of those packages than we do to the maintainers of ndg-httpsclient and pyasn1.
"
929,fixing infinite loop when stream(None) called,2016-07-17T18:09:30Z,2016-07-19T14:55:34Z,,,,"Fixing the issue discussed in #928. Added in test from @Lukasa and reworked `is_fp_closed` to prefer `isclosed` first if available.
"
928,steam(None) can result in an infinite loop,2016-07-17T17:48:41Z,2016-07-19T15:43:55Z,,,,"Currently calling `stream(None)` can result in an `HTTPResponse` trying to read a response forever. 

This occurs [here](https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L361-L365) because `is_fp_closed` won't evaluate to `True` in certain cases. The below test provided by @Lukasa shows the issue in our current implementation of `is_fp_closed`. Currently, in Python 3,  `_close_conn` does not call `super().close()`, which means that the `closed` attribute will never be set. We can fix this by checking `isclosed()` first, if it exists, which will appropriately evaluate to True when the contents of the stream have been completely read.

See related discussion in [kennethreitz/requests#3421](https://github.com/kennethreitz/requests/issues/3421)

``` python
class TestStream(SocketDummyServerTestCase):
    def test_stream_none_unchunked_response_does_not_hang(self):
        done_event = Event()

        def socket_handler(listener):
            sock = listener.accept()[0]

            buf = b''
            while not buf.endswith(b'\r\n\r\n'):
                buf += sock.recv(65536)

            sock.send(
                b'HTTP/1.1 200 OK\r\n'
                b'Content-Length: 12\r\n'
                b'Content-type: text/plain\r\n'
                b'\r\n'
                b'hello, world'
            )
            done_event.wait(5)
            sock.close()

        self._start_server(socket_handler)
        pool = HTTPConnectionPool(self.host, self.port, retries=False)
        r = pool.request('GET', '/', timeout=1, preload_content=False)

        # Stream should read to the end.
        self.assertEqual([b'hello, world'], list(r.stream(None)))

        done_event.set()
```
"
927,Fixed typos,2016-07-17T09:17:15Z,2016-07-17T14:21:43Z,,,,
926,Update connectionpool.py,2016-07-15T13:34:02Z,2016-09-05T16:16:51Z,,,,"Update debug print to proper DEBUG log level
"
925,DNS Caching,2016-07-15T09:31:24Z,2016-07-15T10:20:44Z,,,,"Since urllib3 uses [socket.getaddrinfo](https://docs.python.org/3/library/socket.html#socket.getaddrinfo) to convert a host string into an IP address [1] does that mean we can cache DNS lookups somehow in urllib3?

[1] [urllib3/util/connection.py](https://github.com/shazow/urllib3/blob/e4e49128ac68e4b56e20482cd2bef1700ff30fd9/urllib3/util/connection.py#L75)
"
924,adding in exception for booleans and zero values in timeouts,2016-07-14T21:53:58Z,2016-07-16T14:51:07Z,,,,"This should fix the issue addressed in kennethreitz/requests#2699 where boolean values passed to the `timeout` parameter are being evaluated at 1.0 and 0.0. This also addresses the issue of `timeout=0` opening a non-blocking socket.
"
923,Content-Length incorrect?,2016-07-13T14:01:50Z,2016-07-13T14:04:58Z,,,,
922,Add support for IP address SAN fields.,2016-07-11T09:18:46Z,2016-09-12T09:11:42Z,,,,"Resolves #258.

This is a complex and substantial change, and I'd like a lot of feedback from the other reviewers, as well as @shazow. There are several possible options for supporting this, and this PR is the most radical of the set. However, we may not want to commit to going this far: several of the partial solutions may provide enough value to be worthwhile.

The effect of this pull request is to ensure that, on all Python versions, IP addresses in the subject alternative name field of a TLS certificate can be used to validate the certificate. This is a stark improvement over the status quo, where this only works on Python 3.5 out-of-the-box. However, to achieve it we need to make the following changes:
- backport a newer copy of match_hostname from Python 3.5
- backport and vendor the ipaddress module
- adjust the backported copy of match_hostname to use our vendored copy of ipaddress if it can't find a proper one
- prefer our backported copy of match_hostname to the versions available in Python 2.7.9 through Python 3.4

This is pretty sweeping: in general we've aimed to use as much of the infrastructure that was present in Python as we could, but this change removes that again. It also means that we have not just backported modules but, in the case of match_hostname, are now carrying our patches to it (albeit [only a small one](https://github.com/shazow/urllib3/commit/704fcac89cb0026a290228013ccedbe8795c3f97)).

We could also do the following, less extensive things. In order from smallest to largest:
1. Change our logic to prefer the external match_hostname module from PyPI over the installed version on Python < 3.5, and then add `backports.ssl_match_hostname` and `ipaddress` to the `secure` extra.
2. Do 1, but also do a backport of the newer `match_hostname`. Don't backport `ipaddress`. This means that we still need people to install `ipaddress` to get the IP SANs, but means we don't have to vendor that 2k LoC module.
3. Do what I've done here.

I'm very much interested in what people think here. The version I've done is the most complete and has the effect of ""just working"" in all cases, but is also a really substantial change. I'd be interested in seeing people's opinions as to which of these approaches they like best.

One other note: because we're vendoring new packages, we probably need to alert any downstream Linux distributors because they may want to pull these packages out and make them hard dependencies instead. Do we know who our downstream repackagers are for urllib3?

/cc @sigmavirus24 @haikuginger @jonparrott @paultiplady @fasaxc
"
921,"Cannot match IP hostnames in Python versions >= 3.2, < 3.5",2016-07-09T20:33:58Z,2016-07-09T21:01:23Z,,SSLError,"SSLError: hostname '10.3.240.1' doesn't match either of 'kubernetes', 'kubernetes.default', 'kubernetes.default.svc', 'kubernetes.default.svc.cluster.local'","Possibly related to https://github.com/shazow/urllib3/issues/258, but I'm not sure this is a dupe.

When using urllib3 to connect to https endpoints with certs signed for an IP address (instead of a hostname), urllib throws an error like this:

```
SSLError: hostname '10.3.240.1' doesn't match either of 'kubernetes', 'kubernetes.default', 'kubernetes.default.svc', 'kubernetes.default.svc.cluster.local'
```

The problem appears to be that https://github.com/shazow/urllib3/blob/master/urllib3/packages/ssl_match_hostname/__init__.py uses the standard library implementation of `match_hostname` for Python versions >= 3.2, but that function only gained support for IP subjects in 3.5.

The fallback in that file is to use `backports.ssl_match_hostname`, which _does_ support IP subjects, as it backports 3.5's implementation.

Any chance we can fix urllib3 to use the backports version for Python versions < 3.5? Or perhaps just to preferentially take the backports version over the standard library version if backports is available? 

This issue is causing problems in downstream libraries, e.g. https://github.com/kelproject/pykube/issues/29, which can be resolved with monkey-patching, but the correct fix appears to be in urllib3.
"
920,Fix doc syntax in user-guide.rst,2016-07-07T06:56:03Z,2016-07-07T07:19:34Z,,,,"This PR fixes a syntax error in `user-guide.rst` documentation.
"
919,Updating links to SSL warning help page. Fixes #918,2016-07-06T19:33:00Z,2016-07-07T16:36:59Z,Documentation,,,
918,Doc link for SSL warnings is broken,2016-07-06T19:13:28Z,2016-07-07T16:37:02Z,Documentation,,,"#887 removed docs/security.rst, but it's still referenced as https://urllib3.readthedocs.org/en/latest/security.html in the message for InsecureRequestWarning, InsecurePlatformWarning, and SNIMissingWarning.

For example:

```
requests/packages/urllib3/util/ssl_.py:90: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning
```

```
requests\packages\urllib3\connectionpool.py:768: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
```
"
917,AttributeError: 'Retry' object has no attribute 'history',2016-07-06T16:40:59Z,2016-07-06T21:08:45Z,,AttributeError,AttributeError: 'Retry' object has no attribute 'history',"This is related to #871. I'll add a failing test soon to reproduce this bug:

```
Traceback (most recent call last):
  File ""requests/sessions.py"", line 518, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File ""requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""requests/sessions.py"", line 585, in send
    r = adapter.send(request, **kwargs)
  File ""requests/adapters.py"", line 403, in send
    timeout=timeout
  File ""requests/packages/urllib3/connectionpool.py"", line 642, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""requests/packages/urllib3/util/retry.py"", line 295, in increment
    history = self.history + (RequestHistory(method, url, error, status, redirect_location),)
AttributeError: 'Retry' object has no attribute 'history'
```

Printing `dir(self)` from `urllib3/util/retry.py` on line 294 gives this output:
`['BACKOFF_MAX', 'DEFAULT', 'DEFAULT_METHOD_WHITELIST', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '    __reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_is_connection_error', '_is_read_error', '_observed_errors', 'backoff_factor', 'connect', 'from_int'    , 'get_backoff_time', 'increment', 'is_exhausted', 'is_forced_retry', 'method_whitelist', 'new', 'raise_on_redirect', 'raise_on_status', 'read', 'redirect', 'sleep', 'status_forcelist', 'total']`

Strangely `history` is not an attribute. Printing `inspect.getfile(self.__class__)` shows the correct retry.pyc file so not sure how the `Retry` class is missing the `history` attribute. When opening the source version of retry.pyc, the `Retry` class has history defined in `__init__`, so I'm stumped.

Even though requests is using an old urllib3 version, the urllib3 above is the current master version.
"
916,Undo #915 and apply better solution,2016-07-05T12:36:18Z,2016-07-11T23:36:25Z,,,,"This implements the change discussed in #915.
"
915,Pointing flake8 specifically at the urllib3 package,2016-07-04T13:31:10Z,2016-07-04T13:34:56Z,,,,"Fixes #914.
"
914,Flake8 runs on entire virtualenv inside the repository,2016-07-04T13:30:56Z,2016-07-04T13:34:56Z,,,,"When running `make test-all`, flake8 will run on the entirety of the repository, including the `site-packages` directory of any virtual environment stored therein.

The solution is to ~~create an upstream pull request bringing the entire standard library into line with flake8 standards~~ point flake8 specifically at the urllib3 package.
"
913,Fix RTD.,2016-07-01T07:53:27Z,2016-07-01T08:13:31Z,,,,"Supersedes #910.
"
912,CHANGES for #858 and #887,2016-07-01T07:47:46Z,2016-07-01T08:42:13Z,,,,
911,Normalize the scheme and host in the URL parser,2016-07-01T01:28:43Z,2016-07-01T15:41:26Z,,,,"I don't know that this is the most rigorous way to fix the problem of case-sensitivity, but it seemed like a nice centralized place to normalize things.

According to RFC 3986, the scheme and the host portion of the authority
in a URI are case-insensitive. `urllib3` uses the scheme in particular
as a key for various dictionaries, and oddly-cased schemes can break
this. This patch modifies the Url class to normalize both the scheme and
host.

fixes #833
"
910,Adding docs/requirements.txt for readthedocs.,2016-06-30T20:20:48Z,2016-07-01T07:53:42Z,,,,"The rtd config will need to be updated to point to `docs/requirements.txt` instead of `docs-requirements.txt`.

I think/hope this should work.
"
909,Announcement: @lukasa is the lead maintainer until further notice,2016-06-30T15:27:53Z,2017-12-10T20:33:20Z,Announcement,,,"Hi everyone who watches urllib3 issues. üëã 

I'm taking a 3-6mo hiatus from urllib3. That is, if everything is happy at 3mo I will probably continue for 6mo, until January 1, 2017 **[update: continuing further with quarterly evaluations]**. Logistically this means I will mute Github urllib3 notifications and forego day-to-day involvement, but I'm still reachable if there are questions or I can be helpful in specific situations.

@lukasa will be taking on responsibility as lead maintainer for the direction of the project in the duration, he will have access to the pypi package and whatever other administrative things are needed. @sigmavirus24 will also be around to help push things forward.

While I'm at it, I want to thank @haikuginger and @jonparrott for choosing to get involved and taking on a lot of big tasks lately. Also @dstufft for making his feedback available as we dig deeper into designing our higher-level APIs. I'm grateful to have you all as collaborators on the project. ‚ú® :custard: :sparkles:

I look forward to seeing exciting changes when I'm back, maybe then will be a good time to start a strong sprint towards v2.0!

If anyone has questions or concerns, comments here are welcome or email me. ‚ù§Ô∏è 

For reference (mostly my own), @Lukasa has urllib3 admin privileges on the following platforms: Github, PyPi, ReadTheDocs."
908,Fix chunked transfer-encoding with preload_content,2016-06-24T12:55:29Z,,,**Note**,**Note**:,"When we implemented our own chunked transfer-encoding logic, we broke
the use case (that we did not anticipate) of people not specifying
preload_content=False to stream their data. This provides a fix, sans
tests, for that use case.

Closes gh-907

---

**Note**:
- I'm not sure we want to fix this case (see the issue).
- I didn't write tests, I just wanted to see if I could work in a fix quickly before work.
- An alternative, that forces people to use `preload_content` and avoids this is to not set `chunked` to `True` when `preload_content` is also `True`. I'm just not sure which behaviour we want.
"
907,Chunked downloading is broken in >1.12,2016-06-24T12:31:56Z,2016-11-01T20:52:32Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'readline',"Try this code snippet:

``` python
import urllib3

http = urllib3.PoolManager()
r = http.request('GET', 'http://www.nyaa.se/?page=download&tid=818605')
for chunk in r.stream():
    print len(chunk)
```

on urllib 1.12, the output is:

``` sh
(r)~  Œî python t.py
3862
5535
4150
3813
```

on urllib 1.13 and newer releases (every release, also master):

``` sh
(r)~  Œî python t.py
Traceback (most recent call last):
  File ""t.py"", line 7, in <module>
    for chunk in r.stream():
  File ""/Users/ibrahim/.virtualenvs/r/lib/python2.7/site-packages/urllib3/response.py"", line 353, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File ""/Users/ibrahim/.virtualenvs/r/lib/python2.7/site-packages/urllib3/response.py"", line 502, in read_chunked
    self._update_chunk_length()
  File ""/Users/ibrahim/.virtualenvs/r/lib/python2.7/site-packages/urllib3/response.py"", line 448, in _update_chunk_length
    line = self._fp.fp.readline()
AttributeError: 'NoneType' object has no attribute 'readline'
```
## 
"
906,CHANGES for #835,2016-06-22T17:38:46Z,2016-06-22T17:54:15Z,,,,
905,SSLContext object,2016-06-22T16:53:57Z,2016-06-22T17:05:53Z,,,,"Essentially a dupe of #835 for the SessionManager branch.
"
904,Approved SessionManager changes,2016-06-22T16:25:55Z,2016-09-05T12:17:53Z,,,,"Basic implementation of the SessionManager cookie-handling logic.
"
903,Discrete field types in RequestMethods,2016-06-21T18:31:09Z,,,,,"Implements #885, in support of #892.
"
902,Removing absolute import in NTLMPool,2016-06-20T19:13:46Z,2016-06-20T19:26:27Z,,,,"Continues work of #901.
"
901,Remove absolute import.,2016-06-20T09:53:06Z,2016-06-20T17:32:38Z,,,,"This absolute import breaks vendorizing urllib3: see kennethreitz/requests#3006.
"
900,Unicode filenames in form uploads: Misuse of RFC 2231 in urllib3?,2016-06-20T07:35:33Z,2016-06-20T07:36:47Z,,,,"Hi everyone,

I notices that urllib3 (and therefore requests) uses RFC-2231 extended attributes when uploading `multipart/form-data` with unicode filenames.

```
Content-Disposition: form-data; name=""file""; filename*=utf-8''%E6%9D%B1%E4%BA%AC.png
```

I wanted to start a discussion in this topic.

A quick test showed that RFC-2231 is not fully supported by `werkzeug`, `cgi.FieldStorage` or`multipart`. I suspect that the majority of server-side parsers for multipart form uploads cannot read these parameters. This is probably because modern browsers don't use this standard. Firefox and chrome simply encode headers with UTF-8 and call it a day.

If we look at Apache HttpClient, arguably the most widely used HTTP client library (which is not a browser) out there, we can find this in the source:

https://github.com/apache/httpclient/blob/trunk/httpclient5/src/main/java/org/apache/hc/client5/http/entity/mime/HttpRFC6532Multipart.java#L37

```
 HttpRFC6532Multipart represents a collection of MIME multipart encoded content bodies,
 implementing the strict (RFC 822, RFC 2045, RFC 2046 compliant) interpretation
 of the spec, with the exception of allowing UTF-8 headers, as per RFC6532.
```

So, Apache HTTPClient also just encodes these headers in UTF-8 and does not use RFC 2231.

I propose dropping RFC-2231 headers in favour of plain UTF-8 headers in the `multipart/form-data` generator of urllib3.
"
899,on windows use wincertstore to simulate platform CA,2016-06-18T20:43:41Z,2016-06-18T21:03:39Z,,,,"A proposal for fixing #890
"
898,add domain aware logging to poolmanager,2016-06-17T05:01:54Z,2016-06-17T17:55:13Z,,,,"this also involved making a str method for RecentlyUsedContainer, however I couldn't find an easy way to complete the debug log info as requested.

One of my attempts at #881. I would consider at most one of the two (#897).
"
897,add domain and method aware logging to connectionpool,2016-06-17T04:59:36Z,2016-06-23T15:14:54Z,,,,"One of my attempts at #881. I would consider at most one of the two (#898).
"
896,"Socks supports HTTP_NO_TUNNEL, urllib3 doesn't seem to",2016-06-16T04:31:41Z,2016-06-17T03:42:31Z,,,,"Should urllib3 reflect this in the socks contrib module?
"
895,Allow nested Retry objects in Retry.total,2016-06-15T11:25:35Z,2016-06-15T17:54:50Z,,TypeError,TypeError: unsupported operand type(s) for -=: 'Retry' and 'int',"I had the following exception in the wild:

```
TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'
```

This happened while doing a `pip install` from a `devpi` that was down, causing a `Connection refused` and thus triggering the retry logic of `urllib3`. Seems someone has code like `Retry(r.increment())` or similar ‚Äì instead of hunting that down, I found it easier and more robust to fix it at the source of the exception, especially since the fix is easy. YMMV.
"
894,Override Content-Type when setting body fields,2016-06-15T03:26:23Z,,,,,"Fixes an issue spotted while working on #885 - if the user passes a `Content-Type` header explicitly, then we'll leave it alone when setting up body-encoded fields. This corrects for that; we'll override with the appropriate header for what we implemented.
"
893,updated SNIMissingWarning to address Jython,2016-06-14T21:46:53Z,2016-09-05T16:27:13Z,,,,"In my development environment, i need to disable SSL.  requests makes that easy by use using verify=False.  However, Jython doesn't allow SNI.  In order to disable it, I wrote the mentioned gist.  I'd be glad to make changes to it if anyone would find it helpful.  It was just a quick fix for me.
"
892,Planning for breaking changes on TRFKA1.17,2016-06-14T19:53:30Z,,,,,"So, we're taking a step back and considering/refactoring all the breaking changes we want to make in the near future that'll break current API contracts on some level. This issue is basically a dump of my thoughts/plans so far, which are open for discussion to all comers.
- Method of SSL/TLS verification: There's been an interest in having platform-independent hostname verification pre-enabled for `SessionManager`. From talking to @Lukasa, there are two main ways to go about this - first, to either import or package certifi, or to require PyOpenSSL, certitude, and other binary packages to be able to reliably use the platform cert bundles. At this point, certifi is a more practical option, so we basically need to decide if we want to bundle it or require it.
- Scope of SSL/TLS verification: @shazow has indicated that he'd like autoconfig on `SessionManager`, as a high-level configuration object. Personally, I think that if we're putting in the effort to set up a secure default, all of our connection levels should be able to benefit from it - including `ConnectionPool` and `PoolManager`. This is up for discussion, and brings us to...
- Settings handling in general: I'd like to unify all sorts of autoconfiguration code onto a `Settings` object that can be passed into any `RequestMethods` subclass. This would include things like the `SSLContext` to be used, any platform-specific flags that need to be considered (e.g., GAE), or anything else. My vision would be to have any newly-created `Settings` object carry smart defaults, and any newly-created `RequestMethods` subclass get either a `Settings` object in arguments or one of a fresh instantiation of same or a link to a `urllib3`-wide `Settings` object. It'd look something like this:

``` python
import urllib3

# We can call like this...
sm = urllib3.SessionManager()

# Or like this...
insecure_settings = urllib3.Settings(verify_ssl=False)
sm = urlib3.SessionManager(settings=dumb_settings)

# Or you could even do this!
class InsecurePoolManager(urllib3.PoolManager):

    def __init__(self, *args, **kwargs):
        insecure_settings = Settings(verify_ssl=False)
        kwargs[settings] = insecure_settings
        super(InsecurePoolManager, self).__init__(*args, **kwargs)
```
- While we're on `RequestMethods`, support will be added for explicitly-passed params and body fields. (#885)
- SessionManager will get a rewrite to be more flexible in general. The class will have a default list of `ContextHandlers`, which at first will include just `CookieHandler`, but when instantiating or subclassing `SessionManager`, an alternate list can be provided. Each request/response pair will be passed to each handler in the list for mutation and data extraction. And we're going to include a bonus `BasicAuthHandler` that people can use if they want. This will also solve for #888, as a list without a CookieHandler can be passed to `__init__()`.

``` python
import urllib3
from urllib3.context_handlers import CookieHandler, BasicAuthHandler

basic_auth = BasicAuthHandler('mysite.com', 'username', 'password')
cookies = CookieHandler()
handlers = [cookies, basic_auth]
sm = urllib3.SessionManager(handlers=handlers)
```
- I think I'm going to cut the link between our `Request` object and `urllib.request.Request`. We'll mimic some small features of the interface so that cookielib can still handle our version, but the main goal will be to have data stored in a way that we can natively expand into `urlopen()`. This will also make for a cleaner API for additional `ContextHandlers` to work with.
## 
"
891,Removing symlinks from dummyserver certs to fix test suite on Windows,2016-06-13T14:32:07Z,2016-06-13T15:06:37Z,,,,"Fixes #890.
"
890,Tests fail on Win32,2016-06-12T14:50:10Z,,,"SSLError, nose.proxy.SSLError, ......E..........E..S..........................j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py35\lib\site-packages\nose\util.py:453, urllib3.exceptions.SSLError","SSLError: (""bad handshake: Error([('PEM routines', 'PEM_read_bio', 'no start line'), ('x509 certificate routines',, nose.proxy.SSLError: (""bad handshake: Error([('PEM routines', 'PEM_read_bio', 'no start line'), ('x509 certificate, ......E..........E..S..........................j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py35\lib\site-packages\nose\util.py:453: DeprecationWarning: inspect.getargspec() is deprecated, use, urllib3.exceptions.SSLError: (""bad handshake: Error([('PEM routines', 'PEM_read_bio', 'no start line'), ('x509 certificate routines', 'X509_load_cert_file', 'PEM lib'), ('SSL routines', 'ssl3_get_server_certificate', 'certificate","Tested Python 2.7.11 32bit, 3.4.3 64bit, 3.5.1 64bit:
Also PySocks 1.5.7 is broken for Python 2.7.x on windows due to Anorov/PySocks#35.
**2.7.11:**

```
py27 runtests: commands[1] | nosetests
SSSINFO:urllib3.connectionpool:Starting new HTTPS connection (1): localhost
INFO:tornado.access:200 GET / (::1) 2.00ms
DEBUG:urllib3.connectionpool:""GET / HTTP/1.1"" 200 13
......E..........E..S........................................................................................................E..........S..S..........................................SSS......S..............................................................................................................................................................................................................
======================================================================
ERROR: test_ca_dir_verified (test.contrib.test_pyopenssl.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 66, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 101, in test_ca_dir_verified
    r = https_pool.request('GET', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
SSLError: (""bad handshake: Error([('PEM routines', 'PEM_read_bio', 'no start line'), ('x509 certificate routines',
'X509_load_cert_file', 'PEM lib'), ('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
tornado.general: WARNING: SSL Error on 948 ('::1', 62950, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown
ca (_ssl.c:590)
tornado.general: ERROR: Uncaught exception
Traceback (most recent call last):
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\http1connection.py"", line 693, in _server_request_loop
    ret = yield conn.read_response(request_delegate)
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\gen.py"", line 870, in run
    value = future.result()
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\concurrent.py"", line 215, in result
    raise_exc_info(self._exc_info)
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\gen.py"", line 876, in run
    yielded = self.gen.throw(*exc_info)
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\http1connection.py"", line 168, in _read_message
    quiet_exceptions=iostream.StreamClosedError)
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\gen.py"", line 870, in run
    value = future.result()
  File ""j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py27\lib\site-packages\tornado\concurrent.py"", line 215, in result
    raise_exc_info(self._exc_info)
  File ""<string>"", line 3, in raise_exc_info
SSLError: [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown ca (_ssl.c:590)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: We should rely on the platform CA file to validate authenticity of SSL
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 103, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 217, in test_ssl_verified_with_platform_ca_certs
    https_pool.request('HEAD', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
SSLError: (""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): httpbin.org
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_ca_dir_verified (test.with_dummyserver.test_https.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 66, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 101, in test_ca_dir_verified
    r = https_pool.request('GET', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
--------------------- >> end captured logging << ---------------------
```

**3.4.3**:

```
SSS......E..........E..S........................................................................................................E..........S..S............................................................................................S.................................................S...................................................................................................................
======================================================================
ERROR: test_ca_dir_verified (test.contrib.test_pyopenssl.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 66, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 101, in test_ca_dir_verified
    r = https_pool.request('GET', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
nose.proxy.SSLError: (""bad handshake: Error([('PEM routines', 'PEM_read_bio', 'no start line'), ('x509 certificate
routines', 'X509_load_cert_file', 'PEM lib'), ('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
tornado.general: WARNING: SSL Error on 920 ('::1', 61257, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown
ca (_ssl.c:600)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: We should rely on the platform CA file to validate authenticity of SSL
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 103, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 217, in test_ssl_verified_with_platform_ca_certs
    https_pool.request('HEAD', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
nose.proxy.SSLError: (""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): httpbin.org
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_ca_dir_verified (test.with_dummyserver.test_https.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 66, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 101, in test_ca_dir_verified
    r = https_pool.request('GET', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
nose.proxy.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:600)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
tornado.general: WARNING: SSL Error on 1640 ('::1', 61513, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown ca (_ssl.c:600)
```

**3.5.1**:

```
......E..........E..S..........................j:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\.tox\py35\lib\site-packages\nose\util.py:453: DeprecationWarning: inspect.getargspec() is deprecated, use
inspect.signature() instead
  inspect.getargspec(func)
..............................................................................E..........S..S............................................................................................S.................................................S...................................................................................................................
======================================================================
ERROR: test_ca_dir_verified (test.contrib.test_pyopenssl.transplant_class.<locals>.C)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 66, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 101, in test_ca_dir_verified
    r = https_pool.request('GET', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
urllib3.exceptions.SSLError: (""bad handshake: Error([('PEM routines', 'PEM_read_bio', 'no start line'), ('x509 certificate routines', 'X509_load_cert_file', 'PEM lib'), ('SSL routines', 'ssl3_get_server_certificate', 'certificate
verify failed')],)"",)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
tornado.general: WARNING: SSL Error on 868 ('::1', 62126, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown
ca (_ssl.c:645)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: We should rely on the platform CA file to validate authenticity of SSL
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 103, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 217, in test_ssl_verified_with_platform_ca_certs
    https_pool.request('HEAD', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
urllib3.exceptions.SSLError: (""bad handshake: Error([('SSL routines', 'ssl3_get_server_certificate', 'certificate verify failed')],)"",)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): httpbin.org
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_ca_dir_verified (test.with_dummyserver.test_https.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\__init__.py"", line 66, in wrapper
    return test(*args, **kwargs)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\test\with_dummyserver\test_https.py"", line 101, in test_ca_dir_verified
    r = https_pool.request('GET', '/')
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 69,
in request
    **urlopen_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\request.py"", line 90,
in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""J:\dev\workspace\pythonxy-xy-27\src\python\urllib3\__workspace__\urllib3-git\urllib3\connectionpool.py"", line 621, in urlopen
    raise SSLError(e)
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
tornado.general: WARNING: SSL Error on 1596 ('::1', 62397, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown ca (_ssl.c:645)
--------------------- >> end captured logging << ---------------------
```
## 
"
889,"Tests fail on multiple platforms (trusty, wily)",2016-06-12T14:42:14Z,,,,,"Attempted to run tests from master on the following platforms (used vagrant):
- precise64  - pass
- trusty64 native python 2.7.6- fail
- trusty64 python 2.7.11 - fail
- wily64 python 2.7.10 - fail

The above all fail pretty much the same way:

```
py27 runtests: commands[1] | nosetests
SSS......S.............S...........SEEEEE.EEEEEEEE..............EEEEEE...........................E........E..EE.E.Exception in thread Thread-15:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""/home/vagrant/urllib3/dummyserver/server.py"", line 126, in run
    self.server = self._start_server()
  File ""/home/vagrant/urllib3/dummyserver/server.py"", line 122, in _start_server
    self.socket_handler(sock)
  File ""/home/vagrant/urllib3/test/with_dummyserver/test_socketlevel.py"", line 175, in socket_handler
    sock_timeout = listener.accept()[0]
  File ""/usr/lib/python2.7/socket.py"", line 202, in accept
    sock, addr = self._sock.accept()
timeout: timed out

..............S..........S..S..........................................SSSEEEEEESEEEEEESEEEEEEEE.EEEEEEEE......................................................................................................................................................................................
```
## 
"
888,SessionManager: Ability to disable cookies,2016-06-11T21:43:42Z,,,,,"SessionManager is going to include a bunch of autoconfigured functionality, will be useful to be able to disable portions of it when creating a fresh instance.

(Unless we feel that SessionManager is the wrong place for the autoconfigured functionality, then we need yet another layer.)

To be clear, these disabling/enabling should happen during instantiation, not after. Perhaps another fun opportunity to use `False` as a semantic value. :P

Also another open question: Maybe this should live in SessionContext?
## 
"
887,New docs,2016-06-09T21:21:59Z,2016-06-30T19:42:16Z,,,,"For #884.

Preview here: http://urllib3.theadora.io/

So far:
- Switched to alabaster theme.
- First run at the homepage.
- Started on the user guide.

For the moment I just wanted to give @shazow a chance to look it over & a way to keep track of my progress, not quite ready for feedback yet.
"
886,Proxy handling like urllib2,2016-06-08T06:08:58Z,,,,,"The `ProxyHandler` in urllib2 uses a dictionary of proxies for http and https connections. Like

`urllib2.ProxyHandler({'http': 'http://proxy', 'https': 'https://proxy'})`.

Is there any such support here? There is no such mention in the documentation.
## 
"
885,Expand RequestMethods optional arguments,2016-06-02T00:54:02Z,,,,,"I've come across at least one example of an API where I need to both POST a body of some form and also have query parameters on a request, and this is something I'm accommodated so far in my [beekeeper](https://github.com/haikuginger/beekeeper) project - largely because I've been handling setting those items up in my own code.

I'm now migrating that to urllib3, and one snag I'm seeing is that `RequestMethods.request()` takes a single `fields` argument that turns into either a querystring or `x-www-urlencoded` depending on method- as far as I can tell, there's no way to choose to pass either one or the other explicitly.

Would there be any objection to expanding the `request()` signature to enable those items to be passed explicitly? The way I'm conceiving of it, `fields` would still be encoded according to the request method, while the type-specific arguments (`params` and `form`? Dunno - suggestions welcome) would always be added to the request in their specified format.

If this isn't an appropriate extension, it's not a big deal - I can keep doing it in my own code.
## 
"
884,Documentation improvements,2016-05-31T18:50:06Z,2016-07-30T17:25:08Z,In Progress,,,"Our docs right now are a mishmash of things appended with random pull requests over the years. Would be great to have someone (ahem, @jonparrott kinda just volunteered) go through and re-organize and write some more narratives.

Some tasks:
- [x] Make sure the full TOC is available on the landing page (currently not, ugh! we have a bunch of sections that are effectively hidden.)
- [x] Clean up the landing page to de-emphasize lower-level components and talk about the primary flow more (and make sure examples are as much default-secure as we can).
- [x] Clarify areas of the docs which are higher-level vs lower-level stuff (aka not for beginners). Somekind of colourcoding would be ideal but not sure how easy this would be with Sphinx.
- [x] Reduce emphasis that urllib3 is for ""lower level"" stuff‚Äîit's not _just_ for lower-level stuff. This has been scaring some beginners.
- [x] Recipes section for different usecases
  - [x] appengine
  - [x] request with a body/json blob
  - [x] streaming to/from file (e.g. https://stackoverflow.com/questions/27387783/how-to-download-a-file-with-urllib3)
  - [x] custom tls stuff
  - other things?
- [x] Make sure README is reasonably sync'd with what our latest docs say.
- Any other ideas?
"
883,Drop Python 2 support,2016-05-31T13:55:55Z,,Someday,,,"I'm wondering whether we should start a conversation with our biggest partners (pip, Requests, et. al.) around what the long-term plan for Python 2 ought to be.

The platform itself is planned to have no further support after 2020, so we should probably have a transition plan laid out well in advance so that we can start providing notification and documentation to our users.

Obviously, what that plan ought to be, or even whether we should start considering it yet, is up for discussion.
## 
"
882,Convert readthedocs link for their .org -> .io migration for hosted projects,2016-05-29T13:08:45Z,2016-05-29T13:53:54Z,,,,"As per their email ‚ÄòChanges to project subdomains‚Äô:

> Starting today, Read the Docs will start hosting projects from subdomains on the domain readthedocs.io, instead of on readthedocs.org. This change addresses some security concerns around site cookies while hosting user generated data on the same domain as our dashboard.

Test Plan: Manually visited all the links I‚Äôve modified.
"
881,log server / server info on connectionpool,2016-05-27T19:15:27Z,2016-07-21T12:39:54Z,,,,"The current implementation of connectionpool logs only the ""local path"" of a url - it does not log the server component.

https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L400-L401

this logging is not useful in multithreaded applications _if_ there are as few as 2 concurrent connections. it also becomes increasingly confusing if there are redirects involved.  

logging would be much better if the server were included somewhere on that line.
"
880,Add typing to __init__ and _collections,2016-05-26T08:58:53Z,,,,,"So, here is a first iteration of adding types checking, as discuted in #867.

Having stubs files was harder to make it works, and in general, it would be best to keep related code and typing. So we will use magic comment as proposed by @sigmavirus24.

There is a bunch of TODO in code, with the corresponding bug report (or general issue).

I'll add some commit comments to explain choices made.

With the current state of mypy, there is a bunch of error, like

```
urllib3/_collections.py:22: error: Name 'OrderedDict' already defined
urllib3/_collections.py:27: error: Name 'RLock' already defined
urllib3/_collections.py:25: error: Name 'RLock' already defined
```

which are caused by mypy not handling conditionals, but, by avoiding theses paths while coding, we can have some type error remarks.

Overview
- add types to `urllib3/__init__.py` and `urllib3/_collections.py`
- adding `urllib3.util.typing` module to allow handling both python2 and python3 with an easier interface. We will add some classes during next iterations
- changing some import check based on `ImportError` to one using `sys.version_info`, as it is the recommanded way to do it in PEP 484
"
879,Add ResponseCls class attr to HTTPConnectionPool,2016-05-25T18:51:56Z,2016-05-25T19:18:34Z,,,,"This attribute defaults to `HTTPResponse`, but developers can subclass `HTTPConnectionPool` and substitute their own `HTTPResponse` subclass, as long as it has valid `from_httplib()` and `__init__()` methods, with compatible method signatures.
"
878,urllib3 does not provide a way to send full chain-of-trust when server makes a CertificateRequest ,2016-05-24T14:48:15Z,,,requests.exceptions.SSLError,requests.exceptions.SSLError: Errno bad handshake,"I am on python2.7 installed on CentOS 6. I have OpenSSL 1.0.1e. I have requests version 2.9.1. The HTTP server is performing SSL client authentication. The client's chain has a verification depth of 2, which means there's a single intermediate CA certificate in the client chain. I am not sure if this is a bug or a feature request.

Based on the public api, I sent a request in like so. note i have explicitly chosen not to verify the server's chain of trust (i know that's not secure but this isn't being used in production). The cert.pem contains the client's PEM-encoded certificate, the intermediate CA cert, and the root cert. The final entity in the cert.pem is the client's private key (which is NOT encrypted).

resp = requests.get(url,verify=False,cert='cert.pem')

The following exception is thrown.

requests.exceptions.SSLError: Errno bad handshake

Looking at the server's ssl logs, I can see that the reason the server terminated the SSL connection was caused by the server's inability to construct the client's chain-of-trust. The server could not do this because the request's library only sent its client cert in it. I used openssl s_server to verify this as well.

I could not find anything in the official documentation that would allow me to specify a set of certs to be included in the client's chain-of-trust.

Looking at the pyopenssl wrapper used by requests, I can see the Context.use_certificate method, which sets the leaf cert. I am suspecting that only sets a single certificate. I suspect that for this to work, the requests external api would need to provide an interface for passing in a list of PEM-encoded certs, which is then set via the Context.add_extra_chain_cert method.
## 
"
877,Changelog for #874 and #873.,2016-05-24T14:13:27Z,2016-05-24T19:29:28Z,,,,"Yup, @sigmavirus24 is right, I forgot this.
"
876,``PoolManager`` now references the gobal ``key_fn_by_scheme`` dictionary,2016-05-24T14:02:21Z,2016-05-30T14:05:36Z,,,,"Initially for #830, the `key_fn_by_scheme` was copied for each
PoolManager instance. This is inconcistent with the
`pool_classes_by_scheme` instance variable, which is a reference to
the global dictionary. This commit makes the two consistent by making
the `key_fn_by_scheme` instance variable a reference to the global
dictionary.
"
875,Exclude pkginfo 1.3.0 from installation.,2016-05-24T09:10:52Z,2016-05-24T17:48:40Z,,,,"Note that we only require pkginfo _transitively_ from twine, so arguably we could be a bit more careful and simply not require twine in Travis, but for now this will do just fine.

Resolves the problem affecting #874. This will affect all our CI jobs until resolved, so let's merge this sharpish. @shazow @sigmavirus24 @haikuginger for review.
"
874,fix connectionpool.ConnectionPool.close(),2016-05-24T08:56:42Z,2016-05-24T13:48:34Z,,,,"See #873 for reference.
"
873,`connectionpool.ConnectionPool.close()` has zero args,2016-05-24T08:27:09Z,2016-05-24T18:13:26Z,,,,"As written [here](https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L93), `close()` do not except any argument. I guess it was meant to have `self` (as overriden in `HTTPConnectionPool`). Or maybe having the `staticmethod` decorator. Or is it some weird corner case to keep compatibility?
"
872,PoolManager.request() with invalid scheme produces KeyError,2016-05-23T18:44:58Z,,Contributor Friendly ‚ô•,KeyError,KeyError: 'FAKESCHEME',"```
>>> pm = urllib3.PoolManager
>>> pm.request('GET', 'FAKESCHEME://github.com')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/site-packages/urllib3/request.py"", line 69, in request
    **urlopen_kw)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/request.py"", line 90, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 155, in urlopen
    conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 128, in connection_from_host
    pool = self._new_pool(scheme, host, port)
  File ""/usr/local/lib/python3.5/site-packages/urllib3/poolmanager.py"", line 87, in _new_pool
    pool_cls = self.pool_classes_by_scheme[scheme]
KeyError: 'FAKESCHEME'
```

We should be fetching by that scheme more resiliently, and raising a more specific error message - possibly a `LocationValueError`?
## 
"
871,unhandled exception when creating new Retry object,2016-05-23T14:11:30Z,2016-05-23T18:09:04Z,,AttributeError,AttributeError: 'Retry' object has no attribute 'raise_on_status',"``` python
Traceback (most recent call last):
  File ""urllib3/connectionpool.py"", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""urllib3/util/retry.py"", line 285, in increment
    _observed_errors=_observed_errors)
  File ""urllib3/util/retry.py"", line 151, in new
    raise_on_status=self.raise_on_status,
AttributeError: 'Retry' object has no attribute 'raise_on_status'
```

You can reproduce this by changing line 140 of `urllib3/connection.py` to:

```
140         try:
141             raise SocketTimeout('')
142             conn = connection.create_connection(
143                 (self.host, self.port), self.timeout, **extra_kw)
144
145         except SocketTimeout as e:
```

This forces a `SocketTimeout` and causes the unhandled exception when calling `Retry.new`.
"
870,Release v1.16 (and maybe v1.17),2016-05-23T00:22:12Z,2016-09-26T11:37:24Z,,,,"Current plan is to cut the existing changelist for v1.16 and cut v1.17 shortly after with just #835 and #849.

FYI I'm kinda scrambling to get my PyCon talk ready for next week so this maneuver might have to wait a bit longer unless anyone is hurting. :x
"
869,MaxRetryError when redirecting from http to https through PoolManager(),2016-05-22T02:36:06Z,2016-05-22T03:53:30Z,,urllib3.exceptions.MaxRetryError,"urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: https://en.wikipedia.org/wiki/Main_Page (Caused by ResponseError('too many redirects',))","As just an example http://en.wikipedia.org/wiki/Main_Page redirects to https://en.wikipedia.org/wiki/Main_Page , but requesting http://en.wikipedia.org/wiki/Main_Page via PoolManager fails. 

Using `urllib3.connection_from_url`:

``` bash
python -c ""import ssl, urllib3; r = urllib3.connection_from_url('https://en.wikipedia.org/wiki/Main_Page').request('GET', 'http://en.wikipedia.org/wiki/Main_Page', assert_same_host=False); print(r.status);""
urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
200
```

Using `urllib3.PoolManager().connection_from_url`

``` bash
python -c ""import ssl, urllib3; r = urllib3.PoolManager().connection_from_url('http://en.wikipedia.org/wiki/Main_Page').request('GET', 'http://en.wikipedia.org/wiki/Main_Page', assert_same_host=False); print(r.status);""
```

``` python
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""urllib3/request.py"", line 69, in request
    **urlopen_kw)
  File ""urllib3/request.py"", line 90, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""urllib3/connectionpool.py"", line 653, in urlopen
    release_conn=release_conn, **response_kw)
  File ""urllib3/connectionpool.py"", line 653, in urlopen
    release_conn=release_conn, **response_kw)
  File ""urllib3/connectionpool.py"", line 653, in urlopen
    release_conn=release_conn, **response_kw)
  File ""urllib3/connectionpool.py"", line 638, in urlopen
    retries = retries.increment(method, url, response=response, _pool=self)
  File ""urllib3/util/retry.py"", line 273, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='en.wikipedia.org', port=80): Max retries exceeded with url: https://en.wikipedia.org/wiki/Main_Page (Caused by ResponseError('too many redirects',))
```

Is that correct?  If it is correct, it feels a little confusing seeing `MaxRetryError` here.
"
868,SessionManager: Provide default instance?,2016-05-20T20:16:35Z,,,,,"This is more thinking ahead and depends on #849 but something I want to discuss:

Would be nice if users could do:

``` python
from urllib3 import http # or maybe `session` or `default` or something?

r= http.request('GET', whatever)
```

Two questions:
1. Is it _safe_ to provide a default instance (or magic property) of SessionManager for people to use?
   Need to consider single-threaded, multi-threaded, and possibly multi-process (this actually might be _more_ safe in a multi-process environment than our current approach where people might accidentally re-use a late-instantiated pool).
2. What should it be named?
   I like using `http` in all of the examples like `http = PoolManager()`, `http = ConnectionPool()` etc., so `http` feels natural to me as a default instance you make http requests through, but I'm open to other ideas.
   Originally, we used to use `pool = PoolManager()` but I think `http` is more friendly in practice.
## 
"
867,Add type annotations to urllib3,2016-05-19T12:32:18Z,,,,,"Would that be okay if type annotations of urllib3 were added to the [typeshed repo](https://github.com/python/typeshed)?

It would be nice to have your explicit consent, as stated in [PEP 484](https://www.python.org/dev/peps/pep-0484/#the-typeshed-repo).
## 
"
866,Respect `release_conn=False` after retry,2016-05-17T22:42:45Z,2016-05-25T18:46:16Z,,,,"Respect `ConnectionPool.urlopen(release_conn=False)`, making sure not to release the connection, even after a retry.

If a request failed but then succeeded after a retry, the connection would be released, contrary to what the user requested.

Fixes #651.
"
865,Github: Protected branches?,2016-05-17T21:17:49Z,2016-05-17T21:58:51Z,,,,"Do we want to enable protected branches? If so, for which? (master/release?)
"
864,Adding socks extra to setup.cfg,2016-05-17T18:52:35Z,2016-05-17T21:01:51Z,,,,"Should resolve #863, unless I'm very much mistaken.
"
863,urllib3 1.15.1 does not provide the extra 'socks',2016-05-17T18:06:23Z,2016-05-17T21:01:51Z,,ImportError,ImportError: No module named socks,"When installing urllib3 using `pip` according to the instructions on the [Contrib Modules](https://urllib3.readthedocs.io/en/latest/contrib.html#socks) page, the following messages are produced:

```
$ pip install -U urllib3[socks]
Collecting urllib3[socks]
  Using cached urllib3-1.15.1-py2.py3-none-any.whl
  urllib3 1.15.1 does not provide the extra 'socks'
Installing collected packages: urllib3
Successfully installed urllib3
```

And the `PySocks` package is not installed, resulting in the following error on import:

```
$ python -c 'from urllib3.contrib.socks import SOCKSProxyManager'
/home/user/.local/lib/python2.7/site-packages/urllib3/contrib/socks.py:31: DependencyWarning: SOCKS support in urllib3 requires the installation of optional dependencies: specifically, PySocks.  For more information, see https://urllib3.readthedocs.org/en/latest/contrib.html#socks-proxies
  DependencyWarning
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/user/.local/lib/python2.7/site-packages/urllib3/contrib/socks.py"", line 21, in <module>
    import socks
ImportError: No module named socks
```

This occurs with both Python 2.7 and 3.5.  It appears to be similar to #684 which was resolved by adding `metadata` to `setup.cfg` in #706.  Perhaps a similar fix for `socks` is needed?

Thanks,
Kevin
"
862,late PR for issue #850,2016-05-13T10:37:07Z,,,,,
861,"Hide a confusing ""getresponse() got an unexpected keyword argument 'b‚Ä¶",2016-05-12T12:15:58Z,2016-05-12T19:09:57Z,,TypeError,TypeError: getresponse() got an unexpected keyword argument 'buffering',"‚Ä¶uffering'"" error

Remove the TypeError from the exception chain in Python 3; otherwise it
looks like a programming error was the cause. A quick google search of
the above phrase shows I was not the only one.

Here is how a connection timeout looks like before this patch:

```
urllib3.PoolManager().request('GET', 'google.com:81', retries=False, timeout=1)

Traceback (most recent call last):
File ""/home/ran/src/urllib3/urllib3/connectionpool.py"", line 385, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/home/ran/src/urllib3/urllib3/connectionpool.py"", line 387, in _make_request
    httplib_response = conn.getresponse()
File ""/usr/lib/python3.5/http/client.py"", line 1174, in getresponse
    response.begin()
File ""/usr/lib/python3.5/http/client.py"", line 282, in begin
    version, status, reason = self._read_status()
File ""/usr/lib/python3.5/http/client.py"", line 243, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
File ""/usr/lib/python3.5/socket.py"", line 575, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

[snip]
```

After, the first part is gone, and only the real cause is shown.

In Python2, exceptions are not chained automatically so there is no
difference.
"
860,strict parameter is obsolete in python version > 3.4,2016-05-11T20:29:10Z,2016-05-12T07:28:36Z,,,,"The strict parameter causes twill to break:

```
In [1]: from twill.commands import *

In [2]: go(""http://www.python.org/"")
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-8620e344db1d> in <module>()
----> 1 go(""http://www.python.org/"")

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/twill-1.8.0-py3.4.egg/twill/commands.py in go(url)
    108     Visit the URL given.
    109     """"""
--> 110     browser.go(url)
    111     return browser.get_url()
    112 

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/twill-1.8.0-py3.4.egg/twill/browser.py in go(self, url)
     79         for u in try_urls:
     80             try:
---> 81                 self._journey('open', u)
     82                 success = True
     83                 break

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/twill-1.8.0-py3.4.egg/twill/browser.py in _journey(self, func_name, *args, **kwargs)
    510             auth = None
    511 
--> 512         r = self._session.get(url, auth = auth)
    513 
    514         if _follow_equiv_refresh():

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/sessions.py in get(self, url, **kwargs)
    485 
    486         kwargs.setdefault('allow_redirects', True)
--> 487         return self.request('GET', url, **kwargs)
    488 
    489     def options(self, url, **kwargs):

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    473         }
    474         send_kwargs.update(settings)
--> 475         resp = self.send(prep, **send_kwargs)
    476 
    477         return resp

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/sessions.py in send(self, request, **kwargs)
    583 
    584         # Send the request
--> 585         r = adapter.send(request, **kwargs)
    586 
    587         # Total elapsed time of the request (approximately)

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    401                     decode_content=False,
    402                     retries=self.max_retries,
--> 403                     timeout=timeout
    404                 )
    405 

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)
    564             # Request a connection from the queue.
    565             timeout_obj = self._get_timeout(timeout)
--> 566             conn = self._get_conn(timeout=pool_timeout)
    567 
    568             conn.timeout = timeout_obj.connect_timeout

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py in _get_conn(self, timeout)
    254                 conn = None
    255 
--> 256         return conn or self._new_conn()
    257 
    258     def _put_conn(self, conn):

/home/moritz/projects/twill/venv/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py in _new_conn(self)
    215         conn = self.ConnectionCls(host=self.host, port=self.port,
    216                                   timeout=self.timeout.connect_timeout,
--> 217                                   strict=self.strict, **self.conn_kw)
    218         return conn
    219 
```
"
859,Consider replacing urlparse with something that can handle the WHATWG's 'generous' definition of a URL.,2016-05-11T12:15:20Z,,,,,"For basically its entire lifetime, urllib3 has used urlparse and friends from the standard library to handle the task of working with URLs.

However, it has recently been pointed out that the WHATWG maintains a URL specification that its proponents claim is merely a codification of common browser practice. That URL specification contains a parsing section that is _dramatically_ more generous than urlparse and friends are: for example, it allows the scheme to be separated from the authority portion of the URL by a colon followed by _at least_ two slashes (allowing for the possibility of having an unbounded amount of slashes: @bagder's testing has found that Firefox will handle a string like `'http:' + ('/' * 8000) + 'example.com'` without complaint).

Given this remarkably expansive definition of the allowed format of a URL, and given that this apparently _really does happen_ on the web, and given also that the position of the WHATWG is that this generosity of parsing is a net good, we should at least consider adopting that parser.

It may be sensible to for us to write a standalone library (as part of the hyper project, perhaps) that is then vendored into urllib3/requests as appropriate. Regardless, we should discuss whether we want to do this or whether we think we should be less lenient.
## 
"
858,update timeout warning and default value on App Engine,2016-05-09T14:33:03Z,2016-07-01T07:45:23Z,,,,"motivated by sigmavirus24/requests-toolbelt#146. cc @jonparrott.
"
857,send timeout,2016-05-04T20:47:34Z,,,,,"The [urllib3 timeout docs](http://urllib3.readthedocs.io/en/latest/#timeout) show there's read and connect timeouts, but nothing for write. Per the discussion on [this thread in requests](https://github.com/kennethreitz/requests/issues/3099), it would be awesome if there were a way to control write timeout as well as read. Would it be possible to expose a send timeout that worked like read timeout, but for putting data instead of getting data?

@Lukasa for his input as well
## 
"
856,HTML5 non-ASCII File Names by Default,2016-05-04T20:11:43Z,,"In Progress, Soon",,,"These changes address one point of shazow/urllib3#303: encoding non-ASCII file names according to the HTML5 working draft by default, while still allowing file names to be encoded according to RFC 2231.
"
855,Large POST requests result in SSL3_WRITE_PENDING from urllib3.contrib.pyopenssl#sendall,2016-05-04T17:27:11Z,,,OpenSSL.SSL.Error,"OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_pending', 'bad write retry')]","I currently have a use case where I am sending a large set of data over https via a POST request. Specifically, I am trying to set a POST request to Elasticsearch with an HTTPS URL that has a body of ~2MB.

When sending these types of requests, I get the following stack trace:

```
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/helpers/__init__.py"", line 188, in bulk
    for ok, item in streaming_bulk(client, actions, **kwargs):
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/helpers/__init__.py"", line 160, in streaming_bulk
    for result in _process_bulk_chunk(client, bulk_actions, raise_on_exception, raise_on_error, **kwargs):
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/helpers/__init__.py"", line 85, in _process_bulk_chunk
    resp = client.bulk('\n'.join(bulk_actions) + '\n', **kwargs)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/client/utils.py"", line 69, in _wrapped
    return func(*args, params=params, **kwargs)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/client/__init__.py"", line 782, in bulk
    doc_type, '_bulk'), params=params, body=self._bulk_body(body))
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/transport.py"", line 307, in perform_request
    status, headers, data = connection.perform_request(method, url, params, body, ignore=ignore, timeout=timeout)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/elasticsearch/connection/http_requests.py"", line 62, in perform_request
    response = self.session.request(method, url, data=body, timeout=timeout or self.timeout)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/sessions.py"", line 585, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/adapters.py"", line 403, in send
    timeout=timeout
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 578, in urlopen
    chunked=chunked)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1053, in request
    self._send_request(method, url, body, headers)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1093, in _send_request
    self.endheaders(body)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1049, in endheaders
    self._send_output(message_body)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 893, in _send_output
    self.send(msg)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 869, in send
    self.sock.sendall(data)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 256, in sendall
    sent = self._send_until_done(data[total_sent:total_sent + SSL_WRITE_BLOCKSIZE])
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 245, in _send_until_done
    return self.connection.send(data)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 1274, in send
    self._raise_ssl_error(self._ssl, result)
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 1187, in _raise_ssl_error
    _raise_current_error()
  File ""/usr/local/.virtualenvs/sqs_to_es/lib/python2.7/site-packages/OpenSSL/_util.py"", line 48, in exception_from_error_queue
    raise exception_type(errors)
OpenSSL.SSL.Error: [('SSL routines', 'ssl3_write_pending', 'bad write retry')]
```

At first, I thought this issue was a result of #717, but that issue has been resolved in the version of requests I am using. After further research, I landed on this comment in an [SO answer](http://stackoverflow.com/a/20715438/89342)

> OpenSSL has very strict requirements about how writes can be retried -- specifically the buffer's address and contents must not be changed. **When you retry a write, you must retry with the _exact_ same buffer (the same contents are not sufficient and, of course, different contents is absolutely prohibited)**.

Using this clue, I added some print statements to `urllib3.contrib.pyopenssl#_send_until_done` and `OpenSSL.SSL#send`:

`urllib3.contrib.pyopenssl#sendall`

``` python
def _send_until_done(self, data):
        print('_send_until_done')
        print('no loop')
        while True:
            print('loop')
                print(hex(id(data)))
                return self.connection.send(data)
            except OpenSSL.SSL.WantWriteError:
                _, wlist, _ = select.select([], [self.socket], [],
                                            self.socket.gettimeout())
                if not wlist:
                    raise timeout()
                continue

    def sendall(self, data):
        total_sent = 0
        while total_sent < len(data):
            sent = self._send_until_done(data[total_sent:total_sent + SSL_WRITE_BLOCKSIZE])
            total_sent += sent
```

`OpenSSL.SSL#send`

``` python
    def send(self, buf, flags=0):
        """"""
        Send data on the connection. NOTE: If you get one of the WantRead,
        WantWrite or WantX509Lookup exceptions on this, you have to call the
        method again with the SAME buffer.

        :param buf: The string, buffer or memoryview to send
        :param flags: (optional) Included for compatibility with the socket
                      API, the value is ignored
        :return: The number of bytes written
        """"""
        # Backward compatibility
        buf = _text_to_bytes_and_warn(""buf"", buf)
        print(hex(id(buf)))
        print("""")
        print("""")

        if isinstance(buf, _memoryview):
            buf = buf.tobytes()
        if isinstance(buf, _buffer):
            buf = str(buf)
        if not isinstance(buf, bytes):
            raise TypeError(""data must be a memoryview, buffer or byte string"")

        result = _lib.SSL_write(self._ssl, buf, len(buf))
        self._raise_ssl_error(self._ssl, result)
        return result
    write = send
```

After running this code, I realized that what was occurring was that a specific buffered subset of my data was being retried, BUT, the memory address of the buffered data was changing because of the call to `buf = _text_to_bytes_and_warn(""buf"", buf)` in `OpenSSL.SSL#send`. Here is the output I received:

```
_send_until_done
no loop
0x10a2bba20
loop
0x10a2bba20
0x7fdd6585f400


loop
0x10a2bba20
0x7fdd638f3000
```

As you can see in that output, the buffer of data in `urllib3.contrib.pyopenssl#_send_until_done` is correctly retried when a failure occurred and it has the correct memory address (`0x10a2bba20`), however, on the second attempt, the memory address of the buffer in `OpenSSL.SSL#send` is different between the two attempts (`0x7fdd6585f400` and `0x7fdd638f3000`).

Considering this, it looks like the use of [`OpenSSL.SSL#send`](https://github.com/pyca/pyopenssl/blob/master/src/OpenSSL/SSL.py#L1259) is the incorrect API to use in `urllib3.contrib.pyopenssl#sendall`. A potential solution is to instead use  [`OpenSSL.SSL#sendall`](https://github.com/pyca/pyopenssl/blob/master/src/OpenSSL/SSL.py#L1285-L1313), which seems to account for large requests by performing the [chunking of data](https://github.com/pyca/pyopenssl/blob/master/src/OpenSSL/SSL.py#L1305-L1313) _after_ [`_text_to_bytes_and_warn`](https://github.com/pyca/pyopenssl/blob/master/src/OpenSSL/SSL.py#L1296) has been executed.

Relevant versions in use:

Python 2.7.10
requests 2.10.0
pyOpenSSL 0.15.1
urllib3 1.15.1
## 
"
854,Tests in contrib/ directory are never executed,2016-05-04T16:50:42Z,2016-05-04T17:00:41Z,,,,"When running `make test` locally, tests in `test/contrib/` are never executed:

```
Name                      Stmts   Miss  Cover   Missing
-------------------------------------------------------
urllib3                      20      0   100%   
urllib3._collections        148      0   100%   
urllib3.connection          116      0   100%   
urllib3.connectionpool      235      0   100%   
urllib3.contrib               0      0   100%   
urllib3.contrib.socks        36      0   100%   
urllib3.exceptions           61      0   100%   
urllib3.fields               62      0   100%   
urllib3.filepost             32      0   100%   
urllib3.poolmanager         124      0   100%   
urllib3.request              33      0   100%   
urllib3.response            233      0   100%   
urllib3.util                  1      0   100%   
urllib3.util.connection      48      0   100%   
urllib3.util.request         20      0   100%   
urllib3.util.response        16      0   100%   
urllib3.util.retry           86      0   100%   
urllib3.util.ssl_            63      0   100%   
urllib3.util.timeout         48      0   100%   
urllib3.util.url             91      0   100%   
-------------------------------------------------------
TOTAL                      1473      0   100%   
----------------------------------------------------------------------
```

Example from one of the travis builds: https://travis-ci.org/shazow/urllib3/jobs/127431752#L246
"
853,Improve doc around Retry.status_forcelist and Retry.method_whitelist,2016-05-03T05:51:55Z,2016-05-03T06:38:39Z,,,,"I got confused by some ambiguities in the doc for the Retry class, so I propose these improvements. I've backed it up with a few more tests.
"
852,Unexpected redirect behavior,2016-05-01T02:21:36Z,2016-05-01T07:35:46Z,,,,"I am not sure if this is an issue for requests or urllib3.  requests is getting (incorrectly) stuck in a redirect loop for an image.
If I turn off redirects and manually step through the requests, each time pointing the new url to the value in the Location header, it works as expected (below is the response.headers + the response.status_code):

```
{'Content-Length': '178', 'Server': 'nginx', 'Connection': 'keep-alive', 'Location': 'https://wiki.wildberries.ru/img/2016/03/dffgfgdf-150x150.jpg', 'Date': 'Sun, 01 May 2016 01:27:25 GMT', 'Content-Type': 'text/html'} 301
{'Content-Length': '178', 'Set-Cookie': '__utmp=1', 'Server': 'nginx', 'Connection': 'close', 'Location': 'http://abenz.ru/wt', 'Date': 'Sun, 01 May 2016 01:27:26 GMT', 'Content-Type': 'text/html'} 301
{'Content-Length': '0', 'Expires': 'Thu, 21 Jul 1977 07:30:00 GMT', 'Keep-Alive': 'timeout=5, max=100', 'Server': 'Apache/2.4.10 (Debian)', 'Last-Modified': 'Sun, 01 May 2016 01:27:26 GMT', 'Connection': 'Keep-Alive', 'LOCATION': 'https://wiki.wildberries.ru/img/2016/03/dffgfgdf-150x150.jpg', 'Pragma': 'no-cache', 'Cache-Control': 'max-age=0', 'Date': 'Sun, 01 May 2016 01:27:26 GMT', 'Content-Type': 'text/html; charset=utf-8'} 302
{'Test-Head': '1', 'Accept-Ranges': 'bytes', 'Content-Length': '9753', 'Server': 'nginx', 'Last-Modified': 'Thu, 17 Mar 2016 15:14:49 GMT', 'Connection': 'close', 'ETag': '""56eac9e9-2619""', 'Date': 'Sun, 01 May 2016 01:27:27 GMT', 'Content-Type': 'image/jpeg'} 200
```

If I turn on logging while redirects is enabled, I see this.  Notice that once it hits the abenz[.]ru domain, the location is never updated.

```
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): wiki.wildberries.ru
send: 'GET /img/2016/03/dffgfgdf-150x150.jpg HTTP/1.1\r\nHost: wiki.wildberries.ru\r\nConnection: keep-alive\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nuser-agent: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/5.0)\r\n\r\n'
reply: 'HTTP/1.1 301 Moved Permanently\r\n'
header: Server: nginx
header: Date: Sun, 01 May 2016 01:50:21 GMT
header: Content-Type: text/html
header: Content-Length: 178
header: Connection: keep-alive
header: Location: https://wiki.wildberries.ru/img/2016/03/dffgfgdf-150x150.jpg
DEBUG:requests.packages.urllib3.connectionpool:""GET /img/2016/03/dffgfgdf-150x150.jpg HTTP/1.1"" 301 178
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): wiki.wildberries.ru
send: 'GET /img/2016/03/dffgfgdf-150x150.jpg HTTP/1.1\r\nHost: wiki.wildberries.ru\r\nConnection: keep-alive\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nuser-agent: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/5.0)\r\n\r\n'
reply: 'HTTP/1.1 301 Moved Permanently\r\n'
header: Server: nginx
header: Date: Sun, 01 May 2016 01:50:22 GMT
header: Content-Type: text/html
header: Content-Length: 178
header: Connection: close
header: Location: http://abenz.ru/wt
header: Set-Cookie: __utmp=1
DEBUG:requests.packages.urllib3.connectionpool:""GET /img/2016/03/dffgfgdf-150x150.jpg HTTP/1.1"" 301 178
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): abenz.ru
send: 'GET /wt HTTP/1.1\r\nHost: abenz.ru\r\nConnection: keep-alive\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nuser-agent: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/5.0)\r\n\r\n'
reply: 'HTTP/1.1 302 Found\r\n'
header: Date: Sun, 01 May 2016 01:50:23 GMT
header: Server: Apache/2.4.10 (Debian)
header: Expires: Thu, 21 Jul 1977 07:30:00 GMT
header: Last-Modified: Sun, 01 May 2016 01:50:23 GMT
header: Cache-Control: max-age=0
header: Pragma: no-cache
header: LOCATION: https://wiki.wildberries.ru/img/2016/03/dffgfgdf-150x150.jpg
header: Content-Length: 0
header: Keep-Alive: timeout=5, max=100
header: Connection: Keep-Alive
header: Content-Type: text/html; charset=utf-8
DEBUG:requests.packages.urllib3.connectionpool:""GET /wt HTTP/1.1"" 302 0
send: 'GET /wt HTTP/1.1\r\nHost: abenz.ru\r\nConnection: keep-alive\r\nCookie: __utmp=1\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nuser-agent: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/5.0)\r\n\r\n'
reply: 'HTTP/1.1 302 Found\r\n'
header: Date: Sun, 01 May 2016 01:50:24 GMT
header: Server: Apache/2.4.10 (Debian)
header: Expires: Thu, 21 Jul 1977 07:30:00 GMT
header: Last-Modified: Sun, 01 May 2016 01:50:24 GMT
header: Cache-Control: max-age=0
header: Pragma: no-cache
header: LOCATION: https://wiki.wildberries.ru/img/2016/03/dffgfgdf-150x150.jpg
header: Content-Length: 0
header: Keep-Alive: timeout=5, max=99
header: Connection: Keep-Alive
header: Content-Type: text/html; charset=utf-8
DEBUG:requests.packages.urllib3.connectionpool:""GET /wt HTTP/1.1"" 302 0
send: 'GET /wt HTTP/1.1\r\nHost: abenz.ru\r\nConnection: keep-alive\r\nCookie: __utmp=1\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nuser-agent: Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/5.0)\r\n\r\n'
...
```

The calls above are done via:

```
h = {'user-agent': 'Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/5.0)'}
with requests.Session() as s:
    r = s.get(url, headers = h, timeout = HTTP_TIMEOUT_SECONDS) #, allow_redirects = False)
```
"
851,Correct docstring for Retry backoff factor,2016-04-30T17:54:43Z,2016-04-30T18:30:54Z,,,,"Correct the docstring for the backoff_factor argument to the Retry object, to reflect the fact that a wait is not applied before the second try, as per issue #847 
"
850,Should reset connection pools after fork,2016-04-21T07:31:24Z,2016-04-21T18:11:26Z,,,,"Consider this scenario:

process A creates a connection pool and send a request to `url0`, and then fork twice. Then we got process B & C.
process B and C then send another request to `url0` at nearly the same moment, then the underlying connection they used would be the identical one, causing confused response.
"
849,SessionManager (cookies and autoconfiguration),2016-04-20T05:32:05Z,2016-06-22T16:30:28Z,Ready,,,"Implements #843; supersedes #844.
"
848,Attach last Retry object to response and add Retry history,2016-04-18T18:51:30Z,2016-07-05T07:51:34Z,,,,"This PR is trying to solve both Issue #576 and #578.
The last Retry object of the request is added to the response object. Thus allowing the user to check if any errors occurred during the request. In addition to give more information to the user the Retry object now has a history attribute. This attribute contains a list of the errors that were thrown at each connection retry.

example code:

```
retry = Retry(total=2)
resp = pool.request('GET', '/', retries=retry)
if resp.retries.total != 2:
    print(resp.retries.history)  # will show list of errors
```
"
847,Retry backoff behavior inconsistent with documentation,2016-04-16T21:14:42Z,2016-07-22T14:01:06Z,,,,"The docstring for urllib3.util.retry.Retry indicates that ""If the backoff_factor is 0.1, then :func:`.sleep` will sleep for [0.1s, 0.2s, 0.4s, ...] between retries.""

That isn't consistent with the actual behavior on the first retry, as when `_observed_errors == 1` the sleep function returns 0, so there is no wait after the first retry. This behavior is specifically covered by a unit test requiring the first sleep to return 0. According to the actual behavior, the documentation should read ""[0.0s, 0.2s, 0.4s, ...] between retries."", and probably also note that there is no wait after the first retry.

I'm not sure what the intended behavior is, or what it should be in principle. My vote would be for a wait after the first retry, and I would be happy to contribute a quick fix in either case.
"
846,pyopenssl: add support for IP Address SAN,2016-04-16T02:39:06Z,2017-03-30T14:50:00Z,,,,"See docker/docker-py#1037
"
845,Upgrade to six release 1.10.0,2016-04-16T01:01:50Z,2016-04-17T01:43:13Z,,,,"Blocker for #844.
"
844,Add cookie support,2016-04-15T19:44:03Z,2016-04-20T12:35:04Z,,,,"Initial implementation of #843; updates the bundled copy of `six` to v1.10.
"
843,Support cookies when the PoolManager or ConnectionPool is passed a CookieJar object,2016-04-13T18:45:09Z,,,,,"I think I can support cookie handling with the stdlib `CookieJar` object without too much trouble, and wanted to get some thoughts on implementation first.

The biggest change is that both `CookieJar.extract_cookies` and `CookieJar.add_cookie_header` both expect to work with a `urllib.request.Request`-like object. We can work around this by temporarily loading basic request information (essentially URL) into the appropriate object and then pull the modifications back out, but if we're doing that, it may also be worthwhile to consider a `urllib3.request.Request` object that can be passed around like `urllib.request.Request`.

The second, more minor, change is that `CookieJar.extract_cookies` expects an `info()` method on the response which returns response headers. Given that there's an existing urllib3 header API, for this case, I'd rather just build a disposable shim to give `CookieJar` what it needs.

Regardless, thoughts are appreciated on two questions: is this an appropriate feature for urllib3 to support, and if so, is using the stdlib to this extent an appropriate way to implement it?
## 
"
842,Streamline use of OS-native certificates for HTTPS validation,2016-04-13T04:02:36Z,,,,,"In considering the documentation at:
- https://urllib3.readthedocs.org/en/latest/security.html#using-your-system-s-root-certificates

It is easy to use the Mozilla-provided certifications for HTTPS validation. However these certificates may vary from the OS-native certificates. In such a case, urllib3 might fail to fetch a URL that a native browser app would succeed in fetching. This behavior is confusing to the user.

I propose adding an easy-to-use function to urllib3 that attempts to find the OS-native certificates automatically, so they can then be used for HTTPS validation. Something to automate the platform-specific certificate search alluded to from the StackOverflow post linked from the documentation link above.

Is this something that you (the maintainers) would be interested in supporting?
## 
"
841,Make sure we distribute backports.,2016-04-11T07:19:18Z,2016-04-11T12:40:09Z,,,,"Resolves #839.
"
840,Disables IPv6 DNS when IPv6 connections are not possible,2016-04-11T02:56:02Z,2016-04-17T01:41:02Z,,,,"Fixes #838 

Note that this will only work in cases where either Python has been compiled without IPv6 support or the host has IPv6 networking disabled (an exception is raised when opening a socket to `::1`); beyond that, we have no real way to check IPv6 availability.

Credit where it's due, this is existing code from the test module with unit tests added around it.
"
839,PyOpenSSL: Fails to import after installation with the provided instructions,2016-04-11T01:04:31Z,2016-04-11T12:40:09Z,,ImportError,ImportError: cannot import name '_fileobject',"The [Security documentation page](https://urllib3.readthedocs.org/en/latest/security.html#openssl-pyopenssl) indicates that to use PyOpenSSL that you must first install:
- pip3 install pyopenssl ndg-httpsclient pyasn1

And then in a Python interpreter run:
- import urllib3.contrib.pyopenssl
- urllib3.contrib.pyopenssl.inject_into_urllib3()

However on Python 3.4.4 (the latest 3.4.x) on OS X 10.11 (the latest OS X) I get the following traceback when executing `import urllib3.contrib.pyopenssl`:

```
Traceback (most recent call last):
  File ""/Users/davidf/Projects/webcrystal/venv/lib/python3.4/site-packages/urllib3/contrib/pyopenssl.py"", line 60, in <module>
    from socket import _fileobject
ImportError: cannot import name '_fileobject'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/davidf/Projects/webcrystal/venv/lib/python3.4/site-packages/urllib3/contrib/pyopenssl.py"", line 63, in <module>
    from urllib3.packages.backports.makefile import backport_makefile
ImportError: No module named 'urllib3.packages.backports'
```

Perhaps there is some missing step in the documentation?
"
838,urllib3 attempts to use IPv6 even when IPv6 is disabled,2016-04-10T21:38:26Z,2016-04-17T01:41:02Z,,OSError,OSError: [Errno 97] Address family not supported by protocol,"This is an issue when running on a server without IPv6 (must be disabled because the network does not support it). Example when connecting to https://graph.facebook.com using requests and IPv4 happens to fail:

```
HTTPSConnectionPool(host='graph.facebook.com', port=443): Max retries exceeded with url: /v2.5/me/feed (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f4dbd158518>: Failed to establish a new connection: [Errno 97] Address family not supported by protocol',))
Traceback (most recent call last):
  File ""/home/lib/python3.4/site-packages/requests/packages/urllib3/connection.py"", line 137, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File ""/home/lib/python3.4/site-packages/requests/packages/urllib3/util/connection.py"", line 91, in create_connection
    raise err
  File ""/home/lib/python3.4/site-packages/requests/packages/urllib3/util/connection.py"", line 71, in create_connection
    sock = socket.socket(af, socktype, proto)
  File ""/usr/lib/python3.4/socket.py"", line 126, in __init__
    _socket.socket.__init__(self, family, type, proto, fileno)
OSError: [Errno 97] Address family not supported by protocol
```

urllib3 should throw an exception after exhausting all IPv4 options instead of trying (and invariably failing) IPv6.

See closed issue https://github.com/kennethreitz/requests/issues/3084.
"
837,"Some HTTPS URLs fail to download with ""sslv3 alert handshake failure""",2016-04-08T15:15:59Z,2016-04-08T15:19:58Z,,,,"### Environment
- urllib 1.15
- Python 3.4.0rc1
- OS X 10.11.3 (15D21) - El Capitan
### Repro Steps

```
>>> import urllib3
>>> urllib3.__version__
'1.15'
>>> url = 'https://files.yande.re/sample/02a50945798910f0f13dcab3123e698b/yande.re%20351635%20sample%20bou_nin%20kimono%20landscape%20umbrella.jpg'
>>> http = urllib3.PoolManager()
>>> response = http.request(method='GET', url=url)
```
### Expected Results

No exception. Binary contents of URL available in `response.data`.
### Actual Results

Following exceptions thrown:
- ssl.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:598)
- urllib3.exceptions.SSLError: [SSL: SSLV3_ALERT_HANDSHAKE_FAILURE] sslv3 alert handshake failure (_ssl.c:598)

Full traceback at: https://gist.github.com/davidfstr/9ec1db7dfd1ce5fd3565d88ece4b7e6b
### Notes

I think I remember stumbling across a similar issue in the past. As I recall it was related to an SSL library on OS X being an outdated version, or similar.
"
836,Add support for HTTP Alternative Services,2016-04-08T08:24:48Z,,Someday,,,"HTTP Alternative Services are outlined in [RFC 7838](https://www.rfc-editor.org/rfc/rfc7838.txt). From the text:

> HTTP conflates the identification of resources with their location.  In other words, ""http://"" and ""https://"" URIs are used to both name and find things to interact with.
> 
> In some cases, it is desirable to separate identification and location in HTTP; keeping the same identifier for a resource, but interacting with it at a different location on the network.
> 
> For example:
> - An origin server might wish to redirect a client to a different server when it is under load, or it has found a server in a location that is more local to the client.
> - An origin server might wish to offer access to its resources using a new protocol, such as HTTP/2, or one using improved security, such as Transport Layer Security (TLS).
> - An origin server might wish to segment its clients into groups of capabilities, such as those supporting Server Name Indication (SNI), for operational purposes.
> 
> This specification defines a new concept in HTTP, ""Alternative Services"", that allows an origin server to nominate additional means of interacting with it on the network.

The main interest I have in this specification is that it's likely to be the building block for HTTP Opportunistic Encryption, which I'd very much like urllib3 to support.

In the short term I don't expect anyone to pick this work up: it's not an enormously high priority for me right now, certainly. However, I want an issue to track it, as this will allow anyone who wants this feature to put a bounty on it to get the work done.

Flagging this as ""someday"".
## 
"
835,[Second Attempt] Add ssl_context parameter to Connection classes,2016-04-07T12:03:42Z,2016-06-22T17:15:23Z,Ready,,,"This builds upon the work done by @sigmavirus24 in #507.

The differences between this PR and the original are:
- The tests pass! The original PR had failing tests, and was written at a time when we didn't test PyOpenSSL at all so those tests failed _miserably_. This patch brings everything up to speed.
- Adds PyOpenSSL support, including a new wrapper context object for PyOpenSSL that has a similar interface to the builtin context. The goal here is to keep the experience the same for users who are using PyOpenSSL contexts. As a side benefit, this now allows users to use both PyOpenSSL and the standard library SSL module at the same time if they so choose (by passing appropriate context objects).
- Refactors the code to continue to use as much of the original code as possible. @sigmavirus24 had hoisted a lot of code out of `urllib3.util.ssl_.ssl_wrap_socket`, but that method now takes a `context` argument, which means that we really should just re-use it. This patch has been adjusted to do that, ensuring that code coverage remains high and that logic exists in as few places as possible.

If @sigmavirus24 is happy to agree to it, I'd say that this patch supersedes #507.
"
834,Updating README to note dropping Python 3.2 compatibility,2016-04-06T21:15:45Z,2016-04-06T21:27:28Z,,,,"See title.
"
833,PoolManagers are overly case-sensitive.,2016-04-05T09:03:28Z,2016-07-01T15:41:26Z,,,,"In a URI, both scheme and authority are case-insensitive: that is, the scheme, host, and port should all be considered case insensitively (and basically treated as lower-case).

Unfortunately, the PoolManager and ProxyManager do not consistently do that. The biggest problem is in the ProxyManager, where `connection_from_host` will incorrectly treat any HTTPS scheme with a capital letter in it like a HTTP connection (which isn't great), but the flaw is pretty common elsewhere in those objects (e.g. an uppercased host will get a different connection pool to a lowercased host).

At the very minimum we should be using a lowercased scheme at all points in the urllib3 stack. We should also consider whether we want to be case-sensitive about domains (I'm inclined to say that we do not).

Related: kennethreitz/requests#3075.
"
832,TypeError: unsupported operand type(s) for -=: 'Retry' and 'int',2016-04-02T22:16:44Z,2016-04-03T00:35:15Z,,TypeError,TypeError: unsupported operand type(s) for -=: 'Retry' and 'int',"I am getting this error that i saw in issue #567 whenever i retry a request. I am using python 2.7.11. i have requests version 2.9.1 installed but i did not install urllib3

```
Traceback (most recent call last):
  File ""/home/vagrant/development/demo/api.py"", line 223, in make_request
    response = session.post('https://httpbin.org/status/500',data=self.REQUEST_DICT,timeout=15)
  File ""/home/vagrant/.virtualenvs/vdemo/lib/python2.7/site-packages/requests/sessions.py"", line 511, in post
    return self.request('POST', url, data=data, json=json, **kwargs)
  File ""/home/vagrant/.virtualenvs/vdemo/lib/python2.7/site-packages/requests/sessions.py"", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/vagrant/.virtualenvs/vdemo/lib/python2.7/site-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/home/vagrant/.virtualenvs/vdemo/lib/python2.7/site-packages/requests/adapters.py"", line 376, in send
    timeout=timeout
  File ""/home/vagrant/.virtualenvs/vdemo/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 657, in urlopen
    retries = retries.increment(method, url, response=response, _pool=self)
  File ""/home/vagrant/.virtualenvs/vdemo/lib/python2.7/site-packages/requests/packages/urllib3/util/retry.py"", line 228, in increment
    total -= 1
TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'

```

```
import logging
import requests
from requests.packages.urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
from .exceptions import PaymentError, RequestError, GatewayError
import httplib
import urllib
import xml.dom.minidom

class RetryAdapter(HTTPAdapter):
    def __init__(self, *args, **kwargs):
        super(RetryAdapter, self).__init__(*args, **kwargs)
        method_whitelist=frozenset(['POST','HEAD', 'TRACE', 'GET', 'PUT', 'OPTIONS', 'DELETE'])
        self.max_retries = Retry(total=self.max_retries, backoff_factor=5, status_forcelist=[429, 500, 502, 503], method_whitelist=method_whitelist)

class Base(object):

    def make_request(self, uri):
        session = requests.Session()
        session.mount('https://httpbin.org/status/500', RetryAdapter(max_retries=2))

        try:
            response = session.post('https://httpbin.org/status/500',data=self.REQUEST_DICT,timeout=15)
            response.raise_for_status()

        except requests.exceptions.ConnectionError as e:
            logger.exception(e.message)
            raise GatewayError('Service temporarily unavailable. Please try again')

        except requests.exceptions.HTTPError as e:
            logger.exception(e.message)
            if self.REQUEST_DICT['mode'] == 'query_trans':
                raise GatewayError('Service temporarily unavailable. Please try again')
            else:              
                raise PaymentError('Error getting response from gateway')

        except requests.exceptions.ConnectTimeout as e:
            logger.exception(e.message)
            raise GatewayError('Service temporarily unavailable. Please try again')

        except requests.exceptions.ReadTimeout as e:
            logger.exception(e.message)
            raise GatewayError('Service temporarily unavailable. Please try again')

        except requests.exceptions.RequestException as e:
            logger.exception(e.message)
            raise GatewayError('Service temporarily unavailable. Please try again')            
        except Exception as e:
            logger.exception(e.message)
            raise GatewayError('Service temporarily unavailable. Please try again')

        return response.text
```
"
831,Release v1.15,2016-03-30T18:46:20Z,2016-04-06T19:19:04Z,,,,"[CHANGES](https://github.com/shazow/urllib3/blob/master/CHANGES.rst) is getting pretty meaty. Any last-minute stuff we want to include before I cut v1.15?

/cc @Lukasa @sigmavirus24 and anyone else who wishes to comment
"
830,Create connection pool keys by scheme,2016-03-28T19:44:03Z,2016-04-21T17:56:28Z,Ready,,,"This commit modifies how the PoolManager stores and retrieves `ConnectionPool` objects for later use. It adds a new instance variable to the class, `pool_keys_by_scheme`, which allows a user to define a list of fields from `connection_pool_kw` to consider (by scheme) when storing and retrieving pools.

This is my first stab at addressing the feedback from PR #751. It may or (much more likely) may not be what everyone had in mind, so I thought I would make a pull request before spending too much time polishing. Thanks for taking a look!
"
829,Installing urllib3,2016-03-28T09:27:50Z,2016-03-28T09:33:12Z,,"ValueError, ImportError","ValueError: unsupported hash type md5, ImportError: No module named urllib3","Hi, 

I came here because my pip does not do anything because it say No module named urllib3. However, when I try to install urllib3 with pip it gives me the same error. I guess I messed up my pip. Could you guide me to manually install pip??

Thanks in advance,

Best,

Tunc.

This is the error that I get while installing urllib3.

```

Tuncs-MacBook-Pro:local morova$ pip install urllib3
ERROR:root:code for hash md5 was not found.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>
    globals()[__func_name] = __get_hash(__func_name)
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type md5
ERROR:root:code for hash sha1 was not found.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>
    globals()[__func_name] = __get_hash(__func_name)
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type sha1
ERROR:root:code for hash sha224 was not found.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>
    globals()[__func_name] = __get_hash(__func_name)
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type sha224
ERROR:root:code for hash sha256 was not found.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>
    globals()[__func_name] = __get_hash(__func_name)
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type sha256
ERROR:root:code for hash sha384 was not found.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>
    globals()[__func_name] = __get_hash(__func_name)
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type sha384
ERROR:root:code for hash sha512 was not found.
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 147, in <module>
    globals()[__func_name] = __get_hash(__func_name)
  File ""/usr/local/Cellar/python/2.7.10/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py"", line 97, in __get_builtin_constructor
    raise ValueError('unsupported hash type ' + name)
ValueError: unsupported hash type sha512
Traceback (most recent call last):
  File ""/usr/local/bin/pip"", line 7, in <module>
    from pip import main
  File ""/usr/local/lib/python2.7/site-packages/pip/__init__.py"", line 15, in <module>
    from pip.vcs import git, mercurial, subversion, bazaar  # noqa
  File ""/usr/local/lib/python2.7/site-packages/pip/vcs/mercurial.py"", line 9, in <module>
    from pip.download import path_to_url
  File ""/usr/local/lib/python2.7/site-packages/pip/download.py"", line 39, in <module>
    from pip._vendor import requests, six
  File ""/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/__init__.py"", line 58, in <module>
    from . import utils
  File ""/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/utils.py"", line 26, in <module>
    from .compat import parse_http_list as _parse_list_header
  File ""/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/compat.py"", line 7, in <module>
    from .packages import chardet
  File ""/usr/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/__init__.py"", line 29, in <module>
    import urllib3
ImportError: No module named urllib3
```
"
828,Make each PoolManager get a copy of pool_classes_by_scheme,2016-03-28T02:13:01Z,,Ready,,,"Each PoolManager should have its own copy of `pool_classes_by_scheme` to modify as it sees fit without changing other PoolManager dictionaries.

I ran across this while working on issue #751 and I thought it would be a nice way to get acquainted with the pull request review process for this project. 
"
827,Preserve order of response headers,2016-03-25T02:37:43Z,2016-03-29T08:28:02Z,,,,"Patch for issue: https://github.com/shazow/urllib3/issues/821
"
826,ImportModule error ngd-httpsclient,2016-03-24T02:00:46Z,2016-03-24T02:13:31Z,,ImportError,ImportError: No module named ndg.httpsclient.ssl_peer_verification,"Using AWS Lambda to send push notifications to Pusher app in Python. When I install Pusher and all its dependencies to a directory and zip up to Lambda I run a simple test and get this error.

`No module named ndg.httpsclient.ssl_peer_verification`
Here is the code I'm trying to run.

```
    from pusher import Pusher

    pusher = Pusher(app_id=u'id', key=u'key', secret=u'secret')

    def createPitchZip(context, event):

        pusher.trigger('testchannel', 'testevent', {u'some': u'data'})
```

I've seen several posts about this but installing the dependencies individually doesn't seem to be helping.

Here is the stack trace

```
Traceback (most recent call last):
    File ""pusherlambda.py"", line 2, in <module> 
         pusher = Pusher(app_id=u'*****', key=*****', secret=u'****')
    File ""/home/vagrant/Code/Lamdba/venv/lib/python2.7/site-packages/pusher/pusher.py"", line 42, in __init__
         from pusher.requests import RequestsBackend
    File ""/home/vagrant/Code/Lamdba/venv/lib/python2.7/site-packages/pusher/requests.py"", line 12, in <module>
        import urllib3.contrib.pyopenssl
    File ""/home/vagrant/Code/Lamdba/venv/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py"", line 49, in <module>
        from ndg.httpsclient.ssl_peer_verification import SUBJ_ALT_NAME_SUPPORT
ImportError: No module named ndg.httpsclient.ssl_peer_verification
```

Went from pusher repo issue to requests repo issue and the sent me over this way. Banging my head off the wall for this one. Any help would be appreciated. 
"
825,Preserve order of response headers,2016-03-23T05:57:45Z,2016-03-25T02:38:08Z,,,,"Patch for issue: https://github.com/shazow/urllib3/issues/821
"
824,"Tests fail due to missing ""socks"" dependency",2016-03-23T01:36:46Z,2016-03-23T03:27:15Z,,ImportError,ImportError: No module named 'socks',"#### Repro Steps

```
# (Install Docker from: http://www.docker.com )

# If OS X, start with:
$ docker-machine start default; eval ""$(docker-machine env default)""

# If OS X or Linux, continue with:
$ docker run -it ubuntu:trusty  # Ubuntu 14.04 LTS
$$ apt-get update
$$ apt-get install git -y
$$ apt-get install python3-pip -y
$$ pip3 install virtualenv
$$ git clone https://github.com/shazow/urllib3
$$ cd urllib3/
$$ virtualenv venv
$$ source venv/bin/activate
$$ make test
```
#### Output

https://gist.github.com/davidfstr/b3697cf83b1935f24ef5

In particular:

```
ERROR: Failure: ImportError (No module named 'socks')
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/urllib3/venv/lib/python3.4/site-packages/nose/failure.py"", line 39, in runTest
    raise self.exc_val.with_traceback(self.tb)
  File ""/urllib3/venv/lib/python3.4/site-packages/nose/loader.py"", line 418, in loadTestsFromName
    addr.filename, addr.module)
  File ""/urllib3/venv/lib/python3.4/site-packages/nose/importer.py"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/urllib3/venv/lib/python3.4/site-packages/nose/importer.py"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/urllib3/venv/lib/python3.4/imp.py"", line 235, in load_module
    return load_source(name, filename, file)
  File ""/urllib3/venv/lib/python3.4/imp.py"", line 171, in load_source
    module = methods.load()
  File ""<frozen importlib._bootstrap>"", line 1220, in load
  File ""<frozen importlib._bootstrap>"", line 1200, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1129, in _exec
  File ""<frozen importlib._bootstrap>"", line 1471, in exec_module
  File ""<frozen importlib._bootstrap>"", line 321, in _call_with_frames_removed
  File ""/urllib3/test/contrib/test_socks.py"", line 4, in <module>
    from urllib3.contrib import socks
  File ""/urllib3/urllib3/contrib/socks.py"", line 21, in <module>
    import socks
ImportError: No module named 'socks'
```
#### Notes

Probably some version of PySocks should be declared in `dev-requirements.txt`.
"
823,Fixing setup.py support in ASCII locales,2016-03-22T15:45:05Z,2016-03-22T15:54:28Z,,,,"Fixes #822
"
822,Cannot run tests on Ubuntu 14.04 LTS + Python 3,2016-03-22T02:59:20Z,2016-03-22T15:54:28Z,,UnicodeDecodeError,UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 4597: ordinal not in range(128),"#### Repro Steps

```
# (Install Docker from: http://www.docker.com )

# If OS X, start with:
$ docker-machine start default; eval ""$(docker-machine env default)""

# If OS X or Linux, continue with:
$ docker run -it ubuntu:trusty  # Ubuntu 14.04 LTS
$$ apt-get update
$$ apt-get install git -y
$$ apt-get install python3-pip -y
$$ pip3 install virtualenv
$$ git clone https://github.com/shazow/urllib3
$$ cd urllib3/
$$ virtualenv venv
$$ source venv/bin/activate
$$ make test
```
#### Output

```
python setup.py develop
Traceback (most recent call last):
  File ""setup.py"", line 23, in <module>
    long_description=open('README.rst').read() + '\n\n' + open('CHANGES.rst').read(),
  File ""/urllib3/venv/lib/python3.4/encodings/ascii.py"", line 26, in decode
    return codecs.ascii_decode(input, self.errors)[0]
UnicodeDecodeError: 'ascii' codec can't decode byte 0xf0 in position 4597: ordinal not in range(128)
make: *** [*.egg-info] Error 1
```
#### Notes

My best guess is that the `read()`s in setup.py should have an encoding specified. Probably UTF-8.
"
821,Ordering of HTTP response headers is not preserved,2016-03-18T05:21:50Z,2016-03-29T18:09:36Z,,,,"...although I note that the ordering of the HTTP _request_ headers _are_ preserved. Strange.
#### Environment
- Python 3.4.1
- urllib3 1.14
#### Repro Steps
- Download test program from this gist: https://gist.github.com/davidfstr/fd0c179a232ffbe7798c
- In terminal 1, run: python3 reflect_server.py 
- In terminal 2, run several times: python3 reflect_client_urllib3.py
#### Expected Results
- The response headers in terminal 2 should be printed in the same order each time, since the server running in terminal 1 always sends the same byte-for-byte response.
#### Actual Results
- The response headers are printed in a non-deterministic order.
"
820,Show certificate when there is a hostname mismatch,2016-03-17T15:41:24Z,2016-03-21T17:59:53Z,Ready,"requests.exceptions.SSLError, 'notBefore'","requests.exceptions.SSLError: hostname '10.10.194.37' doesn't match u'vault.service.trouper', 'notBefore': u'Mar 17 01:18:27 2016 GMT', 'serialNumber': u'9BFDB58B08E7DCCA', 'notAfter': 'Mar 15 01:18:27 2026 GMT', 'version': 3L,","I am getting a hostname mismatch error for one of three servers in a pool so I am suspecting that one server has an invalid certificate.

```
requests.exceptions.SSLError: hostname '10.10.194.37' doesn't match u'vault.service.trouper'
```

by itself is not much info to go on. It would be nice to see the server certificate that was presented.

I found that it was surprisingly difficult to see the server certificate, so I decided to see if urllib3 could show the certificate.

With this, urllib3 logs an error with the certificate just before raising the exception:

```
ERROR:requests.packages.urllib3.connection:Certificate did not match expected hostname: 10.10.194.38.
Certificate: {'subjectAltName': (('IP Address', '127.0.0.1'), ('IP Address', '10.10.194.36'), ('IP Address', '10.10.194.37'), ('IP Address', '10.10.194.38')),
'notBefore': u'Mar 17 01:18:27 2016 GMT', 'serialNumber': u'9BFDB58B08E7DCCA', 'notAfter': 'Mar 15 01:18:27 2026 GMT', 'version': 3L,
'subject': ((('countryName', u'US'),), (('stateOrProvinceName', u'California'),), (('localityName', u'Palo Alto'),), (('organizationName', u'DevOps'),), (('commonName', u'vault
```

which allows me to check whether the server is presenting the correct certificate.

I also add the certificate to the exception that is raised so that client code can inspect it.
"
819,Problem with Headers[Location],2016-03-16T22:01:26Z,2016-03-16T23:35:27Z,,"#except, except","#except:, except:","Good Night.
I have this code below
Python 3.4

```
 proxy = urllib3.ProxyManager('http://apache_redirect_address')
 now=time.time()
 URL=url.strip()
 URL=estrelas.globo.com
 g=proxy.request('GET',URL,redirect=False,timeout=5)
 status=g.status
 print(status)
#output 301 here

 print(g.headers['Location'])
#output http://www.globo.com

 print(g.headers)
#output HTTPHeaderDict({'Connection': 'close', 'Date': 'Wed, 16 Mar 2016 21:46:22 GMT', 'Content-Length': '228', 'Location': 'http://www.globo.com', 'Server': 'Apache', 'Content-Type': 'text/html; charset=iso-8859-1'})
```

This information above does not correct this, the output below is correct

```
http_proxy=apache_redirect_addres curl -I estrelas.globo.com
HTTP/1.1 301 Moved Permanently
Date: Wed, 16 Mar 2016 21:51:16 GMT
Server: Apache
Location: http://gshow.globo.com/programas/estrelas/index.html
Connection: close
Content-Type: text/html; charset=iso-8859-1
```

If I use this code in a FOR loop, the first loop I return the wrong Location, but in the next work properly

Below I'm sending all my code function

`def RedirectFarmTest(url,array_redir): #Apenas para testes de redir onde o redir sera o proxy

  print(""Iniciando testes da farma de redir"")
  saida=[]
  linha=[]
  proxy=""""
  print(""Printando o array de redirect dentro da redirectfarmtest"")
  print(array_redir)

  for i in array_redir:

```
#print(i)
try: 
 proxy = urllib3.ProxyManager('http://'+str(i)+':80')
 now=time.time()
 URL=url.strip()
#http=urllib3.PoolManager()
 g=proxy.request('GET',URL,redirect=False,timeout=2)
#status=r.status
#print(status)
#except:
#print(""Deu merda no Redirect server"")
 status=g.status
 print(status)
 #print(r)
#dentro do modulo Url existe as opcoes para fazer o teste do redir
 print(url)
 print(URL)
 print(g.headers['Location'])
 rtime=time.time() - now
 temp={""status"":status,""url_in"":url,""url_out"":g.headers['Location'],""tempo"":rtime,""redir_server"":i,""erro"":""0""}
 saida.append(temp)
 #http=""""
except:
 rtime=time.time() - now
 temp={""status"":404,""url_in"":url,""url_out"":"""",""tempo"":rtime,""redir_server"":i,""erro"":""1""}
 saida.append(temp)
```

  print(""Dentro do Redirect com erro"")
  print(saida)
  return(saida)

can you help me?
thank you

`
"
818,Handling errors around Unicode encoding differently,2016-03-16T18:43:19Z,2016-03-16T20:03:34Z,,,,"Fixes #793; allows unicode characters to come in in the form of a Python 2 bytes/string object without uncaught exceptions.
"
817,Append warnings.,2016-03-16T13:32:06Z,2016-03-16T16:05:50Z,,,,"Should resolve #816.
"
816,Honor PYTHONWARNINGS for SNIMissingWarning,2016-03-16T13:10:29Z,2016-03-16T16:05:50Z,,,,".local/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:315: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#snimissingwarning.
  SNIMissingWarning
.local/lib/python2.7/site-packages/requests/packages/urllib3/util/ssl_.py:120: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For more information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.
  InsecurePlatformWarning

I'm not here to discuss the validity of those warnings, but when I tried to get rid of them with the PYTHONWARNINGS environment variable, it only works for the InsecurePlatformWarning one. The other is always displayed whatever I put in PYTHONWARNINGS.

Please advise what PYTHONWARNINGS value to use to get rid of this one, this is a documented python feature: 
https://docs.python.org/2/library/warnings.html
https://docs.python.org/2/using/cmdline.html?highlight=pythonwarnings#envvar-PYTHONWARNINGS

Also seen the same problem here: in https://github.com/shazow/urllib3/issues/497

Thanks
"
815,Reordering of parse_url to handle separator characters inside auth information,2016-03-15T15:11:25Z,2016-03-16T12:05:31Z,,,,"Fixes #814.
"
814,"parse_url fails when given credentials in the URL with '/', '#', or '?'",2016-03-15T15:10:48Z,2016-03-16T12:08:16Z,,,,"Technically, this behavior is compliant with RFC-3986 in that the ""authority"" section should end at the next instance of one of those characters. However, RFC-2617, which specifies the HTTP Basic Auth scheme, does not specifically exclude those characters as valid, which means that there are valid RFC-2617 usernames and passwords which cannot currently be transmitted inside the URL for handling.

This behavior has received an issue filing on pip in the context of an HTTP proxy.

The attached pull request changes behavior such that any authorization information (separated from the rest of the URL by the reserved character '@') is pulled from the URL prior to scanning for the end of the authority section (separated from the rest of the URL by the aforementioned reserved characters). The PR also includes regression tests to ensure that this behavior change stays in place.

Sources:
https://github.com/pypa/pip/issues/3267
http://tools.ietf.org/html/rfc3986#section-3.2
https://tools.ietf.org/html/rfc2617#section-2
"
813,New PyOpenSSL requires bytes for cipher list.,2016-03-14T18:24:53Z,2016-03-14T19:02:09Z,,,,"PyOpenSSL is planning to add urllib3 as a tested project to make sure that changes there don't break here (that's really convenient and very kind of @hynek). I should note that we only managed to get here because we added PyOpenSSL to our own testing matrix in #795, which is a change I'm feeling increasingly good about!

The current master (and therefore next release) of PyOpenSSL currently works fine with urllib3 _except_ that it emits a deprecation warning when we pass a `str` as the default cipher list. This change fixes that warning, brings our PyOpenSSL tests back to green against the current PyOpenSSL master, and allows the merging of pyca/pyopenssl#446.
"
812,unhandled UnicodeEncodeError,2016-03-06T20:24:49Z,2016-03-06T20:29:27Z,,,,"```
Traceback (most recent call last):
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\main.py\"",
    line 444, in execute
        if send_heartbeat(**kwargs):
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\main.py\"",
    line 356, in send_heartbeat
        proxies=proxies, timeout=timeout)
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\sessions.py\"",
    line 511, in post
        return self.request('POST', url, data=data, json=json, **kwargs)
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\sessions.py\"",
    line 468, in request
        resp = self.send(prep, **send_kwargs)
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\sessions.py\"",
    line 576, in send
        r = adapter.send(request, **kwargs)
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\adapters.py\"",
    line 376, in send
        timeout=timeout
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\packages\\urllib3\\connectionpool.py\"",
    line 559, in urlopen
        body=body, headers=headers)
      File \""C:\\Users\\Marcin\\AppData\\Roaming\\Sublime Text 3\\Packages\\WakaTime\\packages\\wakatime\\packages\\requests\\packages\\urllib3\\connectionpool.py\"",
    line 353, in _make_request
        conn.request(method, url, **httplib_request_kw)
      File \""http\\client.py\"",
    line 1083, in request
      File \""http\\client.py\"",
    line 1123, in _send_request
      File \""http\\client.py\"",
    line 1055, in putheader
    UnicodeEncodeError: 'latin-1' codec can't encode character '\\u017c' in position 2: ordinal not in range(256)
```

Possibly caused by unicode in a header value.

Originally reported at wakatime/sublime-wakatime#62
"
811,installing urllib3[secure] into a virtualenv breaks pip,2016-03-04T01:34:49Z,2016-03-04T08:19:05Z,,,,"I created a virtualenv, activated it, and ran `pip install urllib3[secure]`. Every subsequent call to `pip` then fails:

```
$ pip install argparse
Downloading/unpacking argparse
  Cannot fetch index base URL https://pypi.python.org/simple/
  Could not find any downloads that satisfy the requirement argparse
Cleaning up...
No distributions at all found for argparse
Storing debug log for failure in /nail/home/dgholz/.pip/pip.log
```

I checked `https://pypi.simple.org`. and it seems that the certificate has a common name of '`www.python.org`', which doesn't match the host name '`pypi.python.org`'.
"
810,AttributeError on some response,2016-03-02T04:24:18Z,2016-03-02T04:26:56Z,,`AttributeError,`AttributeError: 'bool' object has no attribute 'close'`,"access url like `http://www.shushuwu.net/top/postdate_1.html` will raise:
`AttributeError: 'bool' object has no attribute 'close'`
"
809,Support with-statement on response objects to improve protection against connection leaking,2016-03-01T08:36:18Z,,,KeyError,KeyError: 'asdfasdf',"As already briefly discussed in #805 there is still a scenario for leaking connections from the connection pool. It happens, when a thread opens a response, but then dies or finishes, before reading from it. E.g. it could die while doing some header parsing:

``` python
def timed_read(cp, i, timeout, host, port, release=None, preload_content=True):
    resp = cp.urlopen('GET', 'http://%s:%s' % (host, port),
                      release_conn=release,
                      preload_content=preload)
    print ""%s opened"" % i
    resp.headers['asdfasdf']
    data = resp.data # Never read, never released
```

And there again is the deadlock on opening connection nr 2:

<pre>
----------  timeout=5 preload=False release=None ----------
1 opened
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py"", line 327, in run
    result = self._run(*self.args, **self.kwargs)
  File ""Schreibtisch/demo3.py"", line 47, in timed_read
    resp.headers['asdfasdf']
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/_collections.py"", line 151, in __getitem__
    val = self._container[key.lower()]
KeyError: 'asdfasdf'
<Greenlet at 0x7fe42a5f8e10: timed_read(<urllib3.connectionpool.HTTPConnectionPool object , 1, 5, '127.0.0.1', 40303, release=None, preload_content=False)> failed with KeyError

0 opened
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py"", line 327, in run
    result = self._run(*self.args, **self.kwargs)
  File ""Schreibtisch/demo3.py"", line 47, in timed_read
    resp.headers['asdfasdf']
  File ""/usr/local/lib/python2.7/dist-packages/urllib3/_collections.py"", line 151, in __getitem__
    val = self._container[key.lower()]
KeyError: 'asdfasdf'
<Greenlet at 0x7fe42a5f8c30: timed_read(<urllib3.connectionpool.HTTPConnectionPool object , 0, 5, '127.0.0.1', 40303, release=None, preload_content=False)> failed with KeyError
</pre>


Supporting the with-statement on response objects may help the user to avoid such situations.
## 
"
808,README: Add maintainers list,2016-02-29T20:17:06Z,2016-02-29T20:35:34Z,,,,"@Lukasa @sigmavirus24 any objections?
"
807,Yet another pass at connection cleanup.,2016-02-29T16:28:04Z,2016-02-29T20:07:27Z,,,,"Resolves #805 (again).

This patch ensures that we correctly cleanup connections that fail inside `HTTPConnectionPool.urlopen`, not just ones that fail after opening. This should ensure that weird gevent timeouts during connection setup also correctly eliminate the connection, rather than leaking it.
"
806,Pull cleanup code into finally.,2016-02-25T08:28:32Z,2016-02-25T17:57:11Z,,,,"As discussed in #805.
"
805,Connection pool queue can still be exhausted by gevent timeouts,2016-02-24T10:35:22Z,2016-03-01T08:32:52Z,,,,"A similar error was discussed here https://github.com/shazow/urllib3/issues/644 already and some fix applied, but the problem still partly exists. Consider this code:

``` python
from gevent import monkey, spawn, sleep, Timeout, joinall
monkey.patch_all()

import sys
from itertools import product
from urllib3.connectionpool import HTTPConnectionPool

def timed_read(cp, i, timeout, release=None, preload_content=True):
    try:
        with Timeout(timeout):
            resp = cp.urlopen('GET', 'http://httpbin.org/redirect/:1',
                              release_conn=release,
                              preload_content=preload)
            print ""%s opened"" % i
            sys.stdout.flush()
            data = resp.data
    except Timeout:
        print ""%s timed out"" % i
    else:
        content_len = int(resp.headers['content-length'])
        if len(data) != content_len:
            print ""%s incomplete read: %s/%s"" % (i, len(data), content_len)
        else:
            print ""%s finished"" % i
    sys.stdout.flush()


def main(timeout=10, release=None, preload=True, poolsize=2):
    print '-' * 10 + ""  timeout=%s preload=%s release=%s "" % (timeout, preload, release) + '-' * 10
    cp = HTTPConnectionPool('httpbin.org', block=True, maxsize=poolsize, timeout=None)
    group = []
    for i in range(3):
        group.append(spawn(timed_read, cp, i, timeout, release=release, preload_content=preload))
    joinall(group)

    current_size = cp.pool.qsize()
    if current_size != poolsize:
        print ""poolsize decreased: %s"" % current_size


for timeout, preload in product([10, 4, 0.1], [True, False]):
    main(timeout=timeout, preload=preload)
```

Setting a timeout that way may seem stupid, as it could be set on the connection itself, but it's just an example for having a timeout running on a longer running task including several calls into the connection pool. What happens is, that the connections are not properly returned back into the queue, and the queue finally runs empty, and any further call to it blocks forever:

<pre>
----------  timeout=10 preload=True release=None ----------
1 opened
1 finished
0 opened
0 finished
2 opened
2 finished
----------  timeout=10 preload=False release=None ----------
0 opened
0 finished
1 opened
1 finished
2 opened
2 finished
----------  timeout=4 preload=True release=None ----------
1 opened
1 finished
0 timed out
2 timed out
----------  timeout=4 preload=False release=None ----------
1 opened
1 finished
0 opened
0 finished
2 timed out
poolsize decreased: 1
----------  timeout=0.1 preload=True release=None ----------
0 timed out
1 timed out
2 timed out
----------  timeout=0.1 preload=False release=None ----------
0 timed out
1 timed out
2 timed out
poolsize decreased: 0
</pre>


Simply setting `preload_content` to `True` is not an option, as e.g. `False` is set by default by requests.

The main problem is in https://github.com/shazow/urllib3/blob/master/urllib3/response.py#L246 . The `Timeout` does not derive from `Exception`, so `self._original_response` is not closed in the except clause, and therefore the connection is not released in the finally clause.
"
804,Support for trailing headers?,2016-02-23T21:02:27Z,,Someday,,,"Are there any plans to support _trailing headers_ as per [RFC 2616](http://tools.ietf.org/html/rfc2616#section-14.40)?

Right now it looks like it is intentionally not supported as per https://github.com/shazow/urllib3/pull/597#discussion_r29098757
## 
"
803,Fix comment about response closing connection,2016-02-11T18:20:57Z,2016-02-11T19:29:08Z,,,,
802,[WIP] Experimental code for cert validation in OS X and Windows,2016-02-10T12:09:37Z,2017-04-24T12:30:32Z,,,,"This is related to kennethreitz/requests#2966.

This change makes it possible to trust the system certificate store explicitly on systems that would like to. The goal is to have this be a cross-platform way to solve the problem, so that Windows and OS X aren't left out in the cold in this kind of world. The goal here is to divide the world into two sets of users:
- Non-Windows and Non-OS X users. These users are assumed to be using OpenSSL, configured appropriately, such that `set_default_verify_paths` will work correctly for them. In that instance, if `trust_system` is `True`, they will automatically use OpenSSL with the system cert stores.
- Windows and OS X users. These users cannot use `set_default_verify_paths` because those platforms have their own, complex, certificate stores that do not and cannot interface properly with OpenSSL. These platforms will therefore need to use certitude to validate their trust stores: essentially, they'll pass the X509 cert chain to certitude which will call into the relevant system functions to do the validation.

This is very much WIP right now, and there are the following limitations:
1. We need to bikeshed the API a bit.
2. This has made `ssl_wrap_socket` a bit crazy: we may want to factor it out a bit to keep the code cleaner.
3. certitude doesn't work on Windows yet.

However, feedback would be appreciated.
"
801,Better handle the IPv6 debacle.,2016-02-10T10:05:26Z,2016-02-18T22:38:35Z,,,,"This is intended to resolve kennethreitz/requests#3002.

It reverts #708 and #760.

The basic problem originally found in #707 was _actually_ because `connection_from_url` would sometimes pass `None` to the connection, which screws up the logic. For that reason, I thought I'd bring in the logic used in `PoolManager.connection_from_url`.
"
800,Multipart mixed responses generate warnings,2016-02-09T20:36:45Z,,"Bounty, Help Wanted",requests.packages.urllib3.exceptions.HeaderParsingError,"requests.packages.urllib3.exceptions.HeaderParsingError: [StartBoundaryNotFoundDefect(), MultipartInvariantViolationDefect()], unparsed data: ''","If you read a multipart/mixed response, the connectionpool issues a couple of warnings about defects in the message. I'm not sure what the expected, correct behavior is, but these warnings seem spurious.

Stick this perl script somewhere:

```
#!/usr/bin/perl

print ""Server: Some Server Name\r\n"";
print ""Content-Type: multipart/mixed; boundary=36eeb8c4e26d842a\r\n"";
print ""Content-Length: 178\r\n"";
print ""\r\n\r\n"";
print ""--36eeb8c4e26d842a\r\n"";
print ""Content-Type: text/plain\r\n"";
print ""\r\n"";
print ""7\r\n"";
print ""--36eeb8c4e26d842a\r\n"";
print ""Content-Type: text/plain\r\n"";
print ""\r\n"";
print ""9\r\n"";
print ""--36eeb8c4e26d842a\r\n"";
print ""Content-Type: text/plain\r\n"";
print ""\r\n"";
print ""11\r\n"";
print ""--36eeb8c4e26d842a--\r\n"";
```

Read it with requests (naturally, you'll have to change the URI to wherever you put the script):

```
import requests, logging

logging.basicConfig(level=logging.WARNING)
logging.getLogger(""requests"").setLevel(logging.DEBUG)

headers = {'accept': ""multipart/mixed""}
r = requests.get(""http://localhost:8124/cgi-bin/mpm.pl"", headers=headers)

print(r)
```

The following errors are displayed:

```
DEBUG:requests.packages.urllib3.connectionpool:""GET http://localhost:8124/cgi-bin/mpm.pl HTTP/1.1"" 200 178
WARNING:requests.packages.urllib3.connectionpool:Failed to parse headers (url=http://localhost:8888/http://localhost:8124/cgi-bin/mpm.pl): [StartBoundaryNotFoundDefect(), MultipartInvariantViolationDefect()], unparsed data: ''
Traceback (most recent call last):
  File ""/home/ndw/.virtualenvs/pyapi/lib/python3.4/site-packages/requests-2.8.0-py3.4.egg/requests/packages/urllib3/connectionpool.py"", line 390, in _make_request
    assert_header_parsing(httplib_response.msg)
  File ""/home/ndw/.virtualenvs/pyapi/lib/python3.4/site-packages/requests-2.8.0-py3.4.egg/requests/packages/urllib3/util/response.py"", line 58, in assert_header_parsing
    raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)
requests.packages.urllib3.exceptions.HeaderParsingError: [StartBoundaryNotFoundDefect(), MultipartInvariantViolationDefect()], unparsed data: ''
```

It took me quite a while to work out that they were spurious (because in real life, the server side that is generating the multipart/mixed is more complicated!)

See also kennethreitz/requests#3001
## 
"
799,Changes to support sending a complete response object when 407 errors‚Ä¶,2016-02-04T12:01:16Z,,,,,"‚Ä¶ is found while doing the CONNECT to proxy. Added a way to stablish a connection using connect over an already connected socket

Trying to connect to a ""https"" web page using a proxy uses a ""HTTP CONNECT"" verb to tell the proxy that we want to connect to a https destination.
With ""basic authentication"" the ""CONNECT"" is sent with the appropriate headers so the proxy can authenticate the request and do the request to the https web server. (That headers are setup by the requests library)
With ""NTLM"" we use a plugin (requests-ntlm) that is set as a hook and it is called on every request response. For example: with HTTP, the requests library sends a ""GET"" to a web page, as a response we get a HTTP-407 that is handled by the plugin hook to do the NTLM negotiation and authentication
In a HTTPS scenario, the first connection to the proxy is being done with the ""CONNECT"". At that time, requests does not set any authentication header (because it is handled by a plugin) and we get an 407 authentication Error.

This Patch tries to avoid raising an exception on ""Proxy Authentication"" errors and it sends back a complete http response so it can be handled by the requests-ntlm plugin and reuse an already connected and authenticated socket to do the connection
"
798,Add more analytics to connection objects,2016-02-03T21:28:18Z,,Someday,,,"Would be nice if our connection pools contained analytics on each connection objects, things like:
- Timestamp of when the connection object was last successfully connected
- Number of times it has been recycled
- Total lifetime of the connection object (time it has spent being connected to the server)
- Other things?

The goal is to have better debugging data for diagnosing whether a server is cooperating with keep-alive.
## 
"
797,Implement Happy Eyeballs to support IPv4/IPv6 combination,2016-01-27T15:23:14Z,,Someday,,,"In case host have IPv4 and IPv6 address but the route for one of them is broken, then user using urllib3 cannot connect to that URL.

This can be avoided (beside fixing the route, which is not always possible) by implementing Happy Eyeballs specification:
http://tools.ietf.org/html/rfc6555
## 
"
796,Doc update: Concatenated gzip data handling,2016-01-26T17:28:00Z,2016-01-27T04:03:34Z,"Feedback Pending, Soon",,,"@shazow I'm not sure if adding a page for concatenated gzip is ok. Please take a look and let me know. thanks.
"
795,Enable PyOpenSSL testing.,2016-01-26T12:51:00Z,2016-02-22T21:14:46Z,Ready,,,"This change brings testing of the PyOpenSSL logic into urllib3's CI and testing frameworks. It also enables testing of the PyOpenSSL code in Python 3, and fixes a few bugs we had that prevented that from functioning properly.

Unfortunately, I had to rewrite the server certificate, because this change revealed that our server certificate would raise warnings under PyOpenSSL because it had no dnsname subject alternative name fields. This didn't affect the standard library because the standard library would also return email fields. This change unfortunately extends the footprint of the total patch quite a bit.

I so far have not enabled code coverage for PyOpenSSL. This is in part because getting accurate numbers for it will be tricky: there's large chunks of code that are specific to error conditions or to individual Python versions that we have not fully investigated. We should investigate how best to obtain code coverage here, which may involve changing our reporting logic to look a bit more like what @hynek suggests [here](https://hynek.me/articles/testing-packaging/).

For the sake of @sigmavirus24 I should also note that this change has a potentially destabilising effect on requests. Previously requests would _not_ automatically use the PyOpenSSL code because the import failed (due to the import of `socket._fileobject`). This change makes that code path functional, which means that Python 3 installations of requests may suddenly take a new and potentially under-tested code path. We'll want to come up with a battle plan here.

Resolves #791.
"
794,Update license year range to 2016,2016-01-26T10:05:13Z,2016-01-26T10:07:11Z,,,,
793,UnicodeDecodeError in format_header_params,2016-01-25T23:23:14Z,2016-03-16T20:03:34Z,,,,"This issue was discussed here: https://github.com/kennethreitz/requests/issues/2639 and it seemed like the consensus was that this should be fixed in urllib3. 
"
792,Cast OpenSSL errors to strings.,2016-01-25T17:51:07Z,2016-01-25T19:58:44Z,,,,"Resolves #556.

I haven't written any tests for this because of #791. I'll start trying to work on #791 and, if we can get it ironed out quickly, we should block the merge of this behind #791 so that we can actually have a test for this.
"
791,Add PyOpenSSL tests to the regular test runs.,2016-01-25T17:49:45Z,2016-02-22T21:14:46Z,,,,"I've just noticed that, while we have tests that can run under PyOpenSSL, they don't run either under tox or Travis. I suggest that we promote those tests up to be full parts of our test suite, especially as PyOpenSSL can be critically important for some features.

While we're there, we should add `TestSSL` to the import from `test_socketlevel`, as the `SSL` tests are probably a good thing to have run under PyOpenSSL as well!

This will give us a hook for writing tests that only run under PyOpenSSL and the expectation that they will actually run on multiple platforms.
"
790,Chunked transfer encoding on HTTP requests,2016-01-25T17:16:26Z,2016-01-26T20:39:22Z,,,,"Supersedes #775.
"
789,Support a concatenated gzip data,2016-01-25T09:50:43Z,2016-01-27T03:28:18Z,,,,"Currently `requests`/`urllib3` doesn't support a concatenated gzip data. Only the first gz content is extracted. Here is an reproducible test case.

1.Create a text file `abc.txt` including three lines of data:

```
1
2
3
```

2.Update `GzipSimpleHttpServer` 
https://github.com/smtakeda/GzipSimpleHTTPServer/commit/68506651620840ba15cc6f60b4d15a3e7719b8c9

3.Launch `GzipSimpleHttpServer`

```
python GzipSimpleHTTPServer.py 8001
```

4.Run this:

```
import requests
ret = requests.get('http://localhost:8001/abc.txt')
print(ret.text)
```

You'll get `1` only, since `zlib.decompress` stops decoding at the end of the first gz data.

The proposed fix is feed https://docs.python.org/3/library/zlib.html#zlib.Decompress.unused_data into a new `Decompress` object until no `unused_data` is observed.
"
788,fix 'bool' object has no attribute 'close',2016-01-24T10:01:46Z,2016-01-27T19:21:58Z,,,,"HTTPConnectionPool.urlopen may pass connection=False to HTTPResponse when release_conn set to True
which can cause AttributeError: 'bool' object has no attribute 'close'
"
787,Drop Python 3.2 support.,2016-01-20T08:10:56Z,2016-01-20T19:48:44Z,,,,"Goodbye Python 3.2. You will be missed.

Resolves #786.

![](http://www.nato.int/ims/graphics/2009/090327/b090327a.jpg)
"
786,Drop Python 3.2 support?,2016-01-20T01:58:11Z,2016-01-20T19:48:44Z,Urgent,,,"Noticed in #785 that virtualenv/pip no longer support Py32, so our travis is going to break if we continue to use them.

Anyone object to dropping Python 3.2 support from our test matrix and README? (Anywhere else?)

Here's some download counts of urllib3 by Python version from the last couple of days via @dstufft: 
<img width=""149"" alt=""screen-shot-2016-01-19-20-53-44"" src=""https://cloud.githubusercontent.com/assets/6292/12437830/2f936bc8-bed6-11e5-95e2-cd8234e72fa1.png"">
"
785,Always use setuptools,2016-01-20T01:41:09Z,2016-01-20T07:48:36Z,Ready,,,"Using distutils instead of setuptools is discouraged, it doesn't properly add all of the metadata that Python packaging requires. In addition, in 2016 you are almost certainly guaranteed to have setuptools exist on a user's machine.
"
784,"When SSL feature warnings, aggressively direct people to upgrade",2016-01-16T01:09:19Z,2016-01-16T01:11:10Z,,,,
783,Close the underlying connection when closing a response.,2016-01-14T13:35:51Z,2016-01-14T19:12:58Z,,,,"If the connection is still on the response object, closing the response should assume the connection is busted and close that too.

Resolves #779.
"
782,Support non-tunnelled https proxying,2016-01-14T06:06:03Z,,,,,"CONNECT is nice and all, but terrible for caching -- the world has happily become much more secure! Turns out Squid supports proxying ""METHOD https://... HTTP/1.1"" requests out-of-the-box, if only a user agent can send them. Yes, you obviously need to do all the SSL verification at the proxy layer, but for a trusted proxy this isn't hard or unreasonable. The alternative this is pseudo-MITM by the proxy which _can_ be done (see squid ssl_bump) but is quite terrible.

PR applies cleanly to 1.7.1 as that's Ubuntu LTS's pinned version -- I don't know if it's still compatible with master.
"
781,Fix typo,2016-01-11T20:53:40Z,2016-01-11T20:58:20Z,,,,
780,assert_hostname and assert_fingerprint attempting to .strip() boolean values in recent version,2016-01-11T17:03:14Z,2016-01-11T17:09:53Z,,,,"Commit that looks to have caused issue (for hostname at least):
https://github.com/shazow/urllib3/commit/cc2d86fc13688e573721ee7a7571cb5315ef2017#diff-ba7cb8edcbac0c25e81c20eba663f317

self.assert_hostname (in addition to self.assert_fingerprint) are booleans. This raises an exception

Edit: Then again, let me make sure this is actually the intended use... I may be wrong and elasticsearch-py docs / urllib3 docs just aren't entirely clear as to what these attempt to do
"
779,HTTPResponse.close may not close underlying connection.,2016-01-09T16:45:02Z,2016-01-14T19:12:57Z,,,,"Found while investigating kennethreitz/requests#2963

The `HTTPResponse` class has a `close` method that rather suggests it will try to close the backing TCP connection behind the given HTTP response. Right now, that's not what happens if the connection is kept alive for any reason (that is, if the server did not send `Connection: close`): instead, the TCP connection will be kept alive and handled as normal.

This seems moderately surprising to me. What it means, in practice, is that calling `HTTPResponse.close()` in both urllib3 and httplib/http.client does not guarantee the closure of the backing TCP connection: instead, in both cases it says ""I'm done with the TCP connection, but the underlying connection is free to re-use it"". The problems this causes can be see in the `_error_catcher` context manager on the HTTPResponse which does not actually call the class `close` method, presumably because it's too deficient to do the job.

This behaviour affects the chunked transfer encoding decoding logic which calls `self.close()` and therefore may incorrectly keep the connection alive, though it does not itself return the connection to the pool.

I believe it _should_ be safe to have `close` close the underlying connection if it is present. As something of an optimisation, we can then safely assume that `close` can call `release_conn`, which will allow us to keep hold of the `HTTPConnection` object in a situation where otherwise we might lose it.
"
778,Some HTTPS website cannot load without changing ssl_version,2016-01-08T15:09:28Z,2016-01-25T17:47:06Z,,,,"Some websites:
1. Default http://prntscr.com/9njwan FAIL | Override ssl_version to 'SSLv3' http://prntscr.com/9njwxr SUCCESS
2. Default http://prntscr.com/9njxdb FAIL | Override ssl_version to 'TLSv1' http://prntscr.com/9njxv5 SUCCESS
3. Default http://prntscr.com/9njzow FAIL | Override ssl_version to 'SSLv3' http://prntscr.com/9nk02x SUCCESS

So i doubt this is a bug because I think urllib3 should automatically detect exact ssl version to use or i should detect website's ssl version myself ? And how to do that ? Thank!

My web browsers like Firefox, IE, Chrome can load those websites without any problem, so I think this is a bug. 
"
777,SOCKS example code from wiki fail to run,2016-01-08T14:54:12Z,2016-01-08T15:12:20Z,,,,"Hi!
I took code from here: https://urllib3.readthedocs.org/en/latest/contrib.html#socks

And here is my result: http://prntscr.com/9njs5b

I'm using Windows 7, Python 3.5, urllib3 newest version.
"
776,v2.0 Wishlist,2016-01-05T20:02:09Z,,Someday,,,"Not sure we'll do this, but if we ever get a v2.0 release, some major changes that would be nice to see:
- Deprecate/remove outdated APIs
  - `pool.urlopen(...)` (in favour of consuming a pre-configured request object; the rest of the RequestMethod helpers will probably remain backwards-compatible)
  - `response.data` (in favour of `.read()`)
  - Default `response.read(preload_content=False)` https://github.com/shazow/urllib3/issues/436
  - (What else am I forgetting?)
- More context managers as a default API! (To allow block-based closing/release, and possibly async-ness) https://github.com/shazow/urllib3/issues/809
- Request history
  - We'll need a pre-configurable request object (instead of using a few dozen keyword params).
- Autoconfig for AppEngine/Proxy settings/SSL (certifi)/others?
  - We'll need a Session-like class in front of PoolManager that is more implicit. Ideally any new feature we add which can get auto-configured based on env should happen magically here.
- More configurable security?
  - I forget what we were doing for this, I remember we were working on some SSLContext abstraction?
- No exposed dict registries, switch to callables. (Related: #828)
- Return proper types on `.read()` (should this be bytes? or header-sensitive encoding?)
- Unlikely long shots:
  - Get rid of httplib dependence.
    - Add trailing header support (#804)
  - Async by default
  - http/2 support
  - Drop a Python, any Python. (Probably the 2nd Python) #883 

TODO: Cross-link existing issue refs.
## 
"
775,WIP: Chunked transfer encoding on HTTP requests,2016-01-03T12:40:55Z,2016-01-25T17:16:46Z,"Feedback Pending, Soon",,,"I recently stumbled upon this quite hacky lines in requests: https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L380 What happens there is, that requests has to do dirty low level work, to get chunked transfer encoding going on in HTTP requests. This low level hack is fairly untested there (if I didn't miss anything obvious), and doesn't really integrate well with all the proxy stuff etc. going on on higher levels, especially error handling, as many higher levels of urllib3 are just bypassed. 

But it looks like currently there was no better way to do this, as urllib3 does not support any chunked request by itself. The original `httplib.HTTPConnection` is also lacking this feature. So this pull request is basically adding some separate `request_chunked` method on HTTPConnection and exposes this functionality to upstream libraries, i.e. requests, so that it can make use of it in some later version and get rid of the misplaced lowlevel code.

In terms of implementation, it would have probably been more elgant to just add a `chunked` flag to the `HTTPConnection.request` method itself, but I thought it may be better for backwards compatibility, to have a separate function.
"
774,test failures related to JSON,2016-01-02T10:16:32Z,2016-01-02T11:02:45Z,,TypeError,TypeError: <tornado.httputil.HTTPHeaders object at 0x7f7fedb01e98> is not JSON serializable,"I've just updated urllib3 in pkgsrc (a packaging system, so all dependencies are installed manually and not using pip or similar).

There are four test failures when running py.test with python-3.5 on NetBSD:

```
===================================================================================== FAILURES ======================================================================================
______________________________________________________________________ TestConnectionPoolTimeouts.test_timeout ______________________________________________________________________

self = <test.with_dummyserver.test_connectionpool.TestConnectionPoolTimeouts testMethod=test_timeout>

    def test_timeout(self):
        # Requests should time out when expected
        block_event = Event()
        ready_event = self.start_basic_handler(block_send=block_event, num=6)

        # Pool-global timeout
        timeout = Timeout(read=SHORT_TIMEOUT)
        pool = HTTPConnectionPool(self.host, self.port, timeout=timeout, retries=False)

        wait_for_socket(ready_event)
        conn = pool._get_conn()
        self.assertRaises(ReadTimeoutError, pool._make_request, conn, 'GET', '/')
        pool._put_conn(conn)
        block_event.set() # Release request

        wait_for_socket(ready_event)
        block_event.clear()
        self.assertRaises(ReadTimeoutError, pool.request, 'GET', '/')
        block_event.set() # Release request

        # Request-specific timeouts should raise errors
        pool = HTTPConnectionPool(self.host, self.port, timeout=LONG_TIMEOUT, retries=False)

        conn = pool._get_conn()
        wait_for_socket(ready_event)
        now = time.time()
        self.assertRaises(ReadTimeoutError, pool._make_request, conn, 'GET', '/', timeout=timeout)
        delta = time.time() - now
        block_event.set() # Release request

>       self.assertTrue(delta < LONG_TIMEOUT, ""timeout was pool-level LONG_TIMEOUT rather than request-level SHORT_TIMEOUT"")
E       AssertionError: False is not true : timeout was pool-level LONG_TIMEOUT rather than request-level SHORT_TIMEOUT

test/with_dummyserver/test_connectionpool.py:122: AssertionError
------------------------------------------------------------------------------- Captured stdout call --------------------------------------------------------------------------------
Starting new HTTP connection (1): localhost
Starting new HTTP connection (1): localhost
Resetting dropped connection: localhost
Resetting dropped connection: localhost
Starting new HTTP connection (1): localhost
Starting new HTTP connection (1): localhost
___________________________________________________________________________ TestPoolManager.test_headers ____________________________________________________________________________

self = <test.with_dummyserver.test_poolmanager.TestPoolManager testMethod=test_headers>

    def test_headers(self):
        http = PoolManager(headers={'Foo': 'bar'})

        r = http.request('GET', '%s/headers' % self.base_url)
>       returned_headers = json.loads(r.data.decode())

test/with_dummyserver/test_poolmanager.py:134:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/usr/pkg/lib/python3.5/json/__init__.py:319: in loads
    return _default_decoder.decode(s)
/usr/pkg/lib/python3.5/json/decoder.py:339: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <json.decoder.JSONDecoder object at 0x7f7ff45505d8>, s = '<html><title>500: Internal Server Error</title><body>500: Internal Server Error</body></html>', idx = 0

    def raw_decode(self, s, idx=0):
        """"""Decode a JSON document from ``s`` (a ``str`` beginning with
            a JSON document) and return a 2-tuple of the Python
            representation and the index in ``s`` where the document ended.

            This can be used to decode a JSON document from a string that may
            have extraneous data at the end.

            """"""
        try:
            obj, end = self.scan_once(s, idx)
        except StopIteration as err:
>           raise JSONDecodeError(""Expecting value"", s, err.value) from None
E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

/usr/pkg/lib/python3.5/json/decoder.py:357: JSONDecodeError
------------------------------------------------------------------------------- Captured stdout call --------------------------------------------------------------------------------
Starting new HTTP connection (1): localhost
Starting new HTTP connection (1): localhost
------------------------------------------------------------------------------- Captured stderr call --------------------------------------------------------------------------------
INFO:urllib3.connectionpool:Starting new HTTP connection (1): localhost
ERROR:tornado.application:Uncaught exception GET /headers (::1)
HTTPServerRequest(protocol='http', host='localhost:64533', method='GET', uri='/headers', version='HTTP/1.1', remote_ip='::1', headers={'Accept-Encoding': 'identity', 'Foo': 'bar', 'Host': 'localhost:64533'})
Traceback (most recent call last):
  File ""/usr/pkg/lib/python3.5/site-packages/tornado/web.py"", line 1443, in _execute
    result = method(*self.path_args, **self.path_kwargs)
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 63, in get
    self._call_method()
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 95, in _call_method
    resp = method(req)
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 209, in headers
    return Response(json.dumps(request.headers))
  File ""/usr/pkg/lib/python3.5/json/__init__.py"", line 230, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 180, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: <tornado.httputil.HTTPHeaders object at 0x7f7fedb01e98> is not JSON serializable
ERROR:tornado.access:500 GET /headers (::1) 2.06ms
_______________________________________________________________________ TestHTTPProxyManager.test_headerdict ________________________________________________________________________

self = <test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager testMethod=test_headerdict>

    def test_headerdict(self):
        default_headers = HTTPHeaderDict(a='b')
        proxy_headers = HTTPHeaderDict()
        proxy_headers.add('foo', 'bar')

        http = proxy_from_url(
            self.proxy_url,
            headers=default_headers,
            proxy_headers=proxy_headers)

        request_headers = HTTPHeaderDict(baz='quux')
        r = http.request('GET', '%s/headers' % self.http_url, headers=request_headers)
>       returned_headers = json.loads(r.data.decode())

test/with_dummyserver/test_proxy_poolmanager.py:239:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/usr/pkg/lib/python3.5/json/__init__.py:319: in loads
    return _default_decoder.decode(s)
/usr/pkg/lib/python3.5/json/decoder.py:339: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())

self = <json.decoder.JSONDecoder object at 0x7f7ff45505d8>, s = '<html><title>500: Internal Server Error</title><body>500: Internal Server Error</body></html>', idx = 0

    def raw_decode(self, s, idx=0):
        """"""Decode a JSON document from ``s`` (a ``str`` beginning with
            a JSON document) and return a 2-tuple of the Python
            representation and the index in ``s`` where the document ended.

            This can be used to decode a JSON document from a string that may
            have extraneous data at the end.

            """"""
        try:
            obj, end = self.scan_once(s, idx)
        except StopIteration as err:
>           raise JSONDecodeError(""Expecting value"", s, err.value) from None
E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

/usr/pkg/lib/python3.5/json/decoder.py:357: JSONDecodeError
------------------------------------------------------------------------------- Captured stdout call --------------------------------------------------------------------------------
Starting new HTTP connection (1): localhost
Starting new HTTP connection (1): localhost
------------------------------------------------------------------------------- Captured stderr call --------------------------------------------------------------------------------
INFO:urllib3.connectionpool:Starting new HTTP connection (1): localhost
ERROR:tornado.application:Uncaught exception GET /headers (::1)
HTTPServerRequest(protocol='http', host='localhost:64520', method='GET', uri='/headers', version='HTTP/1.1', remote_ip='::1', headers={'Accept': '*/*', 'Host': 'localhost:64520', 'Baz': 'quux', 'Accept-Encoding': 'gzip', 'Foo': 'bar', 'Content-Length': '0', 'Connection': 'close'})
Traceback (most recent call last):
  File ""/usr/pkg/lib/python3.5/site-packages/tornado/web.py"", line 1443, in _execute
    result = method(*self.path_args, **self.path_kwargs)
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 63, in get
    self._call_method()
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 95, in _call_method
    resp = method(req)
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 209, in headers
    return Response(json.dumps(request.headers))
  File ""/usr/pkg/lib/python3.5/json/__init__.py"", line 230, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 180, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: <tornado.httputil.HTTPHeaders object at 0x7f7fedf5b818> is not JSON serializable
ERROR:tornado.access:500 GET /headers (::1) 0.66ms
_________________________________________________________________________ TestHTTPProxyManager.test_headers _________________________________________________________________________

self = <test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager testMethod=test_headers>

    def test_headers(self):
        http = proxy_from_url(self.proxy_url,headers={'Foo': 'bar'},
                proxy_headers={'Hickory': 'dickory'})

        r = http.request_encode_url('GET', '%s/headers' % self.http_url)
>       returned_headers = json.loads(r.data.decode())

test/with_dummyserver/test_proxy_poolmanager.py:161:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/usr/pkg/lib/python3.5/json/__init__.py:319: in loads
    return _default_decoder.decode(s)
/usr/pkg/lib/python3.5/json/decoder.py:339: in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <json.decoder.JSONDecoder object at 0x7f7ff45505d8>, s = '<html><title>500: Internal Server Error</title><body>500: Internal Server Error</body></html>', idx = 0

    def raw_decode(self, s, idx=0):
        """"""Decode a JSON document from ``s`` (a ``str`` beginning with
            a JSON document) and return a 2-tuple of the Python
            representation and the index in ``s`` where the document ended.

            This can be used to decode a JSON document from a string that may
            have extraneous data at the end.

            """"""
        try:
            obj, end = self.scan_once(s, idx)
        except StopIteration as err:
>           raise JSONDecodeError(""Expecting value"", s, err.value) from None
E           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

/usr/pkg/lib/python3.5/json/decoder.py:357: JSONDecodeError
------------------------------------------------------------------------------- Captured stdout call --------------------------------------------------------------------------------
Starting new HTTP connection (1): localhost
Starting new HTTP connection (1): localhost
------------------------------------------------------------------------------- Captured stderr call --------------------------------------------------------------------------------
ERROR:tornado.access:500 GET http://localhost:64520/headers (::1) 2.81ms
INFO:urllib3.connectionpool:Starting new HTTP connection (1): localhost
ERROR:tornado.application:Uncaught exception GET /headers (::1)
HTTPServerRequest(protocol='http', host='localhost:64520', method='GET', uri='/headers', version='HTTP/1.1', remote_ip='::1', headers={'Accept': '*/*', 'Host': 'localhost:64520', 'Content-Length': '0', 'Hickory': 'dickory', 'Accept-Encoding': 'gzip', 'Foo': 'bar', 'Connection': 'close'})
Traceback (most recent call last):
  File ""/usr/pkg/lib/python3.5/site-packages/tornado/web.py"", line 1443, in _execute
    result = method(*self.path_args, **self.path_kwargs)
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 63, in get
    self._call_method()
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 95, in _call_method
    resp = method(req)
  File ""/scratch/www/py-urllib3/work/urllib3-1.14/dummyserver/handlers.py"", line 209, in headers
    return Response(json.dumps(request.headers))
  File ""/usr/pkg/lib/python3.5/json/__init__.py"", line 230, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/pkg/lib/python3.5/json/encoder.py"", line 180, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: <tornado.httputil.HTTPHeaders object at 0x7f7fedf45458> is not JSON serializable
ERROR:tornado.access:500 GET /headers (::1) 0.61ms
ERROR:tornado.access:500 GET http://localhost:64520/headers (::1) 2.82ms
================================================================== 4 failed, 303 passed, 7 skipped in 5.79 seconds ==================================================================
*** Error code 1

Stop.
```

This is in an ASCII locale, in case it matters.

installed packages:

```
python35-3.5.1
py35-curl-7.19.3.1
py35-expat-3.5.1
py35-setuptools-19.2
py35-backports_abc-0.4
py35-tornado-4.3
py35-pip-7.1.2
py35-pbr-1.8.1
py35-six-1.10.0
py35-mock-1.3.0nb1
py35-nose-1.3.7
py35-py-1.4.30
py35-test-2.8.3
py35-asn1-0.1.9
py35-cffi-1.4.2
py35-cparser-2.14
py35-idna-2.0
py35-cryptography-1.1.2
py35-OpenSSL-0.15.1
py35-ndg_httpsclient-0.4.0
py35-certifi-2015.9.6.2
py35-Socks-1.5.6
```

Please advise if this is just a packaging problem, or if you need more information.
"
773,Use relative imports in SOCKS module.,2015-12-30T10:34:03Z,2015-12-30T21:54:29Z,,,,"Resolves #772.

It may be worth pushing out a 0.14.1 with this change in it at some point soon, but that's totally up to you @shazow.
"
772,Broken vendoring in socks contrib module,2015-12-30T10:25:54Z,2015-12-30T21:54:29Z,,,,"Awkwardly, the SOCKS contrib module doesn't work properly when vendored because it tries to do an absolute import. We should probably rewrite it to use relative imports.

My bad.
"
771,connection_from_url  hangup forever in proxy environment,2015-12-30T06:24:20Z,2015-12-30T08:12:00Z,,,,"$env | grep proxy
http_proxy=http://proxy.com:8080
$curl --head http://tunein.com/ | head -n 1
HTTP/1.1 200 OK

urllib2 works well but urllib3 hang up at connect forever

> > > import urllib3
> > > print urllib3.**version**
> > > 1.14
> > > conn = urllib3.connection_from_url('http://tunein.com/')
> > > r = conn.request('GET', '/')
> > > it hang up forever
> > > 
> > > import urllib2
> > > print urllib2.**version**
> > > 2.7
> > > response = urllib2.urlopen('http://tunein.com/')
"
770,Add Python 3.5 to Travis,2015-12-26T22:21:04Z,2015-12-26T23:03:17Z,,,,
769,Spelling fixes,2015-12-26T21:46:18Z,2015-12-26T21:47:05Z,,,,
768,RFE: Add Python 3.5 and PyPy3 to Travis,2015-12-26T21:29:29Z,2015-12-27T10:33:10Z,,,,"Would be useful to have Travis run tests against Python 3.5 and PyPy3 as well. I would have submitted a PR but simply adding py35 and pypy3 to .travis.yml and tox.ini didn't work out, it apparently requires something else.
"
767,Let logger format messages on demand,2015-12-26T20:33:21Z,2015-12-26T21:27:19Z,,,,
766,"Match_hostname: hostname '172.25.20.7' doesn't match either of '172.25.20.7', ' 127.0.0.1'",2015-12-24T17:49:06Z,2015-12-24T19:02:30Z,,,,"With the latest release, I am now seeing an error when running etcd with SSL.  When I attempt to run any etcd command, I get this:

```
Connection to etcd failed due to SSLError(CertificateError(""hostname '172.25.20.7' doesn't match 
either of '172.25.20.7', '127.0.0.1'"",),)
```

I was not seeing this previously with my certificates.  @Lukasa, any thoughts with the change in 1.13.1? Does this error imply there is a deeper problem with my certificates?
"
765,docs/security: Upgrading ndg-httpsclient,2015-12-22T21:19:28Z,2015-12-22T21:20:20Z,,,,"Closes #764.
"
764,InsecurePlatformWarning can maybe be fixed by updating ndg-httpsclient?,2015-12-22T21:00:28Z,2015-12-22T21:20:20Z,,,,"The [documentation for InsecurePlatformWarning](https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning) suggests that to fix InsecurePlatformWarning messages you must either upgrade Python or use PyOpenSSL.

However, following the instructions on [this StackOverflow post](http://stackoverflow.com/a/29501899/1979340) I fixed the warning by simply upgrading `ndg-httpsclient`:

``` sh
pip install --upgrade ndg-httpsclient
```

Is there a reason this (much simpler, IMHO) solution isn't in the documentation?
"
763,Fixes the AppEngineManager to work within the requests framework,2015-12-17T10:43:49Z,2015-12-24T15:09:12Z,,,,"- Properly matches HTTPConnectionPool spec for **init**, clear, connection_from_url, and urlopen.
- Supports connection_from_url(url).urlopen(method, host_relative_path) calls
- If we process chunked data from a server through URLFetch, we properly unchunk it for the caller.
- Fix broken timeout detection code.

If I then use this requests_toolbelt code:
https://github.com/sigmavirus24/requests-toolbelt/blob/feature/gae/requests_toolbelt/adapters/appengine.py

I can get requests working on my dev appengine system with URLFetch. This should help fix the issues mentioned in:
https://github.com/kennethreitz/requests/issues/1905
https://github.com/sigmavirus24/requests-toolbelt/pull/114
https://github.com/shazow/urllib3/issues/618
"
762,"Add SOCKS support via contrib module (AKA SOCKS support, take 4)",2015-12-15T17:48:24Z,2015-12-19T19:34:08Z,,,,"@Anorov did fantastic work in #692 but did not have time to go back through and respond to code review, so I've adopted the work done there and refactored it. Credit should be joint between me and @Anorov should this satisfy the bounty: however, @Anorov has previously claimed they don't want the bounty. For that reason, if @Anorov doesn't change their mind and this submission does qualify for it, I'll donate the bounty, in full, to PyLadies. (I don't need it because I'm paid full-time to work on libraries like this.)

This change essentially pulls most of the work done in #692 out into a contrib module. This allows us to avoid having to change too many things in the mainline of urllib3, and also ensures that it is not affected by the requirements to have tests and coverage. This helps us get the work initially off the ground and out into the world.

If we decide to take this change as-is, I'll also volunteer to be ""on the hook"" for supporting it going forward until such time as I can add sufficient test coverage for it.

Feedback is encouraged!
"
761,Strip brackets from hostnames before validating.,2015-12-15T17:05:16Z,2015-12-15T17:20:30Z,,,,"This resolves #760.
"
760,Must remove square brackets from IPv6 addresses when using match_hostname,2015-12-15T16:27:36Z,2015-12-15T17:20:29Z,In Progress,,,"Spotted in [OpenStack](https://bugs.launchpad.net/nova/+bug/1526413). Introduced by my fix #708.

Apparently, while `httplib` itself doesn't care about the square brackets, the `match_hostname` function needs them gone. I suggest we move the stripping down into our `ssl.py` module.
"
759,TLS certificate verification ignores IP SANs,2015-12-07T14:43:33Z,2016-07-20T19:33:06Z,,,,"The current implementation of match_hostname ignores all SANs that are not DNS names.  In particular, it ignores IP address SANs.

Suggested fix: backport [match_hostname](https://hg.python.org/cpython/file/3.5/Lib/ssl.py#l254) from Python 3.5, which supports IP SANs.
"
758,Add ca_cert_dir to SSL_KEYWORDS,2015-12-03T11:27:50Z,2015-12-03T18:21:08Z,,,,"Resolves #750.
"
757,Catch socket errors in error_catcher,2015-12-03T11:04:33Z,2015-12-03T18:23:58Z,,,,"Resolves #756.

This is fundamentally a defensive change because it's pretty hard to hit this, but it clearly does happen (usually involving weird SSL issues and long-running connections).
"
756,Catch socket.error in error_catcher,2015-12-03T09:15:46Z,2015-12-03T18:23:58Z,,,,"Originally spotted in kennethreitz/requests#1236. It turns out that during `stream` a `socket.error` can get raised that isn't wrapped by httplib. We should catch and wrap those ourselves.
"
755,Release v1.13,2015-12-02T19:45:02Z,2015-12-14T21:13:54Z,,,,"What's left to do?
"
754,Define an SNIMissingWarning.,2015-12-02T14:15:01Z,2015-12-03T18:17:17Z,,,,"This fires whenever we make a HTTPS request and don't have SNI present. Should only fire once per run.

Resolves #753.

While we're here, @shazow @sigmavirus24: I'm thinking about having the docs URL be a property on the warning itself, so that requests can override them to point at our own docs when requests is being used. Thoughts?
"
753,Warn if we do not have access to SNI,2015-12-01T22:26:30Z,2015-12-03T18:17:17Z,,,,"Suggested in kennethreitz/requests#2910. This would make it easier for people to correlate unexpected cert verification failures with the specific action they need to take. 
"
752,Repair flaky connection timeout tests.,2015-11-26T12:06:05Z,2015-11-27T17:56:37Z,,,,"This PR changes the tests that previously used the `noop_handler` to instead use the `TARPIT_HOST`, which should fix their flakiness. See also #748.
"
751,Start keying connection pools off SSL arguments as well.,2015-11-26T10:47:31Z,2016-04-07T08:32:53Z,,,,"This is a proposed patch that begins to implement the requirements for kennethreitz/requests#2863. In particular, it adds hooks to provide additional keyword arguments as part of the key for the connection pool.

There are some outstanding questions. For example:
- should we actually just use all the connection pool keyword arguments as part of the key? Why are `headers` et al exempt?
- Are we happy with this API, where `connection_from_host` picks up the new argument but `connection_from_url` does not?

Code review encouraged, please. :custard:
"
750,ca_cert_dir keyword argument may be passed to HTTPConnectionPool by accident.,2015-11-26T10:09:11Z,2015-12-03T18:21:08Z,Urgent,TypeError,TypeError: __init__() got an unexpected keyword argument 'ca_cert_dir',"Seems like as part of #701 I missed the `SSL_KEYWORDS` block in `poolmanager.py`. This means that `ca_cert_dir` may accidentally be passed to the `HTTPConnectionPool`.  This leads to the following error when attempting to use `ca_cert_dir` with a `PoolManager` and then making a plaintext HTTP connection:

```
>>> import urllib3
>>> p = urllib3.PoolManager(ca_cert_dir='/usr/local/etc/openssl')
>>> p.urlopen('GET', 'http://http2bin.org/get')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""urllib3/poolmanager.py"", line 162, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""urllib3/connectionpool.py"", line 548, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""urllib3/connectionpool.py"", line 250, in _get_conn
    return conn or self._new_conn()
  File ""urllib3/connectionpool.py"", line 211, in _new_conn
    strict=self.strict, **self.conn_kw)
  File ""urllib3/connection.py"", line 121, in __init__
    _HTTPConnection.__init__(self, *args, **kw)
TypeError: __init__() got an unexpected keyword argument 'ca_cert_dir'
```
"
749,"requests2.8.1 and urllib1.12 still report SysCallError: (104, 'ECONNRESET')",2015-11-26T05:57:00Z,2015-11-26T08:47:23Z,,SysCallError,"SysCallError: (104, 'ECONNRESET')","  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/sessions.py"", line 511, in post
    return self.request('POST', url, data=data, json=json, *_kwargs)
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/sessions.py"", line 468, in request
    resp = self.send(prep, *_send_kwargs)
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, *_kwargs)
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 559, in urlop
    body=body, headers=headers)
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 376, in _makeuest
    httplib_response = conn.getresponse(buffering=True)
  File ""/usr/lib/python2.7/httplib.py"", line 1034, in getresponse
    response.begin()
  File ""/usr/lib/python2.7/httplib.py"", line 407, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python2.7/httplib.py"", line 365, in _read_status
    line = self.fp.readline()
  File ""/usr/lib/python2.7/socket.py"", line 447, in readline
    data = self._sock.recv(self._rbufsize)
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 196, in re
    return self.recv(_args, *_kwargs)
  File ""/home/fee/runtime/mppython/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 179, in re
    data = self.connection.recv(_args, **kwargs)
  File ""build/bdist.linux-x86_64/egg/OpenSSL/SSL.py"", line 1320, in recv
    self._raise_ssl_error(self._ssl, result)
  File ""build/bdist.linux-x86_64/egg/OpenSSL/SSL.py"", line 1178, in _raise_ssl_error
    raise SysCallError(errno, errorcode.get(errno))
SysCallError: (104, 'ECONNRESET')
"
748,Experiment with flaky project to re-run designated flakey tests,2015-11-23T04:18:11Z,2015-12-03T22:20:51Z,,,,"https://github.com/box/flaky looks like it might help our CI problem over come it's flaky-ness (for a temporary solution until we can make the test suite itself less flaky by default).
"
747,Fix chunked decompression logic,2015-11-22T15:32:52Z,2015-11-25T20:16:06Z,,,,"Resolves #743.
"
746,Set TCP_QUICKACK when available,2015-11-22T00:19:52Z,,Someday,,,"(I think only available on Linux?)

John Nagle said so: https://news.ycombinator.com/item?id=10608356

> That still irks me. The real problem is not tinygram prevention. It's ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful. Unfortunately by the time I found about delayed ACKs, I had changed jobs, was out of networking, and doing a product for Autodesk on non-networked PCs.
> Delayed ACKs are a win only in certain circumstances - mostly character echo for Telnet. (When Berkeley installed delayed ACKs, they were doing a lot of Telnet from terminal concentrators in student terminal rooms to host VAX machines doing the work. For that particular situation, it made sense.) The delayed ACK timer is scaled to expected human response time. A delayed ACK is a bet that the other end will reply to what you just sent almost immediately. Except for some RPC protocols, this is unlikely. So the ACK delay mechanism loses the bet, over and over, delaying the ACK, waiting for a packet on which the ACK can be piggybacked, not getting it, and then sending the ACK, delayed. There's nothing in TCP to automatically turn this off. However, Linux (and I think Windows) now have a TCP_QUICKACK socket option. Turn that on unless you have a very unusual application.
> Turning on TCP_NODELAY has similar effects, but can make throughput worse for small writes. If you write a loop which sends just a few bytes (worst case, one byte) to a socket with ""write()"", and the Nagle algorithm is disabled with TCP_NODELAY, each write becomes one IP packet. This increases traffic by a factor of 40, with IP and TCP headers for each payload. Tinygram prevention won't let you send a second packet if you have one in flight, unless you have enough data to fill the maximum sized packet. It accumulates bytes for one round trip time, then sends everything in the queue. That's almost always what you want. If you have TCP_NODELAY set, you need to be much more aware of buffering and flushing issues.
> None of this matters for bulk one-way transfers, which is most HTTP today. (I've never looked at the impact of this on the SSL handshake, where it might matter.)
> Short version: set TCP_QUICKACK. If you find a case where that makes things worse, let me know.
> John Nagle
## 
"
745,Pin tox for travis,2015-11-19T22:03:38Z,2015-11-19T22:23:27Z,,,,"cc @shazow 
"
744,Specify minimum pyOpenSSL version,2015-11-19T20:48:38Z,2015-11-19T22:41:45Z,Ready,,,"urllib3 requires ""set_tlsext_host_name"" which was only added in
pyOpenSSL 0.13. As some distributions (e.g. Ubuntu 12.04) still ship an
older version enforce the correct minimum version during installation.
"
743,Could calling  self._decoder.decompress after flush() has been called on the decompressobj instance cause some problems ?,2015-11-18T19:08:30Z,2015-11-25T20:16:06Z,,java.lang.NullPointerException,java.lang.NullPointerException: java.lang.NullPointerException: Inflater has been closed,"Disclaimer : I don‚Äôt know much about python. I am calling, from Java (through jython), a python script that uses a 3rd party python API that makes use of urllib3 to call a REST api. This whole setup obviously makes it challenging to troubleshoot issues.
The 3rd party python API we‚Äôre using uses urllib3 version 1.12
While I know it would be very useful, I do not have a small sample to replicate the issue (I‚Äôd have to peel a lot of layers that I do not have a good understanding of at this point). I might need to work on it if nobody can help me without it.

The issue I am having is thrown by the zlib java implementation that is called by jython:

```
java.lang.NullPointerException: java.lang.NullPointerException: Inflater has been closed

File ""C:\Users\laurentd\.gradle\caches\modules-2\files-2.1\org.python\jython-standalone\2.7.0\cdfb38bc6f8343bcf1d6accc2e1147e8e7b63b75\jython-standalone-2.7.0.jar\Lib\zlib.py"", line 202, in _get_inflate_data
    at java.util.zip.Inflater.ensureOpen(Inflater.java:389)
    at java.util.zip.Inflater.inflate(Inflater.java:257)
    at java.util.zip.Inflater.inflate(Inflater.java:280)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
```

The issue seems to be linked to the fact that the flush() method is called on a zlip.decompressobj object but that somehow, somewhere, we‚Äôre still trying to call the decompress method on that zlibdecompressobj object.
This is something that the zlib documentation mentions should not be done:
‚ÄúAfter calling flush(), the decompress() method cannot be called again; the only realistic action is to delete the object.‚Äù (from https://docs.python.org/2/library/zlib.html)

Further debugging took me to the response.py file, in the read_chunked method, especially in the last line of this part:

```
            while True:
                self._update_chunk_length()
                if self.chunk_left == 0:
                    break
                chunk = self._handle_chunk(amt)
                yield self._decode(chunk, decode_content=decode_content,
                                   flush_decoder=True)
```

It looks like when self._decode is called, we always set flush_decoder to True.

Because of this, if I understand correctly, the self._decoder object (which is, in my case, a GzipDecoder, and a decompressobj?) is always flushed at the end of the call to self._decode (line 200).
However, the self._decoder seems to be initialized only once and seems to be reused (at least in my case) throughout the multiple calls to self._decode.

Though I am probably missing a few things, it could explain why decompressobj.decompress is called on a decompressobj object that has been flushed, and this could (?) have unpredictable results, including that NPE in the jython implementation of zlib (and ultimately in java).

Here is a deeper (and sanitized) portion of the call stack when the exception happens.

```
....\scripts\requests\sessions.py"", line 608, in send
    r.content
....\scripts\requests\models.py"", line 734, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
....\scripts\requests\models.py"", line 734, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
....\scripts\requests\models.py"", line 657, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
....\scripts\requests\models.py"", line 657, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
....\scripts\requests\packages\urllib3\response.py"", line 322, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
....\scripts\requests\packages\urllib3\response.py"", line 471, in read_chunked
    yield self._decode(chunk, decode_content=decode_content,
....\scripts\requests\packages\urllib3\response.py"", line 190, in _decode
    data = self._decoder.decompress(data)
....\scripts\requests\packages\urllib3\response.py"", line 190, in _decode
    data = self._decoder.decompress(data)
....\scripts\requests\packages\urllib3\response.py"", line 56, in decompress
    return self._obj.decompress(data)
  File ""C:\Users\laurentd\.gradle\caches\modules-2\files-2.1\org.python\jython-standalone\2.7.0\cdfb38bc6f8343bcf1d6accc2e1147e8e7b63b75\jython-standalone-2.7.0.jar\Lib\zlib.py"", line 155, in decompress
  File ""C:\Users\laurentd\.gradle\caches\modules-2\files-2.1\org.python\jython-standalone\2.7.0\cdfb38bc6f8343bcf1d6accc2e1147e8e7b63b75\jython-standalone-2.7.0.jar\Lib\zlib.py"", line 202, in _get_inflate_data
  File ""C:\Users\laurentd\.gradle\caches\modules-2\files-2.1\org.python\jython-standalone\2.7.0\cdfb38bc6f8343bcf1d6accc2e1147e8e7b63b75\jython-standalone-2.7.0.jar\Lib\zlib.py"", line 202, in _get_inflate_data
```

Is it possible that the _decode method is causing the problem by calling decompress on a decompressobj that has been flushed or am I going down the wrong track?

Note that the problem is gone if I reinitialize the decompressobj at the beginning of _decode by mimicking what _init_decoder does, which I can‚Äôt believe is a good fix (although self._decoder seems to be really used only in _decode):

```
        content_encoding = self.headers.get('content-encoding', '').lower()
        self._decoder = _get_decoder(content_encoding)
```

Thanks in advance.

Laurent.
"
742,Network failure leads to TypeError in retry,2015-11-17T14:26:29Z,2015-12-03T19:28:07Z,,"Exception, socket.gaierror, requests.packages.urllib3.exceptions.NewConnectionError, TypeError","Exception:, socket.gaierror: [Errno -3] Temporary failure in name resolution, requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f86e18d8198>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution, TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'","I got this traceback after a network failure with urllib3 v1.10 on Linux:

```
Exception:
Traceback (most recent call last):
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connection.py"", line 135, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/util/connection.py"", line 66, in create_connection
    for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):
  File ""/usr/lib64/python3.4/socket.py"", line 533, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 559, in urlopen
    body=body, headers=headers)
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 345, in _make_request
    self._validate_conn(conn)
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 782, in _validate_conn
    conn.connect()
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connection.py"", line 215, in connect
    conn = self._new_conn()
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connection.py"", line 144, in _new_conn
    self, ""Failed to establish a new connection: %s"" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f86e18d8198>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib64/python3.4/site-packages/pip/basecommand.py"", line 211, in main
    status = self.run(options, args)
  File ""/usr/lib64/python3.4/site-packages/pip/commands/install.py"", line 294, in run
    requirement_set.prepare_files(finder)
  File ""/usr/lib64/python3.4/site-packages/pip/req/req_set.py"", line 334, in prepare_files
    functools.partial(self._prepare_file, finder))
  File ""/usr/lib64/python3.4/site-packages/pip/req/req_set.py"", line 321, in _walk_req_to_install
    more_reqs = handler(req_to_install)
  File ""/usr/lib64/python3.4/site-packages/pip/req/req_set.py"", line 461, in _prepare_file
    req_to_install.populate_link(finder, self.upgrade)
  File ""/usr/lib64/python3.4/site-packages/pip/req/req_install.py"", line 250, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""/usr/lib64/python3.4/site-packages/pip/index.py"", line 486, in find_requirement
    all_versions = self._find_all_versions(req.name)
  File ""/usr/lib64/python3.4/site-packages/pip/index.py"", line 404, in _find_all_versions
    index_locations = self._get_index_urls_locations(project_name)
  File ""/usr/lib64/python3.4/site-packages/pip/index.py"", line 378, in _get_index_urls_locations
    page = self._get_page(main_index_url)
  File ""/usr/lib64/python3.4/site-packages/pip/index.py"", line 818, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""/usr/lib64/python3.4/site-packages/pip/index.py"", line 928, in get_page
    ""Cache-Control"": ""max-age=600"",
  File ""/usr/lib64/python3.4/site-packages/requests/sessions.py"", line 480, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/lib64/python3.4/site-packages/pip/download.py"", line 373, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/usr/lib64/python3.4/site-packages/requests/sessions.py"", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib64/python3.4/site-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib64/python3.4/site-packages/cachecontrol/adapter.py"", line 46, in send
    resp = super(CacheControlAdapter, self).send(request, **kw)
  File ""/usr/lib64/python3.4/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 609, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/lib64/python3.4/site-packages/requests/packages/urllib3/util/retry.py"", line 226, in increment
    total -= 1
TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'
```
"
741,travisci epic failing,2015-11-14T20:09:08Z,2015-12-14T21:20:11Z,,,,"@Lukasa @sigmavirus24 Any idea what's going on?

Example: https://travis-ci.org/shazow/urllib3/jobs/81394938

Seems to happen for every version of Python. Tried bumping versions in #740 but no help.

```
$ tox
Traceback (most recent call last):
  File ""/home/travis/virtualenv/python2.7.9/bin/tox"", line 11, in <module>
    sys.exit(cmdline())
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/session.py"", line 38, in main
    config = prepare(args)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/session.py"", line 26, in prepare
    config = parseconfig(args)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 223, in parseconfig
    parseini(config, inipath)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 676, in __init__
    self.make_envconfig(name, section, reader._subs, config)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 704, in make_envconfig
    res = meth(env_attr.name, env_attr.default)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 839, in getdict
    s = self.getstring(name, None)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 893, in getstring
    x = self._replace(x)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 993, in _replace
    return RE_ITEM_REF.sub(self._replace_match, x)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 989, in _replace_match
    return handler(match)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 913, in _replace_env
    env_list = self.getdict('setenv')
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 839, in getdict
    s = self.getstring(name, None)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 893, in getstring
    x = self._replace(x)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 993, in _replace
    return RE_ITEM_REF.sub(self._replace_match, x)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 989, in _replace_match
    return handler(match)
  File ""/home/travis/virtualenv/python2.7.9/lib/python2.7/site-packages/tox/config.py"", line 913, in _replace_env
[... repeat a billion times]
```
"
740,dev-requirements piprot bump,2015-11-14T20:05:16Z,2015-12-14T21:21:29Z,,,,"Travis is failing a lot lately, let's try newer versions of things.
"
739,Minor updates to the contrib.appengine module.,2015-11-13T22:03:03Z,2015-11-13T22:48:21Z,,,,
738,'module' object has no attribute 'PROTOCOL_SSLv3',2015-11-12T04:57:30Z,2015-11-12T05:11:34Z,,AttributeError,AttributeError: 'module' object has no attribute 'PROTOCOL_SSLv3',"```
$ http DELETE :3000/api/foo X-Access-Token:login::789bd965-65c4-4219-a0b5-0137e9b86eef
Traceback (most recent call last):
  File ""/usr/local/bin/http"", line 7, in <module>
    from httpie.__main__ import main
  File ""/usr/local/lib/python2.7/dist-packages/httpie/__main__.py"", line 6, in <module>
    from .core import main
  File ""/usr/local/lib/python2.7/dist-packages/httpie/core.py"", line 16, in <module>
    import requests
  File ""/usr/local/lib/python2.7/dist-packages/requests/__init__.py"", line 53, in <module>
    from .packages.urllib3.contrib import pyopenssl
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 70, in <module>
    ssl.PROTOCOL_SSLv3: OpenSSL.SSL.SSLv3_METHOD,
AttributeError: 'module' object has no attribute 'PROTOCOL_SSLv3'
```

Which is strange, because:

```
In [1]: import ssl

In [2]: ssl.PROTOCOL_SSLv23
Out[2]: 2
```

Running Python 2.7.10 under Ubuntu 15.10 x64.
"
737,fix race condition around ready_event (hopefully),2015-11-10T16:31:02Z,2015-11-19T22:33:14Z,Ready,,,"`test_timeout` and friends rely on `ready_event` event variable to indicate that a socket is (about to be) listening. But the set/clear sequence falls on its head if the test thread is not already waiting on `ready_event`.

Which, for some reason, happens really frequently in our build environment.

This patch seems to ensure that the test thread never gets locked out of the `ready_event`.
"
736,test.with_dummyserver.test_connectionpool.TestConnectionPoolTimeouts times out,2015-11-06T15:28:50Z,2015-11-06T15:39:49Z,,,,"```
unpacking sources
unpacking source archive /nix/store/l4fajjsbb6hvmhqmf3zsbwaz9g1bn0z4-urllib3-1.12.tar.gz
source root is urllib3-1.12
patching sources
configuring
cat: test-requirements.txt: No such file or directory
building
running build
running build_py
creating build
creating build/lib
creating build/lib/urllib3
copying urllib3/connectionpool.py -> build/lib/urllib3
copying urllib3/exceptions.py -> build/lib/urllib3
copying urllib3/filepost.py -> build/lib/urllib3
copying urllib3/poolmanager.py -> build/lib/urllib3
copying urllib3/response.py -> build/lib/urllib3
copying urllib3/fields.py -> build/lib/urllib3
copying urllib3/connection.py -> build/lib/urllib3
copying urllib3/request.py -> build/lib/urllib3
copying urllib3/_collections.py -> build/lib/urllib3
copying urllib3/__init__.py -> build/lib/urllib3
creating build/lib/urllib3/packages
copying urllib3/packages/ordered_dict.py -> build/lib/urllib3/packages
copying urllib3/packages/__init__.py -> build/lib/urllib3/packages
copying urllib3/packages/six.py -> build/lib/urllib3/packages
creating build/lib/urllib3/packages/ssl_match_hostname
copying urllib3/packages/ssl_match_hostname/_implementation.py -> build/lib/urllib3/packages/ssl_match_hostname
copying urllib3/packages/ssl_match_hostname/__init__.py -> build/lib/urllib3/packages/ssl_match_hostname
creating build/lib/urllib3/contrib
copying urllib3/contrib/ntlmpool.py -> build/lib/urllib3/contrib
copying urllib3/contrib/appengine.py -> build/lib/urllib3/contrib
copying urllib3/contrib/pyopenssl.py -> build/lib/urllib3/contrib
copying urllib3/contrib/__init__.py -> build/lib/urllib3/contrib
creating build/lib/urllib3/util
copying urllib3/util/timeout.py -> build/lib/urllib3/util
copying urllib3/util/retry.py -> build/lib/urllib3/util
copying urllib3/util/response.py -> build/lib/urllib3/util
copying urllib3/util/connection.py -> build/lib/urllib3/util
copying urllib3/util/request.py -> build/lib/urllib3/util
copying urllib3/util/ssl_.py -> build/lib/urllib3/util
copying urllib3/util/url.py -> build/lib/urllib3/util
copying urllib3/util/__init__.py -> build/lib/urllib3/util
running tests
SKIP: NoseGAE plugin not used.
SKIP: NoseGAE plugin not used.
Failure: SkipTest (App Engine SDK not available.) ... SKIP: App Engine SDK not available.
Failure: SkipTest (Could not import PyOpenSSL: ImportError('No module named ndg',)) ... SKIP: Could not import PyOpenSSL: ImportError('No module named ndg',)
test_bad_connect (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_bad_decode (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_check_deflate (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_check_gzip (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_chunked_gzip (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
Test that connections are recycled to the pool on ... ok
test_connection_count (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_connection_count_bigpool (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
ECONNREFUSED error should raise a connection error, with retries ... ok
Test that modifying the default socket options works. ... ok
Test that passing None disables all socket options. ... ok
test_dns_error (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_for_double_release (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_get (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_keepalive (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_keepalive_close (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_lazy_load_twice (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
Test that connections have TCP_NODELAY turned on ... ok
test_one_name_multiple_values (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_partial_response (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_post_url (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_post_with_multipart (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_post_with_urlencode (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_redirect (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_release_conn_parameter (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_request_method_body (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
Test that connections accept socket options. ... ok
test_source_address (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_source_address_error (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_stream_keepalive (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_timeout_success (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_tunnel (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_unicode_upload (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_upload (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_urlopen_put (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_wrong_specific_method (test.with_dummyserver.test_connectionpool.TestConnectionPool) ... ok
test_conn_closed (test.with_dummyserver.test_connectionpool.TestConnectionPoolTimeouts) ... ok
test_connect_timeout (test.with_dummyserver.test_connectionpool.TestConnectionPoolTimeouts) ... ok
test_create_connection_timeout (test.with_dummyserver.test_connectionpool.TestConnectionPoolTimeouts) ... ok
test_timeout (test.with_dummyserver.test_connectionpool.TestConnectionPoolTimeouts) ... building of ‚Äò/nix/store/iibgvfzkhgkrav1sm7qycmvwnkq9nwny-pypy2.6-urllib3-1.12‚Äô timed out after 7200 seconds of silence
```
"
735,Add absolute_import feature from the future,2015-11-05T22:39:05Z,2015-11-05T23:38:58Z,,,,"This should prevent local files or modules from shadowing standard
library modules and will provide an all around better experience. This
does not, however, help us in the case where someone uses a library
like pies (which installs broken standard library shims for Python 2/
Python 3 compatibility).

Related to: https://github.com/kennethreitz/requests/issues/2771
"
734,Defensively close connections,2015-11-05T11:06:49Z,2015-11-05T19:55:42Z,,,,"Generally speaking, if we hit an exception when working with a connection we should assume that the connection is dead. We've not been defensive enough with this in the past, and that attitude pretty much just leads us to a constant cycle of bug-squashing each time we discover a new connection-invalidating exception.

In this case, the problem (and the fix) were inspired by [this HN post](https://news.ycombinator.com/item?id=10512343). For that reason, I'd like to credit @rkday (in his guise as @rkd-msw) for the actual fix. Rob, if you'd like to, I'm happy for you to open a new pull request with these commits in place. If you don't care about the commit log, I've made sure you're added to CONTRIBUTORS.
"
733,Add proxy authentication example using ProxyManager,2015-11-03T20:23:23Z,2015-11-04T02:45:57Z,,,,"I've struggled a bit while trying to google an example of using basic proxy authentication using urllib3,  therefore decided to add a bit of code sample to ProxyManager docs.
Not sure if I've placed it to the right place, doctests in source code seemed to be not appropriate.
"
732,bug: timeout cause connection stay at FIN_WAIT2,2015-10-30T10:23:02Z,2015-10-31T17:05:00Z,,,,"test code:  https://gist.github.com/cloudfly/a2b781fa1c9e70190707

If server-side is a long-polling action, can not response immediately.

this code will generate lots of `FIN_WAIT2` tcp connection

```
tcp        0      0 192.168.77.10:55785     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55750     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55781     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55776     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55769     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55788     192.168.77.1:8080       ESTABLISHED 27252/python
tcp        0      0 192.168.77.10:55757     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55779     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55774     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55743     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55768     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55762     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55771     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55741     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55783     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55759     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55748     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55751     192.168.77.1:8080       FIN_WAIT2   -
tcp        0      0 192.168.77.10:55746     192.168.77.1:8080       FIN_WAIT2   -
```
"
731,wrap error in named exception,2015-10-29T16:16:37Z,2015-10-29T22:03:54Z,,,,"refs #730
"
730,contrib/pyopenssl should be wrapping SysCallError,2015-10-29T00:29:25Z,2015-10-29T22:04:07Z,,SysCallError,"SysCallError: (10054, 'WSAECONNRESET')","https://github.com/kennethreitz/requests/issues/2852

```
Traceback (most recent call last):
  File ""..\requests-2.8.1-py2.7.egg\requests\api.py"", line 122, in put
    return request('put', url, data=data, **kwargs)
  File ""..\requests-2.8.1-py2.7.egg\requests\api.py"", line 50, in request
    response = session.request(method=method, url=url, **kwargs)
  File ""..\requests-2.8.1-py2.7.egg\requests\sessions.py"", line 468, in request
    resp = self.send(prep, **send_kwargs)
  File ""..\requests-2.8.1-py2.7.egg\requests\sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""..\requests-2.8.1-py2.7.egg\requests\adapters.py"", line 370, in send
    timeout=timeout
  File ""..\requests-2.8.1-py2.7.egg\requests\packages\urllib3\connectionpool.py"", line 559, in urlopen
    body=body, headers=headers)
  File ""..\requests-2.8.1-py2.7.egg\requests\packages\urllib3\connectionpool.py"", line 376, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""C:\Python27\ArcGIS10.3\lib\httplib.py"", line 1067, in getresponse
    response.begin()
  File ""C:\Python27\ArcGIS10.3\lib\httplib.py"", line 409, in begin
    version, status, reason = self._read_status()
  File ""C:\Python27\ArcGIS10.3\lib\httplib.py"", line 365, in _read_status
    line = self.fp.readline(_MAXLINE + 1)
  File ""C:\Python27\ArcGIS10.3\lib\socket.py"", line 476, in readline
    data = self._sock.recv(self._rbufsize)
  File ""..\requests-2.8.1-py2.7.egg\requests\packages\urllib3\contrib\pyopenssl.py"", line 179, in recv
    data = self.connection.recv(*args, **kwargs)
  File ""..\OpenSSL\SSL.py"", line 1320, in recv
    self._raise_ssl_error(self._ssl, result)
  File ""..\OpenSSL\SSL.py"", line 1178, in _raise_ssl_error
    raise SysCallError(errno, errorcode.get(errno))
SysCallError: (10054, 'WSAECONNRESET')
```
"
729,Catch exceptions on connection close,2015-10-27T20:15:44Z,2015-10-29T00:20:49Z,,,,"Typically caused by remote IIS server dropping HTTPS keepalive
connections without shutting down properly

Fixes #728
"
728,conn.close() can raise OpenSSL exception after idle timeout,2015-10-27T20:06:53Z,2015-10-29T00:20:49Z,,,,"When returning a connection to the pool, urllib3 calls `conn.close()`:

```
        if conn and is_connection_dropped(conn):
            log.info(""Resetting dropped connection: %s"" % self.host)
            conn.close()
```

This can raise an exception under the following conditions:
- urllib3 is using pyOpenSSL and https
- Remote webserver is running Windows IIS (7.5 and 8 tested)
- KeepAlives are active (default for IIS)
- Connection drops after idle timeout

tcpdump shows that the remote IIS host sends a RST on idle timeout. This appears to be IIS-specific behaviour (Apache2 and haproxy both close the connection normally)

At that point nothing bad happens immediately. However the connection remains in the pool. The next time you send a request to this host, it is reused; some data is sent down it; this then raises an `OpenSSL.SSL.SysCallError` exception. I am guessing that it it is trying to send an SSL shutdown message down the already-closed socket.

More detail at https://github.com/zatosource/zato/issues/465

Proposed patch:

```
--- a/urllib3/connectionpool.py
+++ b/urllib3/connectionpool.py
@@ -239,11 +239,15 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
         # If this is a persistent connection, check if it got disconnected
         if conn and is_connection_dropped(conn):
             log.info(""Resetting dropped connection: %s"" % self.host)
-            conn.close()
-            if getattr(conn, 'auto_open', 1) == 0:
-                # This is a proxied connection that has been mutated by
-                # httplib._tunnel() and cannot be reused (since it would
-                # attempt to bypass the proxy)
+            try:
+                conn.close()
+                if getattr(conn, 'auto_open', 1) == 0:
+                    # This is a proxied connection that has been mutated by
+                    # httplib._tunnel() and cannot be reused (since it would
+                    # attempt to bypass the proxy)
+                    conn = None
+            except Exception as e:
+                log.info(""Closing connection failed: %s, force new connection"" % str(e))
                 conn = None

         return conn or self._new_conn()
```
"
727,no git tag for 1.12 in the repo,2015-10-22T10:52:00Z,2015-10-22T18:16:44Z,,,,"please push the tags for the 1.12 release
"
726,[POC] Tox combined coverage report,2015-10-22T06:57:10Z,2016-06-12T05:17:00Z,,,,"Store the coverage data from each tox environment,
providing coverage data for each Python version,
and combine them into a final report which requires
100% coverage.
"
725,Fix flake8 violations,2015-10-21T00:19:50Z,2015-11-02T20:44:36Z,,,,"Add tox rule to prevent regressions.
"
724,Sync fetch_gae_sdk.py with oauth2client,2015-10-20T20:25:54Z,2015-10-20T20:35:37Z,,,,"Fixes Python 3 syntax errors, and Python 2.6 str.format incompatibilities.
"
723,Add a way to check if number of bytes received matches declared transfer length(s),2015-10-19T19:33:00Z,2016-08-30T21:06:50Z,,,,"This discussion originated on python requests (https://github.com/kennethreitz/requests/pull/2833). Basically, there are edge cases where a socket can appear to have closed normally (end-of-file) but was actually reset before sending/receiving all of a response. This may happen silently without raising any exceptions from urllib3.

A suggestion was made to allow users to enable a ""strict transfer-length"" check in urllib3. For what it's worth RFC2616 (http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.4) describes when Content-Length should be used by servers. It seems like the chunked logic is probably safe from this, but probably want to confirm that too.

When I get a chance I'll try to submit a PR for comment.
"
722,There's no reason not to preload chunked bodies.,2015-10-19T12:23:49Z,2015-10-19T18:44:58Z,,,,"Resolves #718.

This was introduced during our work on chunked transfer encoding. It's actually a fairly dramatic functional change, and potentially moves the point of exceptions being thrown quite dramatically, as well as changes how long each function call will take. However, I think it's more in line with our internal implementation and our documented behaviour.
"
721,Python 2.6 does not support unindexed parameters,2015-10-18T21:37:03Z,2015-10-18T23:59:50Z,,,,
720,'raise_on_status' option (similar to 'raise_on_redirect'),2015-10-16T01:01:19Z,2016-01-16T20:25:53Z,,,,
719,Remove `memoryview` usage in urllib3.contrib.pyopenssl#sendall,2015-10-15T16:55:25Z,2015-10-17T00:04:25Z,,,,"As described in #717, `urllib3.contrib.pyopenssl#sendall` currently fails when WantWriteError is thrown by OpenSSL due to urllib3 preemptively casting the data to be written as a `memoryview`. This results in duplicate bytestring allocations by pyopenssl and OpenSSL issuing a SSL3_WRITE_PENDING error.

This change removes `memoryview` usage from `urllib3/contrib/pyopenssl.py`. We thus rely on the underlying pyOpenSSL library's ability to cast data to the proper bytestring type.
"
718,"After read timeout, connection broken for a second timeout period",2015-10-15T09:15:36Z,2015-10-19T18:44:58Z,,,,"Using urllib3 1.12 to access etcd with a HTTPConnectionPool.  I do a long-poll read in a retry loop:

```
http = HTTPConnectionPool(""localhost"", 4001, maxsize=2)
while True:
    try:
        resp = http.request(""GET"", ""http://localhost:4001/v2/keys/calico/v1"",
                            fields={""recursive"": ""true"", ""wait"": ""true"",
                                    ""waitIndex"": next_index},
                            timeout=5)
        resp_body = loads(resp.data)
    except ReadTimeoutError:
        _log.info(""Watch read timed out, restarting watch at index %s"",
                  next_index)
        continue
    else:
        ...
```

I expect some reads to time out because there was no data available.  In that case, I expect to hit the ReadTimeoutError, loop again and retry.  If there's then data waiting, I should get it immediately.

I do see the ReadTimeoutError after the first failure.  However, when my code retries, I see a 5s delay and then this log:

```
2015-10-15 09:38:16,311[DEBUG]273: Incremented Retry for (url='http://localhost:4001/v2/keys/calico/v1?waitIndex=7611342&recursive=true&wait=true'): Retry(total=2, connect=None, read=None, redirect=None)
2015-10-15 09:38:16,311[WARNING]625: Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'ReadTimeoutError(""HTTPConnectionPool(host='localhost', port=4001): Read timed out."",)': http://localhost:4001/v2/keys/calico/v1?waitIndex=7611342&recursive=true&wait=true
```

Only then do I receive a response to my retry.
"
717,SSL3_WRITE_PENDING error from urllib3.contrib.pyopenssl#sendall,2015-10-14T21:45:59Z,2015-10-17T00:04:41Z,,Error,"Error: [('SSL routines', 'SSL3_WRITE_PENDING', 'bad write retry')]","I'm writing a program that makes somewhat large (e.g. tens to hundreds of kilobytes) POST requests over HTTPS using `requests`. Such POSTs fail due to `SSL3_WRITE_PENDING` errors from OpenSSL. In short, `SSL3_WRITE_PENDING` occurs when an incomplete write operation (i.e. a write that resulted in a [`WantWriteError`](http://pyopenssl.sourceforge.net/pyOpenSSL.html/openssl-ssl.html)) is retried with arguments not _exactly_ equal to those passed in the initial call (see [here](http://stackoverflow.com/questions/20715408/solved-pyopenssl-ssl3-write-pendingbad-write-retry-return-self-sslobj-write) and [here](http://stackoverflow.com/questions/2997218/why-am-i-getting-error1409f07fssl-routinesssl3-write-pending-bad-write-retr)).

Stracktrace from my call into `requests` down through `urllib3`:

```
File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 511, in post
return self.request('POST', url, data=data, json=json, **kwargs)
File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 468, in request
resp = self.send(prep, **send_kwargs)
File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 576, in send
r = adapter.send(request, **kwargs)
File ""/usr/local/lib/python2.7/site-packages/requests/adapters.py"", line 370, in send
timeout=timeout
File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 559, in urlopen
body=body, headers=headers)
File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 353, in _make_request
conn.request(method, url, **httplib_request_kw)
File ""/usr/local/lib/python2.7/httplib.py"", line 1053, in request
self._send_request(method, url, body, headers)
File ""/usr/local/lib/python2.7/httplib.py"", line 1093, in _send_request
self.endheaders(body)
File ""/usr/local/lib/python2.7/httplib.py"", line 1049, in endheaders
self._send_output(message_body)
File ""/usr/local/lib/python2.7/httplib.py"", line 893, in _send_output
self.send(msg)
File ""/usr/local/lib/python2.7/httplib.py"", line 869, in send
self.sock.sendall(data)
File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 220, in sendall
sent = self._send_until_done(data[total_sent:total_sent+SSL_WRITE_BLOCKSIZE])
File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 206, in _send_until_done
return self.connection.send(data)
File ""/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 1271, in send
self._raise_ssl_error(self._ssl, result)
File ""/usr/local/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 1187, in _raise_ssl_error
_raise_current_error()
File ""/usr/local/lib/python2.7/site-packages/OpenSSL/_util.py"", line 48, in exception_from_error_queue
raise exception_type(errors)
Error: [('SSL routines', 'SSL3_WRITE_PENDING', 'bad write retry')]
```

A bit of digging revealed #412 and #626 as issues that relate to the relevant code in urllib3. `WrappedSocket.sendall` proxies to `WrappedSocket._send_until_done`, which invokes [`OpenSSL.SSL.send`](https://github.com/pyca/pyopenssl/blob/8adee5e7f8759886678ab83919d5956151e825f3/OpenSSL/SSL.py#L1249) in a loop. The conditions for loop termination are a) the write succeeds or b) a timeout expires while waiting for the socket to become writable in the case of a `WantWriteError`.

I think that this issue boils down to:
1. `WrappedSocket.sendall` converts the data to be sent to a `memoryview`.
2. `WrappedSocket._send_until_done` invokes `OpenSSL.SSL.send` in a loop according to the conditions above.
3. [`OpenSSL.SSL.send` calls the `memoryview.tobytes()`](https://github.com/pyca/pyopenssl/blob/8adee5e7f8759886678ab83919d5956151e825f3/OpenSSL/SSL.py#L1264) and passes the bytestring result in the OpenSSL write call.
4. In the `WantWriteError` case, 2 and 3 are repeated and a _new_ bytestring is passed to the OpenSSL write, resulting in a `SSL3_WRITE_PENDING` error.

I was able to get around this issue by patching `urllib3/contrib/pyopenssl.py` to enforce that a single bytestring is used for all calls to `OpenSSL.SSL.send`: https://gist.github.com/evnm/af92092c6a7776e08339

Please advise on whether this is a good way to fix the issue. I tried adding a test case to `test/with_dummyserver/test_https.py`, but haven't figured out how to tickle the specific issue I've run into.

Relevant versions in use:
- Python 2.7.10
- requests 2.8.1
- pyOpenSSL 0.15.1
- urllib3 1.12
"
716,contrib.rst: typo fix: automagically --> automatically,2015-10-11T10:22:13Z,2015-10-11T10:24:29Z,,,,
715,Calling .read() on a request returns an empty string,2015-10-09T20:46:49Z,,,,,"I'm not sure whether this is a bug, or a documentation bug, or not a bug at all, but can anyone tell me why reply ends up as:

``` python
b''
```

?

``` python
import urllib3

http = urllib3.PoolManager(timeout=65)

response = http.request('GET', 'http://google.com', decode_content=True)
reply = response.read()
#reply = response.data  # <- this is non-empty however...
print(""reply.read() returned: "" + reply.decode('utf-8'))
```
## 
"
714,allows user paths for ca_cert verify path,2015-10-05T16:24:54Z,2015-10-05T18:56:12Z,,,,"currently you get the following error:

`SSLError(""bad ca_certs: '~/example.crt'"", Error([('system library', 'fopen', 'No such file or directory'), ('BIO routines', 'BIO_new_file', 'no such file'), ('x509 certificate routines', 'X509_load_cert_crl_file', 'system lib')],))`
"
713,Add optional support for Brotli content-encoding,2015-10-04T15:08:31Z,,,,,"This pull request adds optional support for the [Brotli](http://google-opensource.blogspot.co.uk/2015/09/introducing-brotli-new-compression.html) compression algorithm used as a content-encoding, alongside the current support for gzip and deflate. Chromium have announced their [intention to implement](https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/xdVm8c2GOMQ) Brotli, and Firefox 44 included [their support for Brotli](https://bugzilla.mozilla.org/show_bug.cgi?id=366559). Curl also [intend to implement Brotli](http://curl.haxx.se/docs/todo.html#More_compressions), but it'd be nice to beat @bagder to the punch for once!

Google claims that Brotli achieves 20%-26% higher compression ratios than other compression algorithms. They claim the reference implementation decodes just as quickly as deflate in zlib, but with denser compression than LZMA and bzip2.

This implementation currently relies on the hyper project's [brotlipy](https://github.com/python-hyper/brotlipy) sub-project, a CFFI-based wrapper around the Brotli reference implementation. For obvious reasons, as this adds a third-party dependency, the proposed enhancement is a `contrib` module.

Right now this contrib module monkeypatches its way into urllib3, but in the act of writing it I couldn't help but notice that we're really not very far from being able to have a 'formal' content-encoding plugin system. If that sounds like something worth having let me know, and I could write it and then refactor this contrib module to be the first user of that system. (On the other hand, it may simply not be worth it!)

In the short term, **I don't think we should merge this** just because I'm having trouble actually finding a site that uses Brotli! I think the best way to handle this is for me to offer an enhancement to httpbin that uses brotlipy to provide a `/brotli` endpoint, but that means I need to sit down and actually write that. That's blocked behind python-hyper/brotlipy#2, but I can hopefully get around to that soonish.
"
712,smarter context handling?,2015-09-25T16:32:55Z,,,,,"related to #472 

urllib3 _could_ for example use `create_default_context` in `create_urllib3_context` if it is available

also i'd move certificate path logic into `create_urllib3_context` (why is it separate, anyway?)

i'd also propose to auto-call context.set_default_verify_paths() if available and cert_reqs is other than CERT_NONE

i can send a PR with all or some of this if you want it; just wanted to ask in advance if this is something desirable
## 
"
711,Made SSL cert digest check use a constant-time comparison.,2015-09-23T21:37:28Z,2015-09-24T02:38:13Z,,,,"Resolves #710.
"
710,SSL cert digest check uses non-constant-time comparison,2015-09-23T21:36:44Z,2015-09-24T02:38:13Z,,,,"This is the problematic line:
https://github.com/shazow/urllib3/blob/master/urllib3/util/ssl_.py#L137

The digests being compared are strings, and `==` is not constant-time, which could potentially allow a timing attack. While it may not immediately be obvious whether an adversary could gain any advantage by successfully timing this line, I am a firm believer in ""better safe than sorry.""

The best way to fix this is to use a constant-time digest comparison function, such as the built-in `hmac.compare_digest`. However, this function was only introduced in Python 2.7.7, while this library supports Python 2.6+. The next best solution would be to use something like the `constant_time_compare` function from the following package:
https://github.com/mitsuhiko/itsdangerous

Using a constant-time comparison is semantically equivalent to using `==`, and the performance impact is negligible and comes along with improved security. I believe this is a good tradeoff.
"
709,.read should only return empty (false) when there is nothing left to read (EOS),2015-09-18T16:16:58Z,,,,,"I have some code that does a partial read of a ""raw"" response in requests to look for ""magic bytes"".  Digging into the urllib3 code I find that the size you pass in is used as the pre-decode amount which means that the decompressor doesn't have enough bytes to give you anything.  This confused my code into thinking it had read all of the response, which it hadn't.

Here is a simple example:

In [1]: import urllib3
In [2]: http = urllib3.PoolManager()
In [3]: resp = http.request('GET', 'http://httpbin.org/gzip', preload_content=False)
In [4]: resp.read(8)
Out[4]: b''

I would argue that the amount passed into .read (8 in this case) should be the amount _after_ decoding.

[EDIT]
I originally did not include the `preload_content=False`.  This is necessary.  Without it the content is (correctly) loaded into `.data`.
## 
"
708,We do not require square brackets for httplib.,2015-09-16T15:14:35Z,2015-10-06T18:30:49Z,,,,"Resolves #707.

I need to come up with a test for this that doesn't suck, but it should be considered illuminating that no tests break with this change.
"
707,"Error using https and ipv6: InvalidURL(""nonnumeric port: '4f7b'"",)",2015-09-16T12:24:47Z,2015-10-06T18:30:49Z,,"ValueError, urllib3.exceptions.MaxRetryError","ValueError: invalid literal for int() with base 10: '4f7b', urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='2001:0:53aa:64c:104c:2c10:2bef:4f7b', port=None): Max retries exceeded with url: / (Caused by ProtocolError('Connection aborted.', InvalidURL(""nonnumeric port: '4f7b'"",)))","To reproduce:

``` Python
import urllib3
p = urllib3.connection_from_url('https://[2001:0:53aa:64c:104c:2c10:2bef:4f7b]')
p.request('GET', '/')
```

produces:

``` Python
Traceback (most recent call last):
  File ""/usr/lib/python3.4/http/client.py"", line 771, in _set_hostport
    port = int(host[i+1:])
ValueError: invalid literal for int() with base 10: '4f7b'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 548, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 250, in _get_conn
    return conn or self._new_conn()
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 770, in _new_conn
    strict=self.strict, **self.conn_kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connection.py"", line 171, in __init__
    timeout=timeout, **kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connection.py"", line 119, in __init__
    _HTTPConnection.__init__(self, *args, **kw)
  File ""/usr/lib/python3.4/http/client.py"", line 750, in __init__
    self._set_hostport(host, port)
  File ""/usr/lib/python3.4/http/client.py"", line 776, in _set_hostport
    raise InvalidURL(""nonnumeric port: '%s'"" % host[i+1:])
http.client.InvalidURL: nonnumeric port: '4f7b'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/request.py"", line 89, in request_encode_url
    return self.urlopen(method, url, **extra_kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 629, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 629, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 629, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/connectionpool.py"", line 609, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/local/lib/python3.4/dist-packages/urllib3/util/retry.py"", line 271, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='2001:0:53aa:64c:104c:2c10:2bef:4f7b', port=None): Max retries exceeded with url: / (Caused by ProtocolError('Connection aborted.', InvalidURL(""nonnumeric port: '4f7b'"",)))
```

Using `https://[2001:0:53aa:64c:104c:2c10:2bef:4f7b]:443` instead is a workaround.
"
706,Use proper environment markers,2015-09-15T14:32:36Z,2015-09-22T03:22:37Z,,,,"This will intelligently install the right dependencies for a ""secure""
urllib3 but only if you have a recent enough version of setuptools and
pip.

Closes #684
"
705,Add info about disabling warnings for end users,2015-09-15T00:16:41Z,2015-09-15T03:36:58Z,,,,"For example, you can use `PYTHONWARNINGS` to disable warnings without modifying code (when using Python >= 2.7).

Relates to:
- https://github.com/pypa/pip/pull/3109
- https://github.com/pypa/pip/issues/2681

Thanks to @dstufft and @sigmavirus24 for the idea!
"
704,fix import bug,2015-09-09T09:45:01Z,2015-09-09T10:39:49Z,,,,"fix import bug so the code can be run without modifed
"
703,"Tests cleanup and deflake-ing, added NewConnectionError exception",2015-09-04T01:07:12Z,2015-09-06T18:39:35Z,,,,"- [x] Quarantined tests that depend on the /sleep handler, tests are far less flakey when that whole suite is skipped now.
- [x] Need to migrate tests to not use the /sleep handler (and ideally no inline sleep either, but that's of a lesser priority).
- Added `NewConnectionError` exception.
- Removed /sleep handler, ported tests to use event triggers and socket-level handlers.
- A couple test conditions got pruned because they were annoying, but I'm fairly confident they were extraneous anyways.

Tests seem to be waaaay less flakey now. Some weird failures every 30-50 back-to-back runs, but much better in general.
"
702,Any opinion on a Fedora unbundling question?,2015-09-02T15:15:50Z,,,,,"An [issue was raised](https://bugzilla.redhat.com/show_bug.cgi?id=1231381) in the Fedora bugtracker about handling `ssl_match_hostname` unbundling on Fedora releases that carry Python 2.7.9.  Here's the rundown:
- Currently, we replace the bundled `ssl_match_hostname` shim with a symlink to the system copy of `ssl_match_hostname` (to unbundle).
- In the ticket, Carl points out correctly that we don't need that extra lib/dep on Fedora releases that carry Python 2.7.9, so he wants to remove it.
- In doing so, he's suggesting that we reintroduce some of those import patches that we used to carry.
- Instead, we could replace the bundled `ssl_match_hostname` module with a symlink to the stdlib `ssl` module on Python 2.7.9 systems, and keep the old symlink on older Fedora releases.

How does that sound?  Do you have any opinion on how we do this?  Any different ideas?  Cheers!
## 
"
701,Add support for CA directories.,2015-08-31T09:08:27Z,2015-09-03T23:50:12Z,,,,"This enables support for directories of CAs in urllib3. These are usually used for efficiency purposes, as they allow lazy-lookup of certificate authorities in response to the specific certificates needed for a given connection.

This is a precursor bit of work to kennethreitz/requests#2659.
"
700,Interrupted System Call in select,2015-08-21T17:16:35Z,2016-11-02T14:44:56Z,,select.error,select.error: Interrupted system call,"Hi there,

The `pyopenssl.py` under `contrib/pyopenssl.py` is using `select.select` without trapping the exception raised when the underlying _select(2)_ system call returns EINTR.  This results in an exception (example here using requests):

```
    response = session.request(method, full_url, params=params, timeout=TIMEOUT, headers=HTTP_HEADERS)
  File "".../.virtualenv/local/lib/python2.7/site-packages/requests/sessions.py"", line 465, in request
    resp = self.send(prep, **send_kwargs)
  File "".../.virtualenv/local/lib/python2.7/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File "".../.virtualenv/local/lib/python2.7/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File "".../.virtualenv/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File "".../.virtualenv/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 372, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""/usr/lib/python2.7/httplib.py"", line 1127, in getresponse
    response.begin()
  File ""/usr/lib/python2.7/httplib.py"", line 453, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python2.7/httplib.py"", line 409, in _read_status
    line = self.fp.readline(_MAXLINE + 1)
  File ""/usr/lib/python2.7/socket.py"", line 480, in readline
    data = self._sock.recv(self._rbufsize)
  File "".../.virtualenv/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 184, in recv
    [self.socket], [], [], self.socket.gettimeout())
select.error: Interrupted system call
```

Current versions of pyopenssl don't make these calls; perhaps this can be fixed by updating the internal dependency?
## 
"
699,Fixing appengine tests,2015-08-21T16:33:16Z,2015-08-21T22:48:04Z,,,,"Pulling code from Google/oauth2client to fetch the newest SDK.

Fingers crossed that this will work the first time. Travis can be belligerent about caching.

Fixes #697.
"
698,"Cannot connect to ""https://cdn01.boxcdn.net/_assets/img/not_found_box_logo-Czx5Gh.png""",2015-08-21T10:56:49Z,2015-08-21T17:52:57Z,,,,"My code:

'''
import urllib3
p = urllib3.PoolManager()
p.urlopen('GET', 'https://cdn01.boxcdn.net/_assets/img/not_found_box_logo-Czx5Gh.png')
print(p.data)
'''

My Python console never return me anything, this is Python itself or urllib3 problem ? If this is a Python problem then I think it should be fixed as soon as possible.
"
697,AppEngine tests failing on travis,2015-08-21T05:56:46Z,2015-08-21T22:48:04Z,,ImportError,ImportError: No module named dev_appserver,"```
Traceback (most recent call last):
  File "".tox/gae/bin/nosetests"", line 11, in <module>
    sys.exit(run_exit())
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nose/core.py"", line 121, in __init__
    **extra_args)
  File ""/opt/python/2.7.9/lib/python2.7/unittest/main.py"", line 94, in __init__
    self.parseArgs(argv)
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nose/core.py"", line 145, in parseArgs
    self.config.configure(argv, doc=self.usage())
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nose/config.py"", line 346, in configure
    self.plugins.configure(options, self)
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 284, in configure
    cfg(options, config)
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 99, in __call__
    return self.call(*arg, **kw)
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nose/plugins/manager.py"", line 167, in simple
    result = meth(*arg, **kw)
  File ""/home/travis/build/shazow/urllib3/.tox/gae/lib/python2.7/site-packages/nosegae.py"", line 91, in configure
    import dev_appserver
ImportError: No module named dev_appserver
ERROR: InvocationError: '/home/travis/build/shazow/urllib3/.tox/gae/bin/nosetests -c /home/travis/build/shazow/urllib3/test/appengine/nose.cfg test/appengine'
```

https://travis-ci.org/shazow/urllib3/jobs/76562407

@jonparrott Could you take a look please?
"
696,Add support to pyOpenSSL for TLSv1.1 and TLSv1.2,2015-08-21T01:46:43Z,2015-11-20T20:21:05Z,,,,"Closes #695
"
695,PyOpenSSL support doesn't work with TLSv1.1 and TLSv1.2,2015-08-20T01:26:50Z,2015-11-20T20:21:05Z,,,,"`docker-py` tries to use latest protocol version in the ssl module, so use ssl.PROTOCOL_TLSv1_2. Unfortunately urllib3.contrib.pyopenssl only has SSLv2_3 and TLSv1 so it breaks:

https://github.com/shazow/urllib3/blob/master/urllib3/contrib/pyopenssl.py#L68
"
694,Timeout calculation vulnerable to clock changes,2015-08-19T09:03:30Z,,,,,"The calculation of timeouts in util/timeout.py uses time.time() which is the wall clock time which may be adjusted via various mechanisms such as NTP. This means that an NTP adjustment can make the timeout unduly short or long. Python 3.3 adds time.monotonic() which is a clock that is unaffected by clock changes so will provide more predictable timeouts.
## 
"
693,"Moved the Auth part before doing the split on /, ? and #, to allow those characters in passwords",2015-08-10T09:52:39Z,2015-08-13T17:19:03Z,,,,"This issue i found using PIP. Therefore i made the change.
"
692,"[wip] SOCKS support, take 3",2015-08-09T03:09:03Z,2015-12-29T20:28:13Z,,,,"**[WIP]**

Here is an attempt at SOCKS support with the latest version of PySocks. At the moment there are no tests, though I plan to add them, and definitely encourage others to add them and help with them. I've been testing it with a local script so far. The functionality should all work at the moment, but I can't say that with serious confidence until full tests have been added.

I had to choose between peppering ProxyManager with a few if-branches, or subclassing it and making a SOCKSProxyManager. I decided to go with the former, to preserve full backwards compatibility and allow end users to use the familiar `ProxyManager(""socks5://localhost:9050"")` syntax. A way to make it more OO while preserving compatibility would be to add override **new** for ProxyManager and defer to the SOCKSProxyManager class when a SOCKS URL scheme is seen, but it's debatable whether that is more or less Pythonic.

Unfortunately, a pure-Python SOCKS4 and SOCKS5 server may not be feasible for testing, unless someone has one already written. The ones I found on the Internet have various issues. This is why I've held off on comitting anything since then, and frankly didn't want to spend the time hunting around to get a full test suite working. So, test assistance is very much needed.

I am not interested in the bounty, and would like it split between the other contributors.
"
691,"source IP binding: getsockaddrarg: AF_INET address must be tuple, not str",2015-08-08T22:19:30Z,2015-08-09T08:14:37Z,,TypeError,"TypeError: getsockaddrarg: AF_INET address must be tuple, not str","I'm using [requests-toolbelt](https://github.com/sigmavirus24/requests-toolbelt)'s [source adapter](https://github.com/sigmavirus24/requests-toolbelt/blob/master/requests_toolbelt/adapters/source.py), and I believe this issue lies within urllib3, please correct me if wrong :)

```
Python 2.7.6 (default, Jun 22 2015, 17:58:13)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import requests
>>> from requests_toolbelt.adapters.source import SourceAddressAdapter
>>> s = requests.Session()
>>> s.mount('http://', SourceAddressAdapter('172.31.12.86'))
>>> r = s.get('http://curlmyip.com')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 477, in get
    return self.request('GET', url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 465, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py"", line 349, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python2.7/httplib.py"", line 979, in request
    self._send_request(method, url, body, headers)
  File ""/usr/lib/python2.7/httplib.py"", line 1013, in _send_request
    self.endheaders(body)
  File ""/usr/lib/python2.7/httplib.py"", line 975, in endheaders
    self._send_output(message_body)
  File ""/usr/lib/python2.7/httplib.py"", line 835, in _send_output
    self.send(msg)
  File ""/usr/lib/python2.7/httplib.py"", line 797, in send
    self.connect()
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connection.py"", line 155, in connect
    conn = self._new_conn()
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connection.py"", line 134, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/util/connection.py"", line 77, in create_connection
    sock.bind(source_address)
  File ""/usr/lib/python2.7/socket.py"", line 224, in meth
    return getattr(self._sock,name)(*args)
TypeError: getsockaddrarg: AF_INET address must be tuple, not str
```
"
690,Preliminary SOCKS proxy support,2015-08-07T09:04:39Z,2015-12-19T19:37:04Z,Bounty,,,"We've had some attempts to get this going over the years, this is the umbrella issue tracking these attempts and the goal is to get somekind of baseline support for SOCKS proxies into the urllib3 before we aspire towards more advanced features.

Related attempts (should be a great reference if anyone wants to tackle this):
- https://github.com/shazow/urllib3/pull/68 (original attempt using socksipy; outdated code)
- https://github.com/shazow/urllib3/pull/284 (another attempt using pysocks‚Äîa more up-to-date fork of socksipy; outdated code)
- https://github.com/shazow/urllib3/pull/486 (most recent attempt, refactored version of pysocks attempt)
- https://github.com/shazow/urllib3/pull/692 (another recent attempt, but mixes pysocks logic into the connectionpools)
- https://github.com/shazow/urllib3/issues/630 (recipe for wrapping pysocks)
- https://github.com/shazow/urllib3/pull/762 (@lukasa)

Suggestions:
- SOCKS proxy support should ideally be built into ProxyManager (or a similar manager if combining is not possible).
- Should use pysocks (unless there's anything newer?). Avoid vendoring if possible, we can enable support when pysocks is installed (also include a `urllib3[socks]` install tag which will pull in the dependency).

To complete this issue, the feature needs to be documented and have full test coverage so that it can be merged into urllib3 core (or at least contrib).
## 
"
689,connectionpool.py does not respect disable_warnings(),2015-08-04T08:55:40Z,2015-08-04T09:17:40Z,,,,"On a Centos & system, python-urllib3-1.10.2-1.el7_1.noarch
/usr/lib/python2.7/site-packages/urllib3/connectionpool.py:769: InsecureRequestWarning: Unverified HTTPS request is being made.
Going to said line, there is indeed an extraneous  if not conn.is_verified warnings.warn
"
688,Rely on six to import httplib or http.client,2015-07-29T17:36:09Z,2015-07-29T18:05:56Z,,,,"Regarding https://github.com/elastic/elasticsearch-py/issues/253
"
687,SSLError when reusing connectionpool after a ReadTimeoutError.,2015-07-29T13:47:18Z,2015-07-30T14:50:56Z,,`urllib3.exceptions.SSLError,`urllib3.exceptions.SSLError: Underlying socket connection gone (_ssl.c:1568)`,"I have a `HTTPSConnectionPool` that throws a read time out, then (in a `__del__`) I reuse the same connection (created with urllib3.connection_from_url), then in case of error I do some teardown using the same connection and I get this error when reusing the connection from url object.
`urllib3.exceptions.SSLError: Underlying socket connection gone (_ssl.c:1568)`

reproduce with something like this:

``` Python
conn = urllib3.connection_from_url(my_url)
try:
    conn.request('POST', '/mypath', body=mybody, headers=myheaders, timeout=10, retries=False)
except urllib3.ReadTimeoutError:
    conn.request('DELETE', '/someobect', headers=myheaders,  timeout=10, retries=False)
```

The read time out is due to an internal server error and the following requests to the same server work as expected. A workaround for now is to create a new connection object for each request.
"
686,Use debug instead of info to log new connection,2015-07-29T12:02:54Z,2015-07-29T12:03:44Z,,,,
685,Replace select.select with util functions,2015-07-29T03:51:52Z,2016-11-02T14:44:43Z,,,,"select.poll is preferable when it is available. Unfortunately it isn't
available on every operating system but select.select is, so we can wrap
the logic to fall back to select.select from select.poll in helper
functions in urllib3.util.

Closes #589 

---

It is late here and this is more of a straw man PR than something I'm certain works. I was careful about the logic though so it should work (but again, I haven't done extensive testing).
"
684,urllib3 1.11 does not provide the extra 'secure',2015-07-23T17:10:41Z,2015-09-22T03:22:37Z,,,,"I tried with Python 2.7 and 2.6 inside different virtualenv.

``` bash
pip install 'urllib3[secure]'
```
"
683,Clean changelog,2015-07-22T08:35:10Z,2015-07-22T09:32:38Z,,,,
682,A socket timeout exception in Python 3 shows also a TypeError,2015-07-21T11:06:46Z,2015-07-21T11:18:11Z,,"TypeError, urllib3.exceptions.ReadTimeoutError, requests.exceptions.ReadTimeout","TypeError: getresponse() got an unexpected keyword argument 'buffering', urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commons.wikimedia.org', port=443): Read timed out. (read timeout=30), requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commons.wikimedia.org', port=443): Read timed out. (read timeout=30)","When a timout occurs it also shows the TypeError because you first try to call it with a `buffering` keyword argument, which does not exist in Python 3. I'm not sure if there is a good way in Python 3 to clear that TypeError exception. See also #537.

```
ERROR: Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 331, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 333, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/lib/python3.4/http/client.py"", line 1172, in getresponse
    response.begin()
  File ""/usr/lib/python3.4/http/client.py"", line 351, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python3.4/http/client.py"", line 313, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/usr/lib/python3.4/socket.py"", line 371, in readinto
    return self._sock.recv_into(b)
  File ""/usr/lib/python3.4/ssl.py"", line 745, in recv_into
    return self.read(nbytes, buffer)
  File ""/usr/lib/python3.4/ssl.py"", line 617, in read
    v = self._sslobj.read(len, buffer)
socket.timeout: The read operation timed out

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 362, in send
    timeout=timeout
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File ""/usr/lib/python3/dist-packages/urllib3/util/retry.py"", line 245, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/usr/lib/python3/dist-packages/six.py"", line 625, in reraise
    raise value
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 336, in _make_request
    self, url, ""Read timed out. (read timeout=%s)"" % read_timeout)
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='commons.wikimedia.org', port=443): Read timed out. (read timeout=30)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/matanya/core/pywikibot/data/api.py"", line 1911, in submit
    body=body, headers=headers)
  File ""/home/matanya/core/pywikibot/tools/__init__.py"", line 1227, in wrapper
    return obj(*__args, **__kw)
  File ""/home/matanya/core/pywikibot/comms/http.py"", line 240, in request
    r = fetch(baseuri, method, body, headers, **kwargs)
  File ""/home/matanya/core/pywikibot/comms/http.py"", line 359, in fetch
    error_handling_callback(request)
  File ""/home/matanya/core/pywikibot/comms/http.py"", line 276, in error_handling_callback
    raise request.data
  File ""/home/matanya/core/pywikibot/comms/http.py"", line 255, in _http_process
    auth=auth, timeout=timeout, verify=True)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 457, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 569, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 422, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='commons.wikimedia.org', port=443): Read timed out. (read timeout=30)
```
"
681,pyopenssl: maximum recursion depth exceeded while calling a Python object,2015-07-21T09:57:31Z,,Needs More Information,,,"I opened this issue on [requests](https://github.com/kennethreitz/requests/issues/2681) but just realized it's actually in the vendored urllib3.
## 
"
680,Possible KeyError issue with Python 3.6,2015-07-18T19:18:33Z,2015-08-19T06:27:17Z,,KeyError,"KeyError: ('https', 'eu.wiktionary.org', 443)","As Travis is now using Python 3.6 when using `nightly` we had recently build breakage partially due to a `KeyError` raised inside of `urllib3`. Unfortunately I can't provide something simple to reproduce it, as it only happens (for me) when I do a full test run of pywikibot. Whenever I only tested the parts where the error occurred I got nothing. I'll still try to narrow it down and maybe provide a good test example but I figured you know `urllib3` and might be able to either help me narrow it down or can test it on your own.

```
======================================================================
ERROR: testQueryApiGetter (tests.wikidataquery_tests.TestApiSlowFunctions)
Test that we can actually retreive data and that caching works.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/travis/build/xZise/pywikibot-core/tests/wikidataquery_tests.py"", line 252, in testQueryApiGetter
    data = w.query(q)
  File ""/home/travis/build/xZise/pywikibot-core/pywikibot/data/wikidataquery.py"", line 601, in query
    data = self.getDataFromHost(fullQueryString)
  File ""/home/travis/build/xZise/pywikibot-core/pywikibot/data/wikidataquery.py"", line 563, in getDataFromHost
    resp = http.fetch(url)
  File ""/home/travis/build/xZise/pywikibot-core/pywikibot/comms/http.py"", line 359, in fetch
    error_handling_callback(request)
  File ""/home/travis/build/xZise/pywikibot-core/pywikibot/comms/http.py"", line 276, in error_handling_callback
    raise request.data
  File ""/home/travis/build/xZise/pywikibot-core/pywikibot/comms/http.py"", line 255, in _http_process
    auth=auth, timeout=timeout, verify=True)
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/sessions.py"", line 465, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/adapters.py"", line 337, in send
    conn = self.get_connection(request.url, proxies)
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/adapters.py"", line 251, in get_connection
    conn = self.poolmanager.connection_from_url(url)
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/packages/urllib3/poolmanager.py"", line 139, in connection_from_url
    return self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/packages/urllib3/poolmanager.py"", line 125, in connection_from_host
    self.pools[pool_key] = pool
  File ""/home/travis/virtualenv/python3.6-dev/lib/python3.6/site-packages/requests/packages/urllib3/_collections.py"", line 66, in __setitem__
    _key, evicted_value = self._container.popitem(last=False)
KeyError: ('https', 'eu.wiktionary.org', 443)
```

See also: https://phabricator.wikimedia.org/T106212 and http://bugs.python.org/issue24667
"
679,Use HTTPHeaderDict in request,2015-07-18T04:48:23Z,2015-07-21T13:31:04Z,,,,"Supersedes #633 
"
678,"Add a ""secure"" extras point",2015-07-17T14:49:17Z,2015-07-17T17:03:12Z,,,,"To be user-friendlier, add ""secure"" as an extra so users can do

```
pip install urllib3[secure]
```

Further, only install the dependencies for SNI on versions of Python
that need them (2.6 and ... sort of 2.7). We install certifi on every
version of Python because it's awesome and the stdlib doesn't provide
any of its functionality on any version of Python.

Closes #677
"
677,"setup.py extra_require for certifi, pyopenssl, other?",2015-07-17T14:42:32Z,2015-07-17T17:03:12Z,,,,"Try to setup as many of https://urllib3.readthedocs.org/en/latest/security.html#security's recommended dependencies as possible.

Maybe something like `pip install urllib3[secure]`
"
676,Release v1.11,2015-07-16T10:30:56Z,2015-07-16T10:33:07Z,,,,"Pending https://github.com/shazow/urllib3/milestones/v1.11
"
675,Add jmoldow to CONTRIBUTORS,2015-07-15T20:35:07Z,2015-07-15T20:46:06Z,,,,
674,Make stream() satisfy its v1.10 contract,2015-07-14T20:21:56Z,2015-07-15T20:13:28Z,,,,"Here's my attempt at a fix for #673. Let me know what you think.

In 1.10 through 1.10.2, `HTTPResponse.stream()` always called `HTTPResponse.read()`.  This method catches all `httplib.HTTPException` (including `httplib.IncompleteRead`), and re-raises them as
`ProtocolError`.

In 1.10.3 and 1.10.4, `HTTPResponse.stream()` may call `HTTPResponse.read_chunked()` instead. This method may (via the `_update_chunk_length()` helper method) raise `httplib.IncompleteRead`.
This exception will not be caught, and will be raised out of the call to `HTTPResponse.stream()`.

Fix the contract of `HTTPResponse.stream()` by making `HTTPResponse.read_chunked()` catch the same low-level exceptions that `HTTPResponse.read()` catches. This is accomplished by refactoring this
try/except logic into a shared context manager.

Fixes #673.
"
673,"stream() ""contract"" changed in 1.10.3, can now raise IncompleteRead",2015-07-14T18:52:04Z,2015-07-15T20:13:28Z,,,,"In 1.10.2, `HTTPResponse.stream()` always called `HTTPResponse.read()`. This method catches all `HTTPException` (including `IncompleteRead`), and re-raises them as `ProtocolError`.

In 1.10.3 and 1.10.4, `HTTPResponse.stream()` may call `HTTPResponse.read_chunked()` instead. This method may (via the `_update_chunk_length()` helper method) raise `IncompleteRead`. This exception will not be caught, and will be raised out of the call to `HTTPResponse.stream()`.

A project I work on recently upgraded to requests 2.7.0 (which has vendored urllib3 1.10.4), and has started to see crashes because of `IncompleteRead` being raised from `HTTPResponse.stream()`. We previously would catch `RequestException`, as requests would catch `ProtocolError` (but not `IncompleteRead`) and re-raise a `ChunkedEncodingError` in `Response.iter_content()`.

I'm not familiar with `urllib3.util.retry`, but at a glance it also seems like this change would negatively affect the `_is_read_error()` helper method.
"
672,"pyopenssl broken in contrib/ , throws TypeError",2015-07-14T18:25:13Z,,,,,"Using py 2.7.9 packages

Lines 289 -> 291 in contrib/pyopenssl

makes a call to ssl.SSLError that's broken.

Here's a code snippet that demonstrates

import ssl
import OpenSSL

str(ssl.SSLError('alice',OpenSSL.SSL.Error('bob')))

indeed even

str(ssl.SSLError('foo', Exception('foo')))

reproduces it.

Personally I don't understand wrapping the OpenSSL exception in the ssl exception. Removing the try-except solves the problem handily.
## 
"
671,Indicate hostname in SubjectAltNameWarning,2015-07-09T19:24:34Z,2015-07-09T19:31:40Z,,,,"This adds a new warning subclass for this warning specifically and has
it default to print exactly once (per message and therefore) per
hostname. This will reduce the number of warnings that users see and
provide an overall better user-experience. This allows the user to
determine the offending host and to only see the warning once per time
it connects to that host.
"
670,HTTPHeaderDict incompatible with python-requests' CaseInsensitiveDict,2015-07-09T15:45:47Z,2015-07-09T15:49:16Z,,,,"Hi, please see details here, especially comments #2 and #3 on that bug:
https://bugzilla.redhat.com/show_bug.cgi?id=1241561

```
On line 93 of requests/adapters.py there is:

response.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))
```

where resp['headers'] is now of type urllib3.HTTPHeaderDict so the following happens before and after this line is executed:

```
resp['headers'] is

HTTPHeaderDict({'soapserver': 'SOAP::Lite/Perl/1.11', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'content-encoding': 'gzip', 'transfer-encoding': 'chunked', 'set-cookie': 'Bugzilla_login_request_cookie=or9UbA03it; domain=bugzilla.redhat.com; path=/; HttpOnly', 'vary': 'Accept-Encoding,User-Agent', 'server': 'Apache', 'connection': 'close', 'date': 'Thu, 09 Jul 2015 15:33:28 GMT', 'x-frame-options': 'SAMEORIGIN', 'content-type': 'text/xml'})

response.headers is 

<class 'requests.structures.CaseInsensitiveDict'> {'soapserver': ('soapserver', 'SOAP::Lite/Perl/1.11'), 'x-xss-protection': ('x-xss-protection', '1; mode=block'), 'x-content-type-options': ('x-content-type-options', 'nosniff'), 'content-encoding': ('content-encoding', 'gzip'), 'transfer-encoding': ('transfer-encoding', 'chunked'), 'set-cookie': ('set-cookie', 'Bugzilla_login_request_cookie=or9UbA03it; domain=bugzilla.redhat.com; path=/; HttpOnly'), 'vary': ('vary', 'Accept-Encoding,User-Agent'), 'server': ('server', 'Apache'), 'connection': ('connection', 'close'), 'date': ('date', 'Thu, 09 Jul 2015 15:33:28 GMT'), 'x-frame-options': ('x-frame-options', 'SAMEORIGIN'), 'content-type': ('content-type', 'text/xml')}
```

then the value for content-type becomes ('content-type', 'text/xml') which breaks elsewhere. 

We can either patch python-request to account for that (I've found one place and it works for me) or better provide a backward compatible update to urllib3.  Any idea if we can make the later ? 
"
669,Missed {0},2015-07-06T13:25:38Z,2015-07-06T14:21:14Z,,,,
668,Fix IPv6 HTTPS proxy,2015-07-06T09:39:06Z,2015-07-06T19:05:33Z,,`MaxRetryError,"`MaxRetryError: HTTPSConnectionPool(host='localhost', port=53619): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', gaierror(-2, 'Name or service not known')))`","When using an IPv6 proxy for https connections the connection would fail with

`MaxRetryError: HTTPSConnectionPool(host='localhost', port=53619): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', gaierror(-2, 'Name or service not known')))`

That happens because the hard brackets are not removed before calling getaddrinfo.

This fixes that.
"
667,show invalid fingerprint in exception,2015-07-04T13:56:47Z,2015-07-04T14:01:43Z,,,,
666,Fix Issue #663,2015-07-01T14:21:01Z,2015-07-01T14:32:21Z,,,,
665,Tell real URL from response,2015-07-01T11:50:15Z,2015-07-01T12:03:52Z,,,,"In urllib2's HTTPResponse we have `.geturl()` that retrieves the actual URL that ended up being downloaded after all the redirections.

I saw no such thing in urllib3's documentation, nor any relevant methods in the actual code (as seen with `dir()`). Trying to get `response.getheader('host')` didn't help either.

This is especially important in scenarios involving constructing absolute URLs from relative links inside HTML pages.
"
664,contrib.appengine.AppEngineManager,2015-06-30T19:50:55Z,2015-07-17T08:18:22Z,,,,"This is currently _very_ rough and early. I want to make sure I'm on the right track before continuing.

Outstanding questions:
1. What functionality should be covered by the connection pool? (retries, etc.)
2. Should I just inherit from TestConnectionPool and null out the tests that aren't applicable to GAE instead of duplicating code?
3. How should this connection class be wired in? On GAE Sandbox environments w/o sockets, I assume this should be default.
4. What exact edge cases should this class be aware; e.g, where does URLFetch behavior differ from what's expected of httplib/sockets? Do the tests already cover this?

[_shazow edit: Fixes #664]_
"
663,Problems with documentation TOC,2015-06-29T14:13:38Z,2015-07-01T14:32:37Z,,,,"Several `.. toctree::` commands in index.rst make following problems:
- On page security.html Previous topic is Contrib Modules instead of PoolManager
- Page Collections didn't include in ""Next topic chain"".

`security` included in main toctree twice (begin of index.rst) but `exceptions` not once.

Also there is one build warning: ""urllib3\docs\security.rst:117: WARNING: duplicate label pyopenssl, other instance in urllib3\docs\contrib.rst"".
"
662,Fix attribute name in docsting,2015-06-29T13:58:00Z,2015-06-29T15:35:52Z,,,,
661,Added as contributor.,2015-06-28T16:51:38Z,2015-06-28T17:06:32Z,,,,
660,close response on connection errors during read,2015-06-27T10:19:31Z,2015-06-28T14:49:35Z,,,,"I've added a couple of test cases for this in test_socketlevel.py that just compare the poolsize before and after calls to read (that fail). Without the changes to request.py these tests will fail.
I also moved the finally block to above the decoding code, which means we get to release the connection a little bit sooner. If there's an exception during self._fp.read that code will be skipped anyway.
"
659,Connection not returned to pool when streaming and socket error occurs -> pool becomes exhausted,2015-06-26T10:39:21Z,,,,,"When stream=True and reading large files using requests.models.iter_content, which calls urllib3.response.stream -> self.read, if a SocketTimeout error occurs (and presumably BaseSSLError (and possibly HTTPException)) the self._original_response is not closed and therefore the connection is not released. This means that in blocking connection_pool scenarios the pool will eventually become exhausted.

Hi again!
I've tested this and can confirm that this is the case. It's going to be very hard to add a test case in your test suite to reproduce this in your test env. Looking at the code in stream we iterate over calls to read. If an exception is thrown in there and we haven't read all of the content self._original_response.isclosed() will return False, and thus in your finally block (which btw can be moved above the decoding/decompress code) we will not execute the self.release_conn statement.
Presumably, all errors at this point are connection errors so we're not going to be doing any more reading of the response. In your connection handlers you'll need to do a self._original_response.close() so that we release the conn. Here's the fix (sorry, still don't know how to format code):
response.py.215:

```
        except SocketTimeout:
            # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
            # there is yet no clean way to get at it from this context.
            # the response is not not closed we must do it now to ensure that the
            # connection is released back to the pool
            if self._original_response and not self._original_response.isclosed():
                self._original_response.close()

            raise ReadTimeoutError(self._pool, None, 'Read timed out.')

        except BaseSSLError as e:
            # the response is not not closed we must do it now to ensure that the
            # connection is released back to the pool
            if self._original_response and not self._original_response.isclosed():
                self._original_response.close()

            # FIXME: Is there a better way to differentiate between SSLErrors?
            if 'read operation timed out' not in str(e):  # Defensive:
                # This shouldn't happen but just in case we're missing an edge
                # case, let's avoid swallowing SSL errors.
                raise

            raise ReadTimeoutError(self._pool, None, 'Read timed out.')

        except HTTPException as e:
            # This includes IncompleteRead.
            # the response is not not closed we must do it now to ensure that the
            # connection is released back to the pool
            if self._original_response and not self._original_response.isclosed():
                self._original_response.close()

            raise ProtocolError('Connection broken: %r' % e, e)
   # this can be moved here since you're not using the connection anymore (are you?)
   # and therefore it can go back to the pool now - will speed things up a little
    finally:
        if self._original_response and self._original_response.isclosed():
            self.release_conn()

        self._fp_bytes_read += len(data)

        try:
            if decode_content and self._decoder:
                data = self._decoder.decompress(data)
        except (IOError, zlib.error) as e:
            raise DecodeError(
                ""Received response with content-encoding: %s, but ""
                ""failed to decode it."" % content_encoding, e)

        if flush_decoder and decode_content and self._decoder:
            buf = self._decoder.decompress(binary_type())
            data += buf + self._decoder.flush()

        if cache_content:
            self._body = data

        return data
```
## 
"
658,Adding Disassem to CONTRIBUTORS.txt,2015-06-25T16:49:01Z,2015-06-25T16:51:12Z,,,,
657,Fix usage headers argument for PoolManager,2015-06-25T10:08:48Z,2015-06-25T16:32:53Z,Contributor Friendly ‚ô•,,,"Argument headers for PoolManager doesn't work as it should (""Headers to include with all requests, unless other headers are given explicitly."") for request method.

Sample:

``` python
import urllib3
manager = urllib3.PoolManager(headers = urllib3.make_headers(accept_encoding = True, user_agent = 'urllib3'))
request = manager.request('GET', 'http://httpbin.org/headers')
print request.data
```

Result:

``` json
{
  ""headers"": {
    ""Accept-Encoding"": ""identity"",
    ""Host"": ""httpbin.org""
  }
}
```
"
656,Cannot load fbcdn-profile-a.akamaihd.net/hprofile++ (spdy),2015-06-25T04:18:23Z,,,,,"Urllib3 and urllib.request/urllib2 cannot load this url, can you implement a fix or patch?

```
import urllib3
import urllib.request

p = urllib3.PoolManager()
#urllib2 test
#r = urllib.request.urlopen(""https://fbcdn-profile-a.akamaihd.net/hprofile-ak-xfp1/v/t1.0-1/p32x32/1795556_454025928112183_7252557420120250438_n.jpg?oh=2f63a1f58e29fd34605626a87fdd7202&oe=55EA9D3F&__gda__=1445088810_e91fee57938ed896996ea686b6a0e628"")

r = p.urlopen('GET', ""https://fbcdn-profile-a.akamaihd.net/hprofile-ak-xfp1/v/t1.0-1/p32x32/1795556_454025928112183_7252557420120250438_n.jpg?oh=2f63a1f58e29fd34605626a87fdd7202&oe=55EA9D3F&__gda__=1445088810_e91fee57938ed896996ea686b6a0e628"")
print(r.data)
#urllib2 test
#print(r.read())
```

But without https the above code works. Works on Firefox.

This may interest you:

```
Request URL:    https://fbcdn-profile-a.akamaihd.net/hprofile-ak-xfp1/v/t1.0-1/p32x32/1795556_454025928112183_7252557420120250438_n.jpg?oh=2f63a1f58e29fd34605626a87fdd7202&oe=55EA9D3F&__gda__=1445088810_e91fee57938ed896996ea686b6a0e628
Request Method:     GET
Status Code:    HTTP/1.1 200 OK
Request Headers 02:35:52.000
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.4) Gecko/20150509 Firefox/31.9 PaleMoon/25.4.1
Host:   fbcdn-profile-a.akamaihd.net
Connection: keep-alive
Accept-Language:    en-US,en;q=0.5
Accept-Encoding:    gzip, deflate
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Response Headers Œî350ms
X-Firefox-Spdy: 3
Timing-Allow-Origin:    *
Last-Modified:  Mon, 15 Jun 2015 15:59:16 GMT
Expires:    Wed, 08 Jul 2015 17:35:50 GMT
Date:   Wed, 24 Jun 2015 17:35:50 GMT
Content-Type:   image/jpeg
Content-Length: 1255
Cache-Control:  no-transform, max-age=1209600
Access-Control-Allow-Origin:    *
Received Image 32x32px, Œî0ms
```
## 
"
655,Default timeout,2015-06-24T10:33:07Z,,,,,"o/

In [util.Timeout](https://github.com/kennethreitz/requests/blob/8b5e457b756b2ab4c02473f7a42c2e0201ecc7e9/requests/packages/urllib3/util/timeout.py#L19), urllib3 use the socket global default timeout, but not the ""user set"" default timeout, probably modified by `socket.setdefaulttimeout(10)`, so ignoring any `socket.setdefaulttimeout`. Why ?

I think it would be cool to be able to set a ""setdefaulttimeout"" and have it impact `urllib3`, so also impacting `requests`.

In another hand, one should not know that `urllib3` or `request` use `socket` under the hood, and doing a `socket.setdefaulttimeout` to configure the default timeout of `urllib3` or `requests`  tie them, I mean, if you change the underlying used module, the timeout will still be applied, but to the wrong module, so probably being ignored. Also this is surprising to set a setting on a module A to impact the behavior of module B.

So we have pros and cons here, but totally ignoring socket.setdefaulttimeout may not be the better solution, conversation opened :)
## 
"
654,Chunked responses lack data,2015-06-23T15:41:58Z,2015-09-05T14:37:24Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'read',"Using urllib3 1.10.3 in python-etcd client.

We were hitting a really weird bug when occasonally we'd get 

``` python
Traceback (most recent call last):
  File ""etcd/lib_etcd.py"", line 116, in _result_from_response
    res = json.loads(response.data.decode('utf-8'))
  File ""~/soft/infra-virtenv/local/lib/python2.7/site-packages/urllib3/response.py"", line 166, in data
    return self.read(cache_content=True)
  File ""~/infra-virtenv/local/lib/python2.7/site-packages/urllib3/response.py"", line 239, in read
    data = self._fp.read()
  File ""/usr/lib/python2.7/httplib.py"", line 549, in read
    return self._read_chunked(amt)
  File ""/usr/lib/python2.7/httplib.py"", line 622, in _read_chunked
    self._safe_read(2)      # toss the CRLF at the end of the chunk
  File ""/usr/lib/python2.7/httplib.py"", line 664, in _safe_read
    chunk = self.fp.read(min(amt, MAXAMOUNT))
AttributeError: 'NoneType' object has no attribute 'read'
```

on a chunked response from Etcd. 

It seems to be related to the fact that `HttpConectionPool#urlopen` by default `release_conn=True`, which is dictated by `preload_content`

``` python
~/soft/infra-virtenv/lib/python2.7/site-packages/urllib3/connectionpool.py:510
        if release_conn is None:
            release_conn = response_kw.get('preload_content', True)
```

which means that if it is a chunked response, it will be closed immediately without the requesting library being able to read the content, since

``` python
~/soft/infra-virtenv/lib/python2.7/site-packages/urllib3/response.py:135
        # We certainly don't want to preload content when the response is chunked.
        if not self.chunked and preload_content and not self._body:
            self._body = self.read(decode_content=decode_content)
```

This is fine, as long as the connection doesn't get reused, but under heavy load it causes things to break really badly. 
"
653,Get rid of any trailing whitespace on hostname,2015-06-22T21:00:40Z,2015-07-01T03:01:06Z,,,,"Add port.rstrip to get rid of any trialing whitespace on the hostname.
Any trialing whitespace will cause port.isdigit() to fail. This can be a
very hard bug to find. Many hostnames are taken from files and it is a
very simple thing for someone to leave a trailing space when typing in a
hostname.
"
652,Adding John Krauss to CONTRIBUTORS.txt,2015-06-14T13:42:21Z,2015-06-14T14:51:31Z,,,,
651,"Likely bug in retry/release-conn code, need tests",2015-06-13T09:58:39Z,2016-05-25T18:46:16Z,Help Wanted,,,"In this change: https://github.com/shazow/urllib3/pull/647/files#diff-211abff6a07837e4a89a8663a89d2c84R582

@jlatherfold writes:

> For the (TimeoutError, HTTPException, SocketError, ConnectionError) exception handler you don't raise (presumably because there may be retries) and you will go on to release the conn and make another call to urlopen (if retries). But you call it with release_conn=True (since you've just set it to true) - what happens if that call succeeds? You'll release the conn back to the pool before the response is read. At least that's what it looks like to me.
"
650,Set cert_reqs to 'CERT_REQUIRED' by default if ca_certs is passed.,2015-06-12T22:30:47Z,2015-06-13T09:47:08Z,,,,"Fixes #579 
"
649,is_fp_closed should handle non-file-like body,2015-06-10T23:07:56Z,2015-06-11T07:27:06Z,,,,"is_fp_closed doesn't handle string-type bodies.  It checks whether obj.fp is None, but should probably also check whether obj is None.
"
648,[Docs] Suggest people `pip install cryptography`,2015-06-10T21:12:50Z,2015-06-12T09:33:00Z,,,,"When I tried to follow these instructions to use PyOpenSSL on Ubuntu 14.04, the `import urllib3.contrib.pyopenssl` line failed in Python as the `cryptography` was not already installed in my base Python.  Installing it separately (in the virtualenv in which I was doing this work) made it so these instructions worked.  I think it should be one of the requirements specified here; at worst, it will already be installed in the user's Python and be a no-op.

It was also necessary for me to `sudo apt-get install libffi-dev`, before some of the other `pip` requirements would install.  It may be worth noting that in a platform-agnostic way, or not.
"
647,Replenish pool when release_conn=False and error occurred,2015-06-10T18:26:13Z,2015-06-10T20:32:49Z,,,,"Replaces #646
Fixes #644
"
646,Cleanup properly after timeout or connection-based error.,2015-06-10T06:36:35Z,2015-06-10T20:32:51Z,,,,"If you run the test I've added without the changes it will fail. It should pass with the updates to connectionpool.py.
I've added cleanup code for BaseSSLError, CertificateError, and SSLError as well (which looks like it should be handled in the same way) but no test for this.
"
645,Don't blow up on regular tests if GAE isn't found.,2015-06-05T11:21:41Z,2015-06-05T13:24:42Z,,,,
644,"Connection pool exhausted when connection failures occur, should refill with empty connections",2015-06-04T08:34:00Z,2015-06-10T20:32:49Z,,,,"If preload_content is not specified on a request, connections are implicitly returned to the pool after the request is read. However, when a timeout error occurs or the connection is dropped the connection is closed but not returned in to the pool (since the response is None). This problem is compounded when retries are turned on since the next attempt will grab a new connection from the pool thus depleting it further. With non-blocking pools this is not a problem since we will create a new connection if the pool is empty but when the pool is blocking we have found that eventually the pool size becomes zero (after no.of connections timeout errors) which causes the calling application to hang on it's next request.
This can be fixed in connectionpool.urlopen in the exception handlers by explicitly returning the closed connection to the pool via _put_conn (if release_conn is false), since subsequent calls to _get_conn will check if the connection has been dropped and return a new one.
"
643,may be cause SSLError when has installed pyopenssl,2015-06-03T15:54:23Z,2015-06-03T16:26:27Z,,"Exception, Error","Exception:, Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]","when i install django i found this error:

``` python
pip install django     
Downloading/unpacking django
Cleaning up...
Exception:
Traceback (most recent call last):
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/commands/install.py"", line 274, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/req/req_set.py"", line 286, in prepare_files
    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/index.py"", line 194, in find_requirement
    page = self._get_page(main_index_url, req)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/index.py"", line 568, in _get_page
    session=self.session,
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/index.py"", line 670, in get_page
    resp = session.get(url, headers={""Accept"": ""text/html""})
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/sessions.py"", line 395, in get
    return self.request('GET', url, **kwargs)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/download.py"", line 237, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/sessions.py"", line 383, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/sessions.py"", line 486, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/adapters.py"", line 330, in send
    timeout=timeout
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/packages/urllib3/connectionpool.py"", line 480, in urlopen
    body=body, headers=headers)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/packages/urllib3/connectionpool.py"", line 285, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 973, in request
    self._send_request(method, url, body, headers)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1007, in _send_request
    self.endheaders(body)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 969, in endheaders
    self._send_output(message_body)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 829, in _send_output
    self.send(msg)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 791, in send
    self.connect()
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/packages/urllib3/connection.py"", line 164, in connect
    ssl_version=resolved_ssl_version)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/pip-1.6.dev1-py2.7.egg/pip/_vendor/requests/packages/urllib3/contrib/pyopenssl.py"", line 382, in ssl_wrap_socket
    cnx.do_handshake()
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 1442, in do_handshake
    self._raise_ssl_error(self._ssl, result)
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 1187, in _raise_ssl_error
    _raise_current_error()
  File ""/Users/dongweiming/firefly/venv/lib/python2.7/site-packages/OpenSSL/_util.py"", line 48, in exception_from_error_queue
    raise exception_type(errors)
Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'sslv3 alert handshake failure')]

Storing debug log for failure in /Users/dongweiming/.pip/pip.log
```

This is very confusing Because it always work nice.

Because i install pyopenssl . if i uninstall it. it will be ok.

```
$pip uninstall pyopenssl -q
Proceed (y/n)? y
$pip install django   
Downloading/unpacking django
  Downloading Django-1.8.2-py2.py3-none-any.whl (6.2MB): 6.2MB downloaded
Installing collected packages: django
Successfully installed django
Cleaning up...
```
"
642,[RFC] Warn if header parsing failed,2015-06-02T09:13:34Z,2015-06-12T16:49:28Z,,,,"Useful for broken servers like in kennethreitz/requests#2622

I'd like to get feedback for:
- The idea
- `logging` vs. `warnings`
- Loglevel
- Whatever you can come up with

@shazow @sigmavirus24 @Lukasa 
"
641,proper use of super().__init__() call,2015-06-02T05:12:59Z,2015-06-02T09:11:10Z,,,,"Amends #640...
"
640,Do not use `assert`. Use a ValueError instead,2015-06-01T20:56:19Z,2015-06-01T21:35:49Z,,,,"However we have to be backwards compatible.
See #634
"
639,Universal wheel,2015-05-31T16:44:44Z,2015-05-31T16:45:15Z,,,,":night_with_stars: 
"
638,:custard:,2015-05-31T16:22:29Z,2015-05-31T16:22:49Z,,,,":custard:
"
637,Docs: urllib3's versioning semantics,2015-05-31T10:12:07Z,2015-05-31T15:40:43Z,,,,"We don't use semver, but rather a compatibility-based scheme (which I feel is more popular in-practice for open source anyways). This outlines the details.
"
636,"Revert ""do not use assertions""",2015-05-30T15:09:45Z,2015-05-30T15:09:54Z,,,,"Reverts shazow/urllib3#634

Going to save this for a future major release.
"
635,make `python -m urllib3` work,2015-05-28T21:40:12Z,2015-06-12T16:49:47Z,,,,
634,do not use assertions,2015-05-28T20:04:47Z,2015-05-28T20:11:22Z,,,,"`assert` can be disabled at runtime.
"
633,HTTPHeaderDict in requests,2015-05-28T10:17:37Z,2015-07-19T10:16:49Z,,,,"Do we want to accept HTTPHeaderDict objects in request header fields interchangeably with dicts/lists of tuples? Any reason why not?

Right now if we do that, `foo=bar` headers get encoded as `foo: ('Foo': 'bar')`
- [x] Tests
- [ ] Fix

Fixes #632
"
632,Urllib3 HTTPHeaderDict and proxy_from_url problem,2015-05-28T07:16:36Z,2015-07-21T13:32:20Z,,,,"It is a little bit redundant if I reexplain this problem, here is my post about that problem include code of the script: http://www.prxbx.com/forums/showthread.php?tid=2188&pid=18453#pid18453

The main thing is if I do any proxy_from_url or ProxyManager too, that problem appear: http://pastebin.com/g2mK1CW4
And if I use HTTPHeaderDict, i cannot use proxy, and if I use self.headers from http.server, I CAN use proxy, but ALL other page without proxy cannot even load.
The only and best possible at this time I found is use self.headers from http.server with proxy and HTTPHeaderDict with normal page:

```
            if ""ghacks"" in self.host:
                self.pool = urllib3.proxy_from_url('http://127.0.0.1:7777/')
                headers = self.headers
```

Code: http://www.prxbx.com/forums/showthread.php?tid=2188&pid=17911#pid17911

Need:
pip install colorama
pip install pyOpenSSL
pip install urllib3

and Python 3.4.2
urllib3 1.10.4
"
631,Rough stab at App Engine environment tests.,2015-05-23T20:36:39Z,2015-06-02T09:31:30Z,,,,"- Added test/appengine, containing some code to step the sandbox version of httplib/socket/urllib
- Added test_urlfetch.py, checking the URLFetch is used when making http and https requests (not sure if this is the desired behavior at the moment).
- Added GAE environment to tox, currently requires you to set your PYTHONPATH=~/google-cloud-sdk/platform/google_appengine. I can configure travis to do this automatically.

Travis will definitely barf on this. To run tests locally, just install the cloud sdk (cloud.google.com/sdk) and set the python path as above, then run tox -e gae.

I can add more tests to cover situations where sockets are enabled, but I just wanted to get something basic out here to open the dialog.
"
630,[Small contribution] A way to combine urllib3 with PySocks and finally socks support,2015-05-19T09:19:45Z,2015-12-29T20:29:19Z,,,,"Hi!
I found a way to make urllib3 work with pySocks, I think we just need to modify a little bit to merge it into urllib3:

We will need chardet 2 for requests module:
pip install chardet urllib3
And pySocks for sure:
pip install pySocks
You can try socks proxy with Bitvise SSH Client, change the port 10080 to whatever you want. Hope this topic will at least help people who only want to use Socks with urllib3 or better will have socks5 built-in urllib3. Thank urllib3's author for making such a great library.

```
import time
import requests
from requests.packages.urllib3.connection import HTTPConnection as HTTPConnection_
import socks # https://github.com/Anorov/PySocks

def _init_hook(self, *args, **kw):
    self.socks = kw.pop('socks', None)
    self._init_org(*args, **kw)

def _new_conn_hook(self):
    socks_addr, socks_port = self.socks
    conn = socks.create_connection((self.host, self.port),
        proxy_type=socks.SOCKS5,
        proxy_addr=socks_addr,
        proxy_port=socks_port,
        timeout=self.timeout)
    return conn

HTTPConnection_._new_conn_org = HTTPConnection_._new_conn
HTTPConnection_._new_conn = _new_conn_hook

HTTPConnection_._init_org = HTTPConnection_.__init__
HTTPConnection_.__init__ = _init_hook

class SocksAdapter(requests.adapters.HTTPAdapter):
    def __init__(self, socks_addr, socks_port, *args, **kwargs):
        self.socks = (socks_addr, socks_port)
        super(SocksAdapter, self).__init__(*args, **kwargs)

    def get_connection(self, url, proxies=None):
        self.poolmanager.connection_pool_kw['socks'] = self.socks
        conn = super(SocksAdapter, self).get_connection(url, proxies)       
        return conn

def main():
    sess = requests.Session()
    sess.adapters['http://'] = SocksAdapter(""127.0.0.1"", 10080)
    url = ""http://www.ghacks.net/ip/""
    r = sess.get(url)
    print((r.content))

if __name__ == '__main__':
    main()
```
"
629,Change source_address every request ?,2015-05-18T16:34:35Z,2015-05-18T18:58:46Z,,,,"Hi everybody, I'm writting a proxy server with urllib3 that change source_address every time it open a request, for example:
- Request http://example.com/1.gif, source_address=""192.168.1.3""
- Request http://example.com/2.gif, source_address=""192.168.1.110""
- Request http://example.com/3.gif, source_address=""192.168.1.3""
- Request http://example.com/4.gif, source_address=""192.168.1.110""

...

Again and again, because I have cable and Wifi network, different IP, I want to do load balancing to improve my network speed and use as much bandwidth as possible, problem is how I can change source_address every request create, please me some tips to finish this problem.

Here is my test code, not even a proxy:

import urllib3
http = urllib3.PoolManager(num_pools=10, source_address=('192.168.1.110', 0))
http.source_address = ""192.168.1.3""

r = http.urlopen('GET', 'http://www.ghacks.net/ip/')
print(r.data.decode('utf-8'))
print(http.headers)
print(r._original_response.msg)

The line ""http.source_address = ""192.168.1.3"""" seem do nothing, the IP show still 192.168.1.110's IP.

It is possible because there is software already did that, dispatch-proxy in NodeJS but it is not perfect, at least there is no HTTPS support so I decide to write my own.

Thank!
"
628,Further improvements to pyopenssl sendall(),2015-05-18T01:03:44Z,2015-05-18T04:32:58Z,,,,"Trying to address the feedback from #626 and #627
- no semi-magic number
- use a memoryview, even if PyOpenSSL will still perform a conversion to string (to make cffi happy) today.  Futureproofing!
"
627,Fix performance of sendall() on large files,2015-05-16T21:29:32Z,2015-05-16T21:36:07Z,,,,"Fixes the significant performance issues I'm seeing as a result of the fix for #412 

Fixed #626 
"
626,pyopenssl performance hit from issue#412,2015-05-16T21:22:13Z,2015-05-16T21:36:07Z,,,,"The fix for issue #412 (25755abf27790118cbe1df9bd3b967085c039e9d) introduced a significant performance hit to large file uploads.

In my benchmarks testing with a 50 MB file:
- curl can upload the file in 0.5s (just for context)
- requests + urllib3 using the current implementation takes over 20 seconds (20-22s)
- requests + urllib3 with that fix reverted (`sendall` instead invokes `self.connection.sendall(data)`) uploads the file in 1.15s

The cost appears to be a result of a combination of two factors:
- OpenSSL will only write up to 16384 bytes at a time (the limit derives from the maximum amount of plaintext in a single SSL record)
- sendall() makes a copy of `data` (removing what was already sent), which is copying the entire payload

So with a 50 MB file, urllib3 is performing over 3000 allocations with an average size around 25 MB.  It gets worse when I try uploading even larger files...

Something like the following restores the performance.  There are still a large number of string copies, but they're of a limited (fixed) size.  I should add that I thought about using a memoryview here instead of the slices, but pyOpenSSL immediately turns around and casts back to bytes, so I didn't see a need for the complexity.

``` python
    def sendall(self, data):
        total_sent = 0
        while total_sent < len(data):
            sent = self._send_until_done(data[total_sent:total_sent+16384])
            total_sent += sent

```

With this implementation, I'm again seeing uploads complete in about 1.15s.
"
625,refactor assert_fingerprint further for efficiency and style,2015-05-16T04:02:47Z,2015-05-16T04:08:05Z,,,,"It occurred to me that the code in `assert_fingerprint` could be even simpler. ref: #619
"
624,Twice memory usage in `encode_multipart_formdata`,2015-05-15T00:54:10Z,,,,,"Basically, if you wants to upload file like 1GB it will cost you 2GB RAM.

```
Filename: /Users/kyrylo/.virtualenvs/drsdk/lib/python2.7/site-packages/requests/packages/urllib3/filepost.py

Line #    Mem usage    Increment   Line Contents
================================================
    59    981.3 MiB      0.0 MiB   @profile
    60                             def encode_multipart_formdata(fields, boundary=None):
    61                                 """"""
    62                                 Encode a dictionary of ``fields`` using the multipart/form-data MIME format.
    63                             
    64                                 :param fields:
    65                                     Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).
    66                             
    67                                 :param boundary:
    68                                     If not specified, then a random boundary will be generated using
    69                                     :func:`mimetools.choose_boundary`.
    70                                 """"""
    71    981.3 MiB      0.0 MiB       body = BytesIO()
    72    981.3 MiB      0.0 MiB       if boundary is None:
    73    981.3 MiB      0.0 MiB           boundary = choose_boundary()
    74                             
    75   1920.7 MiB    939.4 MiB       for field in iter_field_objects(fields):
    76    981.3 MiB   -939.4 MiB           body.write(b('--%s\r\n' % (boundary)))    77                             
    78    981.3 MiB      0.0 MiB           writer(body).write(field.render_headers())
    79    981.3 MiB      0.0 MiB           data = field.data
    80                             
    81    981.3 MiB      0.0 MiB           if isinstance(data, int):
    82                                         data = str(data)  # Backwards compatibility
    83                             
    84    981.3 MiB      0.0 MiB           if isinstance(data, six.text_type):
    85                                         writer(body).write(data)
    86                                     else:
    87   1920.7 MiB    939.4 MiB               body.write(data)
    88                             
    89   1920.7 MiB      0.0 MiB           body.write(b'\r\n')
    90                             
    91   1920.7 MiB      0.0 MiB       body.write(b('--%s--\r\n' % (boundary)))
    92                             
    93   1920.7 MiB      0.0 MiB       content_type = str('multipart/form-data; boundary=%s' % boundary)
    94                             
    95   2000.1 MiB     79.4 MiB       return body.getvalue(), content_type
```

P.S. using this profiler https://pypi.python.org/pypi/memory_profiler
## 
"
623,"Change ""Staring new HTTP/S connection..."" log message to DEBUG level",2015-05-14T18:48:15Z,2015-05-14T20:13:07Z,,,,"There are ways to relieve stdout/logs of this programmatically but I thought I'd address the only negative thing I've heard from various developers about urllib3.
"
622,Fix errors casting SSLError to string,2015-05-13T06:49:58Z,2015-05-13T21:07:58Z,,,,"This resolves part of #621.
"
621,failures in tests in 1.10.3 under python2.7,2015-05-13T00:37:08Z,,,"TypeError, AssertionError","TypeError: __str__ returned non-string (type Error), AssertionError: [call('Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)', <class 'urllib3.exceptions.SecurityWarning'>)]","``` bash
======================================================================
ERROR: test_verified_with_bad_ca_certs (test.contrib.test_pyopenssl.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 93, in test_verified_with_bad_ca_certs
    https_pool.request('GET', '/')
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_verified_without_ca_certs (test.contrib.test_pyopenssl.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 106, in test_verified_without_ca_certs
    https_pool.request('GET', '/')
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
tornado.general: WARNING: SSL Error on 18 ('::1', 51276, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown ca (_ssl.c:581)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_discards_connection_on_sslerror (test.contrib.test_pyopenssl.TestHTTPS_TLSv1)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 409, in test_discards_connection_on_sslerror
    self.assertRaises(SSLError, self._pool.request, 'GET', '/')
  File ""/usr/lib64/python2.7/unittest/case.py"", line 473, in assertRaises
    callableObj(*args, **kwargs)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
tornado.general: WARNING: SSL Error on 21 ('::1', 45956, 0, 0): [SSL: TLSV1_ALERT_UNKNOWN_CA] tlsv1 alert unknown ca (_ssl.c:581)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_set_ssl_version_to_sslv3 (test.contrib.test_pyopenssl.TestHTTPS_TLSv1)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 397, in test_set_ssl_version_to_sslv3
    self.assertRaises(SSLError, self._pool.request, 'GET', '/')
  File ""/usr/lib64/python2.7/unittest/case.py"", line 473, in assertRaises
    callableObj(*args, **kwargs)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
tornado.general: WARNING: SSL Error on 21 ('::1', 45957, 0, 0): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:581)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_ssl_version_as_short_string (test.contrib.test_pyopenssl.TestHTTPS_TLSv1)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 405, in test_ssl_version_as_short_string
    self.assertRaises(SSLError, self._pool.request, 'GET', '/')
  File ""/usr/lib64/python2.7/unittest/case.py"", line 473, in assertRaises
    callableObj(*args, **kwargs)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
tornado.general: WARNING: SSL Error on 21 ('::1', 45958, 0, 0): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:581)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_ssl_version_as_string (test.contrib.test_pyopenssl.TestHTTPS_TLSv1)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 401, in test_ssl_version_as_string
    self.assertRaises(SSLError, self._pool.request, 'GET', '/')
  File ""/usr/lib64/python2.7/unittest/case.py"", line 473, in assertRaises
    callableObj(*args, **kwargs)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type Error)
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
tornado.general: WARNING: SSL Error on 21 ('::1', 45959, 0, 0): [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:581)
--------------------- >> end captured logging << ---------------------

======================================================================
ERROR: test_hostname_in_first_request_packet (test.contrib.test_pyopenssl.TestSNI)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_socketlevel.py"", line 69, in test_hostname_in_first_request_packet
    pool.request('GET', '/', retries=0)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 344, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/urllib3/connectionpool.py"", line 314, in _raise_timeout
    if 'timed out' in str(err) or 'did not complete (read)' in str(err):  # Python 2.6
TypeError: __str__ returned non-string (type SysCallError)
-------------------- >> begin captured logging << --------------------
urllib3.util.retry: DEBUG: Converted retries value: 0 -> Retry(total=0, connect=None, read=None, redirect=None)
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
--------------------- >> end captured logging << ---------------------

======================================================================
FAIL: test_verified (test.contrib.test_pyopenssl.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10.3/work/urllib3-1.10.3/test/with_dummyserver/test_https.py"", line 70, in test_verified
    self.assertFalse(warn.called, warn.call_args_list)
AssertionError: [call('Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)', <class 'urllib3.exceptions.SecurityWarning'>)]
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
urllib3.connectionpool: DEBUG: ""GET / HTTP/1.1"" 200 13

TOTAL                                  1767     98    94%   
----------------------------------------------------------------------
Ran 309 tests in 17.232s

FAILED (SKIP=4, errors=7, failures=1)
```

Do you require any further information?
## 
"
620,[WIP] partially unbreak chunked gzip responses on appengine,2015-05-12T23:28:28Z,2015-06-17T11:24:41Z,,,,"see #618
"
619,refactor assert_fingerprint for efficiency + style,2015-05-12T23:21:31Z,2015-05-13T00:38:38Z,,,,
618,Fix AppEngine support,2015-05-12T17:38:53Z,2015-12-03T19:23:14Z,,,,"We claim urllib3 works with AppEngine but looks like it's broken today (as of a few months ago). Nobody on our core team uses AppEngine, so maintaining support for it has been a substantial drain.

Ideally, we'd like official support/resources from the Google Cloud/AppEngine team to maintain support for AppEngine.

Related issues:
- https://github.com/shazow/urllib3/issues/583 (urlfetch is not used when it should be)
- https://github.com/kennethreitz/requests/issues/2595 (chunked-encoding streaming is broken)
"
617,API to use custom SSLContexts in VerifiedHTTPSConnection/HTTPSConnectionPool,2015-05-12T04:11:11Z,2015-05-12T05:04:34Z,,,,"I'd like to request an API for passing a custom `SSLContext` to higher level APIs, so that it eventually gets passed to `ssl_.ssl_wrap_context`. My motivating use case is that I'd like to selectively re-enable RC4 when talking with poorly configured servers. (see #551) It seems like [this](https://gist.github.com/paxswill/5ed7273059b34e90b184) is currently the best way to enable old ciphers, but I think it would be preferable to create a `SSLContext` using `_ssl.create_urllib3_context`, and then pass that to the `HTTPSConnectionPool` constructor.

To that end, I propose the following API change. A `context` keyword argument is added to the constructor of `VerifiedHTTPSConnection`. If present, this `SSLContext` is passed to `ssl_.ssl_wrap_context`. If a `context` is passed to `HTTPSConnectionPool`, it will be passed through to `VerifiedHTTPSConnection`.

Does this approach sound good? If so I'd be happy to start a PR. I see there are tests and docs to update; is there anything else I should look at?
"
616,UnicodeEncodeError handling gzipped responses on Google App Engine,2015-05-11T20:37:26Z,2015-05-11T21:09:55Z,,,,"https://github.com/braintree/braintree_python/issues/53 originally reported the issue. It's due to using requests 2.6.1+, which pulled in a version of urllib3 including f21c2a2b73e4256ba2787f8470dbee6872987d2d, which causes the problem.

I am able to reproduce using app engine development mode when switching https://github.com/agfor/braintree-python-appengine to use requests 2.6.1+ and un-commenting out the development mode enabling code in main.py.
"
615,InsecurePlatformWarning is end-user unfriendly,2015-05-09T19:36:47Z,,,,,"I filed https://github.com/pypa/pip/issues/2681 when pip started throwing IPW's all over the place, and we've had other folk be confused as well.

Now,  I understand the cause and support the intent of exposing insecure situations for users.

However the page linked to is hard to understand for a couple of reasons.

The page as a whole is focused on developers and the history behind security in urllib3, but end users don't need all that context (and the advice for developers, like the InsecureRequestWarning section, is entirely irrelevant to them). Because the link is to a # reference in the page, and one near the end of the page, the actual advice doesn't appear at the top of the users browser screen, but in a narrow section at the bottom (try opening the page in a 4K browser window...). Lastly, _nearly everyone_ that hits this will need the actual advice: https://urllib3.readthedocs.org/en/latest/security.html#openssl-pyopenssl for which they have to chase pointers.

If I may suggest, putting the #insecureplatformwarning reference at the top of the document, and inlining the pyopenssl recipe into it would make this a whole bunch clearer and hopefully make things a lot clearer for users encountering this issue in an application.
## 
"
614,pyopenssl: close != shutdown,2015-05-09T07:58:27Z,2015-05-09T17:46:31Z,,,,"fixes #612, see #613
"
613,Handle EPIPE on shutdown from pyopenssl,2015-05-09T06:30:12Z,2015-05-09T07:58:49Z,,,,"This should resolve #612 and pypa/pip#2753.
"
612,Uncaught exception from pyOpenSSL's shutdown call,2015-05-08T22:03:53Z,2015-05-09T17:46:31Z,,,,"See https://github.com/pypa/pip/issues/2753 for more details
"
611,Issue #610 -- Tests fallback to IPv4 if IPv6 fails,2015-05-07T19:23:43Z,2015-05-12T16:59:25Z,,,,"On some systems binding to an IPv6 address will fail with a socket.gaierror exception even though socket.has_ipv6 is set to True. Added code to catch these exceptions and fallback to using IPv4.
"
610,Unit tests fail with exception gaierror,2015-05-07T19:05:29Z,2015-05-12T17:49:55Z,,gaierror,gaierror: [Errno -2] Name or service not known,"When I run the unit tests for urllib3 v1.10.2 or the latest version (master) in GitHub I receive the following error:

```
urllib3/test$ nosetests
S.............................E.....................................S..S....................................Exception in thread Thread-11:
Traceback (most recent call last):
  File ""/opt/rh/python27/root/usr/lib64/python2.7/threading.py"", line 811, in __bootstrap_inner
    self.run()
  File ""/services/scratch/ddriddle/src/urllib3/dummyserver/server.py"", line 88, in run
    self.server = self._start_server()
  File ""/services/scratch/ddriddle/src/urllib3/dummyserver/server.py"", line 75, in _start_server
    sock.bind((self.host, 0))
  File ""/opt/rh/python27/root/usr/lib64/python2.7/socket.py"", line 224, in meth
    return getattr(self._sock,name)(*args)
gaierror: [Errno -2] Name or service not known
```

The problem seems to be that my system does not support IPv6 but socket.has_ipv6 returns true.
"
609,don't allow path in urls which do not begin with /,2015-04-30T20:59:56Z,2015-05-01T16:40:35Z,,,,"It was possible to create such an object, but it would not
serialize -> parse roundtrip.

The alternative would be to reject paths without a leading /
"
608,[RFC] add preliminary HSTS support,2015-04-30T20:50:40Z,,,,,"_Do not merge yet_

TODO:
- [ ] _more_ tests!
- [ ] reread the RFC and check every point
- [ ] find the proper place for the logic
- [x] Make HSTS failures fatal
- [x] Ignore IP addresses
- [x] make storage abstract
- [x] process HSTS also in lower connection classes
- [ ] documentation

See also #607
"
607,[RFC] HPKP,2015-04-30T16:32:41Z,,,,,"HPKP is cool, and I think we should be able to do it.

This issue tracks us (me?) spiking out how best to do it. I believe we can do it, and continue to be Python HTTP security wizards.

People who may want to watch this issue: @sigmavirus24 @reaperhulk @dstufft @tiran.
"
606,test chunked response with gzip encoding,2015-04-29T22:10:35Z,2015-04-30T00:12:52Z,,,,"duplicates #441
"
605,Learn from httplib how to handle chunked HEAD requests,2015-04-28T16:17:47Z,2015-04-28T18:25:07Z,,,,"When we perform a HEAD request and get a response with a chunked
transfer-encoding, there's no point in looking for a body at all. We
should just close the response immediately.

Fixes #601 

cc @Lukasa this needs (probably) socket-level tests
"
604,SSL session reuse support through pyOpenSSL,2015-04-28T16:16:42Z,,,,,"Added feature to support SSL resume through pyOpenSSL. The more discussion can be found at 
https://github.com/shazow/urllib3/issues/590 
"
603,Fix #602: don't override existing warning config at import,2015-04-28T15:24:07Z,2015-04-28T16:07:24Z,,,,"Filter warnings with append=True to fix issue #602.

No test since it affects import-time behaviour and it's a two-word change, but I've given it a quick try and it seems to do the right thing.

I haven't changed disable_warnings(), since if people are calling that they presumably mean it, and can call the underlying method if not.
"
602,Really don't spam InsecurePlatformWarning,2015-04-28T12:37:11Z,2015-04-28T16:07:25Z,,,,"urllib3 should configure its warnings using append=True to avoid overriding the user's preferences as specified with python -W or PYTHONWARNINGS.

If this issue were fixed, the user could work around pypa/pip#2681 with

```
export PYTHONWARNINGS=""ignore:A true SSLContext object is not available""
```

Additionally, the urllib3 docs are very unclear about why this is considered worth warning the end user about, particularly given that adding this strange message has effectively introduced a bug in hundreds of other projects.
"
601,No tolerance for responses to head requests that indicate chunking.,2015-04-28T10:04:34Z,2015-04-28T18:25:07Z,,,,"As shown [here](https://ptpb.pw/r/AFSJnZhspw4EkC13PK8TBHpNqMDH), if we receive a redirect response that has no body, but claims Transfer-Encoding: chunked, we will hang indefinitely waiting for a chunked response we never see.

This has been observed with requests 2.6.2 (containing the new chunked handling) but not with requests 2.6.0 (containing the old broke stuff), which moves along just fine.

Now, it's worth noting that Google has totally screwed up here, because the response is invalid, as this Wireshark trace shows:

```
HEAD /~r/DeveloperChronicles/~3/iauy0NfhAus/hitchhikers-guide-to-modern-android.html HTTP/1.1
Host: feedproxy.google.com
Connection: keep-alive
Accept-Encoding: gzip, deflate
Accept: */*
User-Agent: python-requests/2.6.2 CPython/2.7.8 Windows/7

HTTP/1.1 301 Moved Permanently
Location: http://www.devchronicles.com/2015/03/hitchhikers-guide-to-modern-android.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+DeveloperChronicles+%28Developer+Chronicles%29
Content-Type: text/html; charset=UTF-8
Date: Tue, 28 Apr 2015 09:56:23 GMT
Expires: Tue, 28 Apr 2015 09:56:23 GMT
Cache-Control: private, max-age=0
X-Content-Type-Options: nosniff
X-XSS-Protection: 1; mode=block
Server: GSE
Alternate-Protocol: 80:quic,p=1
Transfer-Encoding: chunked
Accept-Ranges: none
Vary: Accept-Encoding

```

Note that Google have sent Transfer-Encoding: chunked, but no body. This is, as far as I can tell, against all the relevant RFCs: a zero length body has a valid chunked representation (`0\r\n\r\n`). 301 is also not defined as having no body.

Regardless, however, curl gets this right, so we should too if at all possible. We should also confirm whether this is a problem for older versions of urllib3 or whether they too function correctly.

Arguably this may want to block a requests release, @sigmavirus24.
"
600,Cut a new release for requests-2.6.2,2015-04-27T13:56:04Z,2015-05-03T14:16:22Z,,,,"Requests 2.6.2 bundled urllib3 at commit 7b82da0fa3a13514d11a863e379e7541d0ec46ef.

Could you cut a 1.10.4 release to match them like in #592?
"
599,Fix Chunked keepalives,2015-04-27T05:18:09Z,2015-04-27T16:57:48Z,,,,"I am not sure if this is the right way. The test should be picked anyways.

Fixes #598.

/cc @shazow @sigmavirus24 
"
598,Keep-alive broken in 1.10.3 with chunked transfer-encoding,2015-04-24T14:01:43Z,2015-04-27T16:57:48Z,,,,"Per kennethreitz/requests#2568, starting in urllib3 1.10.3 an exception is thrown if a connection with chunked transfer-encoding is reused.
"
597,Refactor fix for 593,2015-04-24T03:18:05Z,2015-04-25T20:52:49Z,,,,
596,Tornado 4,2015-04-23T18:43:13Z,2015-04-23T18:51:12Z,,,,"Trying out tests on tornado 4.1 (thanks to #594)
"
595,Decode information received from read_chunked in stream,2015-04-23T14:03:54Z,2015-04-23T16:24:17Z,,,,"Previously, when we read chunked data we always decoded it because it
was handled by the read method on the HTTPResponse. Now that it's a
separate method, we need to handle it. This is a quick fix for #593 to
address a regression in behaviour.

Fixes #593 

---

I confirmed this worked by linking my working copy of urllib3 into requests and testing against the site in the original issue on requests, e.g., by doing

``` py
import requests
r = requests.get('https://liquidstate.atlassian.net/rest/api/2/serverInfo')
r.json()
```

I simply didn't have time to write a test case. Also IMO a better solution would be to make `read_chunked` handle this instead of `stream` but this was the fastest hotfix I could produce. I'm adding both @shazow and @Lukasa to my fork so they can dev on this if they want from the same branch
"
594,make test suite more robust and compatible with Tornado 4,2015-04-23T13:37:28Z,2015-04-23T18:40:48Z,,,,"the current test suite relies on WSGI compatibility layer from Tornado 3, which was significantly reworked in version 4. As a result, one can't easily create Tornado HTTPRequests from WSGI parameters (I ran afoul of issue https://github.com/tornadoweb/tornado/issues/1118, which has since been fixed, but it is still safer to avoid it entirely).
fwiw, creating HTTPRequest from WSGI values this way doesn't make sense when the whole point of Tornado WSGI wrapper is to convert HTTPRequest _to_ WSGI values, so, um.

This is a rather crude but practical way to use Tornado RequestHandler directly instead of the WSGI runaround.

In the second commit, I make the socket-based tests time out instead of hanging forever if for whatever reason the server fails to start. A similar change might be useful for the HTTP tests, but it's not as straightforward, so maybe next time ;)
"
593,decode_content parameter to Response.stream() is not respected for chunked bodies,2015-04-23T06:27:23Z,2015-04-23T16:24:17Z,,,,"Originally kennethreitz/requests#2561.

It seems that if you hit an endpoint that sets both Content-Encoding: gzip and Transfer-Encoding: chunked, and then use `Response.stream()` with `decode_content=True`, we don't decompress the content. That represents an API change, and I'd argue a regression.

Should be a fairly simple fix though.
"
592,Cut a new release for requests-2.6.0,2015-04-21T13:24:14Z,2015-04-21T20:07:56Z,,,,"Good morning.  :)  This is just like in #554 and the times before.

It seems that requests bundled another cut of urllib3 for the 2.6.0 release but there's no corresponding urllib3 release on pypi for the unbundlers out there.

https://github.com/kennethreitz/requests/commit/9adaae6

```
  $ git log -p v2.5.3..v2.6.0 --reverse -- requests/packages/urllib3
  commit a0790f37b762ec737b2a4a13c167aad2defa8575
  Author: Ian Cordasco <graffatcolmingov@gmail.com>
  Date:   Tue Mar 10 19:34:34 2015 -0500

      Update urllib3 to 43b5b2b452e4344374de7d08ececcca495079b8d
```
"
591,Drop/deprecate 3.2 support,2015-04-15T07:39:33Z,2016-09-28T18:46:04Z,,,,"Requests has long since dropped support for Python 3.2, and now cryptography wants to do so as well. The 3.2 usage numbers aren't high, so I propose we should abandon our support for Python 3.2 by removing it as a test target from Tox and Travis.
"
590,SSL session reuse not supported,2015-04-14T19:01:14Z,,,,,"Right now, SSL session reuse (partial handshake as client sends the SSL ID with the request) is not support by urllib3 library. This feature would significantly increase SSL performance by improving latency and computing time.  The standard library does not support this feature as there seems to bug opened for this case http://bugs.python.org/issue8106 .

The new release version (0.14) of pyOpenSSL has opened number of callbacks from the subset of OpenSSL library. The particular API called 'set_session' or set_session_id 
(http://pythonhosted.org/pyOpenSSL/api/ssl.html#OpenSSL.SSL.Connection.set_session)
 can be use to enable SSL session reuse through Session Identifiers.  To support this capability two task needs to be executed 
1. Store the initiated established session for particular host:port so that we can reuse the same session for the subsequent request.
2. Use already created context associated for particular host:and port.

Any plans to integrate this feature in future release?.

Thanks
## 
"
589,ValueError: filedescriptor out of range in select() in urllib3,2015-04-11T03:35:23Z,2016-11-02T14:44:24Z,,ValueError,ValueError: filedescriptor out of range in select(),"We are using requests library in our code and after recent urllib3 upgrade we've started seeing following errors in logs on our testing servers:

```
""/usr/lib/python2.7/site-packages/requests-2.6.0-py2.7.egg/requests/packages/urllib3/connectionpool.py"", line 762, in _validate_conn
conn.connect()
File ""/usr/lib/python2.7/site-packages/requests-2.6.0-py2.7.egg/requests/packages/urllib3/connection.py"", line 238, in connect
ssl_version=resolved_ssl_version)
File ""/usr/lib/python2.7/site-packages/requests-2.6.0-py2.7.egg/requests/packages/urllib3/contrib/pyopenssl.py"", line 302, in ssl_wrap_socket
select.select([sock], [], [])
ValueError: filedescriptor out of range in select()
```

What it basically means is that `sock` that is passed to select[1] is `>= 1024`. That may happen for example if application opens 1024 files on start and then gets `OpenSSL.SSL.WantReadError` during TLS handshake while using urllib3.

Regression was introduced in: a49bec58fa5f7aca76d1d0b2f1975eb094648eab [pyopenssl: wait for data before handshake retry]

Simple fix is to factor out functionality for poll -> select fallback from https://github.com/shazow/urllib3/blob/master/urllib3/util/connection.py#L12 and replace all `select.select()` calls in urllib3 with it.

[1] `select(2)` is bound to `FD_SETSIZE` fds which equals to 1024 on most UNIX systems: http://pubs.opengroup.org/onlinepubs/007908799/xsh/select.html
## 
"
588,Ignore stdlib ciphers,2015-04-10T09:52:05Z,2015-04-11T20:06:09Z,,,,"fixes #572, duplicates/supersedes #573
"
587,test import without ssl module,2015-04-09T09:04:02Z,2015-04-10T17:26:09Z,,,,"duplicates #586
"
586,add a new test-nossl Makefile target,2015-04-09T00:02:47Z,2015-04-09T17:30:12Z,,,,"Adds ""make test-nossl"" to run the unittests  without ssl being importable. Right now urllib3 cannot be imported without ssl, but you can only fix it if you can test it.

Its somewhat tricky to test this because nosetests will have urllib3 already imported by the time the unittest is run. Hence this contraption with blocking ssl from the tip of `test/__init__.py`
"
585,Add timeout to ssl_wrap_socket,2015-04-08T19:40:31Z,2015-04-10T17:22:57Z,,,,"Our code is getting stuck sometimes on `select.select([sock], [], [])` forever. It seems to me that we need a timeout in that call. Would adding `sock.gettimeout()` to the `select` call make any sense?
"
584,Support Python without SSL (2),2015-04-08T09:57:54Z,2015-04-10T17:28:27Z,,ImportError,ImportError: No module named _ssl,"This is essentially the same as issue #320, urllib3 fails to import if there is no ssl module (Tested on Python 2.7.9). There are various reasons one might not have it, e.g. self-complied Python without ssl headers, the steady stream of security vulnerabilities, or objections to the not-really-free openssl license. But right now pip requires ssl because it uses urllib3.

```
In [1]: import urllib3
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-dbe53ea169b1> in <module>()
----> 1 import urllib3

/home/vbraun/Code/sage/local/lib/python2.7/site-packages/urllib3/__init__.py in <module>()
      8 
      9 
---> 10 from .connectionpool import (
     11     HTTPConnectionPool,
     12     HTTPSConnectionPool,

/home/vbraun/Code/sage/local/lib/python2.7/site-packages/urllib3/connectionpool.py in <module>()
     29 from .packages.ssl_match_hostname import CertificateError
     30 from .packages import six
---> 31 from .connection import (
     32     port_by_scheme,
     33     DummyConnection,

/home/vbraun/Code/sage/local/lib/python2.7/site-packages/urllib3/connection.py in <module>()
     43 from .packages.ssl_match_hostname import match_hostname
     44 
---> 45 from .util.ssl_ import (
     46     resolve_cert_reqs,
     47     resolve_ssl_version,

/home/vbraun/Code/sage/local/lib/python2.7/site-packages/urllib3/util/__init__.py in <module>()
      3 from .request import make_headers
      4 from .response import is_fp_closed
----> 5 from .ssl_ import (
      6     SSLContext,
      7     HAS_SNI,

/home/vbraun/Code/sage/local/lib/python2.7/site-packages/urllib3/util/ssl_.py in <module>()
     10 
     11 import errno
---> 12 import ssl
     13 
     14 try:  # Test for SSL features

/home/vbraun/Code/sage/local/lib/python/ssl.py in <module>()
     58 import textwrap
     59 
---> 60 import _ssl             # if we can't import it, let the error propagate
     61 
     62 from _ssl import OPENSSL_VERSION_NUMBER, OPENSSL_VERSION_INFO, OPENSSL_VERSION

ImportError: No module named _ssl
```
"
583,urllib3 should use URLFetch to create HTTPS connections on AppEngine,2015-04-08T05:59:55Z,,Help Wanted,,,"AppEngine is listed as a supported platform for `urllib3`.

However, it appears to use the [Sockets API](https://cloud.google.com/appengine/docs/python/sockets/) in order to make HTTPS connections, as a result of it using [`socket` from the standard library](https://docs.python.org/2/library/socket.html) to make the connection.

As a result, this will exhaust quota much faster than if it wrapped the [URLFetch service](https://cloud.google.com/appengine/docs/python/urlfetch/), and is only available to billed (non-free) applications.

Attempting to use it on a free application results in an error.

Additionally, the Sockets API is beta service, and it is [impossible to connect to Google's HTTPS services using the Sockets API](https://cloud.google.com/appengine/docs/python/sockets/#limitations-and-restrictions).

Instead, `urllib3` should use URLFetch by default in order to send HTTPS requests on AppEngine, and [use a similar fallback mechanism to `httplib` in order to fall back to using `socket`](https://cloud.google.com/appengine/docs/python/sockets/#making_httplib_use_sockets).
## 
"
582,"proof of concept: split ProtocolError into socket.connect, socket.write, socket.read",2015-04-06T18:13:57Z,,,,,"In https://github.com/shazow/urllib3/issues/547 I said we could detect which type of error got raised by httplib. Here is a proof of concept showing how to do this. 

The essential `httplib` calls are both in `_make_request`: the first is `conn.request` and the second is `conn.getresponse()`. In order to distinguish these at the `urlopen` level, I had to split `_make_request` into two functions. Otherwise, you have to distinguish between errors between all three of socket connect/write/read; did the `ETIMEDOUT` occur on the connect or the read?
- This gets ugly since we want to pass a lot of state from the first function to the second function, in particular the `conn` object and the `timeout`. There's a failing test `test_total_timeout` that I couldn't figure out what to do with (besides bail on the feature, which I wouldn't be too against)
- a lot of the tests needed to be updated to call both `_make_` functions, which shouldn't be too terrible, since it's an underscored function.
- what is terrible is most of the exception handling has to be duplicated for `_make_connect` and `_make_request`... some of it could probably be killed if we looked harder at what could be thrown by both methods.

So yeah, I'm not amazingly sure about this, and given that I feel bad about code I felt better about when I submitted it, I'm not sure this is worth going down. 

Still, this is evidence that if we _wanted_ to do this, we _could_ without necessarily rewriting httplib.
"
581,"proof of concept: split ProtocolError into socket.connect, socket.write, socket.read",2015-04-06T18:12:55Z,2015-04-06T18:13:08Z,,,,"In https://github.com/shazow/urllib3/issues/547 I said we could detect which type of error got raised by httplib. Here is a proof of concept showing how to do this. 

Summary of the changes - 
- The essential `httplib` calls are both in `_make_request`: the first is `conn.request` and the second is `conn.getresponse()`. In order to distinguish these at the `urlopen` level, I had to split `_make_request` into two functions. Otherwise, you have to distinguish between errors between all three of socket connect/write/read; did the `ETIMEDOUT` occur on the connect or the read?
  - This gets ugly since we want to pass a lot of state from the first function to the second function, in particular the `conn` object and the `timeout`. There's a failing test `test_total_timeout` that I couldn't figure out what to do with (besides bail on the feature, which I wouldn't be too against)
  - a lot of the tests needed to be updated to call both `_make_` functions, which shouldn't be too terrible, since it's an underscored function.
  - what is terrible is most of the exception handling has to be duplicated for `_make_connect` and `_make_request`... some of it could probably be killed if we looked harder at what could be thrown by both methods.

So yeah, I'm not amazingly sure about this, and given that I feel bad about code I felt better about when I submitted it, I'm not sure this is worth going down. 

Still, this is evidence that if we _wanted_ to do this, we _could_ without necessarily rewriting httplib.
"
580,"v2.0 release candidate, finally fixed urllib3's biggest bug",2015-04-01T20:36:19Z,2015-04-03T18:22:03Z,,,,"This is a very important release as urllib3 had a huge bug present since day 1, and it took us this long to fix it: The name. It's a terrible name, by far my biggest regret as far as design decisions go.

Starting with version 2.0, we will shed our shame and rename urllib3 to urllib4.
"
579,cert_reqs default to 'CERT_REQUIRED' when ca_certs is set,2015-03-29T17:57:07Z,2015-06-13T09:47:08Z,Help Wanted,,,"Is this a reasonable change? Would make our boilerplate a little simpler/less crufty.

``` diff
- http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
+ http = urllib3.PoolManager(ca_certs=certifi.where())
```
"
578,Attach last retry object to response before returning,2015-03-25T19:02:22Z,2016-09-28T18:46:58Z,Help Wanted,,,"Would be swell if our `response.HTTPResponse` object had a `retries` attribute (or maybe a better name?) which contained the latest (immutable) Retry object.

At least that way users can get some idea of how many retries occurred (derived from their original retries configuration). Bonus points if we store more useful data in the retry object for the user to inspect without having to do math.

This attachment could happen right before [returning the response here](https://github.com/shazow/urllib3/blob/431e80619efb73201f483467a68ee0c8a215e5d3/urllib3/connectionpool.py#L650).

(Mildly related to #576.)
"
577,"README: ""You might already be using urllib3!"" search is broken",2015-03-24T00:21:05Z,2016-08-16T18:07:46Z,,,,"https://github.com/shazow/urllib3/blob/master/README.rst#you-might-already-be-using-urllib3 links to https://sourcegraph.com/search?q=package+urllib3 , which returns zero results since it seems to be searching within someone's GitHub repo that happens to be named ""package"".
"
576,Retry history (or maybe request history via retries?),2015-03-23T18:18:11Z,2016-09-28T18:46:53Z,Help Wanted,,,"Would be great if we have somekind of retry request history in the retry object. There's a TODO for this in the code.

Ideally we'd do this by replacing our observed errors counter with a list of objects describing the respective requests.

If we end up doing a request history (and requests have a retry attached per #578), then this issue would be obsolete.

Related: #185 which adds redirect_history, but we should be able to do better now that we've refactored some of the code.
"
575,"read chunked: don't close socket, release connection instead",2015-03-18T18:52:32Z,2015-03-19T22:21:00Z,,,,"Fixup for #560.
"
574,Don't spam the platform warning,2015-03-18T01:23:40Z,2015-03-20T01:03:28Z,,,,"The new `InsecurePlatformWarning` inherits from `SecurityWarning`. The platform isn't going to vary based on different requests so it probably makes sense to set that one back to the default of `once`? Otherwise users of urllib3 are getting spammed about their platform for any use instead of just once.
"
573,Stop relying on the stdlib's default cipher list,2015-03-17T11:27:17Z,2015-04-10T11:04:33Z,,,,"urllib3's release cadence is much faster so it's preferable if
urllib3 handles its own default cipher list as it will undoubtedly
be more quickly updated/released.
"
572,Don't prefer the stdlib defined Ciphers,2015-03-16T17:21:25Z,2015-04-11T20:06:09Z,,,,"Right now urllib3 does:

``` python
try:
    from ssl import _DEFAULT_CIPHERS
except ImportError:
    _DEFAULT_CIPHERS = (
        'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:'
        'DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES:!aNULL:'
        '!eNULL:!MD5'
    )
```

Which means that it's going to prefer whatever ciphers Python prefers. However given that it's much easier to upgrade urllib3 (or requests or pip or ...) than it is to upgrade Python and that urllib3 (etc) has a much shorter release cycle I believe it would be a better idea to just ignore the ciphers that Python likes and just have urllib3 set it's own default ciphers. 
"
571,Link the security docs from the front page of the docs.,2015-03-16T09:09:07Z,2015-03-19T22:18:54Z,,,,"I keep getting confused by the fact that I can't find the security docs from the index.
"
570,Properly detect Python 3.0-3.1 in SSLContext,2015-03-15T16:50:14Z,2015-03-15T17:41:08Z,,,,"This stops pip (through its use of requests) from working under Python 3.1 (and probably 3.0 too).
"
569,don't use IPv6 in tests when it's not available,2015-03-15T11:17:13Z,2015-03-17T17:05:16Z,,,,"Fixes #568
"
568,tests are stuck when running `make test`,2015-03-14T09:26:42Z,2015-03-17T17:05:16Z,,,,"I tried to use google's public dns as suggested by @sigmavirus24, but it didn't help.

``` shell
$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 brd 127.255.255.255 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: enp5s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 192.168.0.17/24 brd 192.168.0.255 scope global enp5s0
       valid_lft forever preferred_lft forever
    inet6 fe80::beae:c5ff:fe65:dd71/64 scope link 
       valid_lft forever preferred_lft forever
$ cat /etc/resolv.conf
nameserver 8.8.8.8
nameserver 8.8.4.4
```

strace:

``` shell
$ strace -p 12676
Process 12676 attached
futex(0xbf8dd0, FUTEX_WAIT_PRIVATE, 0, NULL
```

This is gentoo, wicd and python 2.7. I think I had the same problem with Fedora and NetworkManager.

Also, last couple of lines from output:

```
test_oldapi (test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager) ... ok
test_proxy_conn_fail (test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager) ... ok
test_proxy_pooling (test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager) ... ok
test_proxy_pooling_ext (test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager) ... ok
test_proxy_verified (test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager) ... ok
test_redirect (test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager) ... ok
test_multi_setcookie (test.with_dummyserver.test_socketlevel.TestCookies) ... Exception in thread Thread-8:
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""/home/tt/dev/urllib3/dummyserver/server.py"", line 76, in run
    self.server = self._start_server()
  File ""/home/tt/dev/urllib3/dummyserver/server.py"", line 63, in _start_server
    sock.bind((self.host, 0))
  File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
    return getattr(self._sock,name)(*args)
error: getsockaddrarg: bad family
```
"
567,Two different copies of urllib3 don't work interchangeably,2015-03-11T20:37:27Z,2015-03-12T02:12:05Z,,TypeError,TypeError: unsupported operand type(s) for -=: 'Retry' and 'int',"I use urllib3 1.10.2.

In working copy of Requests:

```
$ rm -r requests/packages/urllib3
$ python2.7 test_requests.py -v
...
======================================================================
ERROR: test_connection_error_invalid_domain (__main__.RequestsTestCase)
Connecting to an unknown domain should raise a ConnectionError
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_requests.py"", line 307, in test_connection_error_invalid_domain
    requests.get(""http://doesnotexist.google.com"")
  File ""/tmp/requests/requests/api.py"", line 65, in get
    return request('get', url, **kwargs)
  File ""/tmp/requests/requests/api.py"", line 49, in request
    response = session.request(method=method, url=url, **kwargs)
  File ""/tmp/requests/requests/sessions.py"", line 461, in request
    resp = self.send(prep, **send_kwargs)
  File ""/tmp/requests/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/tmp/requests/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/usr/lib64/python2.7/site-packages/urllib3/connectionpool.py"", line 597, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/lib64/python2.7/site-packages/urllib3/util/retry.py"", line 226, in increment
    total -= 1
TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'

======================================================================
ERROR: test_connection_error_invalid_port (__main__.RequestsTestCase)
Connecting to an invalid port should raise a ConnectionError
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""test_requests.py"", line 312, in test_connection_error_invalid_port
    requests.get(""http://httpbin.org:1"", timeout=1)
  File ""/tmp/requests/requests/api.py"", line 65, in get
    return request('get', url, **kwargs)
  File ""/tmp/requests/requests/api.py"", line 49, in request
    response = session.request(method=method, url=url, **kwargs)
  File ""/tmp/requests/requests/sessions.py"", line 461, in request
    resp = self.send(prep, **send_kwargs)
  File ""/tmp/requests/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/tmp/requests/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/usr/lib64/python2.7/site-packages/urllib3/connectionpool.py"", line 597, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/usr/lib64/python2.7/site-packages/urllib3/util/retry.py"", line 226, in increment
    total -= 1
TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'

----------------------------------------------------------------------
Ran 143 tests in 40.197s

FAILED (errors=2)
```

Problem strangely does not occur when Requests is using its bundled copy of urllib3.
"
566,"Further tests for HTTPHeaderDict, better python2.7 raw httplib header import",2015-03-10T21:33:03Z,2015-03-11T18:23:46Z,,,,"Added full support for python2.7 multiheader import, borrowed from 
https://gist.github.com/wtolson/bf7dada12ddda0482d10
Thanks to @wolston for that code!

Extended test for python2.7 raw header import
Added further header tests borrowed from geventhttpclient
"
565,Test for set-cookies headers are lost after creating a HTTPHeaderDict using an instance of HTTPHeaderDict,2015-03-05T13:58:48Z,2015-03-05T14:33:50Z,,,,
564,set-cookies headers are lost after creating a HTTPHeaderDict using an instance of HTTPHeaderDict,2015-03-05T13:43:25Z,2015-08-06T22:11:38Z,,,,"``` python
headers = HTTPHeaderDict()
headers.add('set-cookie', 'foo')
headers.add('set-cookie', 'bar')
headers.getlist('set-cookie')
Out: ['foo', 'bar']

headers2 = HTTPHeaderDict(headers)
headers2.getlist('set-cookie')
Out: ['foo, bar']
```

I'm gonna send a PR with a test for 

``` python
assert headers2.getlist('set-cookie') == headers.getlist('set-cookie') 
```
"
563,Fix multiheader regression #561,2015-03-05T13:19:28Z,2015-03-06T22:12:38Z,,,,"See #561 for discussion.
"
562,Fix HTTPHeaderDict implementation for multiple headers,2015-03-04T23:20:48Z,2015-03-05T20:40:11Z,,,,"- Allow all headers to have multiple values
- Special-case how cookie, set-cookie, and set-cookie2 are presented/handled
- Trim some extraneous whitespace

Fixes #561
"
561,duplicate/multiple headers are lost,2015-03-04T14:31:07Z,2015-03-06T22:12:39Z,,,,"When the server returns multiple headers of the same type, duplicated headers are lost. Example:

WWW-Authenticate: Negotiate
WWW-Authenticate: NTLM

Results in only NTLM in the www-authenticate header.
"
560,implement generator for chunked responses,2015-03-04T10:00:27Z,2015-03-14T21:43:47Z,,,,"Fixes #550

So, I have figured out first, proof of concept, very hacky, very ugly implementation. I would like to get some feedback.

(Bear in mind that I haven't run a single test yet)
### How to test

I have used this code to generate chunked responses: https://gist.github.com/josiahcarlson/3250376

Client code:

``` python
import urllib3
http = urllib3.PoolManager()
r = http.request('GET', 'http://localhost:8080/')
for x in r.stream():
    print x,
```
"
559,Extend certificate fingerprinting support.,2015-03-03T21:18:45Z,,,,,"While urllib3 has certificate fingerprinting support, it would be extremely powerful to have more fine-grained fingerprinting support. In particular, the current support is very binary: either you fingerprint or you don't. If you want to fingerprint on only some domains you need to abandon the PoolManager entirely, because you need to set `assert_fingerprint` on the ConnectionPool object before you call urlopen.

It would be very nice if it was possible to do fingerprinting on a more granular level. Some options:
- pass `assert_fingerprint` a function that returns a fingerprint for a given hostname. 
- pass `assert_fingerprint` a dict of hostnames to fingerprints

In all cases we'd fall back to asserting hostname or other if the user does not provide a fingerprint for a given hostname.

This would allow users to fingerprint where they can or choose to, while not preventing them from making less-secure HTTPS calls.

Thoughts?
## 
"
558,Emit warnings when SSLContext isn't available.,2015-03-02T20:18:37Z,2015-03-03T20:16:21Z,,,,"This change emits warnings when the `SSLContext` object isn't available.

To restrict the scope of the warning to situations only when it really isn't available, this warning actually gets emitted only if the weird mock object we use is actually being used. In no other circumstances should this warning be emitted.

The test is pretty ghetto, but it turns out to be kinda a pain in the ass to test this really properly. If you'd like me to, I can take another swing at it.
"
557,Emit a warning on platforms where we can't configure OpenSSL as we'd like,2015-03-02T15:04:54Z,2015-03-03T21:09:19Z,,,,"Specifically this means:
- Python < 2.7.9 (Probably something for older Python 3s, but I don't know that off hand)
- and no pyOpenSSL + pyasn1 + ngd-httpsclient

On these platform we can't do all the configuration we'd like, including the fix for verification that we've been discussing. It'd be great if users know they should upgrade their Python (or install the appropriate optional dependencies)
"
556,Need to update error handling for pyOpenSSL,2015-02-28T17:32:14Z,2016-01-25T19:58:44Z,,TypeError,TypeError: __str__ returned non-string (type SysCallError),"If an `SSLError` comes from `pyOpenSSL`, we attempt to determine if it's an error due to request timeouts on python 2.6 being raised as `BaseSSLErrors` [here](https://github.com/shazow/urllib3/blob/29aa09bde9c42cc9a8d79aac47ee3d362b438cca/urllib3/connectionpool.py#L314). If the handshake ends unexpectedly under pyOpenSSL, then we get an `SSLError` that has a captured `SysCallError` which when you call `str(SSLError(msg, SysCallError(code, msg)))` you get

``` py
TypeError: __str__ returned non-string (type SysCallError)
```

So we might want to update how we handle SSLErrors to accomodate pyOpenSSL. To be clear, I observed this on: python 2.7.9 with openssl 1.0.1l (from brew) and I was using requests to debug an issue with `https://apissl.cloudfactory.com`, e.g., `requests.get('https://apissl.cloudfactory.com')`
"
555,Content-length header not set on PUT/POST with empty body,2015-02-25T16:05:46Z,,,,,"Lots of discussion over at Requests: kennethreitz/requests/issues/223 including a PR to work around the issue: kennethreitz/requests/pull/957

Basically, nginx is returning 411 responses when using urllib3 to make PUT or POST requests with an empty body because the content-length header is only set when the body isn't empty.

We're encountering this issue with the elasticsearch-py library, which is relying on urllib3 to set this header appropriately (see elasticsearch/elasticsearch-py@20aa35d791bbcfca555ef059ea1c619019a93ff7)

I've had a poke around the code but can't find the appropriate place to apply a fix for this.
## 
"
554,Cut a new release for requests,2015-02-25T01:16:40Z,2015-02-25T23:22:58Z,,,,"Requests bundled a new snapshot of urllib3 and did a release.  Can you do a release to match, please?

Looks like the latest is in [this commit](https://github.com/kennethreitz/requests/commit/58e513579a0934dbd737077f3069c04d68cf1a52) they updated to 29aa09bde9c42cc9a8d79aac47ee3d362b438cca 
"
553,Failure to raise max retry error in pool manager.,2015-02-23T01:21:37Z,2015-02-23T01:24:17Z,,,,"When retries is set to something like `Retry(total=None, redirect=1)`, a `MaxRetryError` is never raised, even in the case of a infinite redirect loop. This PR makes `PoolManager` act consistently with `HTTPConnectionPool` in respect to this behavior.
"
552,Added myself to CONTRIBUTORS.txt,2015-02-19T01:42:46Z,2015-02-19T01:43:12Z,,,,
551,Drop RC4 from the cipher list,2015-02-19T00:48:12Z,2015-02-19T01:40:41Z,,,,"In addition to having many security concerns, it is a violation of RFC 7465 to include RC4 cipher suites in a ClientHello.
"
550,[rfe] provide access to chunks when server responds with `Transfer-Encoding: chunked`,2015-02-18T14:12:52Z,2015-03-14T21:43:47Z,,,,"I've opened [an issue](https://github.com/kennethreitz/requests/issues/2449) at [requests](https://github.com/kennethreitz/requests) but was advised to move it here. There it goes:

When server responds with `Transfer-Encoding: chunked`, it would be nice if `urllib3` provided access to chunks sent by server. Therefore upper layers, `requests`, could process those encoded chunks and developers wouldn't need to decode those chunks.

More info:
http://en.wikipedia.org/wiki/Chunked_transfer_encoding
http://mihai.ibanescu.net/chunked-encoding-and-python-requests
"
549,Fix file descriptor leak due to stack frame reference,2015-02-14T20:56:49Z,2015-02-16T21:17:17Z,,,,"Fixes #548
"
548,File descriptor leak with HTTPS proxies via CONNECT,2015-02-09T21:28:36Z,2015-02-16T21:17:17Z,,,,"It looks like when making HTTPS requests through an HTTP proxy, there is a file descriptor leak if an error occurs (e.g. socket timeout or 404 error) during httplib.HTTPConnection._tunnel. Tested in versions 1.9, 1.9.1, 1.10.

The way I've been testing is to use netcat to listen for connections but not respond to CONNECT, eg.:

```
$ nc -l localhost 8000
```

Then making a request with that ""proxy"":

```
In [1]: import urllib3
In [2]: pm = urllib3.ProxyManager('http://localhost:8000/')
In [3]: r = pm.request('GET', 'https://httpbin.org/ip', timeout=1.0)
---------------------------------------------------------------------------
MaxRetryError                             Traceback (most recent call last)
```

At that point, `lsof` shows multiple connections (one per retry) in CLOSE_WAIT state, which stay that way permanently.

It looks like the issue is related to a circular reference preventing a socket from being cleaned up properly. For example, this commit fixes the issue: https://github.com/ianpreston/urllib3/commit/a0953a93c877468d9b60a6785c767c5ac31d0dc2 , as does wrapping the `raise` statements in `Retry.increment` with a ""finally: del _stacktrace"".

The workaround above is working for now but I'd like to find the root cause, or at least a workaround that doesn't break stack traces. Any idea where to start looking? Thanks
"
547,Unexpected retry type required for connection failure on LAN,2015-02-09T15:21:59Z,,,,,"Retry-attempts to connect to a disconnected local machine, without setting a timeout, are affected by the 'read' parameter, rather than the 'connect' parameter.  This is confusing, as it does not seem to be in compliance with the documentation, nor is it the same behaviour as when either a) using a connection timeout, or b) attempting to connect to a non-existant remote machine, both of which are correctly affected by the 'connect' parameter.

Looking at the back-trace I suspect this is because the reported OSError is treated as a ProtocolError, rather than a problem establishing the connection: `ProtocolError('Connection aborted.', OSError(113, 'No route to host'))`

Using the latest requests release (2.5.1) the problem is evident using:

``` python
import requests
s = requests.Session()
retry = requests.packages.urllib3.util.Retry(total=None, connect=10, read=2, redirect=5, backoff_factor=0.1)
s.mount('http://', requests.adapters.HTTPAdapter(max_retries=retry))
s.get('http://129.169.154.100/foo.html')
```

(129.169.154.0/24 being my local subnet).
## 
"
546,Test cases (for #534 compatibility),2015-02-06T11:56:08Z,2015-02-06T11:56:24Z,,,,
545,Allow PoolManager and ConnectionPool to be used as a context manager.,2015-02-06T11:52:13Z,2015-02-07T01:24:28Z,,,,"Fixes #514 

I'm not sure if any behaviour beyond closing/clearing the pools is expected from a context manager, let me know if I've missed the mark.
"
544,Faster headers implementation,2015-02-05T10:58:11Z,2015-02-19T06:30:22Z,,,,"I had been trying to speed up urllib3 by monkeypatching with geventhttpclient. Unfortunately there is not too much gain, as lots of copying of response and header objects is done, which could largely be skipped, when the header and response objects would be sufficiently compatible and directly recognized and accepted as compatible, instead of being converted by lots of copying around.

So for the sake of better compatibility and speedups, I would have liked to directly create a header class - the standard urllib3.HTTPHeaderDict - by geventhttpclients c-parser when monkeypatching, which won't need any further processing from urllib3s response object. When I compared both header container implementations, turned out that the current urllib3 version isn't that fast as it could be. Below some timings comparing the old vs the new version, HTTPHeaderDict_ being the old version:

``` python
print ""Empty initializations""
%timeit h = HTTPHeaderDict_()
%timeit x = HTTPHeaderDict()

print ""Initializations with data""
%timeit h = HTTPHeaderDict_(asdf='ddd', abc='100', ddd='200', efg='300')
%timeit x = HTTPHeaderDict(asdf='ddd', abc='100', ddd='200', efg='300')

from random import choice
from string import ascii_lowercase as asciis
hdrs = dict((''.join(choice(asciis) for _ in range(9)),
             ''.join(choice(asciis) for _ in range(9))) for _ in range(200))

print ""Initializations with more data""
%timeit h = HTTPHeaderDict_(hdrs)
%timeit x = HTTPHeaderDict(hdrs)

h = HTTPHeaderDict_(asdf='ddd', abc='100', ddd='200', efg='300')
x = HTTPHeaderDict(asdf='ddd', abc='100', ddd='200', efg='300')

print ""Fetching items""
%timeit h.__getitem__('abc')
%timeit x.__getitem__('abc')

print ""Copying""
%timeit h.copy()
%timeit x.copy()

print ""Adding headers""
%timeit h = HTTPHeaderDict_(); [h.add(k,v) for k, v in hdrs.iteritems()];
%timeit x = HTTPHeaderDict(); [x.add(k,v) for k, v in hdrs.iteritems()];

h = HTTPHeaderDict_(hdrs)
x = HTTPHeaderDict(hdrs)

print ""Iteration of keys""
%timeit [k for k in h]
%timeit [k for k in x]

print ""Iteration of values""
%timeit [h[k] for k in h]
%timeit [x[k] for k in x]

print ""Getting keys""
%timeit h.keys()
%timeit x.keys()

print ""Getting values""
%timeit h.values()
%timeit x.values()
```

For my machine, I get the following timings:

```
Empty initializations
100000 loops, best of 3: 4.38 ¬µs per loop
1000000 loops, best of 3: 858 ns per loop
Initializations with data
100000 loops, best of 3: 11.3 ¬µs per loop
100000 loops, best of 3: 12 ¬µs per loop
Initializations with more data
1000 loops, best of 3: 252 ¬µs per loop
1000 loops, best of 3: 235 ¬µs per loop
Fetching items
100000 loops, best of 3: 2.34 ¬µs per loop
1000000 loops, best of 3: 1 ¬µs per loop
Copying
100000 loops, best of 3: 11.7 ¬µs per loop
100000 loops, best of 3: 5.36 ¬µs per loop
Adding headers
1000 loops, best of 3: 267 ¬µs per loop
1000 loops, best of 3: 242 ¬µs per loop
Iteration of keys
10000 loops, best of 3: 58.3 ¬µs per loop
100000 loops, best of 3: 17.9 ¬µs per loop
Iteration of values
1000 loops, best of 3: 706 ¬µs per loop
1000 loops, best of 3: 271 ¬µs per loop
Getting keys
10000 loops, best of 3: 44.6 ¬µs per loop
100000 loops, best of 3: 2.25 ¬µs per loop
Getting values
1000 loops, best of 3: 671 ¬µs per loop
1000 loops, best of 3: 267 ¬µs per loop
```

The new version passes all current tests, though it comes with one downside, which is worth discussing: It's not storing the original case of the header items. While the current version stores that, and therefore can restore the case if desired e.g. when iterating over the keys, this implementation is consequently using lower case, except when pretty printing the object. The standards require case insensitivity, so not preserving the case seems to me like an acceptable, maybe even desireable behaviour. Furthermore, also the current implementation has to drop the case information, when joining same header items together.

In terms of compatibility, I hope this should work with all python versions, though I mainly tested it on 2.7.

```
python test/test_collections.py
................
----------------------------------------------------------------------
Ran 16 tests in 0.004s

OK
```
"
543,Python 2.7.9 breaks ssl_wrap_socket,2015-02-04T09:25:55Z,2015-02-07T01:28:33Z,,ValueError,ValueError: check_hostname requires server_hostname,"The following code is in `urllib3/util/ssl_.py`, in `ssl_wrap_socket`:

``` python
if HAS_SNI:  # Platform-specific: OpenSSL with enabled SNI
    return context.wrap_socket(sock, server_hostname=server_hostname)
return context.wrap_socket(sock)
```

In essence, we only pass the server hostname if SNI is available on the platform.

Unfortunately, this breaks on Python 2.7.9, as you can see in this requests traceback:

```
Python 2.7.9 (default, Feb  2 2015, 17:03:23)
[GCC 4.1.2 20080704 (Red Hat 4.1.2-52)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import requests
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/api.py"", line 65, in get
    return request('get', url, **kwargs)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/api.py"", line 49, in request
    response = session.request(method=method, url=url, **kwargs)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/sessions.py"", line 461, in request
    resp = self.send(prep, **send_kwargs)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/packages/urllib3/connectionpool.py"", line 518, in urlopen
    body=body, headers=headers)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/packages/urllib3/connectionpool.py"", line 322, in _make_request
    self._validate_conn(conn)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/packages/urllib3/connectionpool.py"", line 727, in _validate_conn
    conn.connect()
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/packages/urllib3/connection.py"", line 238, in connect
    ssl_version=resolved_ssl_version)
  File ""/opt/python2.7/lib/python2.7/site-packages/requests-2.5.1-py2.7.egg/requests/packages/urllib3/util/ssl_.py"", line 254, in ssl_wrap_socket
    return context.wrap_socket(sock)
  File ""/opt/python2.7/lib/python2.7/ssl.py"", line 350, in wrap_socket
    _context=self)
  File ""/opt/python2.7/lib/python2.7/ssl.py"", line 537, in __init__
    raise ValueError(""check_hostname requires server_hostname"")
ValueError: check_hostname requires server_hostname
```

It seems that with `check_hostname` set to true (the default on Python 2.7.9) we _must_ pass the `server_hostname` argument.

Not really sure what the best fix is here.

Follow-on from kennethreitz/requests#2435 and @mjpieters in #517.
"
542,No Retry in HTTPResponse.read() method.,2015-02-03T00:00:21Z,,,,,"It appears that the retry support in urllib3 doesn't cover the HTTPResponse.read() method. This means that if you pass `preload_content=False` to `urlopen()` as requests does, that any errors that occur while `read()`ing the body will not automatically be retried. Doing this transparently would be difficult to do since urllib3 doesn't know what the calling library has done with the parts of the body it's already received.

What _would_ be nice though is to provide the `Retry()` object on the response object and provide instructions for how the caller of the library can use it to implement retries themselves. Perhaps this could look something like:

``` python
response = pool.urlopen(...)
while not response.closed and not response.retries.is_exhausted():
    try:
        data = []
        data.append(response.read(4096))
    except response.retries.exceptions as exc:
        # This would raise an error if there are no more retries
        response = response.retry()
```

Thoughts?
## 
"
541,When using urllib3 with gevent and checking if a socket is closed an exception is raised,2015-02-02T14:34:26Z,2015-02-07T01:28:13Z,,,,"```
Traceback (most recent call last):
  File ""***"", line 285, in request
    return HTTPConnectionPool.request(self, method, url, fields, headers, **urlopen_kw)
  File ""***/lib/python2.7/site-packages/urllib3/request.py"", line 75, in request
    **urlopen_kw)
  File ""***/lib/python2.7/site-packages/urllib3/request.py"", line 88, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""***/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 536, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""***/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 294, in _get_conn
    if conn and is_connection_dropped(conn):
  File ""***/lib/python2.7/site-packages/urllib3/util.py"", line 490, in is_connection_dropped
    return select([sock], [], [], 0.0)[0]
  File ""***/lib/python2.7/site-packages/gevent/select.py"", line 67, in select
    raise error(*ex.args)
error: (9, 'Bad file descriptor')
```

This is because gevent raises `gevent.select.error` instead of `socket.error`.
Is there a way to gracefully handle this?
"
540,add sha256 for fingerprint verification,2015-01-31T20:02:43Z,2015-01-31T20:10:40Z,,,,"fixes #539
"
539,assert_fingerprint SHA256 support is missing,2015-01-31T18:57:59Z,2015-01-31T20:10:40Z,,,,"assert_fingerprint only seems to support MD5 and SHA1.  Would it be possible to add SHA256 support to it?
"
538,Fix misleading comment,2015-01-30T20:18:40Z,2015-01-30T20:22:52Z,,,,"Refs #537.
"
537,Misleading comment in connectionpool.py,2015-01-30T20:14:42Z,2015-01-30T20:23:00Z,,,,"At https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L356, there is a comment implying that in Python 2.7+, we can buffer HTTP responses by passing `buffering=True` to `HTTPConnection.getresponse`.

However, in Python 3, `http.client.HTTPConnection.getresponse` does not take any arguments, so line 356 triggers a `TypeError`, which is caught by the next line.

So, at the least there's a misleading comment, but there's also the possibility that we think we're buffering HTTP responses but we're actually not.
"
536,More Documentation Please,2015-01-18T10:42:52Z,2015-01-20T01:51:50Z,,,,"Hi,

I would like to use your lib to replace some python2 code that I wrote with urllib2 and poster. Now i almost remember no details and I know that it was not easy back then, but your lib does not make it much easier because the documentation is not good:

http://urllib3.readthedocs.org/en/latest/helpers.html#urllib3.filepost.encode_multipart_formdata
What is the return value and in which function should I put the return value?

Maybe I am spoiled by strongly typed languages. But for me it is totally un-obvious how to use the lib. So please add more documentation.

The code is pretty good read and understandable (e.g. https://github.com/shazow/urllib3/blob/master/urllib3/request.py). I am still convinced that an beginner friendly update of read the docs would be a big benefit for new users.
"
535,Decoding Exhausted Stream,2015-01-17T01:05:29Z,2015-01-30T00:26:41Z,,,,"Added a ""reference"" read() test, as well as a demonstration of how it differs from the streaming behavior for deflate or gzip. ""reference"" may be too strong of word in the context of urllib3, but seems to hold true for the standard library:

``` python
>>> from io import BytesIO
>>> f = BytesIO('foo')
>>> f.read(3)
'foo'
>>> f.read()
''
>>> f.read()
''
```

As of this PR opening - I expect 3 existing tests to fail and 1 new test to succeed. No solution yet!
"
534,Replaced concat/split by actual collection stuff,2015-01-12T23:58:09Z,2015-02-11T02:42:13Z,,,,"Addresses #533 
"
533,HTTPHeaderDict breaks if header value contains a comma,2015-01-12T23:40:13Z,2015-02-11T02:43:34Z,,,,"It's not unusual that HTTP headers contains commas inside it. The current implementation of  HTTPHeaderDict relies on commas to copy and to get lists of values and that's not working.

For example:

``` python
p = urllib3.PoolManager()
r = p.request('GET', 'https://gitlab.com/')
r.headers.getlist('set-cookie')

['_gitlab_session=6bad561209aa6c863392c013e7777fa1; path=/; expires=Mon',
 '19 Jan 2015 23:34:33 -0000; secure; HttpOnly',
 'request_method=GET; path=/']
```

In the example about the expired field of set-cookie ended up split.  
"
532,Close connections properly,2015-01-10T14:25:57Z,2015-01-25T01:35:41Z,,,,"Fix #529

I couldn't set up the testcase locally so I'm going to use Travis.
"
531,Raise SSLError exception when using HTTPS via proxy,2015-01-09T07:13:21Z,2015-01-28T06:28:16Z,,SSLError,SSLError: hostname '127.0.0.1' doesn't match 'www.google.com',"Hi,

I used [requests](http://python-requests.org) to make a connection to a HTTPS-enabled site via a HTTP proxy which supports CONNECT.

``` python
import requests
requests.get(""https://www.google.com"", proxies={""https"": ""http://127.0.0.1:8080""})
```

... and got the exception

```
SSLError: hostname '127.0.0.1' doesn't match 'www.google.com'
```

After debugging in current master branch, I found that the problem might due to the change of `actual_host` and `actual_port` in `_new_conn()` to proxy's ones.

``` python
actual_host = self.host
actual_port = self.port
if self.proxy is not None:
    actual_host = self.proxy.host
    actual_port = self.proxy.port
```

https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L722

What should I do to overcome this problem?
"
530,Example downloading files with shutil.copyfileobj,2015-01-08T19:08:45Z,2016-04-07T08:39:25Z,,,,"Added an example on using the response as a file-like object in a context manager to properly handle large files.

Current documentation is not very clear on how to properly download large files in a context manager.

(see http://stackoverflow.com/questions/27387783/how-to-download-a-file-with-urllib3 for more details)
"
529,urllib3 keeps connection open after failed fingerprint validation,2015-01-06T16:18:26Z,2015-01-25T01:35:41Z,,Notes,Notes:,"1.  Launch a single-threaded HTTPS server, like the one from Werkzeug/Flask, with ad-hoc certificate generation.
2.  Paste this into a file called lol.py:
   
   ```
   from urllib3.poolmanager import PoolManager
   
   def x():
       # This is GitHub's certificate fingerprint. This function should not exit successfully.
       fingerprint = 'A0:C4:A7:46:00:ED:A7:2D:C0:BE:CB:9A:8C:B6:07:CA:58:EE:74:5E'  
       p = PoolManager(assert_fingerprint=fingerprint)
       r = p.request('GET', 'https://127.0.0.1:5000')
       p.clear()  # doesn't seem to matter
   ```
3.  Launch `python -i lol.py`, in a Python 3 virtualenv with the latest PyPI-release of urllib3 installed.
4.  Execute `x()` once, an exception for the mismatching fingerprint is thrown.
5.  Execute `x()` again, the function doesn't terminate. Werkzeug is not responding to it because it is busy with another connection, but that one should've been closed.

Notes:
- If one kills the Python process where urllib3 is used, the connections are terminated and the server is usable again.
- This is Python-3-specific.
- This was found during usage with requests, which has the exact same problem. See https://github.com/untitaker/vdirsyncer/pull/161
"
528,Updated Security documentation for proper capture of security warnings,2015-01-02T15:54:13Z,2015-01-08T05:18:10Z,,,,"Per the issue I filed in https://github.com/shazow/urllib3/issues/525#issuecomment-68499803, updated the documentation so that people know the warnings can be easily captured instead of having to ignore them.
"
527,"test failures (failures=1) in release 1.10; test_verified under py2.7, pypy",2014-12-27T02:11:44Z,,,AssertionError,"AssertionError: [call('Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)', <class 'urllib3.exceptions.SecurityWarning'>)]","under py3 I have seen clean runs at first, then on repeated runs an error, so inconsistence causes me to hold back on reporting that fail.  under py2.7 and pypy (the py2 form) with tornado-3.1.1

``` bash
FAIL: test_verified (test.contrib.test_pyopenssl.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/mnt/gen2/TmpDir/portage/dev-python/urllib3-1.10/work/urllib3-1.10/test/with_dummyserver/test_https.py"", line 67, in test_verified
    self.assertFalse(warn.called, warn.call_args_list)
AssertionError: [call('Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)', <class 'urllib3.exceptions.SecurityWarning'>)]
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
urllib3.connectionpool: INFO: Starting new HTTPS connection (2): localhost
urllib3.connectionpool: DEBUG: ""GET / HTTP/1.1"" 200 13
--------------------- >> end captured logging << ---------------------
```

and a report of coverage follows but don't see that's required here.
This seems odd since urllib3-1.9.1 passes fine under py2.7

test_verified (test.contrib.test_pyopenssl.TestHTTPS) ... ok
So 1st. question is can you replicate? 2nd is do you require anything further?

As a note, we are also curious as to dev-requirements.txt:tornado==3.1.1
It appears to work fine under tornado==3.\*  Has this simply missed updating over releases?
## 
"
526,always disable the builtin hostname verification,2014-12-19T16:34:27Z,2015-02-04T18:12:04Z,,,,"It conflicts with our own, more flexible functionality

Fixes #524
"
525,SSL Warnings when SSL is not involved,2014-12-18T22:44:56Z,2014-12-22T14:27:16Z,,,,"I am doing RESTful APIs and am now all of a sudden getting the message:

urllib3/connection.py:251: SecurityWarning: Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)

I understand the position in the referenced issue (#497), however, this is not a very good thing to do. Not every API can/should be behind SSL. For instance, my current API is in dev mode; when deployed it will be behind SSL properly, but not for developing - which is often on a dev system.

This results in multiple incorrect messages being dumped out, littering output (stdout, logs) that only get in the way.

Note: Filed in this WRT the comment https://github.com/shazow/urllib3/issues/497#issuecomment-61354544
"
524,assert_hostname=False seems to be ignored in 1.10,2014-12-18T16:59:44Z,2015-02-04T18:12:04Z,,,,"I have some code that is relying on assert_hostname=False to work.
I upgrade urllib3 to version 1.10 and the code fails with SSLError: hostname 'remote-host' doesn't match 'localhost'

I haven't looked through the code to try to determine why this happens.

Did anyone else notice this? If so, what is the fix.

Essentially I am creating a connection pool like this:

``` python
import urllib3

http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',
                           ca_certs=""cert.pem"",
                           assert_hostname=False)
try:
    r = http.request('GET', 'https://remote-host:4443/')
    print(""Certificate verification NO HOSTNAME successful"")

except urllib3.exceptions.SSLError as e:
    print (""SSL Error:"", e)
    return -1

return 0
```
"
523,False SSL certificate subjectAltName warnings for older Python versions.,2014-12-15T00:07:25Z,2014-12-15T05:59:28Z,,{'notBefore',"{'notBefore': 'Apr  9 00:00:00 2014 GMT', 'serialNumber': '510F0C495E89A1BEEA2AA572D1D4058E', 'notAfter': 'Apr 16 23:59:59 2015 GMT', 'version': 3, 'subject': ((('countryName', u'US'),), (('stateOrProvinceName', u'California'),), (('localityName', u'San Francisco'),), (('organizationName', u'New Relic, Inc.'),), (('commonName', u'*.newrelic.com'),)), 'issuer': ((('countryName', u'US'),), (('organizationName', u'GeoTrust Inc.'),), (('commonName', u'GeoTrust SSL CA - G2'),))}","The change introduced by #497 causes incorrect deprecation warnings to be generated by older Python versions. Specifically, it looks like if using Python 2.7.2 or older, the warning:

```
SecurityWarning: Certificate has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/shazow/urllib3/issues/497 for details.)
  SecurityWarning
```

is generated even though the SSL certificate of the target site has the subjectAltName field.

Test script is:

```
import urllib3
import certifi

http = urllib3.PoolManager(
    cert_reqs='CERT_REQUIRED', # Force certificate check.
    ca_certs=certifi.where(),  # Path to the Certifi bundle.
)

r = http.request('GET', 'https://collector.newrelic.com/')

print r.status, r.data
```

The underlying reason is that older Python versions are not returning the subjectAltName field even though it exists in the certificate.

```
# openssl s_client -connect collector.newrelic.com:443 | tee newrelic.cert
‚Ä¶. lots of output deleted
QUIT

# openssl x509 -inform PEM -in newrelic.cert -text
‚Ä¶. lots of output deleted
        X509v3 extensions:
            X509v3 Subject Alternative Name:
                DNS:*.newrelic.com, DNS:newrelic.com
‚Ä¶. lots more output delete

# python2.6
Python 2.6.9 (default, Oct 22 2014, 19:47:46)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import _ssl
>>> _ssl._test_decode_cert('newrelic.cert')
{'notBefore': 'Apr  9 00:00:00 2014 GMT', 'serialNumber': '510F0C495E89A1BEEA2AA572D1D4058E', 'notAfter': 'Apr 16 23:59:59 2015 GMT', 'version': 3, 'subject': ((('countryName', u'US'),), (('stateOrProvinceName', u'California'),), (('localityName', u'San Francisco'),), (('organizationName', u'New Relic, Inc.'),), (('commonName', u'*.newrelic.com'),)), 'issuer': ((('countryName', u'US'),), (('organizationName', u'GeoTrust Inc.'),), (('commonName', u'GeoTrust SSL CA - G2'),))}

$ python2.7
Python 2.7.2 (default, Oct 11 2012, 20:14:37)
[GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import _ssl
>>> _ssl._test_decode_cert('newrelic.cert')
{'notBefore': 'Apr  9 00:00:00 2014 GMT', 'serialNumber': '510F0C495E89A1BEEA2AA572D1D4058E', 'notAfter': 'Apr 16 23:59:59 2015 GMT', 'version': 3, 'subject': ((('countryName', u'US'),), (('stateOrProvinceName', u'California'),), (('localityName', u'San Francisco'),), (('organizationName', u'New Relic, Inc.'),), (('commonName', u'*.newrelic.com'),)), 'issuer': ((('countryName', u'US'),), (('organizationName', u'GeoTrust Inc.'),), (('commonName', u'GeoTrust SSL CA - G2'),))}

# python2.7
Python 2.7.3 (default, Feb 27 2014, 19:58:35)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import _ssl
>>> _ssl._test_decode_cert('newrelic.cert')
{'subjectAltName': (('DNS', '*.newrelic.com'), ('DNS', 'newrelic.com')), 'notBefore': 'Apr  9 00:00:00 2014 GMT', 'serialNumber': '510F0C495E89A1BEEA2AA572D1D4058E', 'notAfter': 'Apr 16 23:59:59 2015 GMT', 'version': 3, 'subject': ((('countryName', u'US'),), (('stateOrProvinceName', u'California'),), (('localityName', u'San Francisco'),), (('organizationName', u'New Relic, Inc.'),), (('commonName', u'*.newrelic.com'),)), 'issuer': ((('countryName', u'US'),), (('organizationName', u'GeoTrust Inc.'),), (('commonName', u'GeoTrust SSL CA - G2'),))}

# python3.3
Python 3.3.6 (default, Oct 12 2014, 13:56:06)
[GCC 4.6.3] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import _ssl
>>> _ssl._test_decode_cert('newrelic.cert')
{'notBefore': 'Apr  9 00:00:00 2014 GMT', 'subjectAltName': (('DNS', '*.newrelic.com'), ('DNS', 'newrelic.com')), 'version': 3, 'issuer': ((('countryName', 'US'),), (('organizationName', 'GeoTrust Inc.'),), (('commonName', 'GeoTrust SSL CA - G2'),)), 'notAfter': 'Apr 16 23:59:59 2015 GMT', 'serialNumber': '510F0C495E89A1BEEA2AA572D1D4058E', 'subject': ((('countryName', 'US'),), (('stateOrProvinceName', 'California'),), (('localityName', 'San Francisco'),), (('organizationName', 'New Relic, Inc.'),), (('commonName', '*.newrelic.com'),))}
```

This was found by virtue of @kennethreitz requests module bundling latest urllib3 and producing incorrect warnings all the time.

We are having to resort to ignoring the warning:

```
        with warnings.catch_warnings():
            warnings.simplefilter(""ignore"")

            r = session.post(url, params=params, headers=headers,
                    proxies=proxies, timeout=timeout, data=data,
                    verify=cert_loc)
```

If confirmed that older Python versions do not provide this information from a certificate, you possibly should change urllib3 to only generate the warning if using Python 2.7.3 or newer.
"
522,Cut a release?,2014-12-12T02:23:24Z,2014-12-12T02:25:28Z,,,,"requests-2.5.0 came out and it bundles an unreleased version of urllib3 (for instance, it relies on the new `ResponseError` exception).

Can you cut a release of urllib3 to catch up?  This is similar to the old situation in #249.
"
521,Update the comment about SecurityWarning to be accurate,2014-12-09T20:51:28Z,2014-12-09T21:10:14Z,,,,
520,Cleanly handle ZeroReturnError as an expected EOF,2014-12-05T21:08:20Z,2014-12-06T00:53:54Z,,,,"Noticed this in requests, actually!

``` python
>>> import requests
>>> requests.get('https://api.tumblr.com/')
Traceback (most recent call last):
  File ""<console>"", line 1, in <module>
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/api.py"", line 65, in get
    return request('get', url, **kwargs)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/api.py"", line 49, in request
    response = session.request(method=method, url=url, **kwargs)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/sessions.py"", line 461, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/sessions.py"", line 599, in send
    history = [resp for resp in gen] if allow_redirects else []
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/sessions.py"", line 109, in resolve_redirects
    resp.content  # Consume socket so it can be released
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/models.py"", line 728, in content
    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/models.py"", line 653, in generate
    for chunk in self.raw.stream(chunk_size, decode_content=True):
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py"", line 256, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/packages/urllib3/response.py"", line 186, in read
    data = self._fp.read(amt)
  File ""/usr/lib/python2.7/httplib.py"", line 567, in read
    s = self.fp.read(amt)
  File ""/usr/lib/python2.7/socket.py"", line 380, in read
    data = self._sock.recv(left)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 188, in recv
    data = self.connection.recv(*args, **kwargs)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 995, in recv
    self._raise_ssl_error(self._ssl, result)
  File ""/home/ubuntu/.virtualenvs/zapier/local/lib/python2.7/site-packages/OpenSSL/SSL.py"", line 851, in _raise_ssl_error
    raise ZeroReturnError()
ZeroReturnError
```

I haven't been able to reproduce this in pure urllib3, gonna get this and a test case soon!
"
519,Replace no-san certificate with one that expires in 2044,2014-12-03T22:18:52Z,2014-12-03T22:43:34Z,,,,"I'll add a Google Calendar reminder for all of us to update this thirty years from now
"
518,tests on master are failing consistently,2014-12-03T21:27:47Z,2014-12-03T22:43:59Z,,nose.proxy.SSLError,nose.proxy.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:600),"At least it's reproducible!! The failing test is

``` python
class TestHTTPS_NoSAN(HTTPSDummyServerTestCase):
    certs = NO_SAN_CERTS

    def test_warning_for_certs_without_a_san(self):
        """"""A warning should be raised if server cert has no SAN.""""""
        with mock.patch('warnings.warn') as warn:
            https_pool = HTTPSConnectionPool(self.host, self.port,
                                             cert_reqs='CERT_REQUIRED',
                                             ca_certs=NO_SAN_CA)
            r = https_pool.request('GET', '/')
            self.assertEqual(r.status, 200)
            self.assertTrue(warn.called)
```

which fails (on Travis) with:

```
ERROR: Ensure that a warning is raised when the cert from the server has
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/travis/build/kevinburke/urllib3/test/with_dummyserver/test_https.py"", line 394, in test_warning_for_certs_without_a_san
    r = https_pool.request('GET', '/')
  File ""/home/travis/build/kevinburke/urllib3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/home/travis/build/kevinburke/urllib3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/home/travis/build/kevinburke/urllib3/urllib3/connectionpool.py"", line 548, in urlopen
    raise SSLError(e)
nose.proxy.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:600)
```

My current guess is that the logic in the library changed to fail these (or it was always failing and we changed the tests to install pyopenssl). I will try reverting to a far enough back version to try and isolate when this started failing
"
517,Lean on SSLContext to do HTTPS hostname verification,2014-12-01T20:56:35Z,,,,,"In Python 3.4 and 2.7.9 there is a `check_hostname` attribute on `SSLContext` which will have the SSLContext instance handle checking the hostname inside of the `do_handshake`. I think it would be good for urllib3 to rely on this where possible instead of doing the check itself. I think this would go good with the other efforts to use `SSLContext` as the ""bag of configuration"" for TLS stuff.

This can be detected by determining if the `SSLContext` object has a `check_hostname` attribute or not.

There is one downside, this relies on passing the hostname as part of `SSLContext().wrap_socket(server_name=)`. Originally this only worked if the OpenSSL had SNI enabled. However Python 3.4.3 and 2.7.9 will accept `server_name` even if SNI isn't enabled.

This would mean that the code a little ugly until you drop older versions of Python 3, it would look something like (paraphrased and super simplified to just be the ugly parts):

``` python
import ssl
import sys

# In reality these would use the SSLContext shim that urllib3 has in
# order to work on Python 2.6 too and 2.7 earlier than 

if ssl.HAS_SNI or sys.version_info[0] == 2 or sys.version_info[:3] >= (3, 4, 3):
    ctx = SSLContext(ssl.PROTOCOL_SSLv23)
    ctx.check_hostname = True
    sock = ctx.wrap_socket(sock, server_name=""example.com"")
else:
    ctx = SSLContext(ssl.PROTOCOL_SSLv23)
    sock = ctx.wrap_socket(sock)

    if not ssl.match_hostname(sock.getpeercert(), ""example.com""):
        raise Error()
```

Maybe this is a bad idea? I think it would be great to lay the ground work for eventually moving the responsibility for checking hostnames into the stdlib, however you wouldn't actually be able to do that completely without the ugly if statement until 3.4.3 was your minimum supports Python 3 version.
## 
"
516,Handle body param in RequestMethods.,2014-11-27T20:38:52Z,2014-11-27T20:46:44Z,,,,"Fixes #513.
"
515,TLSv1 broken with SNI and Python 2.7.8,2014-11-27T11:00:11Z,2014-12-03T21:39:19Z,,TypeError,TypeError: name must be a byte string,"I have code that was working in Python 2.7.6, but stopped working when I upgraded to 2.7.8.

I'm using the SNI compatibility as described here:
https://urllib3.readthedocs.org/en/latest/contrib.html
The bug only happens with the SNI compatibility activated, but for all https URLs, not just the ones requiring SNI.

Versions:
- urllib3 = 1.9.1
- pyopenssl = 0.14
- ndg-httpsclient = 0.3.2
- pyasn1 = 0.1.7

Whenever I try to fetch an https URL on a server using TLSv1, I get the following error:

  File ""/Library/Python/2.7/site-packages/urllib3/request.py"", line 68, in request
    *_urlopen_kw)
  File ""/Library/Python/2.7/site-packages/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, *_urlopen_kw)
  File ""/Library/Python/2.7/site-packages/urllib3/poolmanager.py"", line 153, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/Library/Python/2.7/site-packages/urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""/Library/Python/2.7/site-packages/urllib3/connectionpool.py"", line 304, in _make_request
    self._validate_conn(conn)
  File ""/Library/Python/2.7/site-packages/urllib3/connectionpool.py"", line 722, in _validate_conn
    conn.connect()
  File ""/Library/Python/2.7/site-packages/urllib3/connection.py"", line 237, in connect
    ssl_version=resolved_ssl_version)
  File ""/Library/Python/2.7/site-packages/urllib3/contrib/pyopenssl.py"", line 272, in ssl_wrap_socket
    cnx.set_tlsext_host_name(server_hostname)
  File ""/Library/Python/2.7/site-packages/OpenSSL/SSL.py"", line 917, in set_tlsext_host_name
    raise TypeError(""name must be a byte string"")
TypeError: name must be a byte string
"
514,Add Context Managers,2014-11-26T05:11:33Z,2015-02-07T01:24:28Z,,,,"The classes like `PoolManager` and `ConnectionPool` and such should act as context managers to make it easier to clean up the resources used by them.
"
513,Support body param in RequestMethods.request,2014-11-26T04:55:35Z,2014-11-27T20:46:44Z,"Contributor Friendly ‚ô•, Soon",,,"Easiest way to do this is by avoiding defining a body kw when no fields are given, then if both are given it will naturally raise a ""passed twice"" error.
"
512,Use a better release process with TLS and Wheels,2014-11-26T04:38:28Z,2014-11-26T04:41:52Z,,,,"This will require `pip install wheel twine` in order to do a release.
"
511,RFC: Session traffic logging,2014-11-23T21:15:38Z,2014-11-23T21:43:15Z,,,,"This is an extension of [requests/#2352](https://github.com/kennethreitz/requests/issues/2352).

Currently it's quite difficult to get usable debug information out of urllib3. Specifically being able to see the request/response data in a similar format as `curl -v`. Logging can be enabled on httplib, but this gets dumped in an unfriendly format, making it difficult to use.

This starts to become apparent when submitting POST data or extra headers, large amounts of data require mangling to make them readable (which involves copy/pasting into a custom tool, which is a pita).

It was suggested that wireshark would be a better approach, but this is not an easy option for everyone depending on how their local environment is set up, and arguably a lot of overhead when you can achieve this easily using `curl` debugging features. That being said, getting `curl` to log POST data is also [troublesome](http://superuser.com/questions/291424/how-do-you-display-post-data-with-curl).

One thing I would agree on is that capturing via Wireshark is going to be more accurate than analysing in code, due to the various layers involved and any bugs in logging code. 

@sigmavirus24 has made it clear he opposes any such feature request, so this ticket can be considered an RFC rather than a feature request. 

The best suggestion I can think of so far would be to add a debugging method on urllib3, however I'm not sure how feasible this is given that the logging output comes from httplib, and monkey-patching isn't really a great idea for obvious reasons.

Here is an example of debugging using urllib3;

```
>>> requests.get('http://httpbin.org/headers')
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): httpbin.org
send: 'GET /headers HTTP/1.1\r\nHost: httpbin.org\r\nConnection: keep-alive\r\nAccept-Encoding: gzip, deflate\r\nAccept: */*\r\nUser-Agent: python-requests/2.4.3 CPython/2.7.5 Darwin/13.3.0\r\n\r\n'
reply: 'HTTP/1.1 200 OK\r\n'
header: Connection: keep-alive
header: Server: gunicorn/18.0
header: Date: Sun, 23 Nov 2014 20:55:23 GMT
header: Content-Type: application/json
header: Content-Length: 353
header: Access-Control-Allow-Origin: *
header: Access-Control-Allow-Credentials: true
header: Via: 1.1 vegur
DEBUG:requests.packages.urllib3.connectionpool:""GET /headers HTTP/1.1"" 200 353
```

And here's an example using curl;

```
$ curl -v http://httpbin.org/headers
* Adding handle: conn: 0x7f8eba004000
* Adding handle: send: 0
* Adding handle: recv: 0
* Curl_addHandleToPipeline: length: 1
* - Conn 0 (0x7f8eba004000) send_pipe: 1, recv_pipe: 0
* About to connect() to httpbin.org port 80 (#0)
*   Trying 54.243.253.52...
* Connected to httpbin.org (54.243.253.52) port 80 (#0)
> GET /headers HTTP/1.1
> User-Agent: curl/7.30.0
> Host: httpbin.org
> Accept: */*
>
< HTTP/1.1 200 OK
< Connection: keep-alive
* Server gunicorn/18.0 is not blacklisted
< Server: gunicorn/18.0
< Date: Sun, 23 Nov 2014 20:56:16 GMT
< Content-Type: application/json
< Content-Length: 274
< Access-Control-Allow-Origin: *
< Access-Control-Allow-Credentials: true
< Via: 1.1 vegur
<
* Connection #0 to host httpbin.org left intact
```
"
510,Publish v1.10,2014-11-22T19:46:22Z,2014-12-14T21:41:59Z,Soon,,,"I recall we're blocking this release on a couple of small-but-important PRs, let's list them here.

@sigmavirus24 @Lukasa?
"
509,Don't break when OpenSSL has SSLv3_METHOD disabled.,2014-11-22T19:27:36Z,2014-11-22T19:44:28Z,,,,
508,benchmark test fail.,2014-11-21T09:19:21Z,2014-11-21T20:01:54Z,,,,"I have changed the List TO_DOWNLOLD as below in [benchmark.py](https://github.com/shazow/urllib3/blob/master/test/benchmark.py).

>  TO_DOWNLOAD = [
>    'http://www.stuzone.com/',
>    'http://baidu.com/',
>    'http://stuzone.com/zone_info/',
>    'http://guwen.stuzone.com/',
>   'http://stuzone.com/zone_vision/',
>    'http://stuzone.com/zone_vision/',
>    'http://stuzone.com/zone_video/ ',
>    'http://stuzone.com/zone_cosa/',
>    'http://stuzone.com/zone_cosa/',
>    'http://www.stuzone.com/zone_deep/?view-3364',
>    'http://www.stuzone.com/zone_scuec/',
>    'http://www.stuzone.com/weibo/index.php?>m=account.login&loginCallBack=%2Fweibo%2Findex.php%3Fm%3Dpub',
>    'https://docs.python.org/2/library/multiprocessing.html#programming-guidelines/',
>  'http://www.ibm.com/developerworks/cn/aix/library/au-threadingpython/index.html',
>   'https://github.com/shazow/urllib3',
>    'http://www.ibm.com/developerworks/cn/aix/library/au-multiprocessing/',
>   ]

Result is : 

> Completed pool_get in 7.366s 
> Completed urllib_get in 5.443s  

What is up?
"
507,Add ssl_context parameter to Connection classes,2014-11-21T05:08:07Z,2016-04-07T12:57:34Z,,,,"After creating a shim SSLContext class in urllib3.util.ssl_, we now want
to start accepting instances of that class in general so users can take
advantage of some features provided only on certain versions of Python.

The first step is to allow users to pass ssl_context to both
HTTPSConnection and VerifiedHTTPSConnection and then pass these down to
the shim ssl_wrap_socket function in urllib3.util.ssl_. This does not
yet account for use with PyOpenSSL.

---

So this basically just works except for the fact that the following will raise an InsecureRequestWarning:

``` python
import ssl
import urllib3
import urllib3.util.ssl_ as ussl


context = ussl.SSLContext(ssl.PROTOCOL_SSLv23)
context.load_verify_locations('../requests/requests/cacert.pem')
context.verify_mode = ssl.CERT_REQUIRED

manager = urllib3.PoolManager(ssl_context=context)
resp = manager.urlopen('GET', 'https://api.github.com/users',
                       preload_content=False)
```

Because the way I wrote `urllib3.util.ssl_.ssl_wrap_socket`, any settings on the the context set needs to be confirmed when creating the pool manager, e.g., the following works:

``` python
import ssl
import urllib3
import urllib3.util.ssl_ as ussl


context = ussl.SSLContext(ssl.PROTOCOL_SSLv23)
context.load_verify_locations('../requests/requests/cacert.pem')
context.verify_mode = ssl.CERT_REQUIRED

manager = urllib3.PoolManager(ssl_context=context,
                              cert_reqs=ssl.CERT_REQUIRED)
resp = manager.urlopen('GET', 'https://api.github.com/users',
                       preload_content=False)
```

I think if we have `urllib3.util.ssl_.ssl_wrap_socket` return early when given a valid context, we won't break any prior behaviour, but I have yet to test that. Does anyone have any thoughts?
"
506,Seek uploaded file-like object to original position before retry (Fix for issue #459),2014-11-14T20:16:19Z,2016-12-02T21:04:39Z,,,,"Fix candidate for #459. @sigmavirus24 can you check if this is good enough?

It doesn't attempt to do anything for non-seekable files. I'm not sure how to properly deal with those.
"
505,HTTPS proxies respect connection timeouts ,2014-11-14T02:21:20Z,2014-12-04T00:19:08Z,,,,"Previously the timeout was set on the the connection too late in the process.
This change passes through the timeout value to _new_conn so it's set for proxy
connection attempts.

Not too happy with the interface change though and open to suggestions.
"
504,HTTPS proxies don't respect connection timeouts,2014-11-13T20:00:15Z,2014-12-04T00:20:09Z,,,,"Code to reproduce (this will take a while to run)

``` python
import urllib3
from urllib3.util.timeout import Timeout
http = urllib3.ProxyManager(proxy_url='https://1.1.1.1', proxy_headers={})
http.request('GET', 'https://google.com', timeout=Timeout(connect=0.3, read=5))
```

The gist of it is, with HTTPS connections connect() gets called earlier in the urlopen() process than with a HTTP connection. Specifically, for HTTPS _new_conn will call _prepare_conn, which calls connect() on the socket, where for HTTP, the connect() call is delayed all the way until httplib.request.

urllib3 sets the connection timeout until after calling _get_conn, but before httplib.request, which explains why the timeout isn't set.

The obvious question is why isn't this an issue when you're not using a proxy? Well, prepare_conn only attempts the connect() if you're using a proxy, with the following note:

```
        # Establish tunnel connection early, because otherwise httplib
        # would improperly set Host: header to proxy's IP:port.
        conn.connect()
```

One way to fix would be to pass the request timeout along with the pool_timeout to `_get_conn` or break out the proxy connect() logic in prepare_conn into its own function.
"
503,Allow starting dummyserver in standalone mode,2014-11-11T21:00:26Z,2014-11-12T00:54:26Z,,,,"I didn't find a way to launch dummyserver stand-alone. Helps writing tests if you can launch the dummyserver and just shoot at it with HTTPie.

Any thoughts how to improve this? Is there a place where this fits on documentation? I'm happy with a doc-only fix if there's another way to run this.

```
(.env)grant:urllib3 joneskoo (dummyserver-standalone) $ python -m dummyserver.server
Listening on http://127.0.0.1:51163
```

```
grant:~ joneskoo $ http get http://127.0.0.1:51163/specific_method?method=GET
HTTP/1.1 200 OK
Content-Length: 0
Content-type: text/plain
Server: TornadoServer/3.1.1

```
"
502,Update README to properly describe certificate,2014-11-07T05:09:16Z,2014-11-07T08:24:34Z,,,,"Addresses @dstufft's comment on #499
"
501,Close a connection in the event of an SSLError,2014-11-05T00:49:31Z,2014-11-07T23:23:56Z,,,,"If an HTTPS connection is made with certificate verification turned on
and the verification of the certificate fails, the connection will be
returned to the connection pool with CERT_REQUIRED set and cannot be
reused. If we close and discard the connection we will avoid further
certificate verification errors.

The only potential problem I see with this approach is that if a user
wants to reconnect with verification but a different pem file, they'll
incur the cost of creating a new connection.

Closes #500 
"
500,Connection retrieved from connection pool has wrong cert_reqs (based on previous request),2014-11-04T23:12:01Z,2014-11-07T23:23:56Z,,,,"Based on the issue on requests library: https://github.com/kennethreitz/requests/issues/2314
I¬¥ve found that when you want to perform a request with `self.cert_reqs=='CERT_NONE'`sometimes (if there has been a previous connection with CERT_REQUIRED) the connection retrieved from the connection pool has CERT_REQUIRED:

```
> /Users/raulcd/Projects/requests/requests/packages/urllib3/connectionpool.py(511)urlopen()
    510             # Request a connection from the queue.
--> 511             conn = self._get_conn(timeout=pool_timeout)
    512

ipdb> self.cert_reqs
'CERT_NONE'
ipdb> n
ipdb> conn.cert_reqs
'CERT_REQUIRED'
```

So the requests should have `CERT_NONE` but the connection retrieved from the connection pool has `CERT_REQUIRED`.

I'll work in a fix.
"
499,Issue warning when certificate has no subjectAltName,2014-11-01T14:11:01Z,2014-11-01T21:12:23Z,,,,"Related bug: #497
"
498,Test on PyPy3,2014-10-31T15:35:23Z,,,,,
497,Feature: Remove support for common names in certificates,2014-10-31T14:32:39Z,,,,,"We should follow the [lead of Chromium](https://code.google.com/p/chromium/issues/detail?id=308330). We should deprecate support for use of common names when a certificate doesn't provide a subjectAltName.

This will bring us into compliance with the >10 year old RFC 2818
## 
"
496,Emit InsecureRequestWarning every time.,2014-10-30T01:49:55Z,2014-10-30T06:55:34Z,,,,"```
urllib3/connectionpool.py:731: InsecureRequestWarning:
Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See:
https://urllib3.readthedocs.org/en/latest/security.html
  InsecureRequestWarning)
```

Every time.
"
495,urllib3 leaks lots of resources,2014-10-28T06:36:04Z,2015-01-20T01:09:48Z,,,,"For fun I decided to run the tests with `PYTHONWARNINGS=always::DeprecationWarning,error::ResourceWarning nosetests`.

According to the warning in the socket module, virtually every test in urllib3 fails to properly close a socket. Example output:

https://travis-ci.org/kevinburke/urllib3/jobs/39233502#L697

```
Exception ignored in: <ssl.SSLSocket fd=62, family=AddressFamily.AF_INET,  
type=SocketType.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57685), 
raddr=('127.0.0.1', 40285)>
```

I'm surprised no one has raised this issue, given how many people use the library. I'm forced to conclude
1. the python library is raising this error erroneously
2. everyone leaks sockets, and it's harmless
3. everyone leaks sockets, and no one cares because sockets are always available
4. the tests leak sockets more than most because they expose error conditions

What's going on? Is there something easy that can be done to fix this @sigmavirus24, @Lukasa, @jschneier , thoughts? 
"
494,move proxy-no-retries test into its own test,2014-10-28T05:21:20Z,2014-10-28T06:22:11Z,,,,"When run at the end of the other test, this fails intermittently, so I split it into its own test, and now it fails 100% of the time on my machine on Python 3.4. Opening this to see what Travis thinks.
"
493,Raise timeout error instead of SSLError on handshake timeout,2014-10-28T03:00:03Z,2014-10-28T04:42:12Z,,,,"Fixed #492 . Adds a test to verify the new behaviour.

So, you can make the if statement in `_raise_timeout` one big conditional but it looks super ugly, see https://gist.github.com/kevinburke/36e6403a1f443092f858.
"
492,timeout can be raised during _validate_conn,2014-10-27T18:55:57Z,2014-10-28T04:42:12Z,,,,"Steps to reproduce:
- set up a port to accept incoming TCP traffic, and never send any data back (for example port 5501 on [hamms](https://github.com/kevinburke/hamms))
- make an HTTPS request to the port with a timeout

``` python
import urllib3, certifi
http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())
r = http.request('GET', 'https://localhost:5501/', timeout=3)
```

The traceback is roughly this:

``` python
  File ""urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""urllib3/poolmanager.py"", line 153, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""urllib3/connectionpool.py"", line 304, in _make_request
    self._validate_conn(conn)
  File ""urllib3/connectionpool.py"", line 722, in _validate_conn
    conn.connect()
  File ""urllib3/connection.py"", line 237, in connect
    ssl_version=resolved_ssl_version)
  File ""urllib3/util/ssl_.py"", line 132, in ssl_wrap_socket
    ssl_version=ssl_version)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.py"", line 392, in wrap_socket
    ciphers=ciphers)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.py"", line 148, in __init__
    self.do_handshake()
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ssl.py"", line 310, in do_handshake
    self._sslobj.do_handshake()
```

The error is a SSLError but it seems like this should be a timeout:

> SSLError: [Errno 0] The handshake operation timed out

We have logic to catch SSLErrors and raise them as timeouts, which would catch this:

```
    except BaseSSLError as e:
        # Catch possible read timeouts thrown as SSL errors. If not the
        # case, rethrow the original. We need to do this because of:
        # http://bugs.python.org/issue10272
        if 'timed out' in str(e) or \
           'did not complete (read)' in str(e):  # Python 2.6
            raise ReadTimeoutError(
                    self, url, ""Read timed out. (read timeout=%s)"" % read_timeout)

        raise
```

Unfortunately this code block doesn't catch this timeout which is raised during `_validate_conn`.

There are a few options here.
- duplicate the `if ""timed out"" in ...` logic around `_validate_conn`
- move the `if ""timed out"" in ...` logic up to urlopen. Existing read timeout errors would get caught in the `except (BaseSSLError, CertificateError) as e` block in urlopen, and not have the connection closed/replaced.
- do nothing and leave ""handshake timed out"" errors as SSLErrors.
"
491,Enable Sourcegraph,2014-10-25T06:44:15Z,2014-10-27T23:15:03Z,,,,"I want to use [Sourcegraph code search and code review](https://sourcegraph.com) with urllib3. A project maintainer needs to enable it to set up a webhook so the code is up-to-date there.

Could you please enable urllib3 on @Sourcegraph by going to https://sourcegraph.com/github.com/shazow/urllib3 and clicking on Settings? (It should only take 15 seconds.)

Thank you!
"
490,use Queens English,2014-10-23T00:11:35Z,2014-10-23T15:24:07Z,,,,
489,Change how ssl_wrap_socket handles certfiles and keyfiles to match the r...,2014-10-22T21:13:57Z,2014-10-23T20:36:48Z,,,,"...egular python ssl library

I first noticed this when I started getting my requests rejected by the server when I started using pyopenssl. I had been using requests and had been passing a client certificate in like `requests.get(..., cert='foo.pem')`

When using the regular ssl library, we call ssl.wrap_socket, which uses the certfile as both the certfile and the keyfile if the keyfile is None [1].

When using pyopenssl, we should keep the same expectation.

[1] https://github.com/python/cpython/blob/455a24625d461ea841ea42b277aa0fb72065a04a/Lib/ssl.py#L491-L492
"
488,Disable SSLv2 and SSLv3 from being used where possible,2014-10-16T19:41:00Z,2016-06-16T20:20:05Z,,,,"This will hopefully mitigate POODLE and all attacks related to SSLv2
and SSLv3 for versions of Python that allow us to disable this via
SSLContext options. Currently that excludes Python 2.6 and Python 2.7.
Python 2.7.9 should include the ability to disable these.

cc @Lukasa @dstufft @alex

Related issues: #471, #487
"
487,SSL v3 is vulnerable,2014-10-15T03:11:19Z,,,,,"I'm trying to figure out whether this means that urllib3 is vulnerable.
- In browsers, failed connections are retried with a different, weaker protocol, which may be SSL v3, which is why Google is freaking out a lot about this. I don't think urllib3 has this problem.
- I am confused by the `ssl_wrap_socket` method in pyopenssl.py because it takes `ssl_version=None` as an argument. However the first line of the method calls 

``` python
OpenSSL.SSL.Context(_openssl_versions[ssl_version])
```

But `_openssl_versions[None]` will raise a KeyError.
- When it's run, CPython 2.7.8 and Pypy (2.7.6) use TLS v1.0 to secure the connection, at least [according to howsmyssl.com](https://gist.github.com/kevinburke/eb02e1e1984b9d2ce806). The only place I can see a default ssl version being set is in `resolve_ssl_version` which sets the version as PROTOCOL_SSLv23 if it's not explicitly set. I can't figure out where in the urllib3 code that TLS v1 gets set.
- This issue suggests a possible path to disabling: https://github.com/shazow/urllib3/issues/471 Though the code path suggests this would only work for environments with an SSLContext. 

I'm still trying to work through all of this.
## 
"
486,SOCKS proxy support (attempt #2),2014-10-14T22:44:47Z,2015-12-29T20:29:10Z,,,,"initial shot at revitalizing https://github.com/shazow/urllib3/pull/284

continuation of work from discussion at #68 -- going to see about using pycurl + Tornado for tests.
"
485,Small fixes to RecentlyUsedContainer,2014-10-10T13:18:27Z,2014-10-10T18:53:56Z,,,,
484,Make the language a little more friendly.,2014-10-04T18:56:18Z,2014-10-06T17:34:12Z,,,,
483,Refactor tox & travis settings to include gevent #482,2014-10-03T16:40:01Z,,,,,"- Add test env with gevent for python 2.7

Other changes:
- Use tox command to run tests from travis
- Set environment variables on .travis.yml to run tests
"
482,wrap_socket does not accept server_hostname in 2.7.x (breaks with gevent),2014-10-03T10:27:06Z,,Bounty,,,"The recent backport of SNI related features into 2.7 series has resulted in urllib looking at python 2.7.8+ as python 3.2+ and ssl_.py calls wrap_socket with the server_hostname as argument. While this works well when not monkey patched with gevent, I am reporting the bug here since one of the features of this library is gevent compat.
## 
"
481,Add ResponseError exception class,2014-10-02T18:20:56Z,2014-10-03T16:11:27Z,,,,"This class is for erroneous states that occur after a response has been
downloaded from the server. Examples include a server that is raising many 500
server errors (with a Retry object coded to forbid this) or a redirect cycle
that exceeds the specified redirect limit.

In addition, ensures that all objects passed as the `reason` parameter to
MaxRetryError are Exception objects.
"
480,"categorizing failures, or Kevin does a bad thing",2014-10-02T17:53:16Z,2014-10-04T16:47:56Z,,,,"So the work in #464 identified some cases where we raise a MaxRetryError but don't have a `reason` (an explicit exception) to be raised - lots of redirects or server failures. 

When trying to work with this new code I realized a) it's difficult to distinguish _why_ the request failed, without a corresponding exception, and b) it's probably not good to mix types for e.reason - currently error is a Exception and cause is a string.

I propose we introduce a `ResponseError` object that covers all errors when we received a full HTTP response from the server. and then change

```
raise MaxRetryError(_pool, url, error or cause)
```

to

```
raise MaxRetryError(_pool, url, error or ResponseError(cause))
```

this way you can check for this in outside code by writing

``` python
if isinstance(e.reason, ResponseError):
    # handle some way
elif isinstance(e.reason, ConnectTimeoutError):
    # do something else
```

etc. etc.
"
479,Dummy commit,2014-10-01T18:10:43Z,2014-10-01T18:20:41Z,,,,"I think master might be broken for pypy. Testing this suspicion.
"
478,"urllib3.connectionpool: Setting read timeout to None & ""'Task got bad yield: <Response [200]>",2014-09-30T10:37:51Z,2014-09-30T15:17:16Z,,,,"Hi,
'm porting the already functioning (in python3.4) code
(https://github.com/KeepSafe/aiohttp/blob/master/examples/crawl.py ) into python2.76 code, through trollius, urllib and requests.

Running the file, the output is:

time python crawl_requests.py http://www.ilsole24ore.com/english-version/front-page.shtml
DEBUG:trollius:Using selector: EpollSelector
('url to do = ', 'http://www.ilsole24ore.com/english-version/front-page.shtml')
('processing:', 'http://www.ilsole24ore.com/english-version/front-page.shtml')
INFO:urllib3.connectionpool:Starting new HTTP connection (1): www.ilsole24ore.com
DEBUG:urllib3.connectionpool:Setting read timeout to None
DEBUG:urllib3.connectionpool:""GET /english-version/front-page.shtml HTTP/1.1"" 200 13699
('...', 'http://www.ilsole24ore.com/english-version/front-page.shtml', 'has error', ""'Task got bad yield: <Response [200]>'"")
('done:', 1, '; ok:', 0)

Here is the code:
http://ipaste.org/fLj

Could pleas give me some hints in solving this issue?
Looking forward to your helpfull hints and suggestions.
Kind regards.
Marco
"
477,disable tunneling for https proxy,2014-09-24T23:10:53Z,,,,,"i dont think https proxying http reqs works now anyway (added test fails without patch). by disabling tunneling for https proxies we can now: https proxy http requests (no http connect tunnel est needed).

closes #476
"
476,allow http requests through https proxy,2014-09-24T23:06:32Z,,,,,"this has been brought up a few times for `requests`:
- https://github.com/kennethreitz/requests/issues/1622
- https://github.com/kennethreitz/requests/issues/1903

and elsewhere:
- http://wiki.squid-cache.org/Features/HTTPS#Direct_SSL.2BAC8-TLS_connection
- http://dev.chromium.org/developers/design-documents/secure-web-proxy

case for me is: send http req --ssl--> https proxy (manipulates and fwds) --> origin. but `ProxyManager` tunnels these reqs unnecessarily (?).

would like to disable tunneling for https proxies so behavior is same as for http proxies just over ssl'd cxn. leaves open  proxying https reqs through https proxy (this doesnt appear to work now, but dont specifically need it).
## 
"
475,fail upload file when the filename contains Chinese,2014-09-24T03:36:18Z,2014-09-24T04:04:51Z,,,,"my python version is 2.7.6,and use the lastest urllib3
my debug code is  

```
r = manager.request('POST', 'http://localhost:8090', fields={'token': 'a2bc35ef2ede22f1fdf9166a7b409dc6', 'file': ('20140312213019GHHGYBfÂØ∫QQ20140312213003.jpg', 'abc')})
```

and it comes with the follow result
![image](https://cloud.githubusercontent.com/assets/6461975/4383136/1ede8898-439a-11e4-9f81-a5ed67f89783.png)  
when I use curl  

```
curl http://localhost:8090/ -F ""file=@20140312213019GHHGYB_QQÂõæÁâá20140312213003.jpg""
```

it comes  
![image](https://cloud.githubusercontent.com/assets/6461975/4383174/21039928-439b-11e4-804e-2fb9dc0b2a43.png)

so ,i have to modify my code to make it work  

```
r = manager.request('POST', 'http://localhost:8090', fields={'token': 'a2bc35ef2ede22f1fdf9166a7b409dc6', 'file': (urllib.quote('20140312213019GHHGYBfÂØ∫QQ20140312213003.jpg'), 'abc')})
```

so,it comes  
![image](https://cloud.githubusercontent.com/assets/6461975/4383209/a41199f0-439b-11e4-8490-16114a7e38af.png)
"
474,Allow ssl certs/keys as buffer instead of file path,2014-09-20T19:21:48Z,2016-10-12T09:47:33Z,,,,"I would like to be able to provide certificates and private keys as buffers instead of file paths to, e.g., `urllib3.util.ssl_wrap_socket`.

I think the reason this wasn't done before was because the `ssl`'s way of handling certificates and private keys was through file paths (see https://docs.python.org/2/library/ssl.html#ssl.SSLContext.wrap_socket). If `urllib3` is making a move to @alekstorm's `backports.ssl` package, then this is something that we can solve! If @shazow and @alekstorm are open to such a change, I don't mind diving in to this, because I have a handful of applications that aren't allowed to serialize their certificates and private keys to disk.

Would you both agree that using `backports.ssl` to solve this problem is correct? Would you like to see this change?
"
473,Use 3.4's ssl.create_default_context when available,2014-09-19T16:05:42Z,2014-10-30T01:24:35Z,,,,"Related #472 
"
472,Use ssl.create_default_context if available,2014-09-19T14:52:28Z,,,,,"Python 3.4 added `ssl.create_default_context()`, ideally urllib3 should use this where possible. It is designed to be a ""give me a SSLContext with best practices applied"" function and will ideally enable improvements in that particular function to be picked up by urllib3 automatically.
## 
"
471,SSLv2 should be disallowed,2014-09-19T14:42:56Z,,,,,"SSLv2 is an extremely broken protocol, many distros have patched it out of their OpenSSL, but not all.

It can be disabled by using the `OP_NO_SSLv2` flag in the `SSLContext`.
## 
"
470,"ProtocolError spans connection, read errors",2014-09-18T03:02:12Z,,,,,"For retry purposes it's grouped as a ""read"" in `_is_read_error` but most types of errors are connection errors.

ProtocolError encompasses SocketErrors (DNS lookup failures, EADDRINUSE, ECONNRESET, ECONNREFUSED, etc), as well as the following exceptions in httplib (taken from CPython 2.7)

``` python
class HTTPException(Exception):
class NotConnected(HTTPException):
class InvalidURL(HTTPException):
class UnknownProtocol(HTTPException):
class UnknownTransferEncoding(HTTPException):
class UnimplementedFileMode(HTTPException):
class IncompleteRead(HTTPException):
class ImproperConnectionState(HTTPException):
class BadStatusLine(HTTPException):
class LineTooLong(HTTPException):
```

Some of these are raised before the connection is established and some aren't. I'm not sure catching them all as read errors is the best approach. It is ""simpler"" for some degree of simpler.

If you are looking for a recommendation I would recommend moving the httplib exceptions that happen after data has been written to the socket (ImproperConnectionState, BadStatusLine, LineTooLong, IncompleteRead) to be instances of RequestError, and then move ProtocolError to `_is_connection_error` in retry.py.
## 
"
469,Not sure status_forcelist is implemented :(,2014-09-18T01:43:44Z,2014-09-18T02:00:20Z,,,,"There's a method called is_forced_retry which checks it but this is never called from anywhere. Probably need a higher-level test for this in test_retry.py (as I am writing this I realize that https://github.com/shazow/urllib3/pull/464 is probably broken). 
"
468,Changed default: preload_content=False,2014-09-16T17:50:53Z,,,,,"Fixes #436. Hopefully this will be backwards compatible to nearly all users. Slated for v1.10.

If there are any concerns, please raise them. :) /cc @Lukasa @sigmavirus24 
"
467,WIP: Add source address to PoolManager,2014-09-13T21:19:14Z,2016-06-16T20:21:02Z,,,,"This is #376 after a rebase. Shortly I'll address the review comments there and get this into a merge ready state.
"
466,"Url: handle parsing of ""oddball schemes"" (data, magnet etc)",2014-09-12T01:24:44Z,,,,,"In `requests`, there has been a commit added, which skips urllib3's `parse_url` in certain cases: https://github.com/kennethreitz/requests/commit/b149be5d864cc7f65ac22113db3d0e4d2ed2b49e

It's meant to not cause e.g. a `LocationParseError`, but then also accidentally skips e.g. `localhost:3128`.

While that issue is specific to `requests`, I think it would be nice if `url_parse` would handle these.
## 
"
465,Url: urlunparse / string representation,2014-09-12T00:21:57Z,2014-09-12T01:03:37Z,,,,"It would be nice to have a `urlunparse` method, which would take an `Url` (from `parse_url`) and return a string.

Currently the following code achieves this:

```
default_scheme = 'http'
Url = parse_url(url)
scheme = Url.scheme or default_scheme
path = Url.path or ''
return urlunparse((scheme, Url.netloc, path, None, Url.query, Url.fragment))
```
"
464,Add failing test with bad message,2014-09-11T17:30:48Z,2014-10-01T18:40:19Z,,,,"We incorrectly state that some errors were ""Caused by redirect"" when they aren't, for example if you retry and get a 500 a few times in a row. 

This adds a failing test to demonstrate this. Not sure how you'd like to handle it, I don't have great suggestions at the moment but I'll keep looking.
"
463,Doc Fix: Disable compression not encryption,2014-09-11T15:01:49Z,2014-09-11T23:00:08Z,,,,
462,Debug message while using elasticsearch,2014-09-10T05:23:16Z,2014-09-10T06:10:34Z,,,,"Hi,

I'm using a library that depends on urllib3 and I'm getting the message below that is apparently from the urllib3 retry module. Is it something I should be concerned about?

`Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)`

Thanks,
John
"
461,Connection keep-alive and error rewrapping,2014-09-07T13:06:32Z,,,,,"- Add connection keep alive as default header.
- Error rewrapping issue
"
460,Added GAE requirements doc in security,2014-09-03T15:32:07Z,2014-09-06T19:01:36Z,,,,"Added the Google App Engine requirements.
"
459,urllib3 upload retry fails to seek file-object,2014-08-29T13:23:10Z,2016-12-02T21:04:49Z,,,,"I believe urllib3 fails to seek file object to beginning when it retries the upload. This bug is probably the true reason why I got timeout in issue #442 .

I overwrote read method in file object to demonstrate this. 

https://gist.github.com/joneskoo/3fe7fb68836d9b387273

The retry handling is non-trivial and gets even more complicated as there's file objects and not strictly strings to deal with.

I believe the retry with file objects should:
1. seek file to beginning or position reported by tell() before reading the file object
2. FAIL if the file object does not support seeking

Currently if I understand correctly the current behaviour will corrupt uploads or result in Timeout error as I experienced before.

Thanks @Lukasa for continued help trying to debug this.
## 
"
458,Wrong error wrapping,2014-08-29T11:09:53Z,,,,,"**Related to**: https://github.com/kennethreitz/requests/issues/2169

**Issue**

If the `keep-alive` header is not send, the connections socket will be closed (_this is normal_) and further connection will raise `SocketError` in `connection_pool.py` (https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L328).

But if the connection go through proxy, the rewraping makes it be a `Proxy Error`, even if its no proy error (https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L553)

I find this confusing and this give me hard time debugging. 

**Solutions**

If one can give me an advice how to test if the conection is closed, I may code to add this verification and add it before rewraaping to `ProxyError`
## 
"
457,specify the test folder,2014-08-25T20:27:26Z,2014-08-25T20:32:34Z,,,,"In case tests exist in a virtualenv, they'll get run twice with the base `nosetests` command.
"
456,Replace TARPIT_HOST with a mock class,2014-08-25T20:04:29Z,2015-04-07T05:36:36Z,,,,"okay i have learned my lesson with git branches and run the tests.
"
455,Source address update,2014-08-25T19:43:22Z,2014-08-25T20:46:43Z,,,,"debugging info for this failure
"
454,test failures on master,2014-08-25T18:53:36Z,,,"AssertionError, ProtocolError, MaxRetryError","AssertionError:, ProtocolError: ('Connection aborted.', error(61, 'Connection refused')), MaxRetryError: HTTPConnectionPool(host='localhost', port=50350): Max retries exceeded with url: / (Caused by ReadTimeoutError(""HTTPConnectionPool(host='localhost', port=50350): Read timed out. (read timeout=0.001)"",))","Running on a Mac, 10.9, with the following command

``` sh
while true; do make test ; if [[ $? -gt 0 ]]; then break; fi; done
```

Some of the failures are below.

ProtocolError not raised (got this one twice):

```
def test_source_address_error(self):
    for addr in INVALID_SOURCE_ADDRESSES:
        pool = HTTPConnectionPool(self.host, self.port,
                source_address=addr, retries=False)
        self.assertRaises(ProtocolError,
                pool.request, 'GET', '/source_address')
```

Not sure what r.data actually was:

``` python
FAIL: test_source_address (test.with_dummyserver.test_connectionpool.TestConnectionPool)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kevin/code/urllib3/test/with_dummyserver/test_connectionpool.py"", line 598, in test_source_address
    assert r.data == b(addr[0])
AssertionError:
```

Connection failed:

``` python
ERROR: test_connection_read_timeout (test.with_dummyserver.test_socketlevel.TestSocketClosing)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kevin/code/urllib3/test/with_dummyserver/test_socketlevel.py"", line 136, in test_connection_read_timeout
    self.assertRaises(ReadTimeoutError, pool.request, 'GET', '/')
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 473, in assertRaises
    callableObj(*args, **kwargs)
  File ""/Users/kevin/code/urllib3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/Users/kevin/code/urllib3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/Users/kevin/code/urllib3/urllib3/connectionpool.py"", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File ""/Users/kevin/code/urllib3/urllib3/util/retry.py"", line 223, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/Users/kevin/code/urllib3/urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""/Users/kevin/code/urllib3/urllib3/connectionpool.py"", line 308, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 995, in request
    self._send_request(method, url, body, headers)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1029, in _send_request
    self.endheaders(body)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 991, in endheaders
    self._send_output(message_body)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 844, in _send_output
    self.send(msg)
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 806, in send
    self.connect()
  File ""/Users/kevin/code/urllib3/urllib3/connection.py"", line 146, in connect
    conn = self._new_conn()
  File ""/Users/kevin/code/urllib3/urllib3/connection.py"", line 125, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File ""/Users/kevin/code/urllib3/urllib3/util/connection.py"", line 87, in create_connection
    raise err
ProtocolError: ('Connection aborted.', error(61, 'Connection refused'))
-------------------- >> begin captured logging << --------------------
urllib3.util.retry: DEBUG: Converted retries value: False -> Retry(total=False, connect=None, read=None, redirect=0)
urllib3.connectionpool: INFO: Starting new HTTP connection (1): localhost
```

Another one

``` python
======================================================================
ERROR: test_timeout_errors_cause_retries (test.with_dummyserver.test_socketlevel.TestSocketClosing)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kevin/code/urllib3/test/with_dummyserver/test_socketlevel.py"", line 175, in test_timeout_errors_cause_retries
    response = pool.request('GET', '/', retries=1)
  File ""/Users/kevin/code/urllib3/urllib3/request.py"", line 68, in request
    **urlopen_kw)
  File ""/Users/kevin/code/urllib3/urllib3/request.py"", line 81, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/Users/kevin/code/urllib3/urllib3/connectionpool.py"", line 579, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/Users/kevin/code/urllib3/urllib3/connectionpool.py"", line 559, in urlopen
    _pool=self, _stacktrace=stacktrace)
  File ""/Users/kevin/code/urllib3/urllib3/util/retry.py"", line 265, in increment
    raise MaxRetryError(_pool, url, error)
MaxRetryError: HTTPConnectionPool(host='localhost', port=50350): Max retries exceeded with url: / (Caused by ReadTimeoutError(""HTTPConnectionPool(host='localhost', port=50350): Read timed out. (read timeout=0.001)"",))
```

The server from the same error

``` python
Exception in thread Thread-22:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py"", line 810, in __bootstrap_inner
    self.run()
  File ""/Users/kevin/code/urllib3/dummyserver/server.py"", line 70, in run
    self.server = self._start_server()
  File ""/Users/kevin/code/urllib3/dummyserver/server.py"", line 66, in _start_server
    self.socket_handler(sock)
  File ""/Users/kevin/code/urllib3/test/with_dummyserver/test_socketlevel.py"", line 145, in socket_handler
    sock = listener.accept()[0]
  File ""/usr/local/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py"", line 202, in accept
    sock, addr = self._sock.accept()
timeout: timed out
```

This is the result of running the tests maybe 100 times, while connected to a stable wifi network. Occasionally I also see `No handlers could be found for logger ""tornado.access""`.
## 
"
453,No tarpit,2014-08-25T17:30:50Z,2014-08-25T19:56:52Z,,,,
452,No tarpit,2014-08-25T04:08:27Z,2014-08-25T17:30:14Z,,,,"This caused problems on some people's machines when their connections would throw a RST packet, I believe, and there's always the possibility that someone has a machine that's listening on 10.255.255.1.

Fixes #448 
"
451,Pyopenssl tests,2014-08-24T14:23:29Z,2014-08-25T20:09:13Z,,,,"Enables unit tests for the contrib.pyopenssl module on python2.
More notes in the commit messages.

@shazow Any ideas how to enable coverage for the pyopenssl module?

This depends on #450
"
450,make our WrappedSocket compatible with pypy,2014-08-24T12:44:41Z,2014-08-25T19:05:49Z,,,,"The changes are inspired by socket.py from pypy and the implementation is
lifted from ssl.py from pypy.

I have another branch where we actually test pyopenssl with tox and travis, we should probably wait until that one is done.

Adresses #449
"
449,pyopenssl fails on pypy,2014-08-23T23:21:53Z,2014-08-25T19:18:08Z,,"Exception, AttributeError","Exception:, AttributeError: 'WrappedSocket' object has no attribute '_reuse'","steps to reproduce:
- ensure your Python is PyPy (I am using 2.3.1)
- pip install openssl ndg-httpsclient pyasn1
- pip install <anything else>

fails with this traceback:

``` python
Exception:
Traceback (most recent call last):
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/commands/install.py"", line 278, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/req.py"", line 1177, in prepare_files
    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/index.py"", line 194, in find_requirement
    page = self._get_page(main_index_url, req)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/index.py"", line 568, in _get_page
    session=self.session,
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/index.py"", line 670, in get_page
    resp = session.get(url, headers={""Accept"": ""text/html""})
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/sessions.py"", line 468, in get
    return self.request('GET', url, **kwargs)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/download.py"", line 237, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/sessions.py"", line 456, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/sessions.py"", line 559, in send
    r = adapter.send(request, **kwargs)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/adapters.py"", line 327, in send
    timeout=timeout
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/packages/urllib3/connectionpool.py"", line 516, in urlopen
    body=body, headers=headers)
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/packages/urllib3/connectionpool.py"", line 331, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""/Users/kevin/.pyenv/versions/pypy-2.3.1/lib-python/2.7/httplib.py"", line 1043, in getresponse
    response = self.response_class(*args, **kwds)
  File ""/Users/kevin/.pyenv/versions/pypy-2.3.1/lib-python/2.7/httplib.py"", line 339, in __init__
    self.fp = sock.makefile('rb')
  File ""/Users/kevin/code/pip/venv/site-packages/pip-1.5.6-py2.7.egg/pip/_vendor/requests/packages/urllib3/contrib/pyopenssl.py"", line 169, in makefile
    return _fileobject(self, mode, bufsize)
  File ""/Users/kevin/.pyenv/versions/pypy-2.3.1/lib-python/2.7/socket.py"", line 300, in __init__
    sock._reuse()
AttributeError: 'WrappedSocket' object has no attribute '_reuse'
```

This is pretty annoying because it means I can't install anything via `pip` if I have those classes installed (which I do with urllib3). 

Maybe we shouldn't be using the internal `_fileobject` method of the socket class? @t-8ch, thoughts?
"
448,rewrite socket connection timeout tests to use mocked socket,2014-08-23T22:14:27Z,2014-11-04T17:32:29Z,,,,"is it even possible? this is the approach used by the standard library. i imagine we could use similar to avoid having to hard code 10.255.255.1 everywhere.

Line 1202 or so here https://github.com/certik/python-2.7/blob/master/Lib/test/test_socket.py

``` python
    @contextlib.contextmanager
    def mocked_socket_module(self):
        """"""Return a socket which times out on connect""""""
        old_socket = socket.socket
        socket.socket = self.MockSocket
        try:
            yield
        finally:
            socket.socket = old_socket

    def test_create_connection_timeout(self):
        # Issue #9792: create_connection() should not recast timeout errors
        # as generic socket errors.
        with self.mocked_socket_module():
            with self.assertRaises(socket.timeout):
                socket.create_connection((HOST, 1234))
```
"
447,Document MaxRetryError parameters.,2014-08-23T20:59:34Z,2014-08-25T17:29:20Z,,,,
446,'HTTPSConnection' object has no attribute 'sock' while using in GAE,2014-08-23T13:49:52Z,2014-09-01T17:09:41Z,,AttributeError,AttributeError: 'HTTPSConnection' object has no attribute 'sock',"I am using requests library in Google App Engine and trying to fetch some tweets for analysis, but facing the following error, as per the conversion with Cory Benfield, he suggested me to raised the issue here.

```
Traceback (most recent call last):
  File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/main.py"", line 94, in main
    run_wsgi_app(application)
File          
""/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/ext/webapp/util.py"",    line 98, in run_wsgi_app
run_bare_wsgi_app(add_wsgi_middleware(application))

File        
""/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/ext/webapp/util.py"",     line 116, in run_bare_wsgi_app
    result = application(env, _start_response)    
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1519, in __call__
response = self._internal_error(e)
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1511, in __call__
rv = self.handle_exception(request, response, e)
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1505, in __call__
rv = self.router.dispatch(request, response)
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1253, in default_dispatcher
return route.handler_adapter(request, response)
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1077, in __call__
return handler.dispatch()
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 547, in dispatch
return self.handle_exception(e, self.app.debug)
File ""/base/data/home/runtimes/python27/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 545, in dispatch
return method(*args, **kwargs)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/main.py"", line 49, in post
tweetTextCotainer = THandler.getTweetsText()
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/main.py"", line 80, in getTweetsText
access_token_secret = self.access_token_secret
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/TwitterSearch/TwitterSearch.py"", line 63, in __init__
self.authenticate(verify)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/TwitterSearch/TwitterSearch.py"", line 83, in authenticate
r = requests.get(self._base_url + self._verify_url, auth=self.__oauth, proxies=self.__proxy)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/api.py"", line 55, in get
return request('get', url, **kwargs)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/api.py"", line 44, in request
return session.request(method=method, url=url, **kwargs)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/sessions.py"", line 468, in request
resp = self.send(prep, **send_kwargs)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/sessions.py"", line 574, in send
r = adapter.send(request, **kwargs)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/adapters.py"", line 345, in send
timeout=timeout
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/packages/urllib3/connectionpool.py"", line 516, in urlopen
body=body, headers=headers)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/packages/urllib3/connectionpool.py"", line 304, in _make_request
self._validate_conn(conn)
File ""/base/data/home/apps/s~analyzetweets/4.378165256673152213/requests/packages/urllib3/connectionpool.py"", line 721, in _validate_conn
if not conn.sock:
AttributeError: 'HTTPSConnection' object has no attribute 'sock'
```

We can reproduce the error on Google App Engine as follow:
- Open the url http://analyzetweets.appspot.com/
- Please enter any text in the form and then press submit 
- Error will be displayed
"
445,warn if the system time is wrong,2014-08-22T10:23:47Z,2014-08-25T17:31:31Z,,,,"addresses kennethreitz/requests#2170

The naming can be improved. We could probably add better support for windows (I think there the time begins at 1900, should be tested)
"
444,allow performing *only* fingerprint verification,2014-08-20T20:35:51Z,2014-08-20T21:29:19Z,,,,"Before if we verified a fingerprint we also required a valid certificate.
Valid in 'signed by a trusted CA, current time in valid timerange', not in the
sense of matching hostnames.
We should allow people to verify certificates via fingerprints that are totally
crap otherwise.
"
443,Catch ConnectionError on Py3.,2014-08-20T19:35:20Z,2014-09-02T04:58:48Z,,,,"This is a very boring possible fix for #442.

@joneskoo, do you might testing this branch out to see if it resolves your problem? Actually testing this is stupid-hard, you basically need to rely on a timing window.
"
442,Connection retry fails with BrokenPipeError on Python3,2014-08-20T11:31:23Z,2014-09-02T04:59:34Z,,,,"After maybe 10 minutes of uploading smallish ~few MB files successfully to S3, using the same session for all requests, requests fails and then fails on all subsquent retries. Ultimately retry counter runs out.

> WARNING requests.packages.urllib3.connectionpool Retrying (10 attempts remain) after connection broken by 'BrokenPipeError(32, 'Broken pipe')': /path
> INFO requests.packages.urllib3.connectionpool Starting new HTTPS connection (2): bucketname.s3.amazonaws.com
> WARNING requests.packages.urllib3.connectionpool Retrying (9 attempts remain) after connection broken by 'ReadTimeoutError(""HTTPSConnectionPool(host='bucketname.s3.amazonaws.com', port=443): Read timed out. (read timeout=20)"",)': /path
> INFO requests.packages.urllib3.connectionpool Starting new HTTPS connection (3): bucketname.s3.amazonaws.com
> WARNING requests.packages.urllib3.connectionpool Retrying (8 attempts remain) after connection broken by 'ReadTimeoutError(""HTTPSConnectionPool(host='bucketname.s3.amazonaws.com', port=443): Read timed out. (read timeout=20)"",)': /path
> INFO requests.packages.urllib3.connectionpool Starting new HTTPS connection (4): bucketname.s3.amazonaws.com
> WARNING requests.packages.urllib3.connectionpool Retrying (7 attempts remain) after connection broken by 'ReadTimeoutError(""HTTPSConnectionPool(host='bucketname.s3.amazonaws.com', port=443): Read timed out. (read timeout=20)"",)': /path

After speaking with @Lukasa on #python-requests, I think this has something to do with Python 3 built-in [BrokenPipeError](https://docs.python.org/3/library/exceptions.html#BrokenPipeError) (subclass of ConnectionError) not being handled.

I think the problem is in this code: [urllib3/connectionpool.py:535](https://github.com/kennethreitz/requests/blob/master/requests/packages/urllib3/connectionpool.py#L535)

> 14:19 < lukasa> Ah, interesting
> 14:19 < lukasa> Yeah, I think we should treat that as a TLS failure
> 14:19 < lukasa> Open an issue against urllib3, we'll sort it out. =)
"
441,Responses with content-encoding:gzip and transfer-encoding:chunked fail,2014-08-09T16:17:55Z,2015-04-29T20:28:54Z,Bounty,,,"If you try to download a file from a server that has ""content-encoding: gzip"" and ""transfer-encoding: chunked"" you get a failure successfully decoding the content because empty strings get passed to the decoder after the last chunk is processed.

I have a tentative patch, but am having a tough time writing a unit test to go along with it since it seems that the httplib HTTPResponse object is the one responsible for handling the transfer-encoding:chunked .

My first attempts to fix it are here:
https://github.com/Ludovicus/urllib3/tree/issue_441

The debugging output there is not intended to be final!
## 
"
440,Release tarball for urllib3-1.9 is missing files required to run tests,2014-08-06T15:19:54Z,2014-08-06T18:45:11Z,,ValueError,ValueError: Attempted relative import in non-package,"The initial thing that I noticed is that the file `tests/__init__.py` is missing; this is causing tests to fail when running them from the release tarball, with fun uninformative errors like:

```
======================================================================
ERROR: Failure: ValueError (Attempted relative import in non-package)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/lib64/python2.7/site-packages/nose/loader.py"", line 414, in loadTestsFromName
    addr.filename, addr.module)
  File ""/usr/lib64/python2.7/site-packages/nose/importer.py"", line 47, in importFromPath
    return self.importFromDir(dir_path, fqname)
  File ""/usr/lib64/python2.7/site-packages/nose/importer.py"", line 94, in importFromDir
    mod = load_module(part_fqname, fh, filename, desc)
  File ""/var/tmp/paludis/build/dev-python-urllib3-1.9/work/PYTHON_ABIS/2.7/urllib3-1.9/test/test_util.py"", line 24, in <module>
    from . import clear_warnings
ValueError: Attempted relative import in non-package
```

While I was looking into this, I noticed that the entire `contrib` and `with_dummyserver` directories are missing from the release tarball as well.
"
439,Expose server IP address as a prat of the Requst object,2014-08-03T13:11:02Z,2014-08-03T16:55:02Z,,,,"Please expose the IP address the request object connects to.  This is very useful for testing/debugging CDN functionality.

Example:
    >>> import urllib3
    >>> http = urllib3.PoolManager()
    >>> r = http.request(""GET"", ""http://www.google.com""
    >>> r.ip
    '74.125.225.242'

Or exposing the socket details with the port number could work as well.
"
438,Requests with large bodies may not be obeying rfc2616 8.2.2,2014-08-01T22:41:20Z,2014-08-01T23:18:16Z,,,,"RFC 2616 8.2.2 states:

<blockquote>
8.2.2 Monitoring Connections for Error Status Messages

   An HTTP/1.1 (or later) client sending a message-body SHOULD monitor
   the network connection for an error status while it is transmitting
   the request. If the client sees an error status, it SHOULD
   immediately cease transmitting the body. If the body is being sent
   using a ""chunked"" encoding (section 3.6), a zero length chunk and
   empty trailer MAY be used to prematurely mark the end of the message.
   If the body was preceded by a Content-Length header, the client MUST
   close the connection.
</blockquote>


I don't have a full repro, but it appears in our tests that when sending a large request body, the httplib client used by urllib3 is not monitoring the receive socket for the error status. (I understand that this might actually be a Python stdlib issue, but I'm trying to understand what can be done about it.)
"
437,HTTPResponse does not observe file-like object protocol if decode_content=True,2014-08-01T08:46:15Z,,,,,"My understanding was that urllib3's `HTTPResponse` is supposed to behave according to the file-like object protocol if `preload_content=False`. In particular, I would like to hand it over to another library that is able to read from a file-like object (e.g. lxml).

Unfortunately, the `HTTPReponse` does not appear to implement the protocol properly if `decode_content=True`. Test case:

```
import urllib3
print('urllib3 v{}'.format(urllib3.__version__))

http = urllib3.PoolManager()

response = http.request('GET', 'https://github.com',
                        headers={'Accept-Encoding': 'gzip'},
                        preload_content=False,
                        decode_content=False)
print('decode_content=False, read(1): expected 1 bytes, got {} bytes'.format(len(response.read(1))))
print('decode_content=False, read(500): expected 500 bytes, got {} bytes'.format(len(response.read(500))))

response = http.request('GET', 'https://github.com',
                        headers={'Accept-Encoding': 'gzip'},
                        preload_content=False,
                        decode_content=True)
print('decode_content=True, read(1): expected 1 bytes, got {} bytes'.format(len(response.read(1))))
print('decode_content=True, read(500): expected 500 bytes, got {} bytes'.format(len(response.read(500))))
```

Output:

```
urllib3 v1.9
decode_content=False, read(1): expected 1 bytes, got 1 bytes
decode_content=False, read(500): expected 500 bytes, got 500 bytes
decode_content=True, read(1): expected 1 bytes, got 0 bytes
decode_content=True, read(500): expected 500 bytes, got 1056 bytes
```

The problem with the first case is that I thought `.read()==''` means that the stream is consumed and/or has been closed. So a library might read 50 bytes, get an empty stream back and assume it has read the entire 'file', which wouldn't be the case with `HTTPResponse`.

Now you can argue that reading just one byte is unreasonable, but what's the minimal chunk size that still works? It appears to be 84 bytes for github.com, but it differs for other hosts.

The problem with the second case is that the file-object protocol defines the first parameter of `read()` as ""[Read at most size bytes from the file](https://docs.python.org/2/library/stdtypes.html?highlight=read#file.read)"". `HTTPResponse` returns more however. lxml for example seems to have internal buffer overflows if `read()` returns more bytes than it requested.

I guess at the very least, the documentation should mention that `HTTPResponse` is not adhering to the file-like object protocol.
## 
"
436,Make `preload_content=False` the new default?,2014-07-31T21:16:59Z,,,,,"More and more people want to use response objects as file-like objects (or streams). The default pre-consume-and-leave-nothing-to-read is confusing.

This would be a fairly substantial change which could cause leaks to old code using urllib3, so it would need to be a major release bump.

See also: https://stackoverflow.com/questions/25067580/passing-web-data-into-beautiful-soup-empty-list/25068934#25068934
## 
"
435,Is fp closed checks closed first,2014-07-30T07:49:05Z,2014-07-31T00:58:21Z,Ready,,,"for #434
"
434,is_fp_closed is should check obj.closed before obj.fp,2014-07-30T07:47:33Z,2014-09-02T17:45:34Z,,,,"This function checks for whether the file-like object used for request processing is closed:

```
def is_fp_closed(obj):
    """"""
    Checks whether a given file-like object is closed.

    :param obj:
        The file-like object to check.
    """"""
    if hasattr(obj, 'fp'):
        # Object is a container for another file-like object that gets released
        # on exhaustion (e.g. HTTPResponse)
        return obj.fp is None

    return obj.closed
```

It is supposed to be treating obj as an opaque file-like object, but actually it is breaking into the object's implementation by checking the status of .fp.

The suggestion in https://docs.python.org/2/library/stdtypes.html#bltin-file-objects is that file-like objects have a .closed attribute.

I understand the issue is that HTTPResponse doesn't implement .closed but we should at least test for .closed -first-.

(Raising this issue because I have a custom file-like-object for which sniffing the .fp is the wrong behaviour and leads to wrong results).
"
433,Add a missing contributor,2014-07-26T20:52:07Z,2014-07-26T20:55:56Z,,,,
432,Docs for `io` reading stuff plus added readinto to fix `io` usage,2014-07-22T23:46:18Z,2014-08-11T01:14:10Z,,,,"This closes #196 by adding some documentation about how one might want to use the `io` library with an `HTTPResponse`.  I wasn't actually sure where to put the example, as there didn't seem to be a relevant specific area.  Hence, I put it as an additional ""usage"" example.  But if it belongs somewhere else, I can move it easily enough.

One important caveat: the example does _not_ work in the current master.  Apparently partial reading of `io` objects require the `readinto` method to also be implemented.  So the last couple commits do that and add a test to make sure it actually works as intended.

cc @shazow
"
431,Give a better error for SNI support?,2014-07-16T09:52:08Z,2014-07-16T16:16:30Z,,,,"I found a issue when sending get request by urllib3, urllib3 says ""urllib3.exceptions.SSLError: hostname 'www.okcoin.com' doesn't match 'www.okcoin.cn'"", and I also have tested with urllib and urllib2, they works well. The URL is https://www.okcoin.com/api/ticker.do, thank you.
"
430,Extra connection diags,2014-07-12T14:40:49Z,2014-07-12T18:13:44Z,,,,"When debugging a heavily loaded system, I wanted to find the TCP stream associated with a specific connection. However, it was hard to correlate that with an individual request from the logs. It would have been useful if we logged out four of the five fields of the connection five-tuple, making it possible to easily identify what TCP stream is associated with a request.

I appreciate this adds some SLOC for something that might be of minimal value, but I wanted to offer it as an option. It's totally reasonable to reject this, won't bother me overmuch: only took 5 minutes to whip up anyway. =)
"
429,Fixed issue #416,2014-07-06T00:03:26Z,2014-07-06T20:28:41Z,,,,"The commits are pretty self-explaining. I fixed the documentation testing.
The next step is to fix issue #414 to make Travis CI fail if docs have warnings.

I didn't get one thing: what are the 'doc-requirements.txt' for? It seems there are too many things there.
To 'make docs' the only dependencies are:
Jinja2==2.7.3
MarkupSafe==0.23
Pygments==1.6
Sphinx==1.2.2
docutils==0.11

And to test them, at the moment the only additional dependency is 'certifi'.
"
428,Dont fail if there's no file to be removed.,2014-07-05T18:20:27Z,2014-07-06T17:46:38Z,,,,
427,Create our own socket connections,2014-07-04T18:45:15Z,2014-07-04T21:59:14Z,,,,"This allows the user to set the socket options before the connection is
established. The previous approach of setting the options after the connection
was established was not correct.
"
426,Security warnings.,2014-07-02T22:36:02Z,2014-07-03T22:17:31Z,,,,"- [x] Add tests
- [x] Limit to one warning
"
425,Retry: Add support for 503 Retry-after?,2014-07-02T02:41:18Z,,,,,"https://twitter.com/akislyuk/status/484141116019580928

Maybe something like `Retry(status_whitelist=...?)` and if it includes 503 then `Retry.sleep()` could do something special when incremented with a 503 response?

Feels slippery...
## 
"
424,Add a helper for loading proxy config from user environment,2014-07-01T22:09:17Z,,,,,"Requests does this:
https://github.com/kennethreitz/requests/blob/master/requests/utils.py#L514

Ours would not be automagic, maybe something like `proxy_from_env(...)` that would return `None` if there isn't one?
## 
"
423,Timeout not working?,2014-07-01T16:32:50Z,2014-07-01T18:16:03Z,,,,"Hi, i am using requests which depends on urllib3 to download some images, and I found that sometimes the timeout option was failing and my program wait forever. I found the problem may be related to urllib3 because requests sucessfully pass the timeout argument to urllib3.

Following is a sample that won't raise the timeout exception but wait forever (EDIT: actually it's not forever but quite a long time):

```
import urllib3
conn = urllib3.connection_from_url('http://filesofjerryblake.netfirms.com',timeout=2)
r1 = conn.request('GET', '/')
```

I've changed the url into something normal such as 'http://baidu.com' or 'http://www.google.com.hk' and the script above works well. When I reduce the timeout into 0.002 and I got the expected timeout exception. (EDIT: conn = urllib3.connection_from_url('http://filesofjerryblake.netfirms.com',timeout=0.002) also raise the timeout exception)

PS: I am using urllib3-1.8.3 and python 2.7

Sorry if i'm posting in wrong place.
"
422,Hostname verification Issue w/ Proxies,2014-06-30T18:47:56Z,2014-06-30T19:13:33Z,,,,"pip is getting a report of an issue verifying hostnames (pypa/pip#1905) when using a proxy and @sigmavirus24 suggested this is probably an urllib3 issue.

Here's the information as I have it:

@chrullrich said:

> When using a proxy (CONNECT via plain HTTP), pip gets the HTTPS certificate verification wrong. Rather than verifying that the certificate received through the tunnel matches the host at the tunnel's end, it compares it to the proxy itself:
> 
> ```
> [user@host ~]# pip --proxy proxy.localdoma.in:3128 -v install --upgrade pip
> Could not fetch URL https://pypi.python.org/simple/pip/: connection error: hostname 'proxy.localdoma.in' doesn't match either of '*.c.ssl.fastly.net', 'c.ssl.fastly.net', '*.target.com', > '*.vhx.tv', '*.snappytv.com', '*.atlassian.net', 'p [...]
> ```
> 
> I'm all in favor of securing PyPI, but there should be _some_ testing involved.

@chrullrich said:

> The proxy is squid 3.3, and no, I do not have any of the three installed, just plain Python 3.4.1 (amd64) plus pywin32-219.

@chrullrich said:

> We do not have any funny business going on, like a transparent proxy configured to do SSL MITM with invented certificates.

@chrullrich said:

> ```
> Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:45:13) [MSC v.1600 64 bit (AMD64)] on win32
> ```

I left out a few things which I don't believe is relevant to urllib3, but it's on the original pip ticket if you want to see it.
"
421,Retries v2: Retries strike back,2014-06-26T17:40:30Z,2014-07-01T00:14:28Z,,,,"Fork of @kevinburke's #326 pull request.

Trying to simplify the code, maybe cut out some less critical features.
"
420,added workaround for missing gzip support under IronPython,2014-06-26T12:51:55Z,2014-06-26T16:59:07Z,,,,"This has been discovered as part of the IronPython pip/ensurepip porting effort.
"
419,Handle SSL read timeouts as ReadTimeoutError,2014-06-25T22:13:09Z,2014-06-25T22:43:34Z,,,,"As reported by @sigmavirus24

Also added try/finally around event sets to avoid tests stalling on failure.
"
418,Raise ConnectionError on IncompleteRead.,2014-06-25T01:25:09Z,2014-06-25T17:34:21Z,,,,"Fixes #207 and #328. 

Once new retry logic gets merged, we should probably retry on `ConnectionError`.
"
417,"Handle blank host with new exception, LocationValueError.",2014-06-24T23:27:16Z,2014-06-24T23:30:24Z,,,,"Fixes #144.
"
416,Fix docs examples to actually pass doctests,2014-06-24T00:09:29Z,2014-07-06T20:28:42Z,Contributor Friendly ‚ô•,,,"```
$ cd docs
$ make doctest
...
Doctest summary
===============
   25 tests
   17 failures in tests
    0 failures in setup code
    0 failures in cleanup code
build finished with problems, 3 warnings.
```

:'(

Once this is fixed, we can shove for travis to test, too.
"
415,Adding a Makefile,2014-06-23T20:34:42Z,2014-06-24T21:34:18Z,,,,"Partly stolen from #317. (Thanks @kevinburke! Thoughts?)

Was considering doing separate `dev-requirements.txt` (sphinx, piprot, etc) and `test-requirements.txt` (nose, tox, etc) but... so many requirements. :(

Then again, we want travisci and co to fail when sphinx build fails (maybe also when piprot says things are out of date?) #414, so merging these things might be a Good Thing.

The plan is to add my release routine in here, too.
"
414,Make travisci fail if docs have warnings/errors?,2014-06-23T19:22:30Z,2016-11-02T10:14:51Z,Contributor Friendly ‚ô•,/Users/shazow/projects/urllib3/docs/exceptions.rst:,/Users/shazow/projects/urllib3/docs/exceptions.rst:: WARNING: document isn't included in any toctree,"Right now getting fun things like....

```
checking consistency...
/Users/shazow/projects/urllib3/docs/exceptions.rst:: WARNING: document isn't included in any toctree
/Users/shazow/projects/urllib3/docs/fields.rst:: WARNING: document isn't included in any toctree
```

This would be handy to know.
## 
"
413,pyOpenSSL: Fix WantWrite issue #412,2014-06-23T11:50:57Z,2014-09-13T18:29:03Z,,,,"Fix for https://github.com/shazow/urllib3/issues/412
"
412,gevent ssl and WantWrite error,2014-06-23T09:46:46Z,2014-09-13T18:29:03Z,,,,"Hi!

Using `gevent`  I got `OpenSSL.SSL.WantWriteError` error.  Here is simple script to reproduce the issue:

``` python
from gevent import monkey
monkey.patch_all()

import requests

payload = 'x' * 1024 * 1000
requests.get('https://example.org/', data=payload, verify=False)
```

Here is possible fix:

``` python
# urllib3/contrib/pyopenssl.py:
class WrappedSocket(object):
....
    def send_until_done(self, data):
        while True:
            try:
                return self.connection.send(data)
            except OpenSSL.SSL.WantWriteError:
                _, wlist, _ = select.select([], [self.socket], [],
                                            self.socket.gettimeout())
                if not wlist:
                    raise
                continue

    def sendall(self, data):
        while len(data):
            sent = self.send_until_done(data)
            data = data[sent:]
```
"
411,Allow _prepare_conn() to run on Python 2.6.4,2014-06-18T01:56:43Z,2014-06-23T18:32:57Z,,,,"When running on Python 2.6.4, a TypeError is thrown in _prepare_conn() 
because httplib's _set_tunnel() only takes two parameters.  This change 
will let the code run on Python 2.6.4 if proxy_headers are not set.
"
410,Change the PY3 detect way for more compatible,2014-06-07T12:04:23Z,2014-06-07T17:33:09Z,,,,"I'm using python 2.7.
I have a file named `http.py`

When start my program, `urllib3.connection` throw an Error,
that's because `urllib3.connection` try import `http.client` module directly.

By this way to detect the python version, and do other imports.
(`http.client`  is a py3 standard library)

So, I make this changes.  using `packages.six.PY3` instead of try import directly.
"
409,Update references to RFC 2616,2014-06-07T09:03:22Z,2014-06-07T17:25:30Z,,,,"This updates all references to RFC 2616 to their new places.

The only reference I left was in the changelog, which I'd argue should be a historical record of the reason for the change, rather than an up-to-date reference of the most recent spec.
"
408,Fix flakey tests,2014-06-04T19:12:03Z,2014-07-09T00:04:15Z,,,,"They seem more flakey than they were a few months ago. What happened?

E.g. [test_retries (test.with_dummyserver.test_socketlevel.TestProxyManager)](https://travis-ci.org/shazow/urllib3/jobs/26728513) was failing moments ago.

Some ideas:
- Use `Event` instead of `time.sleep(...)` wherever possible.
- Unhardcode explicit timeouts (0.05 or whatever we have), replace it with a couple of global test-module constants that we can tweak (I'm thinking `SHORT_TIMEOUT` and `LONG_TIMEOUT` or somesuch).

Anything else?
"
407,Don't include pyc files in packages,2014-06-04T01:03:04Z,2014-06-04T01:05:50Z,,,,"Fixes #405
"
406,Cleanup duplicated and inconsistent logic,2014-06-04T00:10:43Z,2014-06-04T04:12:30Z,,,,"This has been annoying me for a while. There were extra checks for popping off source address, the way we are creating connections is totally inconsistent between `HTTPConnection` and `HTTPSConnection` even accounting for the fact that we need different logic for tunneling https connections and it's all really confusing. Especially that part where we set `conn.conn_kw` in the `connectionpool` when we already have a perfectly good way of doing it!

~~Side note: I don't understand why we have the `getattr(self, _tunnel_host)` logic in `HTTPConnection` at all. `_tunnel_host` is only ever set by `set_tunnel` and that logic only exists in `HTTPSConnectionPool`...I think I am going to remove that too because that block will never get hit.~~ Turns out we test for it just in case someone wants it. Will just leave it for now.
"
405,PyPI tarball includes .pyc files,2014-06-04T00:07:15Z,2014-06-04T01:05:50Z,,,,"```
% wget https://pypi.python.org/packages/source/u/urllib3/urllib3-1.8.2.tar.gz
...
% tar tvf urllib3-1.8.2.tar.gz | grep \.pyc$
-rw-r--r-- shazow/staff    141 2014-03-15 20:20 urllib3-1.8.2/dummyserver/__init__.pyc
-rw-r--r-- shazow/staff    149 2014-02-21 14:33 urllib3-1.8.2/dummyserver/__pycache__/__init__.cpython-33.pyc
-rw-r--r-- shazow/staff  10341 2014-02-21 14:33 urllib3-1.8.2/dummyserver/__pycache__/handlers.cpython-33.pyc
-rw-r--r-- shazow/staff   5809 2014-02-21 14:33 urllib3-1.8.2/dummyserver/__pycache__/proxy.cpython-33.pyc
-rw-r--r-- shazow/staff   6928 2014-03-15 14:04 urllib3-1.8.2/dummyserver/__pycache__/server.cpython-33.pyc
-rw-r--r-- shazow/staff   7203 2014-03-15 14:04 urllib3-1.8.2/dummyserver/__pycache__/testcase.cpython-33.pyc
-rw-r--r-- shazow/staff   8967 2014-04-14 20:40 urllib3-1.8.2/dummyserver/handlers.pyc
-rw-r--r-- shazow/staff   4740 2014-03-15 20:20 urllib3-1.8.2/dummyserver/proxy.pyc
-rw-r--r-- shazow/staff   5992 2014-03-15 20:20 urllib3-1.8.2/dummyserver/server.pyc
-rw-r--r-- shazow/staff   5090 2014-03-15 20:20 urllib3-1.8.2/dummyserver/testcase.pyc
```

Tarballs released on PyPI should _never_ include .pyc files.
"
404,Add docs about how to use certifi with urllib3,2014-06-03T20:03:48Z,2014-07-03T22:57:42Z,,,,"http://certifi.io/
https://github.com/certifi/python-certifi
"
403,Add a warning when user makes an unverified HTTPS request,2014-06-03T20:02:28Z,2014-07-03T22:58:05Z,,,,"I'm thinking a log.warning(...) that appears just once for the first time, maybe with an explicit way to turn it off (probably by manually toggling the global state var we'll need to track whether it has happened once or not).

Might be worth adding an stderr logging handler just for this?

Also related, would be nice to have some docs about how to do verified HTTPS requests using things like certifi. (Opened issue: #404)
"
402,fix test failures on newest pythons,2014-06-02T22:54:36Z,2014-06-02T23:01:35Z,,,,"The newly released versions of python put self._tunnel_host in the _make_request critical path causing this test to fail with an attribute error. Since this is only a 2.6 feature we are testing it makes sense to only run the test on 2.6. The decorator was also wrong because (2, 6, 1) > (2, 6).
"
401,tests broken on python 2.7.7 :(,2014-06-02T22:40:09Z,2014-06-02T23:02:02Z,,AttributeError,AttributeError: 'VerifiedHTTPSConnection' object has no attribute '_tunnel_host',"see this commit:

http://hg.python.org/cpython/rev/568041fd8090

stack trace:

```
Traceback (most recent call last):
  File ""/Users/kevin/code/urllib3-dev/test/with_dummyserver/test_https.py"", line 258, in test_tunnel_old_python
    self._pool._make_request(conn, 'GET', '/')
  File ""/Users/kevin/code/urllib3-dev/urllib3/connectionpool.py"", line 292, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/local/Cellar/python/2.7.7/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 995, in request
    self._send_request(method, url, body, headers)
  File ""/usr/local/Cellar/python/2.7.7/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1023, in _send_request
    self.putrequest(method, url, **skips)
  File ""/usr/local/Cellar/python/2.7.7/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 925, in putrequest
    if self._tunnel_host:
AttributeError: 'VerifiedHTTPSConnection' object has no attribute '_tunnel_host'
```
"
400,Include request information in `HTTPResponse`?,2014-06-01T03:22:26Z,,,,,"This would be useful for raising more descriptive errors inside of `HTTPResponse` as well as making responses more user-friendly.

Forked from #399:

> Also, the more I think of it, the more I'm starting to feel that we should be including a little more state information in our `HTTPResponse`. Namely, some of the immutable things which describe the request, at least the method and url.
> 
> Although I am reluctant to go all the way and have a full prepared request object to store all the state (including headers, params, settings, etc). Maybe I'll open another issue for discussion.

Downsides to consider:
- Where do we draw the line? (Can we come up with a good rule of thumb for what request information the HTTPResponse tracks vs what it should not?)
- How do we stay relatively efficient? (If we decide to store things like passed params, we may be pulling in and holding large payloads in the process.)
- We need to make sure we retain minimal coupling across the different levels (pool -> connection -> request, and back).
## 
"
399,catch socket.timeout exception,2014-05-30T19:07:06Z,2014-06-01T03:08:52Z,,,,"Fixes #296. Cleaned up version of #297. I had to add the actual request_url to the response so that we could raise it in the exception.
"
398,Flake8 fixes,2014-05-29T13:13:07Z,2014-05-30T01:14:23Z,,,,
397,Start accepting extra socket options,2014-05-29T03:01:46Z,2014-06-12T21:26:50Z,,,,"Early review is always welcome (not that I've much done yet).
"
396,document the util module again,2014-05-24T22:21:53Z,2014-05-27T17:27:21Z,,,,"This was never fixed up when `urllib3.util` was broken up. I guess it's minorly annoying that the paths are now listed as `urllib3.util.request` et al but I'm not that familiar with rst to fix it. Also, note that I intentionally didn't include `urllib3.util.timeout` because it's documented elsewhere and seems specific to urllib3 unlike the rest of util.
"
395,add disable_cache to make_headers,2014-05-24T22:16:54Z,2014-05-27T17:28:12Z,,,,"Woohoo I get to add a feature! This is for #393 which we talked about on that issue. I did some research about which headers to add and what I discovered is that this is all we need to be HTTP/1.1 compatible. Things like 'pragma: no-cache' is actually HTTP/1.0 and 'expires: 1994 etc' is even earlier, I can't imagine there are many web servers that aren't 1.1 compatible at this point in the world...interestingly though the spec does say the following about pragma:

> Clients SHOULD include both header fields when a no-cache request is sent to a server not known to be HTTP/1.1 compliant. 

Meh.
"
394,Add unparse_url to urllib3.util.url,2014-05-23T18:46:19Z,2014-09-13T18:25:27Z,,,,"This round-trips with parse_url (or at least equivalently by the RFC).

You should verify that the logic is correct. I haven't actually read the RFC. 

Also, how do you run the doctests? I didn't verify that they were correct. The ones for `parse_url` look incorrect since they don't include `auth=None` in the returned Url objects. 
"
393,Add cache control helper to urllib3.util.make_headers?,2014-05-20T22:26:02Z,2014-06-29T23:36:18Z,Contributor Friendly ‚ô•,,,"Related to: http://stackoverflow.com/questions/11335825/authentication-with-urllib3/11388614#comment36543731_11388614

Would be fun to have something like `make_headers(disable_cache=True)`, or maybe `make_headers(cache_control='no-cache')` but that may be too specific and of little value-add to just adding the header yourself.
"
392,Run Travis on OSX.,2014-05-13T20:05:40Z,2014-05-13T20:08:00Z,,,,
391,make active ssl backend accessible to users,2014-05-12T22:41:33Z,2014-05-13T03:21:28Z,,,,"This especially useful for requests users, as they get whatever gets
autodetected without knowing what.

Also see kennethreitz/requests#2023
"
390,refactor ssl tests,2014-05-12T21:56:02Z,2014-05-17T22:57:17Z,,,,"Changing the verified tests so that we aren't changing the certs of a pool after we've used the pool. It produces weird undefined behavior and logic that we shouldn't support.

cc @t-8ch Is there anything stupid I'm doing?
"
389,fix skip test copy/paste typo,2014-05-12T20:40:30Z,2014-05-12T22:37:02Z,,,,"There was a copy paste error where the 2.6 tests say they need to be run on 2.7, was kind of confusing.
"
388,DO NOT MERGE - properly close connections and apply pool certs for https pools,2014-05-12T20:29:15Z,2014-05-12T21:47:26Z,,,,"cc @Lukasa @shazow

In my debugging of #369 I came across the fact that we aren't propagating changes to connections if we change the pool certs settings. This change fixes all of the current test but adds a failing test that illustrates a case that my change does not fix; since we are reusing the connection object (and its underlying connected socket) `connect` will never get invoked again and thus neither will `ssl_wrap_socket`. I don't see a great way of fixing this outside of some fairly hacky things (e.g. make `self.ca_certs` a property and trigger a `.close` on every connection when it gets set) and I'm not sure about the ramifications this has on proxies.
"
387,Remove the silly yearly copyright header in source files,2014-05-12T18:45:54Z,2014-07-03T22:58:40Z,Contributor Friendly ‚ô•,,,"It's not actually needed and I'm sick of updating it. :P
"
386,Better PyOpenSSL docs,2014-05-12T18:04:30Z,2014-05-12T22:36:16Z,,,,"Originally #341.
"
385,TLS Verification will break in Python 3.4.1 when using a Proxy,2014-05-11T20:35:20Z,2014-05-12T18:31:19Z,,,,"There is a change in Python 3.4.1 which will break the hostname verification when used in conjunction with a proxy. This got discovered when someone attempted to use pip with 3.4.1rc1.

Take a look at http://bugs.python.org/issue7776 for more details. I'm not entirely familiar with this code but I'm going to try and familarize myself with it to see what can be done here.
"
384,Question about HttpConnectionPool,2014-05-11T17:49:15Z,2014-05-11T18:14:39Z,,,,"I'm trying to understand this issue in a little more detail: 

```
urllib3.connectionpool:HttpConnectionPool is full, discarding connection
```

http://stackoverflow.com/questions/18466079/can-i-change-the-connection-pool-size-for-pythons-requests-module

I'm aware that I can increase the size of the HttpConnectionPool easily but I don't quite understand what it is and what value I can set to make it stop erroring out.

Reading http://urllib3.readthedocs.org/en/latest/pools.html#urllib3.connectionpool.HTTPConnectionPool

> maxsize ‚Äì Number of connections to save that can be reused. More than 1 is useful in multithreaded situations. If block is set to false, more connections will be created but they will not be saved once they‚Äôve been used.
> block ‚Äì If set to True, no more than maxsize connections will be used at a time. When no free connections are available, the call will block until a connection has been released. This is a useful side effect for particular multithreaded situations where one does not want to use more than maxsize connections per host to prevent flooding.

It seems as though the point is that it ""saves connections"" in order to reuse them.

I'm using greenlet threads (via celery/gevent) to make requests to a single domain, `https://subdomain.example.com`. My requests are all to different `:path` on this domain.

I'd like to reuse the saved cookies which is why I pickle my requests Session object and unpickle within each thread, but this is a huge overhead for no good reason.

My question is essentially:
1. what does saving these connections do in urllib3
2. if I have a concurrency of say 20, and I'm running 1000 tasks - do I need to set my HttpConnectionPool maxsize to 20 or 1000? 20 definitely didn't work when I tried it. Is this because I did not set block to True?
3. I don't care how many simultaneous connections I make to example.com at this time, my end goal is to just use the shared cookies (immutable) across my threads without pickling and unpickling.
"
383,Ability to disable connection pooling,2014-05-08T12:12:17Z,,,,,"My proxy doesn't play nice with connection pooling, it constantly resets the connection:

```
2014-05-08 13:54:30,207 [INFO ] requests.packages.urllib3.connectionpool: Resetting dropped connection: pypi.python.org
2014-05-08 13:54:30,209 [INFO ] requests.packages.urllib3.connectionpool: Starting new HTTPS connection (2): pypi.python.org
2014-05-08 13:55:31,239 [INFO ] requests.packages.urllib3.connectionpool: Resetting dropped connection: pypi.python.org
2014-05-08 13:55:31,242 [INFO ] requests.packages.urllib3.connectionpool: Starting new HTTPS connection (3): pypi.python.org
2014-05-08 13:56:32,026 [INFO ] requests.packages.urllib3.connectionpool: Resetting dropped connection: pypi.python.org
```

So in this case it doesn't make sense to keep the connection at all. It would be great if there is an option to disable connection pooling. 

As this is a system wide setting it should be configurable independent of the caller, preferably a environment variable. I'm thinking about something like `URLLIB3_DISABLE_POOLS=1`.

Not sure which entry point would be most suitable for such a behavior. Right now I've patched `ConnectionPool._put_conn` to throw away the connection but there might be a better place.
## 
"
382,CPython 3.4 is a thing,2014-05-06T18:03:39Z,2014-05-06T18:40:51Z,,,,
381,Fix typo,2014-05-02T06:38:16Z,2014-05-02T07:21:17Z,,,,"Fix typo
"
380,workaround missing _tunnel_host attr in python 2.6,2014-05-01T14:26:50Z,2014-05-04T17:58:28Z,,,,"Adds the same workaround as on line 175, which allows this lib to work on python2.6
"
379,Can I count on HTTPResponse.closed and HTTPResponse.close?,2014-04-28T14:37:09Z,2014-04-28T21:05:42Z,,,,"It seems like I can't ""count"" on `HTTPResponse.closed` or `HTTPResponse.close`:
- `HTTPResponse.closed` looks fine, except that https://github.com/kennethreitz/requests/blob/b8128d6b1ec52bc57499a87eb3f7a73e359c6a6a/requests/packages/urllib3/response.py#L235 uses `util.is_fp_closed` to test for close.  Which is correct?  Why isn't `util.is_fp_closed` also in `HTTPResponse.closed`?
- If I want to know when a response is closed, it's not sufficient to wrap `HTTPResponse.close` as https://github.com/kennethreitz/requests/blob/b8128d6b1ec52bc57499a87eb3f7a73e359c6a6a/requests/packages/urllib3/response.py#L192 short-circuits the call.  Why can't `HTTPResponse.read` call `HTTPResponse.close` on exhaustion?

Here's some context: I'm writing a library that ultimately uses urllib3 in a concurrent programming setting.  I've discovered that it's possible to keep connections open long enough that a ""Too many files open"" exception can be thrown.  So, I need to attach a semaphore acquire after a connection is made (I'd use `HTTPResponse.closed` to test whether it's a ""streaming"" connection or not), and then wrap `HTTPResponse.close` with a bit of code that releases the semaphore.
"
378,Setting custom setsockopt options on new sockets,2014-04-28T10:17:50Z,2014-06-12T23:41:14Z,,,,"To reuse the connections which status is time_wait, I have to set ""setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)"".
 What I wonder is how, or what else I can do to achieve my goal.
Any answers wiil be appreciated.
"
377,AttributeError: 'ApplicationError' object has no attribute 'errno',2014-04-24T14:38:29Z,2014-04-24T21:48:39Z,,AttributeError,AttributeError: 'ApplicationError' object has no attribute 'errno',"I have the following error that has started to pop up in mass, I can't tell if it's a bug with requests or urllib3, but since the error is being thrown fairly deep inside urllib3 I will assume that it could be involved, any help would be appreciated, thanks!

On app engine using 1.8.2 I am getting the stack trace:

line 55, in get
    return request('get', url, *_kwargs)
  line 44, in request
    return session.request(method=method, url=url, *_kwargs)
  /libs/requests/sessions.py"", line 452, in request
    resp = self.send(prep, *_send_kwargs)
  /libs/requests/sessions.py"", line 555, in send
    r = adapter.send(request, *_kwargs)
  /libs/requests/adapters.py"", line 327, in send
    timeout=timeout
  /libs/requests/packages/urllib3/connectionpool.py"", line 493, in urlopen
    body=body, headers=headers)
  /libs/requests/packages/urllib3/connectionpool.py"", line 291, in _make_request
    conn.request(method, url, *_httplib_request_kw)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/python_std_lib/httplib.py"", line 973, in request
    self._send_request(method, url, body, headers)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/python_std_lib/httplib.py"", line 1007, in _send_request
    self.endheaders(body)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/python_std_lib/httplib.py"", line 969, in endheaders
    self._send_output(message_body)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/python_std_lib/httplib.py"", line 829, in _send_output
    self.send(msg)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/python_std_lib/httplib.py"", line 791, in send
    self.connect()
  /libs/requests/packages/urllib3/connection.py"", line 156, in connect
    *_self.conn_kw)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/socket.py"", line 560, in create_connection
    sock.connect(sa, host)
  File ""/base/data/home/runtimes/python27/python27_dist/lib/python2.7/socket.py"", line 222, in meth
    return getattr(self._sock,name)(*args)
  File ""/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/api/remote_socket/_remote_socket.py"", line 776, in connect
    if translated_e.errno == errno.EISCONN:
AttributeError: 'ApplicationError' object has no attribute 'errno'
"
376,Add source_address support to PoolManager. Issue warnings if source_address is used with Python 2.6. Fix Python 2.6 testing bug.,2014-04-18T23:59:19Z,2014-09-15T19:11:50Z,,,,
375,Pyopenssl refactor,2014-04-18T16:56:44Z,2014-06-27T16:28:42Z,Ready,,,"~~_Do not merge (yet)_~~

See #370.
Introduces the `suppress_ragged_eofs` parameter for stdlib compat.

I'd like to have the testsuite also cover pyopenssl before merging this.
Maybe @alekstorm would provide a pull request with only his testsuite changes, so we could do this.
"
374,Add Arthur Grunseid to contributors.,2014-04-16T07:58:55Z,2014-04-16T17:12:35Z,,,,
373,HTTPResponse.read() returns empty string when response has `Connection: close`,2014-04-16T04:22:42Z,2014-04-16T04:57:52Z,,,,"Compare:

```
import urllib3
http = urllib3.PoolManager()
res = http.request('GET', 'https://signin.infusionsoft.com/login')
print res.read()
```

(Prints `''`)

```
import httplib
conn = httplib.HTTPSConnection('signin.infusionsoft.com')
conn.request('GET', '/login')
res = conn.getresponse()
print res.read()
```

(Prints a large HTML entity body)
"
372,Add Arthur Grunseid to contributors.,2014-04-15T03:42:11Z,2014-04-16T07:59:27Z,,,,
371,"[WIP] Make `ssl` module implementation injectable in SSL-related classes, remove `contrib.pyopenssl` and `packages.ssl_match_hostname` in favor of `backports.ssl`",2014-04-12T05:50:48Z,,,,,"Addresses #370.

Also tests both the standard and backported `ssl` implementations, skipping each if unavailable.

I've incorporated the `subjectAltName` support from the `contrib.pyopenssl` module and the `ndg-httpsclient` package into `backports.ssl` (thanks @kirkeby!), obviating the need for the former.

This shouldn't be merged yet (see TODO below), but feedback would be much appreciated (@shazow @lukasa @t-8ch).

TODO:
- ~~Vendor `backports.ssl`, rather than listing it in `test-requirements.txt` (currently undone for quicker iteration).~~
- [x] Figure out `install_requires`/`extras_requires` rules for `pyOpenSSL` and `pyasn1` in urllib3/`backports.ssl`.
- [x] Properly catch and re-throw more `pyOpenSSL` exceptions as their `ssl` equivalents.
- [x] Fix the hanging bug in the `backports.ssl` implementation of `TestHTTPS` - it's happening on Travis CI (though I've got it disabled for now), so it can't be just my machine. Could it be a bug in `pyOpenSSL` itself? Seems to be fixed by monkey-patching the Tornado dummy server's `ssl` with `backports.ssl.monkey.patch()`, so could also be an OpenSSL version problem.
"
370,Vendor `backports.ssl` package,2014-04-10T22:24:50Z,,,,,"(Continuing the conversation from [#367](https://github.com/shazow/urllib3/issues/367#issuecomment-39660734))

I admit I'm at a bit of a loss as to how to do this - `backports.ssl` is designed to be a drop-in replacement for the standard `ssl` module, but since it depends on pyOpenSSL, we can't automatically switch to it unless the latter is installed as well. Does that mean pyOpenSSL should be vendored in as well, or specified in the `extras_requires` parameter to `setuptools.setup()`? Should we be automatically switching at all (with explanatory logging statements), or allow the user to configure which `ssl` implementation is used, or both? Should `backports.ssl` even be vendored, since it can be monkey-patched in by the user a la gevent with `backports.ssl.monkey.patch()`?

Note also that since OS X ships with an ancient OpenSSL by default, which Tornado relies on, the urllib3 test suite hangs on each of the methods in `test_https.TestHTTPS_TLSv1` unless `backports.ssl.monkey.patch()` is called or the user recompiles their Python against a newer (probably Homebrew-installed) OpenSSL.

Guidance here would be much appreciated; I'm prepared to make all necessary changes to `urllib3` and `backports.ssl` to get them to play nicely together, but I need to know your (@shazow's) vision for the interop first.
## 
"
369,Test proxy behavior when origin server disconnects,2014-04-07T22:57:15Z,2014-06-03T21:40:19Z,,,,"Add a test for a CONNECT proxy where the remote server closes the connection, and then we try to reuse it.

This test demonstrates the problem described in #366 (and apparently reported earlier as #295)
"
368,Stop using None to designate (state of) connection,2014-04-04T08:04:07Z,2014-07-03T23:01:09Z,,,,"Following issues for some time now it becomes clear that the meaning of None in the context of connection objects is being overloaded. What I mean is that tcp/http connection during its lifetime goes through many different states and this must be reflected in any implementation. Yet we continue to use None for many types of different states of connection and throw away information about _real_ state. This is wrong and it's the root cause of many bugs. Fixing these bugs requires hunting down the place the connection became None what reveals the _true_ state of connection down the call stack where something bad has happened.
"
367,"Differences between standard SSL and Pyopenssl (ZeroReturnError, SysCallError)",2014-04-03T22:00:12Z,,,,,"We use urllib3 via requests with pyopenssl so we can have SNI support in python 2.7.3.

Since we switched, we've noticed more errors from requests. Specifically:
- ZeroReturnError
- SysCallError: (104, 'Connection reset by peer')
- SysCallError: (-1, 'Unexpected EOF')

It seems like these should all be caught somehow in contrib/pyopenssl.py so at the requests level, we don't have to worry about pyopenssl being different than the standard ssl module.

Here's a few of the ideas I'm considering:
- in `fileobject.read`, everywhere we are catching `WantReadError` also catch `ZeroReturnError` and make sure that data is an empty string and continuing
- In `inject_into_urllib3`, setting `connection.BaseSSLError = OpenSSL.SSL.Error`

Based on my limited understanding of pyopenssl (and python's standard _ssl.c) I'm probably missing something in trying to unify these interfaces. If somebody with more familiarity with this stuff can point me in the right direction I can try to formalize this stuff into a pull request.

Here's a gist of how I started investigating the `ZeroReturnError` specifically: https://gist.github.com/mthurman/9963741
## 
"
366,reconnect issues via CONNECT proxies,2014-04-03T21:14:15Z,2014-06-03T21:51:14Z,,,,"if I connect to an https server via a CONNECT proxy (ProxyManager), and either the origin server or the proxy disconnects, then subsequent requests connect directly to the origin server and attempt to use it as a proxy. This is because the first time httplib.HTTPConnection._tunnel() is called, it changes the .host and .port to be the origin server's.

I have a patch that fixes this for me, but I don't think I will be able to produce a test case that tests for this issue. 
Two changes are required, and there is another that I think makes sense.
- is_connection_dropped should return True if the socket is None
- HTTPConnectionPool._get_conn should not reuse a dropped connection. (maybe only if it has proxies)
- The optional bit: self.auto_open should be set to 0 after self._tunnel() is called on a connection object. This prevents httplib from reconnecting on send. Instead, it will throw NotConnected(). auto_open being 0 could also be used as a flag to trigger the rest of the behavior.

Assuming you have a functioning proxy (for testing, I set up a simple proxy on apache on localhost), the following script should fail:

p=ProxyManager(proxy_url=""http://somewhere"")
r=p.request('GET', 'https://api.box.com/', redirect=False, headers={'Connection':'close'})
r=p.request('GET', 'https://api.box.com/', redirect=False, headers={'Connection':'close'})

(my real code uses requests, and I'm not sure that this simple test will fail in all the same cases as my requesrs code)
"
365,make it more clear why we are closing connections,2014-04-02T02:02:53Z,2014-04-02T02:41:07Z,,,,"I/Lukasa noticed this while investigating #295. Basically the `getattr(conn, 'sock', False)` followed by `if not sock` is catching the case where the Connection: close header is set because calling `response.close()`  sets `sock=None`. This pull request makes it clearer when we are passing on closing due to AppEngine craziness as opposed to do to someone already calling `response.close` such as `httplib` doing it for us.
"
364,Python 2.6.0-2.6.4 doesn't have a headers parameter for _set_tunnel,2014-04-01T14:45:37Z,2014-07-01T17:38:47Z,,,,"This bit of code https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L629-L633 uses `_set_tunnel` on Python 2.6, however the third parameter `headers` was only added in 2.6.5 in this commit: http://hg.python.org/cpython/rev/3a736ad661fb

Not sure if it makes sense at all for this to handle that case, but pip had a user report an exception because of it.
"
363,Make initial boundaries of multipart encoded form data compliant with RFC 1521,2014-03-21T17:48:13Z,2014-03-26T21:33:28Z,,,,"`urllib3.filepost.encode_multipart_formdata` is not fully compliant with [RFC 1521](https://www.ietf.org/rfc/rfc1521.txt); specifically, boundaries technically include `\r\n` at the start, which is omitted for the initial part in what that produces.

The portion of the RFC I'm talking about is in [RFC 1521](https://www.ietf.org/rfc/rfc1521.txt), ""MIME (Multipurpose Internet Mail Extensions) Part One: Mechanisms for Specifying and Describing the Format of Internet Message Bodies"", Subsection 7.2.1 (""Multipart:  The common syntax""), on page 30:

> Note that the encapsulation boundary must occur at the beginning of a
>   line, i.e., following a CRLF, and that the initial CRLF is considered
>  to be attached to the encapsulation boundary rather than part of the
>  preceding part.  The boundary must be followed immediately either by
>  another CRLF and the header fields for the next part, or by two
>  CRLFs, in which case there are no header fields for the next part
>  (and it is therefore assumed to be of Content-Type text/plain).

Key part there is ""the initial CRLF is considered to be attached to the encapsulation boundary rather than part of the preceding part.""
"
362,Don't install dummyserver into site-packages,2014-03-20T14:43:39Z,2014-03-20T18:24:29Z,,,,"It is pure example code with self-signed certificates. It pollutes the
global site-packages spaces and is potentially dangerous (if used
accidentally). It doesn't make sense to install it as 'package_data' or
'date_files' either. Since it servers as an example it should be part
of the source distribution.
"
361,Don't pin dependency to exact version,2014-03-20T14:37:28Z,2014-06-24T22:38:47Z,,,,"While this expresses with which versions urllib3 is tested to work with,
almost all distros ship different package versions. To accomodate that
(and to avoid having them to patch away these hard requirements) only
use '>='.
"
360,Intermittent test failure,2014-03-20T08:13:38Z,2014-07-03T23:22:20Z,,OSError,OSError: [Errno 98] Address already in use,"in bind_sockets. There's a note there about something being merged into tornado, did it ever do that?

```
======================================================================
ERROR: test suite for <class 'test.with_dummyserver.test_proxy_poolmanager.TestHTTPProxyManager'>
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/travis/virtualenv/python3.3/lib/python3.3/site-packages/nose/suite.py"", line 209, in run
    self.setUp()
  File ""/home/travis/virtualenv/python3.3/lib/python3.3/site-packages/nose/suite.py"", line 292, in setUp
    self.setupContext(ancestor)
  File ""/home/travis/virtualenv/python3.3/lib/python3.3/site-packages/nose/suite.py"", line 315, in setupContext
    try_run(context, names)
  File ""/home/travis/virtualenv/python3.3/lib/python3.3/site-packages/nose/util.py"", line 469, in try_run
    return func()
  File ""/home/travis/build/shazow/urllib3/dummyserver/testcase.py"", line 95, in setUpClass
    app, cls.io_loop, None, 'http', cls.http_host)
  File ""/home/travis/build/shazow/urllib3/dummyserver/server.py"", line 157, in run_tornado_app
    sockets = bind_sockets(None, address=host)
  File ""/home/travis/build/shazow/urllib3/dummyserver/server.py"", line 143, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
```
"
359,Fix up grammar in test comment,2014-03-20T08:09:55Z,2014-03-20T18:27:09Z,,,,
358,Ssl docs,2014-03-16T12:03:14Z,2014-03-16T19:00:55Z,,,,"Attempts to fix representation of the ssl/sni docs on the urllib3 page. note, you'll need to go in and change the ""requirements file"" to docs/doc-requirements.txt in readthedocs for this to work.

I also added a few changes to the pyopenssl docs to make it clear to people that they're in the right place.
"
357,Raise LocationParseError if host is None,2014-03-16T06:40:59Z,2014-03-16T08:52:07Z,,,,"Also fixes double-encoding of the error message ""Failed to parse:"" for
LocationParseErrors, and adds tests that parse_url and poolmanager behave in
the appropriate ways.

Should fix #355.
"
356,SSL Does not work on Appengine,2014-03-14T05:31:29Z,2014-03-15T18:05:05Z,,,,"I'm trying to access to an HTTPS website like :  
https://www.eff.org/https-everywhere 

When I run this code on appengine : 
        http = urllib3.PoolManager()
        response = http.urlopen(""GET"", ""https://www.eff.org/https-everywhere"")

The request is effectively done as HTTP instead of HTTPS, so the eff website redirect me in loop into the https version.

As a fix, I did add the field _protocol to HTTPSConnection to ensure the value is https and not http. That solved the issue. I'm not sure if this is the proper fix.
"
355,"""AttributeError: 'NoneType' object has no attribute 'strip'"" with malformed URL",2014-03-13T19:52:54Z,2014-03-16T08:52:07Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'strip',"```
>>> import urllib3
>>> http = urllib3.PoolManager()
>>> r = http.request('GET', 'http://@')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File "".../urllib3/request.py"", line 75, in request
    **urlopen_kw)
  File "".../urllib3/request.py"", line 88, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File "".../urllib3/poolmanager.py"", line 145, in urlopen
    conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
  File "".../urllib3/poolmanager.py"", line 119, in connection_from_host
    pool = self._new_pool(scheme, host, port)
  File "".../urllib3/poolmanager.py"", line 86, in _new_pool
    return pool_cls(host, port, **kwargs)
  File "".../urllib3/connectionpool.py"", line 226, in __init__
    ConnectionPool.__init__(self, host, port)
  File "".../urllib3/connectionpool.py"", line 156, in __init__
    host = host.strip('[]')
AttributeError: 'NoneType' object has no attribute 'strip'
```

The fundamental issue here seems to be that `connection_from_host` assumes `host` will never be `None`, but

```
>>> urllib3.util.parse_url('http://@')
Url(scheme='http', auth='', host=None, port=None, path=None, query=None, fragment=None)
```

I suggest that `LocationParseError` should be thrown instead under these conditions, and that perhaps `parse_url` should do it (since it does not appear to support URL-schemes that don't have a netloc, and netloc-ful schemes with an empty host don't make sense).

For the record, this is _not_ academic - I actually encountered a webserver that returned a redirection to ""`http://@`"" while doing a moderate-scale survey of the interwebs.
"
354,RecentlyUsedContainer goes wrong,2014-03-13T10:19:52Z,2014-03-14T03:12:44Z,,,,"i got 'pool is closed' in multithreads situation and i debuged into the code (_collection.py,method __setitem__),i printed the parameter key and value and the evicted_value before line 75 (before dispose_func executed coz after it executed the pool must be closed),i got this output:
('http', 'jl.sina.com.cn', 80) HTTPConnectionPool(host='jl.sina.com.cn', port=80) HTTPConnectionPool(host='haikou.newshainan.com', port=80)
how could the dict(self.container) got a value by a mismatched key,before i printed this i have checked that the dict didn't containd the given key.(self.container is big so i don't copy here).can u give me some suggestions point out where im wrong.
"
353,Refactor urllib3/util.py into submodules,2014-03-11T01:25:08Z,2014-03-16T00:21:46Z,,,,"No code changes, just refactoring.

(shazow edit: For rebasing #326)
"
352,Support source_address,2014-03-08T22:01:06Z,2014-04-15T00:27:31Z,,,,"This still doesn't work for `HTTPSConnection`s (the default address is used instead of `source_address`). I'll try to fix this while moving over to using `**connection_kw` instead of an explicit parameter.

issue #9
"
351,when data is long convert to string before posting,2014-03-08T11:06:24Z,2014-03-09T01:05:16Z,,`TypeError,`TypeError: 'long' does not have the buffer interface`,"For some reason when the data is a long there is an issue in `filepost.py`

It creates the error

`TypeError: 'long' does not have the buffer interface`
"
350,Publish default accept encoding as constant.,2014-03-06T16:32:35Z,2014-03-07T23:05:34Z,,,,"This makes it better reusable for other code, e.g. requests.
"
349,Use unittest.mock with Python >=3.3,2014-03-06T06:44:06Z,2014-03-06T06:51:43Z,,,,"mock is in standard library as unittest.mock in Python >=3.3.

```
--- test/test_util.py
+++ test/test_util.py
@@ -1,7 +1,11 @@
 import logging
 import unittest

-from mock import patch
+try:
+    # Python >=3.3
+    from unittest.mock import patch
+except ImportError:
+    from mock import patch

 from urllib3 import add_stderr_logger
 from urllib3.util import (
--- test/with_dummyserver/test_connectionpool.py
+++ test/with_dummyserver/test_connectionpool.py
@@ -3,7 +3,11 @@ import socket
 import sys
 import unittest

-import mock
+try:
+    # Python >=3.3
+    from unittest import mock
+except ImportError:
+    import mock

 try:
     from urllib.parse import urlencode
--- test/with_dummyserver/test_https.py
+++ test/with_dummyserver/test_https.py
@@ -3,7 +3,12 @@ import ssl
 import sys
 import unittest

-import mock
+try:
+    # Python >=3.3
+    from unittest import mock
+except ImportError:
+    import mock
+
 from nose.plugins.skip import SkipTest

 from dummyserver.testcase import HTTPSDummyServerTestCase
```
"
348,Fix Socket closing on connection errors,2014-03-01T00:07:15Z,2014-03-04T19:05:47Z,,,,"- Merged #344 
- Merged #346 
- Adds `ConnectionError` exception which wraps socket errors and HTTPExceptions from httplib.
- Adds support for `retries=False` which raises exceptions immediately without wrapping them with `MaxRetryError`.
- Minor cleanup.
"
347,looks like TARPIT_HOST doesn't work in all cases :(,2014-02-27T17:54:29Z,2014-03-16T06:55:57Z,,,,"I'm getting this error when running the tests on Mac 10.9.1 and connected to a wifi network. Travis still does the correct thing. May be VPN related, but I'm not sure.

```
======================================================================
ERROR: test_connect_timeout (test.with_dummyserver.test_connectionpool.TestConnectionPool)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/kevin/code/urllib3/test/__init__.py"", line 34, in wrapper
    return test(*args, **kwargs)
  File ""/Users/kevin/code/urllib3/venv/lib/python2.7/site-packages/nose/tools/nontrivial.py"", line 97, in newfunc
    result = func(*arg, **kw)
  File ""/Users/kevin/code/urllib3/test/with_dummyserver/test_connectionpool.py"", line 178, in test_connect_timeout
    self.assertRaises(ConnectTimeoutError, pool._make_request, conn, 'GET', url)
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/unittest/case.py"", line 475, in assertRaises
    callableObj(*args, **kwargs)
  File ""/Users/kevin/code/urllib3/urllib3/connectionpool.py"", line 282, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 973, in request
    self._send_request(method, url, body, headers)
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 1007, in _send_request
    self.endheaders(body)
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 969, in endheaders
    self._send_output(message_body)
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 829, in _send_output
    self.send(msg)
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 791, in send
    self.connect()
  File ""/Users/kevin/code/urllib3/urllib3/connection.py"", line 103, in connect
    conn = self._new_conn()
  File ""/Users/kevin/code/urllib3/urllib3/connection.py"", line 90, in _new_conn
    *extra_args
  File ""/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py"", line 571, in create_connection
    raise err
error: [Errno 61] Connection refused
```
"
346,Add a test for socket closing,2014-02-27T17:19:45Z,2014-03-04T19:05:48Z,,,,"This is a failing test which exposes the problem in #344, I believe.
"
345,Automatically detecting non-relative imports,2014-02-26T18:37:09Z,2014-02-26T19:09:02Z,,,,"Lately there was an issue with a non-relative import breaking pip's vendoring of urllib3. I was wondering if we can add an automated check for a non-relative import that would break vendoring.

I tried adding this to tox.ini

```
egrep -R ""^(\s+)from urllib3|^(\s+)import urllib3"" urllib3
```

However this matches a false positive in pyopenssl.py, sadly.

Wondering if there's a best practice here or something we can do.
"
344,ConnectionPool::urlopen(): Closing opened connections in case of error,2014-02-26T01:23:39Z,2014-03-04T19:05:48Z,,,,"If there's an error occur during operation, urlopen() function doesn't
close connection properly.

In my current project, I'm using the timeout and retry feature
frequently,  so a single request can be timed-out and retried as many as
20 times.
The number of requests can go up to more than 100,000, and it leads to
crash in open SSL (libcrypto.so.1.0.0.0 segment fault or double free
memory, etc.), assumingly due to the excessive number of sockets
remaining open.

After applying the modification to package code, I could get rid of such
crashes, and my project code runs stable now.
"
343,Fix HTTPConnection strict-related DeprecationWarning in Py3.,2014-02-21T19:36:21Z,2014-02-22T19:49:33Z,,,,"Resolves #342.
"
342,DeprecationWarning regarding strict argument in connection.py,2014-02-21T14:50:47Z,2014-02-22T19:49:33Z,,,,"When using requests which includes urllib3 I get this deprecationwarning that traces back to connection.py.

It doesn't break code but just clutters logs.

'C:\Python33\lib\sitepackages\requests\packages\urllib3\connection.py:99: DeprecationWarning: the ¬¥strict¬¥ argument isn¬¥t supported anymore; http.client now always assumes HTTP/1.x compliant servers.
 HTTPConnection.**init**(self, host, port, strict, timeout, source_address)'
"
341,Add better documentation on how to use pyopenssl,2014-02-21T05:52:21Z,2014-05-12T18:05:01Z,,,,"Per our conversation
"
340,contrib/pyopenssl: fix busyloop with timeouts,2014-02-19T19:31:08Z,2014-02-19T23:34:13Z,,,,"For rationale and history see kennethreitz/requests#1910
"
339,"fix decorator/test case, update CHANGES and add to CONTRIBUTORS",2014-02-09T20:44:24Z,2014-02-09T22:10:13Z,,,,"I forgot to return the wrapped function in the decorator so the test case wasn't running/was producing a no-op, I was a bit confused why a removed method wasn't throwing an `AttributeError`, oops.
"
338,fix import for python3s without ssl,2014-02-09T03:39:45Z,2014-02-09T04:17:21Z,,,,"This resolves issue #320 doing what @Lukasa suggested but with no test :( I mocked out not having `ssl` on my machine by creating my own `ssl.py` file first on my `sys.path` with `raise ImportError`  and confirmed that I could create an `HTTPConnectionPool` and make a `request` to reddit.com under python3.3 with this fix. This previously failed.
"
337,fix the benchmark file to run,2014-02-09T00:57:26Z,2014-02-09T02:39:23Z,,,,"`get_url` doesn't seem to exist anymore and these http links get redirected to https which doesn't seem to work with an`HTTPConnectionPool` which is the bug I reported in #336 so I switched to using a `PoolManager`. Also, the redirect switches the host so I had to add in the `assert_same_host=False`. Or I guess we could switch the benchmark urls instead :)
"
336,redirect from http to https using HTTPConnectionPool,2014-02-09T00:39:19Z,2014-02-09T22:11:13Z,,,,"Possibly related to #312. Trying to fix the broken benchmarks.py file. Requesting http://code.google.com/apis/apps/ issues a 301 redirect to https://developers.google.com/google-apps/?csw=1 and when we go to make a request we re use the connection from the `HTTPConnectionPool` which ends up with a 400 response. Also we might not want to merge the proxy headers at that point if the scheme has changed.

Is this something we want to support, blanket redirects from http to https is increasingly the norm so we could at least raise some sort of meaningful exception.
"
335,fix skipping tests on OSX,2014-02-08T04:04:21Z,2014-02-08T05:21:14Z,,,,"On OSX 10.7 Python 2.7 not being connected to the network raises `errno.EHOSTUNREACH` instead of `errno.ENETUNREACH` which meant that skipping the network tests failed.
"
334,httpbin.org is just the right tool for this.,2014-02-07T13:14:03Z,2014-02-07T22:48:05Z,,,,"When working on requests, I just noticed they use this service to perform real http connections in their tests.
It is probably better to use this dedicated testing service, than harassing travis-ci.org servers.
"
333,"document HTTPHeaderDict, update CHANGES for bugfix",2014-02-07T01:52:59Z,2014-02-07T04:35:09Z,,,,"First shot at documenting this class, I'm having a bit of difficulty nailing down the wording. As written it seems way too verbose in some places and that it doesn't properly explain the functionality in others.

Side note...I'm thinking of removing `raw_header` from the class, what do you think?
"
332,Platform ca location,2014-02-04T13:28:41Z,2014-02-06T19:31:08Z,,,,"fixes for #330 

WIP do not merge yet.

@shazow I'm not sure I understood properly your advice, if not, I'm willing to re-implement it.

For the tests. I decided to perform a real connection to `https://travis-ci.org/` . If it is not acceptable
I see one alternative, that can be performed on CI environment.

I can add https://github.com/shazow/urllib3/blob/master/dummyserver/certs/cacert.pem to list of
root-ca of the system. For ubuntu (travis-ci.org ) it is like

``` bash
$ cp dummyserver/certs/cacert.pem /usr/local/share/ca-certificates/
$ update-ca-certificates
```

But this change will break tests that check https connections without a specified ca_certs raises `SSLError`, because `/etc/ssl/certs/ca-certificates.crt` will be used by default.
Not mentioning that modifying global ca-certs of the system of developers who wants to run the tests, is probably not acceptable.

 Unless you see a third option, I think it is for the best like this.
"
331,Adding Timeout examples on docs landing page,2014-02-04T07:07:06Z,2014-03-15T00:05:48Z,,,,"Getting more ambitious with my docs patches :)

Is this in the right spot and do the examples/description look ok? 
"
330, Use the platform-specific CA certificate locations ?,2014-02-03T11:31:50Z,2014-02-06T19:31:27Z,,,,"Hi,
I would like to know if it is a good idea to tell SSL.Context to use the default cacert.pem file provided by OpenSSL if none is given ?

It will then become the default file used for every SSL connections.

It can be easily implemented using:

``` python
OpenSSL.SSL.Context.set_default_verify_paths()
```

I can make a PR if the idea sounds reasonable.
Thx.
"
329,add HTTPHeaderDict,2014-02-01T00:30:17Z,2014-02-03T02:02:31Z,,,,"First split up of shazow/urllib3/pull/299. Still outstanding are deciding on which methods are really necessary, e.g. is `copy` or how about about access to the ""raw header"".

~~Have to add a `SkipTest` for the actual insensitive header checking due to the earlier mentioned `httplib` bug.~~ (EDIT: Done)

Technically this is a breaking api change even if only in the slightest. Any code that relied on the following check would now break:

``` python
isinstance(r.headers, dict) == True
```
"
328,Wrap httplib exceptions?,2014-01-31T06:52:54Z,2014-06-25T18:14:29Z,,,,"Do we wrap httplib exceptions in general? Getting this snip of a traceback, would be happy to submit a pull request if so.

``` python
for l in r.iter_lines():
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 550, in iter_lines
    decode_unicode=decode_unicode):
  File ""/usr/local/lib/python2.7/dist-packages/requests/utils.py"", line 363, in stream_decompress
    for chunk in iterator:
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 528, in generate
    chunk = self.raw.read(chunk_size)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/response.py"", line 148, in read
    data = self._fp.read(amt)
  File ""/usr/lib/python2.7/httplib.py"", line 541, in read
    return self._read_chunked(amt)
  File ""/usr/lib/python2.7/httplib.py"", line 586, in _read_chunked
    raise IncompleteRead(''.join(value))
httplib.IncompleteRead: IncompleteRead(311 bytes read)
```

This is the same as issue #207 which I don't really think is an issue, will do some more debugging over the next few days. Definitely think we should be wrapping the exception regardless.
"
327,Fixing minor typo on docs landing page,2014-01-31T03:48:49Z,2014-01-31T18:27:50Z,,,,"Making my first ever pr on an open source project. Yep.
"
326,Retries,2014-01-28T01:52:07Z,2014-07-02T00:26:54Z,,,,"Okay:
- Adds support for retries, per the spec we laid out (mostly).
- Adds docs etc.
- Splits out util.py into submodules - I need to know what you want to do here because as written it would change from `urllib3.util.Timeout` into `urllib3.util.timeout.Timeout` which is not good.
- adds a new /successful_retry handler to the DummyTestCase which keys based on a `test-name` header and returns 200 only after the request has been retried once.
- I believe there are some API changes here.
  - some subclasses of httplib.HTTPException can actually be raised as connection errors because they imply the request never actually got sent to the server.
  - urlopen previously would retry on read timeouts, which violated the urlopen contract (as I understood it) of only retrying things that couldn't possibly be side effecting. this code does not retry read timeouts by default.

I am also testing this in two new environments - in the office which places my IP on 10.\* subnet and I think has weird/different behavior when connecting to TARPIT_HOST than do standard wifi networks, and without an Internet connection, in which case a bunch of tests fail. Also, it's difficult to test some of these exceptional cases because the errors raised rely on the network stack, which (I think) is why the tests are failing on the branch. I'm still looking into it.

Either way I am losing some confidence in the connection timeout tests; getting a network to reliably generate ECONNREFUSED, or not generate it and tie up a connection, is tricky, unless we want to go down a path like this: http://bugs.python.org/file26890/test_timeout.patch

[Edit: This is an implementation of #260]
"
325,Skip network tests if machine isn't on a network,2014-01-27T18:52:20Z,2014-01-27T19:06:34Z,,,,"So, I tried running the tests this morning on the train and found out that some of the unit tests fail if your machine is not connected to the Internet. To reproduce, unplug wifi etc. and run the tests and they should fail - at least, on my Mac they do.

This PR adds a new `requires_network` decorator which should be used if the test
requires a connection to a network resource like TARPIT_HOST. Otherwise, the
unit tests fail when running on an isolated machine.
"
324,Tracking state in the dummyserver,2014-01-27T05:38:05Z,2014-07-03T23:22:13Z,,,,"I'm trying to work on the retry branch again. To test things I'd like to make a request that fails once or twice and then finally succeeds. However, each of the methods in `dummyserver/handlers.py` is idempotent, which makes this a little bit tricky.

I'm considering the following:
- Adding a method to handlers.py that tracks state on the class, or resets a class property (""attempts"") every test. Then add a method that checks the attempts property and returns a response based on its value.
- Setting a ""attempts"" cookie on the response and adding cookie parsing to urllib3. Are we even supposed to do this, for example if someone sets a cookie on a 3xx response? However I am not looking forward to adding cookies.
- Mocking `_make_request` to side effect different things - I know it's not great to use mocks but the mock library does have support for this.
- Using the same approach as the retries test in test_socketlevel.py, which is a bit lower level than I'd like but will do just fine.
"
323,Intermittent socketlevel retry test failure,2014-01-27T05:25:52Z,2014-07-03T23:22:01Z,,,,"See https://travis-ci.org/shazow/urllib3/jobs/17679149 for the just-merged PR. Not sure what is going on, it doesn't look like it's related to the uncommented test.
"
322,Uncomment DNS error test,2014-01-27T05:07:03Z,2014-01-27T05:11:55Z,,,,"This test passes for me, and Travis... should we uncomment it?
"
321,update DEFAULT_SSL_CIPHER_LIST for PyOpenSSL,2014-01-26T19:11:25Z,2014-01-26T19:42:53Z,,,,"The default cipher suites set in the pyopenssl contrib are now slightly out of date. SSLLabs has updated their recommendations and now suggests not using RC4 (see: [Is BEAST still a threat?](http://blog.ivanristic.com/2013/09/is-beast-still-a-threat.html)).

This particular suite has been lifted from the [current master of twisted](https://github.com/twisted/twisted/blob/trunk/twisted/internet/_sslverify.py#L995), but provides a whitelist that prefers forward secrecy for key exchange, AES-GCM for encryption (with fallbacks for CBC and 3DES if AES is unavailable), and disables null auth, etc. This was derived by @hynek, so if there questions about it we can drag him in to answer.
"
320,Support Python Without SSL,2014-01-24T16:02:20Z,2014-02-09T04:18:22Z,,ImportError,ImportError: No module named '_ssl',"Right now it's impossible to use urllib3 from a Python that was compiled without SSL. Ideally urllib3 would still continue to function, but if you tried to do something that required the ssl module it would _then_ bail. This would allow pip to support a limited set of functionality on non SSL Pythons.

```
Python 3.4.0b2+ (default, Jan 24 2014, 15:56:34)
[GCC 4.8.1] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import requests
Traceback (most recent call last):
  File ""/root/requests/requests/packages/urllib3/packages/ssl_match_hostname/__init__.py"", line 3, in <module>
    from ssl import CertificateError, match_hostname
  File ""/usr/local/lib/python3.4/ssl.py"", line 97, in <module>
    import _ssl             # if we can't import it, let the error propagate
ImportError: No module named '_ssl'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/root/requests/requests/packages/urllib3/packages/ssl_match_hostname/__init__.py"", line 7, in <module>
    from backports.ssl_match_hostname import CertificateError, match_hostname
ImportError: No module named 'backports'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/requests/requests/__init__.py"", line 58, in <module>
    from . import utils
  File ""/root/requests/requests/utils.py"", line 25, in <module>
    from .compat import parse_http_list as _parse_list_header
  File ""/root/requests/requests/compat.py"", line 7, in <module>
    from .packages import chardet
  File ""/root/requests/requests/packages/__init__.py"", line 3, in <module>
    from . import urllib3
  File ""/root/requests/requests/packages/urllib3/__init__.py"", line 16, in <module>
    from .connectionpool import (
  File ""/root/requests/requests/packages/urllib3/connectionpool.py"", line 31, in <module>
    from .packages.ssl_match_hostname import CertificateError
  File ""/root/requests/requests/packages/urllib3/packages/__init__.py"", line 3, in <module>
    from . import ssl_match_hostname
  File ""/root/requests/requests/packages/urllib3/packages/ssl_match_hostname/__init__.py"", line 10, in <module>
    from _implementation import CertificateError, match_hostname
ImportError: No module named '_implementation'
```
"
319,Speed up tests on Windows,2014-01-23T13:06:25Z,2014-01-23T19:17:31Z,,,,"This reduces test duration from 120s to about 4s (!!). See #256
"
318,Support up to 1024 SANs in a single certificate,2014-01-22T12:27:59Z,2014-01-22T18:47:21Z,,,,"This is affecting pip because PyPI has more than 64 (the default) SANs on it's SSL certificate.
"
317,Add basic makefile,2014-01-16T23:24:20Z,2014-06-23T20:33:42Z,,,,"Adds targets for testing, building docs, installing the project
"
316,Doc links,2014-01-16T17:56:04Z,2014-01-16T21:45:03Z,,,,"Fix some warnings when building the docs, update the year
"
315,Disable Nagle's algorithm for non-proxies,2014-01-16T04:16:55Z,2014-01-27T20:04:01Z,,,,"Nagle's algorithm can be useful in some scenarios to limit packet overhead and
bandwidth over the wire. This change allows consumers to disable Nagle's
algorithm by subclassing HTTPConnection in urllib3/connection.py.
"
314,cherry-picking a commit; just ensure connection class gets reassigned if...,2014-01-16T02:31:55Z,2014-01-16T02:37:11Z,,,,"... a test fails
"
313,typo,2014-01-16T02:26:42Z,2014-01-16T02:26:59Z,,,,"Typo
"
312,Redirect from https -> https sends wrong host header,2014-01-10T21:54:16Z,,,,,"I cant give the actual url causing this problem, but this is the gist of the issue...

1) sending request to a HTTP server via proxy. Using ProxyManager. assume url is http://foo.com/image.jpg
2) Receive a redirect to a https://foo.com/image.jpg
3) GET https://foo.com/image.jpg
4) 301 redirect: http://foo.com/image.jpg
5) GET http://foo.com/image.jpg
6) 301 redirect https://foo.com/image.jpg
7) GET https://foo.com/image.jpg
8 301 redirect https://bar.com/image.jpg

at this point, I expect the request to https://bar.com/image.jpb to succeed. However, urllib3 does not send the correct request. The host header that it sends in the request is for foo.com, due to which bar.com rejects the request.

Here is the stack where it is happening

```
-> return self.urlopen(method, redirect_location, **kw)
  /home/botouser/git/myapp/src/main/python/eggs/urllib3-1.7.1-py2.7.egg/urllib3/poolmanager.py(256)urlopen()
-> return super(ProxyManager, self).urlopen(method, url, redirect, **kw)
  /home/botouser/git/myapp/src/main/python/eggs/urllib3-1.7.1-py2.7.egg/urllib3/poolmanager.py(169)urlopen()
-> log.info(""Redirecting %s -> %s"" % (url, redirect_location))
  /home/botouser/git/myapp/src/main/python/eggs/urllib3-1.7.1-py2.7.egg/urllib3/poolmanager.py(256)urlopen()
-> return super(ProxyManager, self).urlopen(method, url, redirect, **kw)
> /home/botouser/git/myapp/src/main/python/eggs/urllib3-1.7.1-py2.7.egg/urllib3/poolmanager.py(172)urlopen()
-> return self.urlopen(method, redirect_location, **kw)
```

and the current state:

```
(Pdb) p (url,redirect_location)
('https://foo.com/image.jpg', 'https://bar.com/image.jpg')
(Pdb) p kw
{'redirect': True, 'retries': 1, 'assert_same_host': False, 'headers': {'Host': u'foo.com', 'Accept': '*/*', 'user-agent': 'my-app'}}
(Pdb) 
```

As you can see, the url is pointing to the correct location, but the host header in the headers collection is wrong.

Is there any way to work around this bug?
## 
"
311,Message body length not matching Content-Length should raise httplib.IncompleteRead even when using stream(),2014-01-10T21:44:23Z,2014-01-29T20:06:50Z,,,,"More details in the issue I originally filed with requests (https://github.com/kennethreitz/requests/issues/1855).  Here is a repro, slightly modified from the repro in that issue:

``` python
import SocketServer as socketserver
import threading
import urllib3
import time


class MyTCPHandler(socketserver.BaseRequestHandler):
    def handle(self):
        self.data = self.request.recv(1024)
        self.request.sendall('HTTP/1.1 200 OK\r\n'
                             'Server: truncator/0.0\r\n'
                             'Content-Length: 20\r\n'
                             'Connection: close\r\n\r\n'
                             '12345')

server = None
def background_server():
    global server
    HOST, PORT = ""localhost"", 9999
    server = socketserver.TCPServer((HOST, PORT), MyTCPHandler)
    server.serve_forever()


if __name__ == ""__main__"":
    t = threading.Thread(target=background_server)
    t.daemon = True
    t.start()
    time.sleep(1)
    http = urllib3.PoolManager()
    try:
        g = http.request('GET', 'http://localhost:9999', preload_content=False).stream()
        data = list(g) # This should raise httplib.IncompleteRead
        print(data)
    finally:
        server.shutdown()
```
"
310,Account retries on ProxyError,2014-01-09T07:45:36Z,2014-01-10T17:36:53Z,,,,"Current implementation fails if it encounters a proxy error no
matter how many retries left.

Change the behaviour to try connecting until retry limit has reached
and then throw an appropriate exception: ProxyError or MaxRetryError.
"
309,Disable TLS compression (CRIME),2014-01-08T01:40:45Z,2014-01-10T07:49:11Z,,,,"Right now, urllib3 has SSL/TLS compression enabled by default:

``` python
>>> import json
>>> import urllib3
>>> http = urllib3.PoolManager()
>>> r = http.request('GET', 'https://www.howsmyssl.com/a/check')
>>> data = json.loads(r.data.decode('utf8'))
>>> data['rating']
'Bad'
>>> data['tls_compression_supported']
True
```

SSL/TLS compression should be disabled by default to migitate the CRIME attack: https://community.qualys.com/blogs/securitylabs/2012/09/14/crime-information-leakage-attack-against-ssltls I tried to create a patch but did not find the right place in the codebase to do so.

Most browsers (if not all) have already disabled the compression (or never implemented it in the first place), but many client libraries like urllib3 still use it.
"
308,_encode_body_methods isn't referenced anywhere,2014-01-07T03:38:18Z,2014-01-07T04:09:43Z,,,,
307,_tunnel_host compatibility for Python 2.6.(0-2),2014-01-06T20:34:05Z,2014-01-06T21:09:17Z,,,,"The _tunnel_host attribute was added in Python 2.6.3, so older Python 2.6's
failed when urllib3 would attempt to access the _tunnel_host attribute. This
change checks for the attribute's existence before accessing it. This fixes
Github issue #306.

Adds tests for this change.
"
306,python 2.6 compatibility issue: VerifiedHTTPSConnection instance has no attribute '_tunnel_host',2014-01-06T15:26:38Z,2014-01-06T21:10:08Z,,"Exception, AttributeError","Exception:, AttributeError: VerifiedHTTPSConnection instance has no attribute '_tunnel_host'","Hi,

I got the following error in pip on Darwin-10.8.0-i386-64bit :

<pre>
$ pip install --allow-unverified mock mock
Downloading/unpacking mock
Cleaning up...
Exception:
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/commands/install.py"", line 270, in run
    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/req.py"", line 1157, in prepare_files
    url = finder.find_requirement(req_to_install, upgrade=self.upgrade)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/index.py"", line 202, in find_requirement
    page = self._get_page(main_index_url, req)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/index.py"", line 576, in _get_page
    session=self.session,
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/index.py"", line 678, in get_page
    resp = session.get(url)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/sessions.py"", line 394, in get
    return self.request('GET', url, **kwargs)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/download.py"", line 236, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/sessions.py"", line 382, in request
    resp = self.send(prep, **send_kwargs)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/sessions.py"", line 485, in send
    r = adapter.send(request, **kwargs)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/adapters.py"", line 324, in send
    timeout=timeout
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/packages/urllib3/connectionpool.py"", line 478, in urlopen
    body=body, headers=headers)
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/packages/urllib3/connectionpool.py"", line 285, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/httplib.py"", line 874, in request
    self._send_request(method, url, body, headers)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/httplib.py"", line 911, in _send_request
    self.endheaders()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/httplib.py"", line 868, in endheaders
    self._send_output()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/httplib.py"", line 740, in _send_output
    self.send(msg)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.6/lib/python2.6/httplib.py"", line 699, in send
    self.connect()
  File ""/Library/Python/2.6/site-packages/pip-1.5-py2.6.egg/pip/_vendor/requests/packages/urllib3/connection.py"", line 83, in connect
    if self._tunnel_host:
AttributeError: VerifiedHTTPSConnection instance has no attribute '_tunnel_host'

Storing debug log for failure in /Users/haypo/Library/Logs/pip.log
</pre>


My python version is:

<pre>
Python 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) 
[GCC 4.2.1 (Apple Inc. build 5646)] on darwin
</pre>


It looks like a compatibility issue with Python 2.6. ""python"" on my Mac OS X is python 2.6.
"
305,Use a default port when comparing hosts to a connection pool,2014-01-06T02:26:09Z,2014-01-06T04:53:20Z,,,,"Added tests to verify and show expected behavior
"
304,Follow HTML 5 draft and avoid RFC 2231 by default.,2014-01-06T02:19:32Z,,,,,"This passes the request object along for field rendering. Two attributes of the request object control that rendering:
- `field_encoding_style` which may be `HTML5` or `RFC2231` controls
  the encoding of non-ascii field names and file names.
- `form_data_encoding` can be set to encode the form with something other than UTF-8.

See [the current (as of this commit) draft version of the HTML 5 standard](http://www.w3.org/TR/2013/WD-html51-20130528/) for details on their idea of [how to format multipart/form-data](http://www.w3.org/TR/2013/WD-html51-20130528/forms.html#multipart-form-data).

These changes addresses shazow/urllib3#303.
"
303,Consider HTML 5 draft for multipart/form-data,2014-01-04T23:59:35Z,,,,,"In several past posts, I've tried to make urllib3 more standards-compliant, the way I saw things. In particular, #120 aimed at adding a proper `Content-Type` header for _every_ form field, and #119 / #223 introduced [RFC 2231](http://tools.ietf.org/html/rfc2231) format for file name encoding.

One consequence of that last change was my noticing that tornado doesn't decode that format, causing some test cases to fail. e11e03628f8b82021a74ba7f5b39ce9ad9011382 monkey-patched tornado to deal with that, and I filed facebook/tornado#868 to take this upstream. There [bdarnell pointed out](https://github.com/facebook/tornado/pull/869#issuecomment-23632083) the following:

**The current HTML 5 draft has [a section on multipart/form-data](http://www.w3.org/html/wg/drafts/html/master/forms.html#multipart-form-data) which explicitely forbids the use of `Content-Type` for non-file fields, and also the use of RFC 2231 format for file names. It also provides its own mechanism for encoding field names.**

Although I'm reluctant to abandon conformance with Standards Track RFCs for the sake of some lines in a draft, it appears that the HTML 5 draft might better reflect what current web servers actually implement, or are going to implement in the intermediate future. Perople have already [reported problems](http://stackoverflow.com/q/20591599/1468366) with our format in the wild. For this reason, I wonder whether the current choice of always using RFC 2231 might have to be revised.

I can imagine two approaches, one would be dropping RFC 2231 support completely, the other would be introducing some switch to distinguish cases. I guess (although I don't like it) that the switch should default to the HTML 5 way. I guess I'd implement the switch as a global variable, since every field needs acces to it and concurrent use of multiple standards should be very unlikely. Do you have a better suggestion?

So what would be needed for HTML 5 conforming handling of encodings? We'd need to properly distinguish between files and non-files. The [`RequestField` class](https://github.com/shazow/urllib3/blob/master/urllib3/fields.py#L53) could probably check whether both file name and content type are `None` to recognize non-file fields. That choice could be used to control the addition of the `Content-Type` header.

For file names, we'd have to encode them to UTF-8 (or some user-configurable request encoding?), after making sure the result does not contain any invalid data: newlines must be stripped (or converted to spaces), quotation marks and backslashes quoted by `\` or substituted. I'm not sure which of these, and how much control we want to give our users over that process.

If we allow for encodings other than UTF-8, which are not able to represent all Unicode codepoints, then we might need some more work. File name characters which are not representable must then be substituted or approximated somehow. For filed names we'd have to make sure we replace non-representable characters as XML decimal character entities instead. Other non-ASCII header fields should usually not occur, but if they do, I guess they should be treated the same way file names are.

I notice that I made a mistake in reading [RFC 2388](http://tools.ietf.org/html/rfc2388): its [section 5.4](http://tools.ietf.org/html/rfc2388#section-5.4) states that field names are subject to a different encoding than file names. Grrr! Whoever though of that?!? In any case, to be conforming to that as well, we might even want to revise the RFC2231-using mode if we decide to keep it at all.

I might find the time to write some of this myself, but before I start, I'd like to discuss the open questions above as to how this should get implemented.
## 
"
302,Remove incorrect comments,2013-12-24T08:33:52Z,2014-01-02T15:57:09Z,,,,"httplib.HTTPConnection.getresponse() doesn't accept any keyword arguments in Python 3.x

See:
1) http://docs.python.org/3.3/library/http.client.html?highlight=httpconnection#http.client.HTTPConnection.getresponse
2) http://hg.python.org/cpython/file/3.3/Lib/http/client.py#l1101

And as per https://github.com/shazow/urllib3/issues/301, the comments are incorrect at https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L313:

```
   try: # Python 2.7+, use buffering of HTTP responses
            httplib_response = conn.getresponse(buffering=True)
        except TypeError: # Python 2.6 and older
            httplib_response = conn.getresponse()
```
"
301,TypeError: getresponse() got an unexpected keyword argument 'buffering',2013-12-24T08:15:53Z,2013-12-25T05:08:45Z,,"TypeError, ConnectionResetError, requests.packages.urllib3.exceptions.MaxRetryError, requests.exceptions.ConnectionError","TypeError: getresponse() got an unexpected keyword argument 'buffering', ConnectionResetError: [Errno 104] Connection reset by peer, requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='la.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?bltitle=Britannia+Minor&list=backlinks&action=query&format=json&maxlag=60&blnamespace=0&blredirect=true&bllimit=500 (Caused by <class 'ConnectionResetError'>: [Errno 104] Connection reset by peer), requests.exceptions.ConnectionError: HTTPConnectionPool(host='la.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?bltitle=Britannia+Minor&list=backlinks&action=query&format=json&maxlag=60&blnamespace=0&blredirect=true&bllimit=500 (Caused by <class 'ConnectionResetError'>: [Errno 104] Connection reset by peer)","Why isn't the TypeError caught by the exception catch clause? https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L314 

I'm using urllib3 through Requests 1.2.3 and Python 3.3.1 

This issue has been dicussed here as well: https://github.com/kennethreitz/requests/issues/1289

Stacktrace:

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.3/dist-packages/requests/packages/urllib3/connectionpool.py"", line 288, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.3/dist-packages/requests/packages/urllib3/connectionpool.py"", line 428, in urlopen
    body=body, headers=headers)
  File ""/usr/local/lib/python3.3/dist-packages/requests/packages/urllib3/connectionpool.py"", line 290, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/lib/python3.3/http/client.py"", line 1143, in getresponse
    response.begin()
  File ""/usr/lib/python3.3/http/client.py"", line 354, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python3.3/http/client.py"", line 316, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/usr/lib/python3.3/socket.py"", line 297, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.3/dist-packages/requests/adapters.py"", line 292, in send
    timeout=timeout
  File ""/usr/local/lib/python3.3/dist-packages/requests/packages/urllib3/connectionpool.py"", line 474, in urlopen
    raise MaxRetryError(self, url, e)
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='la.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?bltitle=Britannia+Minor&list=backlinks&action=query&format=json&maxlag=60&blnamespace=0&blredirect=true&bllimit=500 (Caused by <class 'ConnectionResetError'>: [Errno 104] Connection reset by peer)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""remote_coords_finder_5.py"", line 185, in <module>
    {'list': 'backlinks', 'bltitle': output['title'], 'blnamespace': 0, 'bllimit': 500, 'blredirect': 'true'}, 'backlinks', 'blcontinue'):
  File ""remote_coords_finder_5.py"", line 55, in query
    response = requests.get('http://' + language + '.wikipedia.org/w/api.php', params=req, headers=headers, timeout=60)
  File ""/usr/local/lib/python3.3/dist-packages/requests/api.py"", line 55, in get
    return request('get', url, **kwargs)
  File ""/usr/local/lib/python3.3/dist-packages/requests/api.py"", line 44, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python3.3/dist-packages/requests/sessions.py"", line 335, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python3.3/dist-packages/requests/sessions.py"", line 438, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python3.3/dist-packages/requests/adapters.py"", line 327, in send
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='la.wikipedia.org', port=80): Max retries exceeded with url: /w/api.php?bltitle=Britannia+Minor&list=backlinks&action=query&format=json&maxlag=60&blnamespace=0&blredirect=true&bllimit=500 (Caused by <class 'ConnectionResetError'>: [Errno 104] Connection reset by peer)
```
"
300,make requests compatible with SAE,2013-12-19T03:59:29Z,2014-07-03T23:19:16Z,,,,"make requests compatible with SAE

Sina App Engine conn has a None valued sock
"
299,Use CaseInsensitiveMultiDict for headers,2013-12-18T05:14:12Z,2014-02-03T08:08:45Z,,,,"(Re: issue #236)

First go at implementing this. Not 100% sure of the scope so there might be some features missing/extraneous items, would like to start discussion regardless.
"
298,#297 exception form socket not wrapped,2013-12-14T19:32:50Z,2013-12-15T07:11:19Z,,,,
297,aims to wrap an obscure socket problem that is not being caught with a g...,2013-12-13T16:19:26Z,2014-06-25T21:50:43Z,,,,"this wraps an exception into the urllib3.exceptions.ConnectTimeoutError .

Related to #296
"
296,socket.timeout not wrapped properly,2013-12-13T14:01:49Z,2014-06-01T03:08:52Z,,,,"As mentioned in a [comment](https://github.com/kennethreitz/requests/issues/1787#issuecomment-30510686) on kennethreitz/requests#1787 urllib3 apparently lets a `socket.timeout` exception propagate outside library code instead of wrapping it e.g. with `urllib3.exceptions.TimeoutError`. See the linked issue for a full backtrace.
"
295,HTTPS Proxy Connections don't play with with Connection: Close.,2013-12-07T09:41:52Z,2014-04-02T02:11:36Z,,urllib3.exceptions.ProxyError,urllib3.exceptions.ProxyError: Cannot connect to proxy. Socket error: [Errno 54] Connection reset by peer.,"This was actually spotted on [StackOverflow](http://stackoverflow.com/questions/20437753/python-requests-library-cant-handle-redirects-for-https-urls-when-behind-a-prox).

To reproduce, set up a proxy that can handle the CONNECT verb (e.g. mitmproxy), then do as follows:

``` python
pman = urllib3.proxy_from_url('http://127.0.0.1:8080')
res = pman.urlopen('GET', 'https://www.paypal.com/')
```

You hit a nice big fancy error:

```
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 254, in urlopen
    return super(ProxyManager, self).urlopen(method, url, redirect, **kw)
  File ""/usr/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 155, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/usr/local/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 520, in urlopen
    'Socket error: %s.' % e)
urllib3.exceptions.ProxyError: Cannot connect to proxy. Socket error: [Errno 54] Connection reset by peer.
```

This can be more clearly reproduced by turning off redirects:

``` python
pman = urllib3.proxy_from_url('http://127.0.0.1:8080')
res = pman.urlopen('GET', 'https://www.paypal.com/', redirect=False)
res = pman.urlopen('GET', 'https://www.paypal.com/', redirect=False) # Same error is hit here.
```

This is almost certainly because PayPal is sending back `Connection: Close` in the headers. This causes mitmproxy to close the upstream connection _and_ our connection to it, but we don't remove the connection from the pool.
"
294,This should be enough to handle kennethreitz/requests#239,2013-12-04T18:57:06Z,2013-12-05T20:19:45Z,,,,"- The comment in the method explains why we need to check self.pool first
- Fixes #291
"
293,Minor Suggestion,2013-12-04T04:36:21Z,2013-12-04T04:42:07Z,,,,"In `urllib3/_collections.py`, `RecentlyUsedContainer` has a pattern of

``` python
if self.dispose_func:
    self.dispose_func(value)
```

This could be DRY'd up to:

```
def dispose_of(self, value):
     if self.dispose_func:
         self.dispose_func(value)
```

And then all the other instances could just call:  `self.dispose_of(value)`. I have this on a [branch](https://github.com/sigmavirus24/urllib3/tree/dispose-of-method) but wasn't sure if it was of any value. (In my humble opinion those usages now read a lot better and no longer need to consider directly whether that function was provided.)

In reality it is a micro-optimization and it's one of personal preference.
"
292,Exceptions being ignored in the tests,2013-12-04T03:10:31Z,2013-12-04T03:20:05Z,,,,"```
S..................................................Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10ddd6110>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e5321d0>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532450>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532190>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532250>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532690>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532490>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532410>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532250>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532c50>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e532fd0>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e532d90>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e539a50>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e539e90>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e539410>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e54d310>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e539950>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e539d90>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e539c90>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e539890>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e539350>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e54db10>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e539950>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e54d590>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e54d650>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e54dc50>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e54dc90>> ignored
..Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e551090>> ignored
..Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e5510d0>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e551090>> ignored
.S................Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e5582d0>> ignored
.....................Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e558bd0>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e559190>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPSConnectionPool.__del__ of <urllib3.connectionpool.HTTPSConnectionPool object at 0x10e558ed0>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e558d10>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e558d90>> ignored
Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e559050>> ignored
.Exception AttributeError: AttributeError(""'NoneType' object has no attribute 'get'"",) in <bound method HTTPConnectionPool.__del__ of <urllib3.connectionpool.HTTPConnectionPool object at 0x10e558cd0>> ignored
........................................
Name                     Stmts   Miss  Cover   Missing
------------------------------------------------------
urllib3                     14      0   100%
urllib3._collections        46      0   100%
urllib3.connection          35      0   100%
urllib3.connectionpool     194      0   100%
urllib3.contrib              0      0   100%
urllib3.exceptions          40      0   100%
urllib3.fields              62      0   100%
urllib3.filepost            32      0   100%
urllib3.poolmanager         89      0   100%
urllib3.request             25      0   100%
urllib3.response           128      0   100%
urllib3.util               173      0   100%
------------------------------------------------------
TOTAL                      838      0   100%
----------------------------------------------------------------------
Ran 152 tests in 0.959s

OK (SKIP=2)
```

I get that output running the tests from OSX 10.9 on Python 2. Using tox it seems to happen for all the pythons I have installed (2.6, 2.7, 3.3)
"
291,Ensure connections are properly closed on objects destruction,2013-12-03T14:42:16Z,2013-12-03T16:32:00Z,,,,"This fixes the issue raised by @tardyp on https://github.com/kennethreitz/requests/issues/239.

We have not been able to reproduce the scenario on a simple testcase. However, in our environment involving Buildbot, Twisted, and txrequests, the number of connections in a CLOSE_WAIT state grows until we reach the maximum open files limit per process!

While it's still obscure to me what would prevent an HTTPConnection object from being destroyed once it's no longer used, this fix appears to work in our environment, at least we no longer see this CLOSE_WAIT connections leak...
"
290,ssl_wrap_socket capability to override default _verify_callback,2013-11-28T20:21:37Z,2013-11-30T19:19:14Z,,,,"This is to allow customized server certificate validation.

Use cases include:
- a certificate that passes all of chain validation with the exception of the hostname (instances where the IP is specified during connection instead of the hostname)
- you wish to restrict connections to only servers providing specific certificates ie. a single allowable CN (common name).
"
289,urllib3 should work without threads,2013-11-26T13:29:36Z,2013-11-26T15:52:13Z,,ImportError,ImportError: No module named '_thread',"Hi, Python 3.4 now has an ensurepip module. The virtualenv (venv) test fails to install pip on our ""without threads"" buildbot because urllib3 requires the threading module:

```
$ cd /home/haypo/pip/PIP/pip/_vendor/requests/packages
$ /home/haypo/prog/python/default/venv/bin/python -c 'import urllib3'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/haypo/pip/PIP/pip/_vendor/requests/packages/urllib3/__init__.py"", line 24, in <module>
    from .poolmanager import PoolManager, ProxyManager, proxy_from_url
  File ""/home/haypo/pip/PIP/pip/_vendor/requests/packages/urllib3/poolmanager.py"", line 14, in <module>
    from ._collections import RecentlyUsedContainer
  File ""/home/haypo/pip/PIP/pip/_vendor/requests/packages/urllib3/_collections.py"", line 8, in <module>
    from threading import RLock
  File ""/home/haypo/prog/python/default/Lib/threading.py"", line 4, in <module>
    import _thread
ImportError: No module named '_thread'
```

Python issue:
http://bugs.python.org/issue19766
"
288,Fixed check for IPv6 support.,2013-11-25T12:29:32Z,2013-11-25T16:04:14Z,,,,"The `hasattr` check doesn't make any sense according to the docs: http://docs.python.org/2/library/socket.html#socket.has_ipv6

Was added by @t-8ch in b794413042e8b2ab8ca362b2c8ebd6b8aa5bb9e9
"
287,Refactored tornado test cases.,2013-11-25T10:36:50Z,2013-11-25T16:11:32Z,,,,"Use an explicit IOLoop instance so it should be more clear what's going
on internally.
"
286,Fixed test_fields on Windows and some PEP8 love.,2013-11-25T09:36:11Z,2013-11-25T16:07:18Z,,,,"Fix is in line 10/11:

```
self.assertTrue(guess_content_type('image.jpg') in
                      ['image/jpeg', 'image/pjpeg'])
```

The rest is indentation and max line length corrections.
"
285,Increase timeout for Windows.,2013-11-25T09:23:50Z,2013-11-25T16:05:48Z,,,,"Yes, we need this much time on Windows...
"
284,SOCKS proxy support,2013-11-23T03:36:35Z,2015-12-29T20:29:03Z,,,,"Here's the SOCKS proxy support patch.

Usage is identical to setting HTTP proxies: `ProxyManager(""socks4://localhost:1080"")` or `ProxyManager(""socks5://localhost:1080"")` (or `proxy_from_url()`). Default ports for both are 1080.

I added new SOCKS Connection classes in `connection.py`, and added a `_is_socks` attribute to the proxy URL that is passed to `ProxyManager` (on the `Url` object). The `_is_socks` attribute is used by the connection pools so that HTTP proxy negotiation steps aren't taken when the proxy has a SOCKS scheme. This could possibly be refactored (separate `socks_proxy` and `http_proxy` attributes?); tell me how you feel about the `_is_socks` solution.

I also touched up some other parts of the codebase, as follows, which I noticed while working through files and debugging.
- Fixed inconsistent indentation in a few places.
- Fixed some typos in comments.
- Changed `__str__` methods to `__repr__`. `__str__` automatically defers to `__repr__` if it isn't already defined, but the reverse isn't true. So this keeps the same behavior while also allowing easier debugging from the REPL.
- Added a newline in the middle of the proxy connection exception message, to make it a little easier to read.

I believe this branch should currently work fine for regular usage; it seems to have no trouble with any SOCKS proxies.

However, I had many confusing issues and bugs while trying to modify the test suite. The tests are currently broken (I added `raise SkipTest()` in the proxy tests, for the time being). I am using Twisted for the SOCKS4 and SOCKS5 proxy servers, and added that to the test requirements.

The test issues seem to stem from the Tornado HTTP and HTTPS dummy servers. On my computer, they spat out odd SSL errors (like `SSLError: [Errno 1] _ssl.c:504: error:1411B072:SSL routines:SSL3_GET_NEW_SESSION_TICKET:bad message type
` and many others), and would hang seemingly randomly.

I upgraded the Tornado version in `test-requirements.txt` because the version listed before appeared to have a bug that prevented me (or others) from running in IPv6 mode, see here: https://github.com/facebook/tornado/pull/593; I had to upgrade to get IPv6-related tests to pass. I'm not sure if the upgrade has anything to do with the current issues.

I think it has something to do with trying to run Tornado in separate threads. I was not able to get multithreading to work with Twisted, so I spawned a new process for each SOCKS proxy server instead; Twisted seems to work fine, as I can connect to and use the proxy servers while the tests are running.

I would appreciate if someone could modify the tests so the HTTP servers work properly, or give me some advice on how to fix the issue. And of course, I'd appreciate any suggestions or changes for the SOCKS patch itself.
"
283,Nagle,2013-11-18T16:57:10Z,2014-01-16T20:24:58Z,,,,
282,add tox as test dependency,2013-11-18T10:00:43Z,2013-11-18T14:47:41Z,,,,
281,Nagle,2013-11-18T01:59:42Z,2013-11-18T16:56:19Z,,,,"I merged master into the branch, which kind of blew up the diff, but this gets the tests passing on python 2.6-3.3 for this branch.

I still want to add a tcp_nodelay flag to the connection classes, will work on that next
"
280,Set an explicit log level for the library.,2013-11-08T20:40:05Z,2013-11-08T21:22:04Z,,,,"Not sure if this is typical of Python packages, but I always find it a surprise when I set the log level of a Python application to DEBUG and then all my imported packages begin chattering. It seems more logical to me that packages should set their own explicit log levels so that the basicConfig doesn't propagate.

It is always easy enough for the application developer to explicit set a package's log level if so desired.
"
279,Add proxy_basic_auth option to util.make_headers(),2013-11-03T13:54:08Z,2013-11-03T13:57:05Z,,,,"With this option, `util.make_headers()` will return a dictionary
containing the key `Proxy-Authorization` needed to authenticate against
proxy servers.
"
278,Add myself to CONTRIBUTORS.txt,2013-11-03T13:30:35Z,2013-11-03T13:45:38Z,,,,"xref #277
"
277,Add HTTPResponse.tell() as per #276,2013-11-02T17:39:42Z,2013-11-02T19:21:07Z,,,,"See #276.

This new method gives a way of telling how many bytes have
been transferred over the wire so far, even if they are being
transparently decoded.
"
276,Method to determine stream size pre-decoding,2013-11-01T22:49:26Z,2013-11-02T19:22:41Z,,,,"This is a repost of the issue I reported at kennethreitz/requests#1721 before realising it belonged here.

I would like to know the amount of data transferred over the wire for accounting purposes when streaming large compressed files.

Currently, I do:

```
req = requests.get(url, stream=True)
```

followed by `req.iter_content()`. Unfortunately the server doesn't respond with anything telling me the length and looking at the source of requests I can't find a way of knowing the original size. It looks like that information is [effectively lost at this point in the code](https://github.com/shazow/urllib3/blob/fc116510ebaa61d4754c6e4a1c3368ac3a71b1d2/urllib3/response.py#L188). Any chance we could claw it back?
"
275,"When 3316f5b reverted the removal of special headers for proxies, the un...",2013-10-29T02:55:46Z,2013-10-29T08:52:31Z,,,,"...ittest change should have been reverted as well.

This fixes a unittest failure introduced by pull #271 
"
274,Feature/vendored backports ssl match hostname,2013-10-29T01:21:19Z,2013-10-29T16:24:33Z,,,,"This pull request vendors the most recent version of backports.ssl_match_hostname and uses it as a fallback in case neither the stdlib nor the backports.ssl_match_hostname module contain a match_hostname() function.  Also note in a comment that the ssl_match_hostname function is under the PSF license as it's originally copied from the Python stdlib.
"
273,pyopenssl: fix monkeypatching for new code,2013-10-26T08:09:19Z,2013-10-26T08:17:40Z,,,,"fixes kennethreitz/requests#1705
"
272,Logging levels in `connectionpool`,2013-10-23T11:56:37Z,2013-10-23T13:05:05Z,,,,"There's three places where logging at level `INFO` is made but these aren't particularly informative; perhaps just `DEBUG`?
"
271,Resolves #270.,2013-10-22T21:32:45Z,2013-10-28T22:36:57Z,,,,"See #270 for the 'discussion'.
"
270,Proxy Headers don't get applied when using ProxyManager.connection_from_url(),2013-10-22T20:30:17Z,2013-10-28T22:36:57Z,,,,"Brought up in the Requests IRC room: it turns out Requests no longer correctly applies Proxy-Authentication headers to HTTP messages via proxies.

If you use `ProxyManager.urlopen()`, [this block of code](https://github.com/shazow/urllib3/blob/master/urllib3/poolmanager.py#L247-L253) applies some HTTP headers:

``` python
if u.scheme == ""http"":
    # It's too late to set proxy headers on per-request basis for
    # tunnelled HTTPS connections, should use
    # constructor's proxy_headers instead.
    kw['headers'] = self._set_proxy_headers(url, kw.get('headers',
                                                        self.headers))
    kw['headers'].update(self.proxy_headers)
```

But no such code back exists when using `ProxyManager.connection_from_url`.

I'm open to providing code to fix this, but am not sure I can do so tonight, so if someone else would like to they should feel free.
"
269,Potential implementation of urllib3.util.Retry,2013-10-20T23:01:29Z,2014-06-01T03:13:39Z,,,,"Reference PR for #260 , as mentioned in the last commit

No tests, no docs, etc, just trying to push through this to see what the
logic's like. Some things I don't like:
- it's fairly repetitive, we call increment/exhausted every time there's an
  error, and once when we have a response.
- checking the response twice, once in increment and once in is_retryable
- calling `_connect_error_count` from outside the class
- conflict between individual retry and total retry values causing tests to
  fail, need to nail down this behavior
- setting default timeout for a specific test instead of all of them
"
268,Set default timeout on the server socket,2013-10-20T22:08:50Z,2013-10-21T08:03:20Z,,,,"This probably isn't the best way to solve this, but in some cases (for me, test_timeout_errors_cause_retries) if the main thread fails with an exception, the server thread will never get an accept() and it will hang indefinitely. Control won't be returned to the caller and you can't kill it with a KeyboardInterrupt.

This sets a default timeout on the socket so control is eventually returned and the tests complete.
"
267,At least some HTTPExceptions imply the client sent data,2013-10-20T17:57:56Z,2014-07-03T23:23:43Z,,,,"Generally `MaxRetryError`s are handled as a `ConnectionError`, which implies by its name that the connection did not succeed and is safe for the client to retry, see for example https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L356

The code [here](https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L510) aggregates HTTPExceptions and socket errors. However some instances of HTTPException imply the request made it to the server - `BadStatusLine`, `UnknownProtocol`, `IncompleteRead`, `UnknownTransferEncoding` (which doesn't seem to be used anywhere but is defined and seems like it applies to the response) and `LineTooLong` in particular. Other HTTPExceptions imply the connection was not sent.

I'm not sure about the best way to tease these apart but it seems like lumping everything into MaxRetryError is maybe not the best way forward.
"
266,consider raising socket.error as a ConnectionError,2013-10-20T08:55:53Z,2014-07-03T23:21:30Z,,,,"In other places in the library exceptions from the underlying classes (httplib, socket) are caught and rethrown, it may be nice to do the same for `socket.error`. 

There are also a whole bunch of errors in the socket.error class, may be good to catch/raise where appropriate but I'm not even sure which ones could be raised. http://docs.python.org/2/library/errno.html#module-errno

There are some compatibility issues, for example the code here https://github.com/kennethreitz/requests/blob/master/requests/adapters.py#L352 would have to change.
"
265,Abuse SSL to fix test coverage.,2013-10-19T11:17:49Z,2013-10-19T13:45:18Z,,,,"We have a branch of the code that catches SSL errors thrown by `getresponse()`. This currently doesn't happen during any of our tests. It can only really happen if we're sent non-SSL (or bad SSL) data when receiving a response to an SSL request. This is _totally crazy_, but it's better to be safe than sorry.

This test replicates that level of craziness, involving no fewer than three sockets and a lot of sadness, but it successfully tests that path and confirms that we do roughly the right thing. It also returns test coverage to 100%.
"
264,Return test coverage to 100%.,2013-10-18T21:54:42Z,2013-10-18T22:01:40Z,,,,"Make @shazow happy following [this travesty](https://github.com/shazow/urllib3/pull/252#issuecomment-26631728). Whichever moron proposed that pull request should be dragged out and shot.

On a separate note, this is a very low level test. Let me know if you'd like me to try to do this at a broader scope.
"
263,Put the total first,2013-10-18T21:30:02Z,2013-10-18T21:38:20Z,,,,
262,Reset the timeout after a connect,2013-10-18T18:56:26Z,2013-10-18T21:38:51Z,,,,"Previously, if the read timeout was not set, or set to None, we would not reset
the timeout on the socket, so the read would get the same timeout as the
connect timeout. This resets the read timeout in every case.

Fixes #261
"
261,read timeout is not properly getting un-set,2013-10-18T18:08:04Z,2013-10-18T21:38:51Z,,,,"I discovered this issue when digging into the code here:

https://github.com/kennethreitz/requests/commit/c64c0ab1215168adf2384888f3d52bd99217d723#diff-31f6e77c031977d33226530924b4337aR303

On my machine at least, creating a Timeout like this

``` python
from urllib3.util import Timeout
t = Timeout(connect=5)
make_some_request(timeout=t)
```

The socket timeout for the connect is properly set to 5 seconds. [This line](https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L297), however, keeps us from resetting the timeout for the read, so it's still at 5 seconds for the read. It should be set instead to `socket.getdefaulttimeout()`.
"
260,More granular retry support,2013-10-16T18:07:30Z,2014-07-03T23:17:33Z,,,,"I'm not sure what the interface for this would look like, or even at what level this would be implemented, but essentially, more function-level control over retries would be awesome. This is roughly the behavior we build in around a `requests.request`:
- We want to retry pretty much everything on idempotent requests that's not a 2xx level response - for GET, PUT, DELETE methods, because these are safe to retry.
- We want to retry **all** connection errors and connection timeouts - these indicate the request never made it to the other machine. We usually use an exponential backoff here, in the case of network failure
- In general, we don't want to retry failed POST requests, whether they were timeouts, closed connections, or 5xx errors - consider a customer getting billed multiple times, or duplicate accounts being created, or something else bad.

Again, this might not be appropriate to build in at the library level (maybe we need better primitives for this internally) but adding all this logic around our http requests leads to pretty gnarly code. 

Some kind of  `Retries` object would be nice, especially because it mirrors the `Timeout` interface and there are a lot of things you could specify if you wanted to run wild with it, say the following:
- `connection_retries`, the number of times to retry a connection error
- what i'd like to call `read_retries` or the number of times to retry in a situation where a connection **was** made to the server. a timeout, a closed connection, or an unacceptable HTTP error code, like 500
- `retry_codes`, a list of integers representing HTTP status codes, could also have named constants for `NON_200`, `5XX_ERROR`, etc, would default to 500 range
- `method_whitelist`, a list of HTTP methods to retry, defaults to HEAD, GET, PUT, DELETE, OPTIONS, TRACE, per the [spec](http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html)
- `backoff_factor`, some kind of multiplier to control how fast backoff occurs. Defaults to 0. Algorithm would be something like `backoff_factor * (2 ** (retry_attempt_number))`, so 1, 2, 4, 8...
"
259,Reference PR for wrapping HTTP(S)Connection and disabling Nagle by default,2013-10-16T09:56:11Z,2014-01-27T20:14:28Z,,,,"This is a broken branch for now, but wanted to share my (little) progress...

Related to #253 and the discussion in #254. 

Outstanding issues:
- [ ] Can't inherit from httplib classes and use super(...) because they're old-style classes.
- [ ] I'd like to reduce repeating logic by reusing it in `_new_conn` and `_prepare_conn`. This is half-done.
- [ ] Consider whether we want to vendor in more of httplib/http.client logic into urllib3. Or maybe roll our own? Doesn't seem like that much... #famouslastwords (cc @lukasa)
  - If we do roll our own, we can put it inside `urllib3.yolo`. #halfjoking
- [ ] Allow some kind of backwards compat access to `source_address` (Py26 sockets don't have it).
- ...

Anyone interested in picking this up?
"
258,support ip address typed subject alterative names when connecting to an ip address using https,2013-10-10T13:19:09Z,2016-09-12T09:11:42Z,,,,"According to rfc6125 section 3.1.3.2 subjectAltName values of type iPAddress should be considered when the https connection is made directly to an ip address rather than a dns domain name. If a server uses such a certificate, urllib3 will fail to validate it, however other tools such as curl and go does. go even refuses to consider CN when verifying the certificate when connected by the 
"
257,Initial implementation of SSL peer certificates custom verifiers,2013-10-09T08:40:34Z,,,,,"There were some talks and ideas to generalize peer certificate validation, and not limit to fingerprint and hostname validations.
This is my first attempt to address this feature.
"
256,Windows testing problems,2013-10-07T08:58:06Z,2014-01-23T13:32:20Z,,AssertionError,AssertionError: 'image/pjpeg' != 'image/jpeg',"urllib3's test suite runs dog slow on Windows, and has a pair of failing tests. On my badass development machine at work, running `nosetests` takes 90 seconds to run, and provides the following output:

```
S......................F..............................................S.........
...............F..................................................
======================================================================
FAIL: test_total_timeout (test.with_dummyserver.test_connectionpool.TestConnecti
onPool)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\cb2\Documents\urllib3\env\lib\site-packages\nose\tools\nontrivi
al.py"", line 100, in newfunc
    raise TimeExpired(""Time limit (%s) exceeded"" % limit)
TimeExpired: Time limit (0.1) exceeded
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTP connection (1): 10.255.255.1
urllib3.connectionpool: INFO: Starting new HTTP connection (1): localhost
urllib3.connectionpool: DEBUG: Setting read timeout to 0
urllib3.connectionpool: INFO: Starting new HTTP connection (1): localhost
urllib3.connectionpool: DEBUG: Setting read timeout to 0
root: WARNING: Read error on 804: [Errno 10053] An established connection was ab
orted by the software in your host machine
urllib3.connectionpool: INFO: Starting new HTTP connection (1): 10.255.255.1
--------------------- >> end captured logging << ---------------------

======================================================================
FAIL: test_guess_content_type (test.test_fields.TestRequestField)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\cb2\Documents\urllib3\test\test_fields.py"", line 10, in test_gu
ess_content_type
    self.assertEqual(guess_content_type('image.jpg'), 'image/jpeg')
AssertionError: 'image/pjpeg' != 'image/jpeg'

Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Coverage.py warning: Trace function changed, measurement is likely wrong: None
Name                     Stmts   Miss  Cover   Missing
------------------------------------------------------
urllib3                     14      0   100%
urllib3._collections        43      0   100%
urllib3.connectionpool     214      0   100%
urllib3.contrib              0      0   100%
urllib3.exceptions          40      0   100%
urllib3.fields              62      1    98%   24
urllib3.filepost            32      0   100%
urllib3.poolmanager         88      0   100%
urllib3.request             25      0   100%
urllib3.response           124      0   100%
urllib3.util               169      0   100%
------------------------------------------------------
TOTAL                      811      1    99%
No handlers could be found for logger ""nose.plugins.cover""
```

I'll try to dig into this at some point when I have time, but if someone else is interested in working out why the tests are so slow (and indeed failing) they should just go right ahead.
"
255,removed deprecated BaseException,2013-10-05T17:19:39Z,2013-10-05T18:15:12Z,,,,"Removed deprecated `BaseException`
"
254,"Factor out HTTP(S)Connection -> ConnectionCls, and cleanup.",2013-10-04T15:52:41Z,2013-10-16T09:58:14Z,,,,"Also removed the need to None-ify global `ssl` module in tests.

This should address #253 for now. @HonzaKral, thoughts?

Also, @kevinburke, can you comment on the lines I commented out in `test/with_dummyserver/test_https.py`? Ctrl+f ""kevinburke"". :)
"
253,Support passing TCP_NODELAY to underlying socket,2013-10-04T14:30:56Z,2014-01-27T20:13:57Z,,,,"Many people (including me) use urllib3 (or requests) to consume an API or to write a client for potential high-volume connections. In many cases performance of this can be severly impacted by the [Nagle's Algorithm](http://en.wikipedia.org/wiki/Nagle%27s_algorithm#Negative_effect_on_non-small_writes). A solution to this is to run `.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)` on the underlying socket. This is however very hard to do with urllib3. The code to do so:

``` python
import socket
from urllib3.connectionpool import HTTPConnectionPool, HTTPConnection,\
    HTTPSConnectionPool, log as urllib3_log

class NoNagleHTTPConnection(HTTPConnection):
    def connect(self):
        super(NoNagleHTTPConnection, self).connect()
        self.sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)

class NoNagleHTTPConnectionPool(HTTPConnectionPool):
    def _new_conn(self):    
        """"""    
        Return a fresh :class:`httplib.HTTPConnection`.
        """"""    
        self.num_connections += 1      
        urllib3_log.info(""Starting new HTTP connection (%d): %s"" %
                (self.num_connections, self.host))
        return NoNagleHTTPConnection(host=self.host,
                            port=self.port,
                            strict=self.strict)
```

... and this is just when you use `HTTPConnectionPool` directly and for http only (more difficult for HTTPS).

I'd like to propose a solution to this problem, either being able to pass in a `connection_class` to be able to inject your own class (`NoNagleHTTPConnection` in my case). This could still get a little tricky with http/https.

Another possible solution is to be able to provide a callback that will be passed to the underlying connection class and called after the socket has been created.

Third solution that comes to mind is to simply support a flag (or just always specify `TCP_NODELAY`) to turn this behavior on.

I am leaning towards third but want to check with people before I do anything - I am more than happy to work on the solution since I use urllib3 as the default transport for the new [Elasticsearch client](https://github.com/elasticsearch/elasticsearch-py) - needless to say that performance is crucial for us there.
"
252,TimeoutErrors should cause retries.,2013-10-03T21:06:12Z,2013-10-16T21:19:03Z,,,,"This contains the fix for #251 and a very very broken test. Submitted for your perusal and to prevent my head exploding.
"
251,max_retries does not appear to apply to timeout,2013-10-03T17:19:01Z,2013-10-16T21:20:31Z,,,,"This server

```
import time

request = 0

def application(environ, start):
    start(""200 OK"", [])
    global request
    request += 1
    if request >= 5:
        return [""GOODBYE\n""]
    time.sleep(1)
    return [""HELLO\n""]
```

seems to fails immediately.

(Warning: I tested this via requests, though requests appears to be just passing it along. I'm sure you get this kind of bug report a lot -- so in advance, I have not done any work trying to tear their integration apart, it would appear that they're just passing it along).
"
250,pyopenssl: wait for data before handshake retry,2013-10-01T19:56:14Z,2013-10-01T20:12:16Z,,,,"Affects requests made to SSL servers running in the same thread via green
threads.

http://stackoverflow.com/questions/19109436/gevent-ssl-wsgiserver-blocks-when-it-shouldnt

`select` is not available on AppEngine, but fortunately _cough_ AppEngine doesn't support C extensions and therefore PyOpenSSL anyways.
"
249,Release a new version compatible with requests-2.0.0,2013-09-25T13:23:49Z,2013-09-25T16:09:36Z,,,,"The latest release of requests (version 2.0.0) bundles changes to urllib3 that don't appear in the urllib3-1.7 tarball on PyPI.

When possible, can you cut a new release of urllib3 to match it up?
"
248,support source_address when create HTTPConnection in python 2.7,2013-09-24T17:40:13Z,2013-09-24T18:38:52Z,,,,"Python 2.6, socket.create_connection(host,port,timeout),
in Py 2.7, socket.create_connection(host,port,timeout,source_address) which allow the user to specify the source address when the host has multiple interfaces can reach the same destination.

urllib3 using httplib.HTTPConnection() which allow caller to specify source_address.

Can urllib3 support the source_address in 2.7, and ideally can support 2.6 with some patches in py stdlib.
"
247,Do not shift retries counter on fails from stale connections,2013-09-13T20:59:33Z,2014-06-23T18:45:22Z,,,,"See https://github.com/shazow/urllib3/issues/245 for details.
"
246,Also call ReadTimeout on sockets that receive an EAGAIN,2013-09-12T17:18:47Z,2013-09-15T10:10:44Z,,,,"In the event the socket timeout is set to zero, and recv() is called on it, the
socket will raise a socket.error with the errno set to errno.EAGAIN. To us this
is a timeout because there's no time left for the read to complete.

This adds an extra exception handler which checks specifically for EAGAIN as
a socket error and raises it as a ReadTimeout if it is received.

Also adds tests that cover this case
"
245,Max retries exceeded exception on stale pooled connections,2013-09-12T08:31:34Z,2013-10-16T18:13:05Z,,ConnectionError,"ConnectionError: HTTPConnectionPool(host='node-testing.sugarlabs.org', port=80): Max retries exceeded with url: //packages/OLPC/11.3.1/poppler-data-0.4.4-1.fc14.noarch.rpm (Caused by <class 'httplib.BadStatusLine'>: '')","The application code uses the same requests.Session object from requests library (thus, urllib3 at the end) to download bunch of files. In rare cases, downloading fails with the following trace (here, there is one successful download and one failed for the same Session object):

<pre>
2013-09-08 22:57:38,553 INFO root: Download 'http://node-testing.sugarlabs.org//packages/OLPC/11.3.1/ghostscript-8.71-16.fc14.i686.rpm' package
2013-09-08 22:57:38,754 DEBUG requests.packages.urllib3.connectionpool: ""GET //packages/OLPC/11.3.1/ghostscript-8.71-16.fc14.i686.rpm HTTP/1.1"" 200 None
2013-09-08 22:58:23,276 INFO root: Download 'http://node-testing.sugarlabs.org//packages/OLPC/11.3.1/poppler-data-0.4.4-1.fc14.noarch.rpm' package
2013-09-08 22:58:23,442 ERROR root: Failed to install
Traceback (most recent call last):
  File ""/usr/share/PackageKit/helpers/presolve/presolveBackend.py"", line 65, in _install_packages
    presolve.install(backend.split_package_id(i)[0])
  File ""/usr/share/PackageKit/helpers/presolve/presolve.py"", line 133, in install
    _install_bundles(_load_bundles(package))
  File ""/usr/share/PackageKit/helpers/presolve/presolve.py"", line 291, in _load_bundles
    _fetcher.download(url, bundle)
  File ""/usr/lib/python2.7/site-packages/sugar_network/toolkit/http.py"", line 161, in download
    reply = self.request('GET', path, allow_redirects=True)
  File ""/usr/lib/python2.7/site-packages/sugar_network/toolkit/http.py"", line 206, in request
    headers=headers, params=params, **kwargs)
  File ""/usr/lib/python2.7/site-packages/sugar_network/toolkit/../lib/requests/requests/sessions.py"", line 335, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/lib/python2.7/site-packages/sugar_network/toolkit/../lib/requests/requests/sessions.py"", line 438, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.7/site-packages/sugar_network/toolkit/../lib/requests/requests/adapters.py"", line 327, in send
    raise ConnectionError(e)
ConnectionError: HTTPConnectionPool(host='node-testing.sugarlabs.org', port=80): Max retries exceeded with url: //packages/OLPC/11.3.1/poppler-data-0.4.4-1.fc14.noarch.rpm (Caused by <class 'httplib.BadStatusLine'>: '')
</pre>


The server side was not heavy loaded and runing wget for the same urls was not capable to reproduce the issue. But in logs, I found that application generates bunch of entries like

<pre>
Resetting dropped connection..
</pre>


So, the guess was that urllib3 tries to reuse pooled http connection which is arranged to be closed on server side or so. After the applying the fix

<pre>
diff --git a/requests/packages/urllib3/connectionpool.py b/requests/packages/urllib3/connectionpool.py
index 4e851f0..c7dfa05 100644
--- a/requests/packages/urllib3/connectionpool.py
+++ b/requests/packages/urllib3/connectionpool.py
@@ -331,7 +331,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
 
     def urlopen(self, method, url, body=None, headers=None, retries=3,
                 redirect=True, assert_same_host=True, timeout=_Default,
-                pool_timeout=None, release_conn=None, **response_kw):
+                pool_timeout=None, release_conn=None, force_new=False, **response_kw):
         """"""
         Get a connection from the pool and perform an HTTP request. This is the
         lowest level call for making a request, so you'll need to specify all
@@ -420,7 +420,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
 
         try:
             # Request a connection from the queue
-            conn = self._get_conn(timeout=pool_timeout)
+            conn = self._new_conn() if force_new else self._get_conn(timeout=pool_timeout)
 
             # Make the request on the httplib connection object
             httplib_response = self._make_request(conn, method, url,
@@ -471,7 +471,10 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
             err = e
 
             if retries == 0:
-                raise MaxRetryError(self, url, e)
+                if force_new:
+                    raise MaxRetryError(self, url, e)
+                force_new = True
+                retries = 1
 
         finally:
             if release_conn:
@@ -487,7 +490,7 @@ class HTTPConnectionPool(ConnectionPool, RequestMethods):
             return self.urlopen(method, url, body, headers, retries - 1,
                                 redirect, assert_same_host,
                                 timeout=timeout, pool_timeout=pool_timeout,
-                                release_conn=release_conn, **response_kw)
+                                release_conn=release_conn, force_new=force_new, **response_kw)
 
         # Handle redirect?
         redirect_location = redirect and response.get_redirect_location()
</pre>
"
244,Timeout test flakey,2013-09-11T20:06:22Z,2013-10-18T11:08:50Z,,,,"Looks like our new Timeout class and its respective test is failing about once every 4 runs.

My output on OSX 10.6 with Python 2.7.2:

```
ERROR: test_total_timeout (test.with_dummyserver.test_connectionpool.TestConnectionPool)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/shazow/env/urllib3/lib/python2.7/site-packages/nose/tools/nontrivial.py"", line 97, in newfunc
    result = func(*arg, **kw)
  File ""/Users/shazow/projects/urllib3/test/with_dummyserver/test_connectionpool.py"", line 188, in test_total_timeout
    self.assertRaises(ReadTimeoutError, pool._make_request, conn, 'GET', url)
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/unittest/case.py"", line 471, in assertRaises
    callableObj(*args, **kwargs)
  File ""/Users/shazow/projects/urllib3/urllib3/connectionpool.py"", line 381, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/httplib.py"", line 1027, in getresponse
    response.begin()
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/httplib.py"", line 407, in begin
    version, status, reason = self._read_status()
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/httplib.py"", line 365, in _read_status
    line = self.fp.readline()
  File ""/usr/local/Cellar/python/2.7.2/lib/python2.7/socket.py"", line 447, in readline
    data = self._sock.recv(self._rbufsize)
error: [Errno 35] Resource temporarily unavailable
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTP connection (1): 10.255.255.1
urllib3.connectionpool: INFO: Starting new HTTP connection (1): localhost
urllib3.connectionpool: DEBUG: Setting read timeout to 0
--------------------- >> end captured logging << ---------------------
```

@kevinburke Could you take a look at it please? :)
"
243,Support redirect on 308,2013-09-11T18:43:01Z,2013-09-11T20:03:57Z,,,,"As http://greenbytes.de/tech/webdav/draft-reschke-http-status-308-07.html has
been accepted by IESG (but is still waiting for official publication).
"
242,Allow custom authentication (in particular NTLM) to proxies,2013-09-07T21:38:41Z,,,,,"(Upstreamed from https://github.com/kennethreitz/requests/issues/1582 )

While Requests allows custom authentication (any class implementing AuthBase) to content servers, it only supports Basic auth to proxies. It would be great to support custom authentication for proxies too.

In particular, NTLM proxies are common in Windows corporate networks.  Python development can't get off the ground in my office because the package manager falls over at the proxy.

I note  urllib3's ProxyManager can only do Basic authentication, and not any other kind. https://github.com/shazow/urllib3/blob/master/urllib3/poolmanager.py#L174
## 
"
241,Authentication with large POSTs not working (with aggressive servers),2013-09-06T10:38:32Z,2014-07-03T23:05:22Z,,,,"When sending a large (multipart) POST request to a server requiring digest access authentication the ""401 Unauthorized"" response might be received and the connection terminated before the HTTPConnection.request() in HTTPConnectionPool._make_request() has returned causing the underlying socket routines to fail with EPIPE.

For more details, see:
https://github.com/kennethreitz/requests/issues/1574
"
240,RFC: Introduce SSLHandshakeError,2013-09-04T18:02:31Z,2014-07-03T23:05:41Z,,,,"_DO NOT MERGE_ (yet)

For the complete rationale see kennethreitz/requests#1570 and linked issues.

Use a dedicated error class to indicate that the SSL handshake failed for technical reasons.
The code is a bit repetitive.
The other possibility would be to trigger the handshake in `connectionpool.py` which would require less duplication.
Keeping the actual handshake inside `ssl_wrap_socket` consistent with the current behaviour.

Another possibility would be to wrap the `ssl_wrap_socket` function inside `utils.py`
"
239,Ignore strict Connection param in Py3,2013-09-04T15:07:51Z,2013-09-04T15:11:12Z,,,," Fixes #238.
"
238,Python 3 HTTPClient doesn't support strict flag,2013-09-04T03:53:14Z,2013-09-04T15:11:12Z,,,,"The connectionpool.py module passes strict=<bool> to (http.client.) HTTPClient, but this parameter is no longer supported in Python 3. It was deprecated in Python 3.2 (or maybe earlier) and in 3.4 it is gone. Please fix this before Python 3.4 is released.
"
237,ConnectionPool example in urllib3 docs doesn't work,2013-09-03T22:03:51Z,2013-09-04T02:01:00Z,,,,"1. pip install urllib3
2. Run this:
   
   ```
   import urllib3
   conn = urllib3.connection_from_url('http://google.com')
   r1 = conn.request('GET', 'http://google.com/')
   #or
   r2 = conn.request('GET', '/mail')
   ```

Gives this error:

```
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/home/x/.virtualenvs/search/lib/python2.7/site-packages/urllib3/request.py"", line 75, in request
        **urlopen_kw)
      File ""/home/x/.virtualenvs/search/lib/python2.7/site-packages/urllib3/request.py"", line 88, in request_encode_url
        return self.urlopen(method, url, **urlopen_kw)
      File ""/home/x/.virtualenvs/search/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 530, in urlopen
        release_conn=release_conn, **response_kw)
      File ""/home/x/.virtualenvs/search/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 442, in urlopen
        raise HostChangedError(self, url, retries - 1)
    urllib3.exceptions.HostChangedError: HTTPConnectionPool(host='google.com', port=None): Tried to open a foreign host with url: http://www.google.com/mail
```

Link:http://urllib3.readthedocs.org/en/latest/#connectionpool
"
236,"Headers are always lower case, regardless of how they come on the wire.",2013-08-31T05:50:51Z,2017-01-07T11:14:36Z,,,,"Initially shown by kennethreitz/requests#1561. Is this intended behaviour, or are you open to me changing that to maintain case?
## 
"
235,don't raise ProxyError in VerifiedHTTPSConnection,2013-08-30T14:40:50Z,2013-08-30T16:13:38Z,,,,"Reverts a part of 51d485a (introduced in #221)

Before this path SocketErrors were always wrapped in ProxyErrors.
This should only happen if the connection is to a proxy.
Just remove the check as it will happen anyway in HTTPConnectionPool.url_open
"
234,avoid calling 'close' on a None object,2013-08-29T05:10:56Z,2013-08-29T13:27:56Z,,,,"#### Ran into this in the field when used by the pyes client:

Unfortunately I am unable to give exact details on how to replicate this behavior at this time.
It appears that when the ES server is under heavy load that there are times when the `conn` variable is not initialized with a HTTPConnection and then the code attempts to 'close' it when it is put back in the pool.
This changeset merely validates that the `conn` variable passed in to `def _put_conn` is `truthy` before calling it's `close` method.

```
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/pyes/es.py"", line 1171, in force_bulk 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] return self.flush_bulk(True) 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/pyes/es.py"", line 1163, in flush_bulk 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] return self.bulker.flush_bulk(forced) 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/pyes/es.py"", line 305, in flush_bulk 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] ""\\n"".join(batch) + ""\\n"") 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/pyes/es.py"", line 583, in _send_request 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] response = self.connection.execute(request) 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/pyes/connection_http.py"", line 94, in execute 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] response = conn.urlopen(**kwargs) 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/urllib3/connectionpool.py"", line 463, in urlopen 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] self._put_conn(conn) 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] File ""/usr/lib/python2.6/site-packages/urllib3/connectionpool.py"", line 253, in _put_conn 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] conn.close() 
[Wed Aug 28 15:51:32 2013] [error] [client 103.8.52.41] AttributeError: 'NoneType' object has no attribute 'close' 
```
"
233,RFC: Fix WantReadError when using pyOpenSSL and Timeouts,2013-08-27T17:32:15Z,2013-08-27T19:40:55Z,,,,"Fixes the issues mentioned in kennethreitz/requests#749

Prevent WantReadError by using a fileobject which correctly handles those.
socket._fileobject which was previously used only handled builtin `EINTR` which
are is the corresponding error from the stdlib to WantReadError.
The new implementation inherits from socket._fileobject overwriting read() and
readline(). The overwritten methods are taken straight from CPython 2.7.5 and
modified as follows in the relevant places:

```
-except error, e:
-    if e.args[0] == EINTR:
-        continue
-    raise
+except OpenSSL.SSL.WantReadError:
+    continue
```

urllib3 itself only needs `readline()` overwritten, but requests also needs
`read()`.
The handshake could have been handled automatically by writes and reads, but
when requesting the certificate directly after connection setup an error is
thrown.
"
232,Proxy autodetection on Windows,2013-08-25T05:42:51Z,2013-08-25T05:48:28Z,,,,"Hi, It would be nice if urllib3 and subsequently requests could support automatic proxy detection on Windows.

Both urllib2 and urllib accomplish this by reading windows registry via urllib.getproxies()

It would help me debug using Fiddler.
"
231,Add a Timeout class,2013-08-18T04:24:55Z,2013-09-11T19:52:20Z,,,,"You can now specify Timeout values with 

``` python
from urllib3 import util
urlopen(timeout=util.Timeout(connect=5, read=20))
```

in addition to the usual float/int ways of specifying it.

Also adds a test case for the old and the new way of specifying a timeout.
"
230,"Add a Makefile with convenience install, test, clean commands",2013-08-18T02:57:08Z,2013-10-18T11:14:01Z,,,,"This lets developers install urllib3 locally by running ""make install"" and to run the tests by running ""make test"".

I also had to add two packages to test-requirements.txt to get the requirements file working on my local machine (a Mac running 10.8, and Homebrew python 2.7). I'm possibly missing something in the way the tests should be run however.
"
229,AttributeError thrown in connectionpool.py,2013-08-17T11:32:25Z,2013-08-17T18:14:39Z,,,,"While using Requests' post function (requests.post(<some url>)),
This following exception was thrown:

> File ""C:\Python27\lib\site-packages\requests\api.py"", line 88, in post
> return request('post', url, data=data, *_kwargs)
> File ""C:\Python27\lib\site-packages\requests\api.py"", line 44, in request
> return session.request(method=method, url=url, *_kwargs)
> File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 335, in request
> resp = self.send(prep, *_send_kwargs)
> File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 438, in send
> r = adapter.send(request, *_kwargs)
> File ""C:\Python27\lib\site-packages\requests\adapters.py"", line 272, in send
> conn = self.get_connection(request.url, proxies)
> File ""C:\Python27\lib\site-packages\requests\adapters.py"", line 199, in get_connection
> conn = self.poolmanager.connection_from_url(url)
> File ""C:\Python27\lib\site-packages\requests\packages\urllib3\poolmanager.py"", line 123, in connection_from_url
> return self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
> File ""C:\Python27\lib\site-packages\requests\packages\urllib3\poolmanager.py"", line 109, in connection_from_host
> pool = self._new_pool(scheme, host, port)
> File ""C:\Python27\lib\site-packages\requests\packages\urllib3\poolmanager.py"", line 79, in _new_pool
> return pool_cls(host, port, **kwargs)
> File ""C:\Python27\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 186, in __init__
> self.pool = self.QueueCls(maxsize)
> File ""C:\Python27\Lib\Queue.py"", line 35, in **init**
> self.mutex = _threading.Lock()
> AttributeError: 'NoneType' object has no attribute 'Lock'

LifoQueue inherits from Queue, which uses _threading (imported in Queue.py).
Because the Queue module isn't explicitly imported, _threading does not exist.

connectionpool.py, imports:
from Queue import LifoQueue, Empty, Full
import Queue # Duct tape solution
"
228,Small tweaks to the test setup/docs,2013-08-14T21:43:49Z,2013-08-14T21:50:07Z,,,,"- Erasing coverage data before each run prevents the problems we faced initially on travis about coverage data being cached
- Statement about py3.2 being tested was removed in december. Testsuite disagrees :-)
- Contributors should use `tox` to streamline testing
"
227,match_hostname() doesn't implement RFC 6125 wildcard matching rules,2013-08-12T17:00:56Z,2013-11-02T19:22:54Z,,,,"Please see http://bugs.python.org/issue17997 for more information and a preliminary patch
"
226,Sane solution to automatic port allocation.,2013-08-07T18:25:36Z,2013-08-07T18:35:30Z,,,,
225,Remove need for `time.sleep(0.1)` in test setUpClass,2013-08-07T02:04:27Z,2013-08-07T18:36:48Z,,,,"https://github.com/shazow/urllib3/blob/master/dummyserver/testcase.py#L60
https://github.com/shazow/urllib3/blob/master/dummyserver/testcase.py#L113

If the sleeps aren't there, tests randomly fail. Need some way to block until the server is ready. Maybe there's something in the Tornado API? Or maybe we need some socket loop waiting for the server...
"
224,Unify indentation to 4 spaces.,2013-08-07T01:34:45Z,2013-08-07T01:36:52Z,,,,"As requested per https://github.com/shazow/urllib3/issues/120#issuecomment-22224010
"
223,Handle non-ascii headers in multipart/form-data,2013-08-07T00:41:17Z,2013-08-07T16:15:08Z,,,,"Second stab at shazow/urllib3#119, after I left #121 lying around for far too long. Now this is integrated with the RequestField infrastructure. If you want me to, I can add another test for the overall structure of a non-ASCII form submission later on. Basically copy gagern/urllib3@71f0ed22cda47dc80495ca10479564d07de5c933 here.
"
222,Close socket used for port allocation.,2013-08-06T16:48:05Z,2013-08-07T18:26:16Z,,,,"I have seen some strange issues with such half-opened sockets, e.g. on Windows where you can't use SO_REUSEADDR.
"
221,Raise special error if connection to proxy fails,2013-08-06T16:43:26Z,2013-08-06T17:47:01Z,,,,
220,Add request params,2013-07-29T21:48:11Z,2013-08-01T22:24:56Z,,,,"Creating pull request to track the conversation we're having on this.

@shazow, can you clarify what you meant by ""IMO it should be a member of the RequestParam class, as per the example in my comment.""? Which example are you referring to?
"
219,Added units to the doc for timeout,2013-07-29T06:45:45Z,2013-07-29T06:47:23Z,,,,"timeout is specified in seconds, but in the doc, it was unclear.
"
218,Url netloc,2013-07-28T19:28:23Z,2013-07-28T22:39:06Z,,,,"For rationale see #217
Introduces `urllib3.util.Url.netloc`.
Also removes superfluous code.

@gavrie You can rebase your PR against this one, or I could do this. Whatever you prefer.
"
217,Correctly set Host header when using a proxy,2013-07-28T15:08:39Z,2013-07-29T20:27:28Z,,,,"Fixed a bug in _set_proxy_headers that ignores the port when setting the
'Host' HTTP header while a proxy is used.
"
216,Be able to retrieve Peer SSL certificate from an HTTPResponse,2013-07-27T21:57:32Z,2013-09-26T09:33:46Z,,,,"I'm using python requests lib to create a request to an https/ssl server. Internally, it uses urllib3. In my specific use case, I need not to verify the server's certificate CA, but to store the ssl certificate itself to be able to check later in other connections that it's still using the same one.

This is not possible to do easily with urllib3 because the HTTPResponse class from response.py does not expose this information, specially after release_conn() has been called (before, one could still get that using response._connection.sock.getpeercert(binary_form=True)), but I can't do that as an user of requests library.

I propose to add the patched lines in the constructor of HTTPResponse so that the certificate will be always retrievable in an ssl req using ""request.peer_cert"".
"
215,Can't set headers per part of multipart requests,2013-07-25T21:12:57Z,2013-08-01T22:31:18Z,,,,"I think it would be helpful to allow users to specify per-part headers in `encode_multipart_formdata` at https://github.com/shazow/urllib3/blob/master/urllib3/filepost.py#L42.

As it is now, I cannot specify a `Content-Location` header per part, which makes it impossible to encapsulate subsidiary parts of aggregate documents (whose `Content-Location` can be different per part) -- see http://tools.ietf.org/html/rfc2557.

Can anyone thing of any major objects to adding this functionality here, or is this out of the scope of this library?
"
214,Close sockets from server connections.,2013-07-23T10:34:03Z,2013-07-23T18:22:23Z,,,,
213,Do not use REUSEADDR on Windows.,2013-07-23T10:28:28Z,2013-07-23T19:03:02Z,,,,
212,Made tornado test server thread safe.,2013-07-23T09:50:35Z,2013-07-23T18:23:29Z,,,,"`add_callback` is the only thread-safe operation.
"
211,Only invoke decoder if decode_content parameter is True. Resolve shazow/...,2013-07-22T12:59:58Z,2013-07-29T21:57:24Z,,,,"Resolve shazow/urllib#209 
"
210,include reason for failed decoding,2013-07-21T17:17:26Z,2013-07-21T18:33:44Z,,,,
209,Possible bug in decoding,2013-07-20T21:23:46Z,2013-07-29T22:14:27Z,,,,"I was getting a stacktrace in my app today from ContextIO, which uses rauth, which uses requests, which uses urllib3:

```
  File ""/Users/rbp/Projects/Python-ContextIO/contextio/__init__.py"", line 1592, in get_content
    return StringIO.StringIO(response.read())
  File ""/Users/rbp/Projects/tengahdb/default/lib/python2.7/site-packages/requests/packages/urllib3/response.py"", line 191, in read
    buf = self._decoder.decompress(binary_type())
error: Error -2 while decompressing: inconsistent stream state
```

This was traced to a bug elsewhere, but I did notice that `get_content` was being called with  `decode_content=False`, so I developed this patch, which helped me ultimately find the root cause

--bob

```
***************
*** 191,193 ****  
!             if flush_decoder and self._decoder:
                  buf = self._decoder.decompress(binary_type())
                  data += buf + self._decoder.flush()

--- 191,193 ----
!             if flush_decoder and decode_content and self._decoder:
                  buf = self._decoder.decompress(binary_type())
                  data += buf + self._decoder.flush()
```
"
208,Use real error class as fallback for BaseSSLError,2013-07-19T19:08:06Z,2013-07-19T19:09:34Z,,,,"If BaseSSLError is None, it is not a valid exception class and when evaluating
the try-except-block where all BaseSSLErrors are meant to be catched the python
interpreter blows up because of this.
"
207,This seems to fix the IncompleteRead errors I was getting,2013-07-19T17:26:03Z,2014-06-25T17:34:21Z,,,,"I'm sometimes getting IncompleteRead errors when using twython which would bring down the connection.  Found a fix online and it seems to have solved the issue so far.

Fix came from:
http://bobrochel.blogspot.co.nz/2010/11/bad-servers-chunked-encoding-and.html
"
206,GZip decoding regression,2013-07-19T16:20:53Z,2013-10-18T11:07:31Z,,DecodeError,"DecodeError: Received response with content-encoding: gzip, but failed to decode it.","See this issue on Requests: https://github.com/kennethreitz/requests/issues/1472

This is the truncated traceback:

```
...
/project/virtualenv/lib/python2.6/requests/packages/urllib3/response.py"", line 188, in read
    ""failed to decode it."" % content_encoding)
DecodeError: Received response with content-encoding: gzip, but failed to decode it.
```

cc @schlamar
"
205,Add reason phrases to responses from dummyserver,2013-07-19T14:15:18Z,2013-07-19T17:50:40Z,,,,"fixes failing test mentioned in https://github.com/shazow/urllib3/pull/170#issuecomment-21234450
Only the reason for `303` is necessary. Reasons for 400 are only for consistency.
"
204,Fix a race condition between pools.get and pools.set,2013-07-18T17:00:29Z,2013-07-18T17:13:18Z,,,,"We have parallel threads opening multiple connections to the same destination which lead to discovery of this.

Sequence of events:
T1: pools.get results a None
T2: pools.get results a None
T1: creates a new pool with key (x,y,z)
T2: creates a new pool with key (x,y,z)
T1: adds the pool to pools container under key (x,y,z)
T2: adds the pool to pools container under key (x,y,z) replacing the pool set by T1, and as a result applying dispose_func (closing the pool)
T1: returns the now closed pool for use
T1: calls pool.urlopen and throws an exception
"
203,Allow ipv6 address in url,2013-06-30T13:28:15Z,2013-08-12T01:36:50Z,,,,"Followup for #199

Maybe it would be nicer to put all those changes into one single file.
"
202,Add missing WrappedSocket.fileno method in PyOpenSSL (issue #188),2013-06-29T12:42:16Z,2013-07-03T19:32:39Z,,,,"This fixes (at least for myself) issue #188 where a missing fileno() method in the WrappedSocket object is required later in urllib3.util method is_connection_dropped() (or more precisely by the poll object select.poll.register() method called there).

I could try to provide a test though I'm not sure where to start, here for preliminary review if you think this fix is appropriate.

From Python documentation (http://docs.python.org/2/library/select) for poll.register method:

> ...
>  fd can be either an integer, or an object with a fileno() method that returns an integer.
> ...
"
201,python 3.x  ImportError: No module named 'urllib.request'; urllib is not a package,2013-06-25T13:44:58Z,2013-06-25T16:40:59Z,,ImportError,ImportError: No module named 'urllib.request'; urllib is not a package,"code:
import urllib.request;

error:
ImportError: No module named 'urllib.request'; urllib is not a package

python version:3.3.2
os:window 8
"
200,HTTP 302 redirect not handle when use urllib3.connection_from_url(url),2013-06-23T08:43:21Z,2013-06-24T04:57:40Z,,,,"I am sorry that think it is not appropriate to provide the real example at here because I am just crawling an adult comic site :P

The situation is:
step 1.
url = 'http://xxx.com/fullimg.php?gid=54406&key=6c663b82de' 
http_pool = urllib3.connection_from_url(url)

if use browser to view above url,will 302  redirect to a url like http://55.66.169.75/image.php?f=61aa13f2c2bcabf896b585d95.............

step 2.
r=http_pool.urlopen('GET',url,headers=headers,assert_same_host=False)
or 
r=http_pool.urlopen('GET',url,headers=headers,redirect=True,assert_same_host=False)

print (r.status)  always shows 404

---

when I change to use http_pool= urllib3.PoolManager() in step 1,
r.status shows 200

So I think there is something wrong at urllib3.connection_from_url()  :)
"
199,urlparse returns brackets of IPv6 address,2013-06-21T11:13:02Z,2013-06-21T16:55:01Z,,,,"Fixes https://github.com/kennethreitz/requests/issues/1426#issuecomment-19649944

Code is kept close to the original, so it still throws a ValueError, if the
closing bracket is missing.
Using slices would have changed the behaviour.
"
198,Initial pass at stream method,2013-06-08T18:48:04Z,2013-06-17T21:07:08Z,,,,"As you requested in #186, this is my first pass at the `.stream()` method we talked about. Obviously this isn't final code, and has no tests: it was me considering ideas.

The problem here is that it assumes knowledge of the underlying file object. Right now we only call `self.fp.read()` and `self.fp.close()`, which means `fp` can be any file-like object. This method uses the knowledge of `httplib` to work out when we've actually exhausted the file-like object, which makes it harder to test.

Let me know.
"
197,contrib: pyopenssl: correctly close connections,2013-06-07T09:34:55Z,2013-06-07T18:32:03Z,,,,"fixes https://github.com/shazow/urllib3/issues/195
"
196,Add documentation for HTTPResponse `io` support,2013-06-07T05:22:54Z,2014-08-11T01:14:10Z,,,,"Reminder for @eteq re #187:

> Would you be up for adding a bit to the documentation/comments which talks about the use cases or benefits of this? That would be greatly appreciated. :)
"
195,Stack dump attempting to use SNI,2013-06-06T21:51:06Z,2013-06-07T18:32:48Z,,AttributeError,AttributeError: 'WrappedSocket' object has no attribute 'close',"SNI support was added in #156 but I'm seeing a stack dump failure when trying to use it for a GET request. I have pyopenssl, ndg-httpsclient, and pyasn1 installed and use the setup code. The same issue happens when indirectly using urllib3 via the requests library.

``` python
import urllib3
import urllib3.contrib.pyopenssl
urllib3.contrib.pyopenssl.inject_into_urllib3()
http = urllib3.PoolManager()
r = http.request('GET', 'https://payswarm.com/contexts/payswarm-v1.jsonld')
print r.status, r.data
```

```
  ...
  File "".../urllib3/connectionpool.py"", line 288, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""/usr/lib/python2.7/httplib.py"", line 1051, in getresponse
    self.close()
  File ""/usr/lib/python2.7/httplib.py"", line 780, in close
    self.sock.close()   # close it manually... there may be other refs
AttributeError: 'WrappedSocket' object has no attribute 'close'
```
"
194,Setting assert_hostname to False will disable SSL hostname verification,2013-06-06T20:12:36Z,2013-06-06T22:59:49Z,,,,"This commit enables the use of `assert_hostname=False` to disable SSL hostname verification.
"
193,Pass DontVerify singleton to assert_hostname to bypass SSL hostname verification,2013-06-06T17:27:21Z,2013-06-06T23:01:03Z,,,,"This commit adds a `DontVerify` singleton to pass via `assert_hostname` to disable SSL hostname verification.

This is the result of the discussion at #191 .
"
192,Allow to skip SSL hostname verification,2013-06-04T03:53:11Z,2013-06-07T03:18:47Z,,,,"The commit implements #191.
"
191,Add an option to disable SSL host name verification,2013-06-04T03:11:33Z,2013-06-07T10:12:59Z,,,,"When verifying a certificate, there are 2 optional tests: verifying the signature against a trusted CA and verifying the CN against the used host name.
The first test is mandatory in all cases, but the second test is _not_.

In the regular usage, where a known public server is accessed, it is mandatory to verify the CN.

On the other hand, when a custom small chain of trust is used, verifying the CN, even if it is a constant one, is completely useless.
"
190,iter_content(size=1) behaves oddly with gzip,2013-06-03T09:17:19Z,2013-06-06T17:33:48Z,,,,"If the remote side is using gzip and you specify an iter_content size of 1, then the result is None because it cannot decode anything. This would leave the person to think that if None is returned then no more data is available to read.

Perhaps this should be increased to a higher block size to compensate for the gzip, and then any left over data would be stored in a buffer var. On the next read() call, it would check if the buffer has any data from the last read() and return that first, if there was not enough data it would also do another read().

Discussed briefly here;
https://github.com/kennethreitz/requests/issues/1401#issuecomment-18829272

Thoughts?

Cal
"
189,raw.read() unexpected behavior with gzip compression - needs docs,2013-06-03T09:14:04Z,2013-06-07T00:32:04Z,,,,"Hello,

Discussed briefly here;
https://github.com/kennethreitz/requests/issues/1401#issuecomment-18829272

If you use .raw.read() on a gzip compressed request, it returns a bunch of unicode because of the compression. By default, I think it should use 'decode_content=True', as this is again a bit confusing. I can't think of any situation where we would want to read in non decoded data, or at the very least the majority of situations would not require this.

Thoughts??

Cal
"
188,Py2 + SNI gets a weird error about sock being the wrong type,2013-06-03T04:58:52Z,2013-08-07T02:06:50Z,,TypeError,"TypeError: argument must be an int, or have a fileno() method.","MacOSX + Python 2.7.5 + latest requests Traceback :

```
r = session_var.post(url, data=payload, verify=False)
  File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 377, in post
    return self.request('POST', url, data=data, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 335, in request
    resp = self.send(prep, **send_kwargs)
  File ""/usr/local/lib/python2.7/site-packages/requests/sessions.py"", line 438, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/local/lib/python2.7/site-packages/requests/adapters.py"", line 292, in send
    timeout=timeout
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 423, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 234, in _get_conn
    if conn and is_connection_dropped(conn):
  File ""/usr/local/lib/python2.7/site-packages/requests/packages/urllib3/util.py"", line 256, in is_connection_dropped
    return select([sock], [], [], 0.0)[0]
TypeError: argument must be an int, or have a fileno() method.
```

Perhaps the exception raised should be a bit more easy to read? Or am I missing something obvious.
"
187,HTTPResponse io package compatibility,2013-05-31T07:20:43Z,2013-06-06T17:45:29Z,,,,"This PR addresses issue #182.  It does so by adding a few key methods to HTTPResponse, and changes it to subclass from `io.IOBase` to fill in the rest (mostly the relevant boilerplate stuff for `io` objects that are neither writeable nor seekable).

This also adds tests to maintain 100% coverage.
"
186,Be more obvious when out of data,2013-05-28T21:24:46Z,2013-06-28T08:03:17Z,,,,"With the Twitter streaming API, if it has no tweets to send it will occasionally send the two byte string `'\x1f\x8b'`. When passed through the decompression routine in `Response.read()`, this returns an empty string, which looks the same as the completed read. You can avoid this by using large values as the argument of `Response.read()`, but that can lead to other awkward behaviour.

Do we need a way to be more explicit about the difference between no data and this case?

/c @michaelhelmick
"
185,Added redirect_history property to HTTPResponse instances,2013-05-28T13:33:38Z,2016-05-03T16:59:25Z,,,,"Hey guys, first of all, thank you for your awesome work on urllib3.

One thing I missed though was being able to figure out what were the redirects my request had to jump through before arriving at the final destination, and what was this final destination.

To solve this problem I added a new property to instances of HTTPResponse called `redirect_history`. It contains a list of tuples with each status code and redirect location sent by the servers before the request was completed. I also added some test cases to illustrate this behavior.

Please let me know if you think this useful. I'm open to any changes you might require, just let me know and I'll work on it.
"
184,small typo fix,2013-05-24T05:25:40Z,2013-05-24T05:28:04Z,,,,
183,urllib3.contrib.ntlmpool: SyntaxError with Python 3,2013-05-22T19:19:59Z,2013-05-22T21:13:44Z,,,,"Pull request from shazow/urllib3#177
"
182,Support `io` framework for HTTPResponse,2013-05-17T00:57:36Z,2013-06-06T17:45:29Z,,AttributeError,AttributeError: 'HTTPResponse' object has no attribute 'readable',"This suggestion is motivated by the following issue that pops up in `requests` (see kennethreitz/requests#1364 - they redirected me here):

```
>>> import io, requests
>>> r = requests.get('http://www.google.com',stream=True)
>>> io.BufferedReader(r.raw).read()
AttributeError: 'HTTPResponse' object has no attribute 'readable'
```

The base problem here is that `requests.packages.urllib3.response.HTTPResponse`  object (that's what `r.raw` is) does not respect the `io` API.  The following _does_ work, however:

```
>>> r = requests.get('http://www.google.com',stream=True)
>>> r.raw.readable=lambda:True
>>> r.raw.closed=False
>>> io.BufferedReader(r.raw).read()
```

Suggesting that the main thing that is neeeded is adding a `readable` method and a `closed` attribute to `HTTPResponse`.  (I think `flush` and `close` are probably also necessary, and maybe also `writable` and `seekable`).

Is there a reason this is not possible, or would it be fine to just add the relevant methods to `HTTPResponse`?
"
181,"url[scheme, host, ...] parsed in different ways in HTTPConnectionPool()",2013-05-16T14:13:33Z,2013-05-16T18:52:42Z,,urllib3.exceptions.HostChangedError,"urllib3.exceptions.HostChangedError: HTTPConnectionPool(host='http://www.google.com', port=None): Tried to open a foreign host with url: http://www.google.com","```
$python
Python 2.7.3 (default, Jul 24 2012, 10:05:38) 
[GCC 4.7.0 20120507 (Red Hat 4.7.0-5)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import urllib3, logging, httplib
>>> logging.basicConfig(level = logging.DEBUG)
>>> httplib.HTTPConnection.debuglevel = 3
>>> http = urllib3.HTTPConnectionPool('http://www.google.com')
>>> rsp = http.request('GET', 'http://www.google.com')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/root/Downloads/Fedora_thing/SELF/Workspace/simplesite/Orange-Juice/urllib3/urllib3/request.py"", line 75, in request
    **urlopen_kw)
  File ""/root/Downloads/Fedora_thing/SELF/Workspace/simplesite/Orange-Juice/urllib3/urllib3/request.py"", line 88, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/root/Downloads/Fedora_thing/SELF/Workspace/simplesite/Orange-Juice/urllib3/urllib3/connectionpool.py"", line 417, in urlopen
    raise HostChangedError(self, url, retries - 1)
urllib3.exceptions.HostChangedError: HTTPConnectionPool(host='http://www.google.com', port=None): Tried to open a foreign host with url: http://www.google.com
```

The reason is that: 
is_same_host() would parse 'http:www.google.com' as ('http', 'www.google.com'), 
however, request.**init**() would use 'http://www.google.com' as self.host, 
so 'http://www.google.com' == 'www.google.com' would be false.
And we should parse(url) first in request.**init**() to get the proper host.
"
180,Test for Support for relative urls in Location header ,2013-05-16T05:16:53Z,2013-05-17T03:29:47Z,,,,"A simple test for ""support for relative urls in Location header ""
"
179,Support for relative urls in Location header,2013-05-10T08:29:10Z,2013-05-17T03:29:49Z,,,,
178,Relative urls in 'Location:' response header.,2013-05-09T09:19:20Z,2013-05-17T03:30:08Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'encode',"Though [RFC forces use of absolute URL in Location header](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.30), real life web servers do not obey that.
For example [http://www.microsoft.com/](http://www.microsoft.com/):

<pre>
$ python3
Python 3.2.3 (default, Oct 19 2012, 20:10:41) 
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import logging
>>> logging.basicConfig(level=logging.INFO)
>>> import urllib3
>>> pm = urllib3.PoolManager()
>>> c = pm.request('GET', 'http://www.microsoft.com/', None, {'User-Agent': 'Opera/9.80 (X11; Linux x86_64) Presto/2.12.388 Version/12.15'})
INFO:urllib3.connectionpool:Starting new HTTP connection (1): www.microsoft.com
INFO:urllib3.poolmanager:Redirecting http://www.microsoft.com/ -> /en-us/default.aspx
INFO:urllib3.connectionpool:Starting new HTTP connection (1): None
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.2/dist-packages/urllib3-dev-py3.2.egg/urllib3/request.py"", line 75, in request
    **urlopen_kw)
  File ""/usr/local/lib/python3.2/dist-packages/urllib3-dev-py3.2.egg/urllib3/request.py"", line 88, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/local/lib/python3.2/dist-packages/urllib3-dev-py3.2.egg/urllib3/poolmanager.py"", line 154, in urlopen
    return self.urlopen(method, redirect_location, **kw)
  File ""/usr/local/lib/python3.2/dist-packages/urllib3-dev-py3.2.egg/urllib3/poolmanager.py"", line 142, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/usr/local/lib/python3.2/dist-packages/urllib3-dev-py3.2.egg/urllib3/connectionpool.py"", line 428, in urlopen
    body=body, headers=headers)
  File ""/usr/local/lib/python3.2/dist-packages/urllib3-dev-py3.2.egg/urllib3/connectionpool.py"", line 280, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib/python3.2/http/client.py"", line 968, in request
    self._send_request(method, url, body, headers)
  File ""/usr/lib/python3.2/http/client.py"", line 996, in _send_request
    self.putrequest(method, url, **skips)
  File ""/usr/lib/python3.2/http/client.py"", line 892, in putrequest
    host_enc = self.host.encode(""ascii"")
AttributeError: 'NoneType' object has no attribute 'encode'
</pre>


The solution I see is to use something like urllib.parse.urljoin(url, redirect_location) in urlopen function (urllib3/poolmanager.py:125).
"
177,urllib3.contrib.ntlmpool: SyntaxError with Python 3,2013-04-28T08:05:07Z,2013-05-22T21:13:48Z,,SyntaxError,SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 130-131: truncated \uXXXX escape,"```
$ python3.3 -c 'import urllib3.contrib.ntlmpool'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""./urllib3/contrib/ntlmpool.py"", line 38
    """"""
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 130-131: truncated \uXXXX escape
```
"
176,Make RequestException unpickleable,2013-04-27T19:49:34Z,2013-04-27T21:09:43Z,,,,"And provide complete cycle (pickle/unpickle) tests.
"
175,Add RequestError subclass to PoolError.,2013-04-27T02:40:03Z,2013-04-27T17:21:36Z,,,,"The former PoolError `__reduce__` method was broken as it assumed the `url` attribute was set, when in fact it was not.

As per issue #174, this commit creates a `RequestError` subclass of `PoolError` and makes `MaxRetryError` and `TimeoutError` a subclass of `RequestError`.
"
174,TimeoutError is not pickleable,2013-04-27T01:14:40Z,2013-04-27T17:22:06Z,,AttributeError,AttributeError: 'TimeoutError' object has no attribute 'url',"When attempting to pickle TimeoutErrors, I receive the following:

``` python
    cPickle.dumps(exc)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/exceptions.py"", line 23, in __reduce__
    return self.__class__, (None, self.url)
AttributeError: 'TimeoutError' object has no attribute 'url'
```
"
173,restructure ssl tests,2013-04-05T13:31:52Z,2013-04-05T14:10:14Z,,,,"fixes #169
obsoletes #172
"
172,Fixes TLSv1 tests on Windows.,2013-04-05T12:02:38Z,2013-04-05T14:11:56Z,,,,
171,Fixes possible deadlock in threads.,2013-04-05T11:18:19Z,2013-04-05T14:13:45Z,,,,"If you don't wait for the thread to be finished, there could be two tornado threads active at one time, which results occasionally in a deadlock because they share the same IOLoop, see https://github.com/shazow/urllib3/issues/169.  
"
170,Clean rebased version of https-proxy,2013-04-05T09:46:51Z,2013-08-01T17:34:51Z,,,,"This is a cleaned up version of #139 rebased against current master.

Current todo:

**All discussions in the diff**
- [x] proxy\* private: https://github.com/shazow/urllib3/pull/170#discussion-diff-4087371
- [x] ProxyManager API https://github.com/shazow/urllib3/pull/170#discussion-diff-4087377
- [x] PoolManager attributes: https://github.com/shazow/urllib3/pull/170#discussion-diff-4087384
- [x] extract connection related stuff: https://github.com/shazow/urllib3/pull/170#discussion-diff-4087386
- [x] PEP8: https://github.com/shazow/urllib3/pull/170#discussion-diff-4087390
- [x] comment: https://github.com/shazow/urllib3/pull/170#pullrequestreviewcomment-5512536
- [x] style: https://github.com/shazow/urllib3/pull/170#pullrequestreviewcomment-5512583

**Other**
- [x] #139 Bug if SSL is not available.
- [x] #170 TODO comments. I think only cleaning them up is relevant here, proxy auth can be addressed separately. The best would be to create an issue for it so it won't be forgotten :)
- [x] #170 Failing tests on Windows.
- [x] https://github.com/shazow/urllib3/pull/170#issuecomment-21345677 Address in use on Travis
"
169,TestHTTPS_TLSv1 fail on Windows ,2013-04-05T07:20:56Z,2013-04-05T15:10:19Z,,,,"Log: https://gist.github.com/schlamar/fba8c5682756da509a6f
"
168,Content-Type header values should be native strings,2013-04-02T19:27:50Z,2013-04-02T20:07:55Z,,,,"In response to the discussion kennethreitz/requests#1252, this PR would ensure that any calculated header key is returned as a native string.

Obviously, feel free to code review this. Let me know if you want me to go about this in a different way, or anything. =)
"
167,"On some https sites, urllib3/requests latest github version hangs",2013-03-25T17:37:11Z,2013-04-25T18:55:45Z,,,,"I installed Commit 7f9d8ee3f5 on python 27 and then tried some https sites with SNI. It seems to be hanging. Is SNI fixed on python2 yet, or was it only for python3?
"
166,Fix coverage for Windows.,2013-03-25T12:30:43Z,2013-03-25T14:54:46Z,,,,
165,Added Travis status to README.,2013-03-25T12:19:43Z,2013-03-25T12:20:26Z,,,,
164,Support for @ in proxy password and username,2013-03-22T11:16:43Z,2013-03-24T15:01:31Z,,,,"The only change(which makes a (BIG) difference) is on the line 140.
which now splits the `url` from the last `@` to get auth and proxy.
##### Effect of the change:

If the proxy is `'http://abc:abc@123@abc.com:80'`,
Currently, it is parsed as:
- auth => `abc:abc`
  - user => `abc`
  - password => `abc`
- url => `123@abc.com:80`
  - host => `123@abc.com` >>> Causes Error!
  - port => `80`

After my edit, it is parsed as:
- auth => `abc:abc@123`
  - user => `abc`
  - password => `abc@123`
- url => `abc.com:80`
  - host => `abc.com` >>> NO Error! Yay!!
  - port => `80`
"
163,Enforce 100% code coverage when testing,2013-03-21T16:56:54Z,2013-03-21T19:18:38Z,,,,
162,add fingerprint support to pyopenssl.py,2013-03-20T20:01:26Z,2013-03-20T20:30:14Z,,,,"preparation for #140
"
161,test all the versions,2013-03-20T16:49:45Z,2013-03-20T16:59:38Z,,,,
160,Turn down INFO log messages,2013-03-20T14:08:51Z,2013-03-20T15:21:55Z,,,,"There are a number of log messages set at INFO (""Starting new HTTPS connection"" being the one I noticed first) that seem (to me) to be closer to DEBUG than INFO? Looking through our logs we have a lot of extraneous INFO messages - most of which are urllib3. Is there any chance of turning them down?
"
159,Support streaming decompression of HTTPResponse.,2013-03-19T12:39:16Z,2013-04-09T17:09:41Z,,,,"This PR will change HTTPResponse.read to support chunked reading (with `amt` parameter) in combination with `decode_content`. This is quite useful if you want HTTPResponse to be a decompressed file-like object. 

Additionally, it exposes `HTTPResponse._decode_content` as public member so you can control the decompression behavior after creation. This will allow request's `response.raw` to be a decompressed file-like object by simply setting `response.raw.decode_content = True`.

The streaming decompression logic is a merge of the previous logic and `requests.utils.stream_decompress`.
"
158,connection pool does not release connection on http error with preload_content=False,2013-03-18T09:57:38Z,2013-03-18T16:38:07Z,,,,"``` python
import urllib3
cp = urllib3.HTTPConnectionPool('some-error-site.org', 80, maxsize=1, block=True)

try:
    # this line will raise an http exception
    r = cp.urlopen('GET', '/', timeout=1, pool_timeout=0, preload_content=False)
except Exception as e:
    print e

# here the cp.pool is empty! so no more requests can handle on this pool
# below line will raise an EmptyPoolError
r2 = cp.urlopen('GET', '/', timeout=1, pool_timeout=0, preload_content=False)
```
"
157,API doesn't let user choose the kind of LifoQueue to be used in connectionpool.py,2013-03-13T12:08:50Z,2013-03-15T00:47:30Z,,,,"For my purposes I replaced
    from Queue import LifoQueue, Empty, Full
by
    from gevent.queue import LifoQueue, Empty, Full
in connectionpool.py because it deadlocks otherwise (when using gevent).

I don't want to have to modify the library code to achieve this.
"
156,PyOpenSSL for SNI-support on Python2,2013-03-12T07:05:03Z,2013-03-20T19:26:50Z,,,,"This adds optional SNI-support for Python2, if pyOpenSSL, pyasn1 and ng-httpsclient are installed (pyasn1 and ng-httpsclient are used to parse the subjectAltName of certificates).
"
155,Speed up tests on Windows,2013-03-01T07:11:21Z,2013-03-01T18:07:16Z,,,,"This reduces the test duration from ~13 seconds to ~7 seconds on Windows (Python 2.7).
"
154,Prevent passing SSL keywoards to HTTP connection,2013-02-27T20:22:38Z,2013-03-25T14:28:04Z,,,,"See #153.
"
153,PoolManager with SSL settings crashes on HTTP connection,2013-02-27T06:57:11Z,2013-03-25T15:17:06Z,,TypeError,TypeError: __init__() got an unexpected keyword argument 'ca_certs',"A PoolManager created with SSL related arguments passes them to HTTP connections, too. This results in a crash:

```
>>> from urllib3 import PoolManager
>>> http = PoolManager(ca_certs='REQUIRED')
>>> http.request('GET', 'https://google.com')
<urllib3.response.HTTPResponse object at 0x02D0B730>
>>> http.request('GET', 'http://google.com')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""urllib3\request.py"", line 75, in request
    **urlopen_kw)
  File ""urllib3\request.py"", line 88, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""urllib3\poolmanager.py"", line 126, in urlopen
    conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
  File ""urllib3\poolmanager.py"", line 100, in connection_from_host
    pool = self._new_pool(scheme, host, port)
  File ""urllib3\poolmanager.py"", line 70, in _new_pool
    return pool_cls(host, port, **self.connection_pool_kw)
TypeError: __init__() got an unexpected keyword argument 'ca_certs'
```
"
152,Python 2.7.3rc2 fails to import urllib3,2013-02-19T14:03:22Z,2013-04-25T18:54:54Z,,,,"On a system with Python 2.7.3rc2 I am unable to import urllib3:

user@box$ python -V
Python 2.7.3rc2
user@box$ python -c ""import urllib3""
Aborted
user@box$ 

I've tried installing urllib3 from source (via 'python setup.py install') and via pip ('pip install urllib3').
"
151,PoolManager urlopen() redirect at most once (issue #149),2013-02-19T06:47:10Z,2013-04-25T18:43:54Z,,,,"This pull request fixes the PoolManager urlopen() redirect at most once issue (#149).

Summary of changes:
1. Add a test case for server redirecting twice (with the dummy server);
2. Set argument _redirect_ for recursive urlopen() call to the value of the passed in param.

New test passed with Python 2.6 on 32bit Debian 6. Since the fix is just a 1-liner, I think it's safe to assume it works on other platforms.
"
150,Default CERT_NONE and other ssl imports,2013-02-19T03:27:26Z,2013-04-25T18:57:53Z,,,,"For platforms without ssl, we need to make sure we have sane defaults when we import things. Specifically util.py

See: http://stackoverflow.com/questions/14919557/how-to-get-urllib3-and-requests-working-with-jython-2-7-beta-1
"
149,PoolManager only redirect once,2013-02-18T10:26:46Z,2013-04-25T18:45:39Z,,,,"In PoolManager.urlopen() method, _redirect_ was set as _False_ in the _kw_ dict. If the first response is a redirection, the same _kw_ is used to invoke a subsequence, **self**.urlopen(), which disables further redirections.

Add a line:

``` python
  __kw['redirect'] = redirect
```

in front of the recursive self.urlopen() should fix it.
"
148,Does urllib3 support oauth ?,2013-02-15T13:34:19Z,2013-02-19T02:43:07Z,,,,"Hi there,

I'm using issue since I couldn't find any other mean to contact oyu (ML or IRC). I was wondering if urllib3 support oauth ? more specifically for twitter api v1.1. I've googled but no answer, also i'm newbie to oauth so i'm might ve missed some obvious tricks...

That bein siad, urllib3 is awesone, thanks for your hard work on it !

Best,
Antoine
"
147,Fix pypy tests,2013-02-02T01:27:54Z,2013-02-09T02:25:11Z,,,,"This makes all tests pass on pypy2.0.0-beta1
On 1.9, the current stable version there is still one SSLError, which should be fixed by my other PR.
"
146,Generate 'Host' header with proxies.,2013-02-01T20:38:14Z,2013-02-09T18:39:54Z,,,,"This PR is intended to resolve a problem in Requests: specifically, kennethreitz/requests#1053. It seems like there are a few proxy servers that really don't like the absence of a Host header, so this PR will add one. Let me know what you think.
"
145,"Add ""connection timeout"" feature in addition to operation timeout.",2013-02-01T18:05:08Z,2013-08-23T16:07:36Z,Feedback Pending,,,"Hi!

I'd like to suggest a new feature - two types of timeouts.
Regular one - happens when we wait for response from server on existing connection.
And a new one - connection timeout. Raised only when we try to create a new socket connection.
I added new HTTPConnectionPool.**init** parameter - connect_timeout and two new exceptions ConnectionTimeout and OperationTimeout.
## 

But why anybody would need such a thing?

It can be useful when a call to some HTTP API takes a long time to answer the query (for ex. reading lots of data), but not so much to connect to the server.
With this new feature we can set bigger timeout (or even None which blocks until some data is read) and smaller connect_timeout, so network failures, dead API servers or DNS problems could be detected faster.

And thank you so much for the great library!
## 

Pasha Kirichenko
"
144,Check for a blank host while opening a connection,2013-02-01T03:00:16Z,2014-06-24T23:30:24Z,,,,"Because we can't open a http(s) connection to host, which equals to None.

I'm not sure, if this check should be done in the `urllib3.parse_url`, because it can be used to parse URLs like a `file:///tmp/some_file.txt`.

Also, see issue #143 for a first part of the discussion.
"
143,Check for incorrect input while creating connection from the URL string,2013-01-31T08:21:35Z,2014-07-03T23:24:03Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'rfind',"I got an very rare case: incorrect URL doesn't contains host part (note for a three slashes), so it can't be loaded in fact, and library raises an unobvious exception like a

```
>>> from urllib3.connectionpool import connection_from_url
>>> conn = connection_from_url(""http:///example.org"")
>>> conn.request(""GET"", ""/"")
```

```
Traceback (most recent call last):
  File ""test.py"", line 4, in <module>
    response = conn.request(""GET"", ""/"")
  File ""/home/svartalf/projects/test/.env/lib/python2.7/site-packages/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/home/svartalf/projects/test/.env/lib/python2.7/site-packages/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/home/svartalf/projects/test/.env/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 410, in urlopen
    conn = self._get_conn(timeout=pool_timeout)
  File ""/home/svartalf/projects/test/.env/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 225, in _get_conn
    return conn or self._new_conn()
  File ""/home/svartalf/projects/test/.env/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 192, in _new_conn
    return HTTPConnection(host=self.host, port=self.port)
  File ""/usr/lib/python2.7/httplib.py"", line 693, in __init__
    self._set_hostport(host, port)
  File ""/usr/lib/python2.7/httplib.py"", line 712, in _set_hostport
    i = host.rfind(':')
AttributeError: 'NoneType' object has no attribute 'rfind'
```

Shouldn't there be raised something like a `ValueError('Unknown host: %s' % host)`, if the returned from the `urllib3.parse_url` `Url` object  has `host` equals to `None`?
"
142,Fix ssl test on old openssl,2013-01-25T22:39:19Z,2013-02-09T02:27:13Z,,,,"Fixes #141
"
141,Failing SSL test in master,2013-01-24T05:19:15Z,2013-02-09T06:05:18Z,,AssertionError,AssertionError: SSLError not raised,"On 10.8 with 2.7.2:

```
(urllib3)~/code/urllib3 nosetests
...........................F..............................................................
======================================================================
FAIL: test_set_ssl_version_to_sslv2 (test.with_dummyserver.test_https.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/wolever/code/urllib3/test/with_dummyserver/test_https.py"", line 39, in test_set_ssl_version_to_sslv2
    fields={'method': 'GET'})
AssertionError: SSLError not raised
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
root: INFO: 200 GET /specific_method?method=GET (::1) 0.36ms
urllib3.connectionpool: DEBUG: ""GET /specific_method?method=GET HTTP/1.1"" 200 0
--------------------- >> end captured logging << ---------------------

Name                     Stmts   Miss  Cover   Missing
------------------------------------------------------
urllib3                     14      0   100%   
urllib3._collections        43      0   100%   
urllib3.connectionpool     164      0   100%   
urllib3.contrib              0      0   100%   
urllib3.exceptions          32      0   100%   
urllib3.filepost            34      0   100%   
urllib3.poolmanager         57      0   100%   
urllib3.request             25      0   100%   
urllib3.response            75      0   100%   
urllib3.util               104      5    95%   259-264
------------------------------------------------------
TOTAL                      548      5    99%   
----------------------------------------------------------------------
Ran 90 tests in 0.907s

FAILED (failures=1)
(urllib3)~/code/urllib3 git show
commit 44423a2985b95c4a6cb34ee67123cbab6ec1be0d
Author: Andrey Petrov <andrey.petrov@shazow.net>
Date:   Sat Jan 19 11:22:37 2013 -0800

    SSL CHANGES.

(urllib3)~/code/urllib3 uname -a
Darwin Chaos.local 12.2.0 Darwin Kernel Version 12.2.0: Sat Aug 25 00:48:52 PDT 2012; root:xnu-2050.18.24~1/RELEASE_X86_64 x86_64
(urllib3)~/code/urllib3 python --version
Python 2.7.2
(urllib3)~/code/urllib3 pip freeze
coverage==3.6
nose==1.2.1
tornado==2.4.1
-e git://github.com/shazow/urllib3.git@44423a2985b95c4a6cb34ee67123cbab6ec1be0d#egg=urllib3-dev
wsgiref==0.1.2
```
"
140,Manual specification of hostname to verify against,2013-01-22T15:49:24Z,2013-03-25T17:01:46Z,Ready,,,"This allows people to manually specify the hostname they want to verify their ssl certs against.
This should help if SNI insn't available, the server returns the wrong cert or one connects directly to IP addresses.

(Reference: kennethreitz/requests#1124, kennethreitz/requests#749)
This could be enhanced to verify against fingerprints.
"
139,HTTPS proxy support (issue #50),2013-01-09T02:52:21Z,2013-07-19T17:57:19Z,,,,"This Pull Request resolves #50 adding HTTPS (CONNECT) proxy support to ProxyManager with minimal API and code changes.

A short summary of the changes:
- ProxyManager is a subclass of PoolManager now and its logic has been changed completely, though its constructor still accepts HTTPConnectionPool as the main argument so it's backwards compatible
- Tiny change to VerifiedHTTPSConnection.connect() that introduces CONNECT tunneling support (copied from HTTPConnection.connect() code)
- HTTPSConnectionPool._new_conn() has been changed, but its logic for non-proxied connections stays the same
- HTTPSConnectionPool._new_conn() actually establishes the connection for proxied requests by calling HTTPSConnection.connect(), otherwise Host: header is set improperly due to httplib logic. As stated above, the behavior for non-proxied requests hasn't changed
- [tornado-proxy](https://github.com/senko/tornado-proxy) has been added for tests
- TornadoServerThread has been changed a bit - now it's possible to run multiple server threads in a single test case
- Added proxy tests
- Docs updated

All tests run fine with Python 2.6, 2.7 and 3.2 on Mac OS 10.7.5, except for sporadic timeouts and SSL errors that occur against the unpatched library as well (and should be fixed separately). The code doesn't introduce any new regressions.

Possible TODOs and improvements:
- Proxy authentication (though that could be another libraries' responsibility, like [requests](https://github.com/kennethreitz/requests))
- When accepting proxy_pool as the first argument for ProxyManager, copy all pool properties and use them for other pool instances generation or even use it as the pool for HTTP proxy requests
- Usage of HTTPConnectionPool and, especially, HTTPSConnectionPool against proxy isn't really clear and requires some external logic - much like ProxyManager.connection_from_host() 
"
138,Enable skip_host and skip_accept_encoding,2012-12-31T09:40:50Z,2012-12-31T19:48:49Z,,,,"Py2.4 added skip_host and skip_accept_encoding to httplib, but it's only
available when manually sending data over the wire with connect() and
putheader().

In httpconnection.py, httplib and http.client are wrapped with a small
fix to enable support for skip_host and skip_accept_encoding.

Also unit tests.
"
137,Import failed if no ssl module is found,2012-12-23T12:11:09Z,2013-01-19T19:25:01Z,,,,"In util.py, the `ssl_wrap_socket` function will throw error when there is no ssl module in the sys (like Py25 or GAE Py 27)
Because CERT_NONE or PROTOCOL_SSLv23 is not defined.

> Traceback (most recent call last):
>   File ""/python27_runtime/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 196, in Handle
>     handler = _config_handle.add_wsgi_middleware(self._LoadHandler())
>   File ""/python27_runtime/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 255, in _LoadHandler
>     handler = __import__(path[0])
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/application/**init**.py"", line 12, in <module>
>     from utils import monkey_patch
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/application/utils/monkey_patch.py"", line 3, in <module>
>     import requests
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/**init**.py"", line 52, in <module>
>     from . import utils
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/utils.py"", line 22, in <module>
>     from .compat import parse_http_list as _parse_list_header
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/compat.py"", line 7, in <module>
>     from .packages import charade as chardet
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/packages/__init__.py"", line 3, in <module>
>     from . import urllib3
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/packages/urllib3/**init**.py"", line 16, in <module>
>     from .connectionpool import (
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/packages/urllib3/connectionpool.py"", line 45, in <module>
>     from .util import get_host, is_connection_dropped, ssl_wrap_socket
>   File ""/base/data/home/apps/s~dabr-gae/1.364069325046980719/lib/requests-1.0.4/requests/packages/urllib3/util.py"", line 293, in <module>
>     def ssl_wrap_socket(sock, keyfile=None, certfile=None, cert_reqs=CERT_NONE,
> NameError: name 'CERT_NONE' is not defined
> 
> Traceback (most recent call last):
>   File ""/python27_runtime/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 196, in Handle
>     handler = _config_handle.add_wsgi_middleware(self._LoadHandler())
>   File ""/python27_runtime/python27_lib/versions/1/google/appengine/runtime/wsgi.py"", line 255, in _LoadHandler
>     handler = __import__(path[0])
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/application/**init**.py"", line 12, in <module>
>     from utils import monkey_patch
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/application/utils/monkey_patch.py"", line 3, in <module>
>     import requests
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/**init**.py"", line 52, in <module>
>     from . import utils
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/utils.py"", line 22, in <module>
>     from .compat import parse_http_list as _parse_list_header
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/compat.py"", line 7, in <module>
>     from .packages import charade as chardet
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/packages/__init__.py"", line 3, in <module>
>     from . import urllib3
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/packages/urllib3/**init**.py"", line 16, in <module>
>     from .connectionpool import (
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/packages/urllib3/connectionpool.py"", line 45, in <module>
>     from .util import get_host, is_connection_dropped, ssl_wrap_socket
>   File ""/base/data/home/apps/s~dabr-gae/1.364069540401230145/lib/requests-1.0.4/requests/packages/urllib3/util.py"", line 295, in <module>
>     ssl_version=PROTOCOL_SSLv23):
> NameError: name 'PROTOCOL_SSLv23' is not defined
"
136,Over the Wire Trace Capability,2012-12-20T16:46:40Z,2012-12-20T17:09:12Z,,,,"I have started using requests which is based on urllib3. Much to like. However relative to httplib2 or even httplib, there is one significant feature lack in requests/urllib3, that is, detailed logging of what gets sent over the wire for debugging purposes.

httplib2.debuglevel =1 will log to console explicit detail of the request and response

Whereas urllib3.add_stderr_logger() will only log the hardly useful

2012-12-20 09:30:12,493 INFO Starting new HTTPS connection (1): example.com
2012-12-20 09:30:12,806 DEBUG ""POST /soap/default HTTP/1.1"" 500 572

I find that in developing interfaces to backend api's that the most convenient way to report bugs
is to provide a trace of the full request response. There is no convenient way that I can see to do this
with urllib3 and hence with requests.
"
135,Update urllib3/packages/socksipy/socks.py,2012-12-20T07:31:22Z,2013-01-19T18:32:48Z,,,,"Fix for HTTP proxy negotiation.
"
134,Safely access to 'pool_classes_by_scheme' avoiding possible KeyErrors.,2012-12-19T23:39:31Z,2013-01-19T19:07:02Z,,,,"Access safely to ""pool_classes_by_scheme"" because ""scheme"" can be None, and we must provide a default action to be taken or a KeyError will ocurr.

Copied the same structure like the line 77 on the same file, that takes the HTTP protocol as default.
"
133,Close connection when no data is read.,2012-12-19T09:39:58Z,2013-01-03T23:20:29Z,,,,"As per the long discussion in PR #132 that originated with kennethreitz/requests#1041 the solution is to make sure to read the EOF marker in order for httplib to automatically shutdown the connection. This actually occurs already with httplib in all but one condition (how requests uses it).

While there is an [upstream fix](http://hg.python.org/cpython/rev/5d6c2c7bc5d4), not everyone will upgrade to the newest releases of python 2.7 and 3.2+. This pull request fixes the behavior on urllib3 in a way that won't break if someone is running a version of python with the upstream fix (subsequent `close` calls have no effect).

I have verified that this fixes kennethreitz/requests#1041 when I applied this code to the version of urllib3 included with requests.
"
132,Add close method to HTTPResponse,2012-12-19T03:26:50Z,2013-01-19T18:48:40Z,,,,"See: kennethreitz/requests#1041
"
131,RFC: Check ssl cert time,2012-12-18T18:12:29Z,2012-12-18T20:55:57Z,,,,"This adds a check if the certificate of the server is still valid.
It certainly needs lots of error handling and sanity checks.
`NotBefore` should be checked also.

Please tell me what you think.
"
130,Unify ssl api,2012-12-17T23:10:32Z,2013-01-19T18:13:31Z,,,,"Allows the user to specify arguments as strings or numeric constants, exportet from the ssl module.
Also allow abbrevations like `SSLv23` -> `PROTOCOL_SSLv23`, although I don't know if this is to fancy.

Should also fix https://github.com/kennethreitz/requests/issues/1025
"
129,fix mergeconflict in connectionpool.py,2012-12-16T20:28:09Z,2012-12-16T20:29:26Z,,,,"docstring of HTTPSConnectionPool
"
128,SSL-related tests failing on OSX Py32,2012-12-16T20:19:24Z,2012-12-16T22:07:47Z,"Contributor Friendly ‚ô•, Has Failing Test, Urgent",AssertionError,"AssertionError: <socket.socket object, fd=17, family=2, type=1, proto=6> != None","This is blocking the next release of urllib3. Help would be greatly appreciated.

Good chance that the tests simply need to be updated, but an investigation would be valuable.

_Update:_ This is on OSX.

```
........F...............F...........................................................
======================================================================
FAIL: test_keepalive_close (test.with_dummyserver.test_connectionpool.TestConnectionPool)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/shazow/projects/urllib3/test/with_dummyserver/test_connectionpool.py"", line 173, in test_keepalive_close
    self.assertEqual(conn.sock, None)
AssertionError: <socket.socket object, fd=17, family=2, type=1, proto=6> != None
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTP connection (1): localhost
root: INFO: 200 GET /keepalive?close=1 (127.0.0.1) 0.60ms
urllib3.connectionpool: DEBUG: ""GET /keepalive?close=1 HTTP/1.1"" 200 13
--------------------- >> end captured logging << ---------------------

======================================================================
FAIL: test_set_ssl_version_to_sslv2 (test.with_dummyserver.test_https.TestHTTPS)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/shazow/projects/urllib3/test/with_dummyserver/test_https.py"", line 38, in test_set_ssl_version_to_sslv2
    fields={'method': 'GET'})
AssertionError: SSLError not raised by request
-------------------- >> begin captured logging << --------------------
urllib3.connectionpool: INFO: Starting new HTTPS connection (1): localhost
root: INFO: 200 GET /specific_method?method=GET (127.0.0.1) 0.50ms
urllib3.connectionpool: DEBUG: ""GET /specific_method?method=GET HTTP/1.1"" 200 0
--------------------- >> end captured logging << ---------------------
```

There were some recent merges which probably affected this. See: #89, #109, and maybe others. 
"
127,Fix SSL3 error on Ubuntu 12.04 ,2012-12-13T01:43:23Z,2012-12-13T02:14:00Z,,,,"Taken from here. https://github.com/kennethreitz/requests/pull/799
"
126,Allow explicit specification of content type when encoding file fields.,2012-12-12T11:55:42Z,2012-12-15T23:54:07Z,"Ready, Soon",,,"Because sometimes we know better than the guessing game mimetools plays.
"
125,Switch uses of `with self.assertRaises(...)` to standard calls.,2012-12-03T09:52:26Z,2012-12-16T01:02:40Z,,,,"In Python 2.6, `self.assertRaises()` is not a ContextManager, and can't be
used with the `with` statement.
"
124,"Tests error with ""Address already in use"" on Linux",2012-12-02T03:14:16Z,2012-12-02T03:26:36Z,,,,"```
$ nosetests -v
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 551, in __bootstrap_inner
    self.run()
  File ""/home/shazow/projects/urllib3/dummyserver/server.py"", line 90, in run
    self.server = self._start_server()
  File ""/home/shazow/projects/urllib3/dummyserver/server.py"", line 86, in _start_server
    http_server.listen(self.port, address=self.host)
  File ""/home/shazow/env/urllib3/lib/python2.7/site-packages/tornado/netutil.py"", line 100, in listen
    sockets = bind_sockets(port, address=address)
  File ""/home/shazow/env/urllib3/lib/python2.7/site-packages/tornado/netutil.py"", line 265, in bind_sockets
    sock.bind(sockaddr)
  File ""/usr/lib/python2.7/socket.py"", line 224, in meth
    return getattr(self._sock,name)(*args)
error: [Errno 98] Address already in use
```

Not sure what's causing it. Could be some specific version of Python/Tornado/Linux/urllib3. Need to investigate.asda
"
123,Fix some unittests for py3,2012-12-02T00:31:24Z,2012-12-02T03:09:24Z,,,,"100% pass with this on py3.3 (and 2.7.3) on linux
"
122,connectionpool: pass strict to the HTTP and HTTPSConnections.,2012-12-01T22:15:34Z,2012-12-01T22:38:06Z,,,,"While writing a small app with Requests, I discovered that an app I want to integrate with is pretty brain-dead with  SSL, and will return garbage at you if you accidentally use HTTP vs. HTTPS.  Unfortunately, Requests was happy to say it was successful.  Come to find out that turning on strict is the answer (as I don't care about operating with an HTTP 0.9 server), but urllib3 doesn't actually pass the strict parameter to the underlying connection.  This PR solves that problem.

FYI: I tried running tests, but they fail (without my patch).  It looks like several servers are being started, and one of them is unhappy because another one is listening on the same port.  Then it dies, and the tests don't proceed any further.
"
121,Handle non-ascii headers in multipart/form-data,2012-11-27T22:05:02Z,2013-08-07T00:43:11Z,Doesn't Meet Project Standards,,,"These commits fix shazow/urllib3#119 as well as a broken indentation of another test case.
"
120,More configurable metadata in filepost encoding requests,2012-11-23T16:49:23Z,,Someday,,,"This issue report (just like #119) is based solely on the specifications, and not backed by any real world problem I encountered _yet._

[RFC 2388 Section 4.5](http://tools.ietf.org/html/rfc2388#section-4.5) states:

> In the case where a field element is text, the `charset` parameter for the text indicates the character encoding used.

The content type is optional in the case of `text/plain`, but in that case, the default charset is ASCII. So to ensure correct treatment, you should always add a header

```
 Content-Type: text/plain; charset=""utf-8""
```

to text fields. At least if they are not ascii-only, but always adding it might be easier.

On the other hand, if the data is binary, i.e. considered to be the content of a file, then [Section 3](http://tools.ietf.org/html/rfc2388#section-3) mandates:

> If the contents of a file are returned via filling out a form, then the file input is identified as the appropriate media type, if known, or ""`application/octet-stream`"".

So in those cases you should send a header of

```
Content-Type: application/octet-stream
```

I'm not sure how you'd best differentiate these two cases in the library, as you don't strictly distinguish between file content and text content. If the data is unicode, then I'd treat it as text. If the input is binary and not strictly ascii, then I'd treat it as a file. For binary ascii-only data, you might use the presence or absence of a file name to distinguish.

Furthermore, [RFC 2388 Section 4.3](http://tools.ietf.org/html/rfc2388#section-4.3) states:

> The value supplied for a part may need to be encoded and the ""`content-transfer-encoding`"" header supplied if the value does not conform to the default encoding.

I haven't yet figured out what that ‚Äúdefault encoding‚Äù is in this case, and where it is specified. But just to be sure, I'd suggest to add either of these to every content:

```
Content-Transfer-Encoding: 8bit
Content-Transfer-Encoding: binary
```

I must confess that I'm quite unsure about how these two differ, if at all. I have the gut feeling that `8bit` might be more suitable for text, particularly if its lines are not too long, and `binary` more suitable for file content. But as this is based mostly on my (very theoretic) understanding of how things work with SMTP as the underlying transport, I'm far from sure for HTTP.
## 
"
119,Use international headers when posting file names,2012-11-23T16:25:32Z,2013-08-07T16:15:41Z,Someday,,,"[RFC 2388 Section 4.4](http://tools.ietf.org/html/rfc2388#section-4.4) states:

> if the file name of the sender's operating system is not in US-ASCII, the file name might be approximated, or encoded using the method of [RFC 2231](http://tools.ietf.org/html/rfc2231).

[Currently](https://github.com/shazow/urllib3/blob/8b610a305fe0c2c4123bf963b9b53423db8c423d/urllib3/filepost.py#L67), urllib3 does neither. Instead, it will simply UTF-8 encode the file name. While many receiving parties might be able to handle that situation, it appears to me like a violation of the specification, with all the impact this might have on interoperability.

To be more precise, when encountering a non-ascii file name like `T√§st.txt`, it should send a header like this:

```
Content-Disposition: form-data; name=""fieldname""; file*=utf-8''T%C3%A4st.txt
```

The same holds for non-ascii names of form fields. You can use [the `email.utils.encode_rfc2231` function](http://docs.python.org/2/library/email.util.html#email.utils.encode_rfc2231) to perform this kind of header formatting. For increased portability, you might want to only do so for cases that actually require it, keeping the current syntax for ascii-only names.
"
118,Added SNI support,2012-11-21T14:30:02Z,2012-12-01T23:03:42Z,,,,"In Python 3.2 support for [SNI](http://en.wikipedia.org/wiki/Server_Name_Indication) was added to the `ssl` module. But you have to use the new `SSLContext` API, in order to enable it. So I made the `VerifiedHTTPSConnection` class use `SSLContext` if it exists and fallback to the old implementation if it doesn't exist.
"
117,Patch for issue #116,2012-11-17T16:30:29Z,2012-12-16T01:12:27Z,,,,"Resolves issue #116:   https://github.com/shazow/urllib3/issues/116
"
116,Setting timeout on celery worker causes TypeError when attempting to throw TimeoutError ,2012-11-17T16:22:54Z,2012-12-16T01:18:04Z,,,,"# Problem

Setting timeout on celery worker causes TypeError when attempting to throw TimeoutError   which is inherits from PoolError.  Keep in mind I'm not using urllib3 directly but using requests library and setting timeout.

I have the following in my celery worker:

```
result = requests.head( ""http://looney.com"", allow_redirects=True, timeout=1.0 )
```

I expect a TimeoutError but I end up getting a TypeError...
# Possible solution

Set a default message in the PoolError constructor.

```
#def __init__(self, pool, message):
def __init__(self, pool, message=''):
```
# Traceback

```
 [2012-11-17 15:47:24,980: ERROR/MainProcess] Unrecoverable error: TypeError('__init__() takes exactly 3 arguments (2 given)', <class 'requests.packages.urllib3.exceptions.TimeoutError'>, (""HTTPConnectionPool(host='looney.com', port=80):   Request timed out. (timeout=1.0)"",))

 Traceback (most recent call last):
   File ""/home/user/virtpy/lib/python2.6/site-packages/celery/worker/__init__.py"", line 347, in start
     component.start()
   File ""/home/user/virtpy/lib/python2.6/site-packages/celery/worker/consumer.py"", line 390, in start
     self.consume_messages()
   File ""/home/user/virtpy/lib/python2.6/site-packages/celery/worker/consumer.py"", line 474, in consume_messages
     readers[fileno](fileno, event)
   File ""/home/user/virtpy/lib/python2.6/site-packages/billiard/pool.py"", line 669, in handle_event
     self._it.next()
   File ""/home/user/virtpy/lib/python2.6/site-packages/billiard/pool.py"", line 642, in _process_result
     ready, task = poll(timeout)
   File ""/home/user/virtpy/lib/python2.6/site-packages/billiard/pool.py"", line 1059, in _poll_result
     return True, self._quick_get()
 TypeError: ('__init__() takes exactly 3 arguments (2 given)', <class 'requests.packages.urllib3.exceptions.TimeoutError'>,(""HTTPConnectionPool(host='looney.com', port=80): Request timed out. (timeout=1.0)"",))
```
"
115,Update to the most recent six commit to prevent conflict with six.moves.,2012-11-07T08:45:46Z,2012-11-12T16:11:02Z,,,,"Ran and passed all 78 tests with python 2.7 and 3.2.

Verified that this fixes issue #113.
"
114,socket.error handling: for #92,2012-11-06T17:23:59Z,2012-11-12T18:07:06Z,,,,"For shazow/urllib3#92...because I can't add to a pull request.

2 things:
- Combines HTTPException and SocketError into one handler, checking max retry, and raising an error
- MaxRetryError includes a reason to help with debugging

As a side note, does this probably impacts shazow/urllib3#104.
"
113,urllib3 doesn't play nice with other packages that use six.moves,2012-11-05T22:24:06Z,2012-11-12T16:11:58Z,,ImportError,ImportError: cannot import name urljoin,"``` python
from six import MovedAttribute, add_move
add_move(MovedAttribute('urljoin', 'urlparse', 'urllib.parse'))

import urllib3
from six.moves import urljoin
```

gives an ImportError:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name urljoin
```

The above code works fine if I don't import urllib3, or if I import urllib3 first (not a good solution).

It appears that urllib3 replaces any previously added moves to the six library. This issue is preventing me from using requests (that in-turn uses urllib3) in one of my packages. While I **can** try to restructure my imports to prevent the issue, it's just defers the error until another point.
"
112,Corrected multipart/form-data behavior with params which aren't a file,2012-10-30T19:22:45Z,2012-11-13T00:10:02Z,,,,"Patch for issue #111 (https://github.com/shazow/urllib3/issues/111).

I hope it matches the urllib3 pull requirements/way to do things. Anyway, thanks!
"
111,urllib3/filepost.py settings wrong form-data format,2012-10-30T00:04:48Z,2012-11-13T00:10:26Z,,,,"First, thanks for great library!

I think I may have found an issue with urllib3 handles parameters that aren't files when doing file upload. (multipart/form-data mime format).

In the last version (4d477a6ee695ccb64277f49f836f32a2e023cf4e), when we pass a field that's not file, it adds a `'Content-Type: text/plain\r\n\r\n'` between `'Content-Disposition: form-data; name=""%s""\r\n'` and the actual value of the field. Normally, if I understood well the specifications of the W3C (http://www.w3.org/TR/html401/interact/forms.html) and after sniffing browser's behaviors with POST incorporating both file and params, the `'Content-Type: text/plain\r\n\r\n'` must not be here.

Basically, urllib3 is generating this : 

```
   Content-Type: multipart/form-data; boundary=AaB03x

   --AaB03x
   Content-Disposition: form-data; name=""submit-name""
   Content-Type: text/plain

   Larry
   --AaB03x
   Content-Disposition: form-data; name=""files""; filename=""file1.txt""
   Content-Type: text/plain

   ... contents of file1.txt ...
   --AaB03x--
```

when it should have generated this :

```
   Content-Type: multipart/form-data; boundary=AaB03x

   --AaB03x
   Content-Disposition: form-data; name=""submit-name""

   Larry
   --AaB03x
   Content-Disposition: form-data; name=""files""; filename=""file1.txt""
   Content-Type: text/plain

   ... contents of file1.txt ...
   --AaB03x--
```

A simple correction will be to replace filepost.py:75 by `body.write(b'\r\n')`

Hope it's helping and I am not using wrongly urllib3 (I was using requests library in fact and found that urllib3 might be the issue following the path of my real problem)
"
110,Make the Content-Encoding value case insensitive per RFC 2616.,2012-10-19T19:12:26Z,2012-10-21T19:03:03Z,,,,"Make the Content-Encoding value case insensitive per RFC 2616.
"
109,Added ability to choose SSL version,2012-10-19T18:29:59Z,2012-12-19T02:20:18Z,"Ready, Soon",,,"This is the same as a change that I did for the requests project - which
required this urllib3 change - so I thought it might be appropriate to
contribute the change to the upstream library as well.
"
108,Adding URL openers that support automated cookie handling,2012-10-01T06:50:50Z,2013-01-19T19:23:51Z,Doesn't Meet Project Standards,,,"See urllib3/contrib/cookies.

Basically, I created a class that can take strings from the Set-Cookie header from a request and put them in a jar. The cookies can also be extracted and converted to a Cookie header string.

Then, I subclassed the ConnectionPools and overrode their urlopen methods to send a Cookie header using cookies from my class, and feed the Set-Cookie headers from the response into my class.
"
107,Provide way to enable httplib debug logging,2012-09-25T15:59:52Z,,,,,"The only way I have found to get the really low-level, over-the-wire debugging I need in some situations is to use the set_debuglevel() method of HTTPConnection object in httplib.  If you use this method to set the debug level to something > 2, it will give you this kind of logging:

```
send: 'POST /?Action=ListUsers&PathPrefix=%2F&Version=2010-05-08 HTTP/1.1\r\nHost:    iam.amazonaws.com\r\nAccept-Encoding: identity\r\nContent-Length: 0\r\nAuthorization: AWS4-HMAC-  SHA256 Credential=AKIAJOTCCJRP4C3NSMYA/20120925/us-east- 1/iam/aws4_request,SignedHeaders=host;x-amz- date,Signature=<signature-removed>\r\nX- Amz-Date: 20120925T155303Z\r\nUser-Agent: Boto/2.6.0-dev (darwin)\r\n\r\n'
reply: 'HTTP/1.1 200 OK\r\n'
header: x-amzn-RequestId: 16ef212a-0729-11e2-af7e-533ffc505ce6
header: Content-Type: text/xml
header: Content-Length: 805
header: Date: Tue, 25 Sep 2012 15:53:02 GMT
```

I find this level of info invaluable at times.  I am currently coercing urllib3 to give me this logging by hacking _get_conn method of the connection pool objects.  That works but it would be nice to enable it without hacking code.
## 
"
106,Fix testcase for issue #104,2012-09-24T17:36:19Z,2013-10-18T11:16:16Z,,,,"Fix testcase for issue #104
"
105,Add testcase for issue #104,2012-09-24T16:38:41Z,2012-09-24T17:38:32Z,,,,"Add testcase for issue #104 (socket.error unhandled exception)
"
104,socket.error improperly handled in urlopen(),2012-09-24T14:28:49Z,2014-07-03T23:06:57Z,"Has Failing Test, Soon",socket.error,socket.error: [Errno 104] Connection reset by peer,"I think the socket.error exception is improperly handled inside urlopen() or one of the functions it calls (such as _make_requests), with regards to retries.

Unless I am missing something, I believe that when setting retries to a value greater than 1, a retry should take place if something breaks with the socket. However, I am getting the following stack trace:

```
...
  File ""/home/eran/sandsquid/sspy/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 415, in urlopen
    body=body, headers=headers)
  File ""/home/eran/sandsquid/sspy/local/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 275, in _make_request
    httplib_response = conn.getresponse(buffering=True)
  File ""/usr/lib/python2.7/httplib.py"", line 1030, in getresponse
    response.begin()
  File ""/usr/lib/python2.7/httplib.py"", line 407, in begin
    version, status, reason = self._read_status()
  File ""/usr/lib/python2.7/httplib.py"", line 365, in _read_status
    line = self.fp.readline()
  File ""/usr/lib/python2.7/socket.py"", line 447, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer
```

and no retry takes place...
"
103,Reading from response allowed after the connection has been released,2012-09-18T10:51:40Z,2013-10-18T11:18:57Z,,,,"It appears that you can read uncached data from a response even after the connection has been released.

``` python
from urllib3 import PoolManager, add_stderr_logger

add_stderr_logger()

pool_manager = PoolManager(1)

response = pool_manager.request('GET', 'https://www.google.com',
    preload_content=False, release_conn=False)

# Since no content has been pre-loaded, _body should be None
assert response._body is None

response.release_conn()

# Releasing the connection should prevent any further reading of the data on
# that connection, otherwise request interleaving may occur
assert len(response.read()) == 0
```
"
102,Not obvious why `r.read()` returns '' on `preload_content=True`.,2012-09-14T18:25:18Z,2014-07-03T23:07:18Z,,,,"See http://stackoverflow.com/posts/12429816
"
101,"MaxRetryError is not pickleable (has failing test, needs fix)",2012-09-13T03:57:58Z,2012-09-13T15:46:54Z,"Has Failing Test, Soon",,,"Progress branch for #100.

Related bug report: https://github.com/celery/billiard/issues/18
"
100,MaxRetryError is not pickleable,2012-09-12T14:29:47Z,2012-09-13T04:01:07Z,,,,"https://twitter.com/asksol/status/245831282007883776
"
99,request_encode_body will empty the potential headers defined when HTTPConnectionPool init,2012-09-11T08:05:11Z,2012-09-15T19:30:01Z,,,,"When creating a `HTTPConnectionPool` object, you can pass `headers` which should be included in all requests. However the `request_encode_body` method from parent class will empty these default headers and add a `Content-Type` value, unless you pass `headers` explicitly when calling this method.

I think it's a bug and there is a rough fix.
"
98,urlopen does not work as documented - or like urllib2,2012-09-09T17:15:25Z,2012-09-09T17:25:03Z,,TypeError,TypeError: urlopen() takes at least 3 arguments (2 given),"http://urllib3.readthedocs.org/en/latest/managers.html has an example using urlopen without a GET/POST param (i.e. the api looks like urllib2) this does not work with urllib3-1.2.1-102-ge905248

```
D:\>j:\Python26\python
Python 2.6.4 (r264:75708, Oct 26 2009, 08:23:19) [MSC v.1500 32 bit (Intel)] on
win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from urllib3 import PoolManager
>>> manager = PoolManager(num_pools=2)
>>> r = manager.urlopen(""http://google.com/"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: urlopen() takes at least 3 arguments (2 given)
```

Is it possible to open urls using the old urllib2 api?
"
97,Fixing coverage regex,2012-09-04T05:29:04Z,2012-09-04T05:29:36Z,,,,"Escaping parentheses in regex
"
96,Change info to debug logging,2012-09-03T20:47:18Z,2012-10-21T19:14:39Z,,,,"I think that debug output will be more appropriate in this cases.
"
95,Please merge trivial change to allow some overrides,2012-08-31T21:34:48Z,2013-01-19T19:11:18Z,,,,"This commit allows code as this example:

https://gist.github.com/3559286

It is also a step in to support stuff like smart card based authentication as it allows replacing the connection factories for a single PoolManager. I used something like this to inject a SSL.Connection instance with a SSLContext that refers to a private key on a PKCS #11 token.
"
94,Expose pool and connection creation to allow overriding,2012-08-31T21:26:51Z,2013-01-19T19:26:03Z,,,,"I would like to tunnel HTTP requests through a tunnel. My use case is developing an application that is only locally accessible but I have SSH access to the network.
With Firefox etc. I can use a SOCKS5 proxy (via ssh -D) to access the site. It is off limits for my Python application though.

It would be great if urllib3 would allow plugging custom ways of creating a connection into the PoolManager.
"
93,Fix overwritten Accept header when proxy is used,2012-08-30T04:24:33Z,2012-09-01T22:57:14Z,Ready,,,"When a request specifies both an Accept header and a proxy server, the Accept header value is overwritten.
(https://github.com/kennethreitz/requests/issues/821)
"
92,Bugfix for socket.error being raised from connection pool,2012-08-08T19:43:34Z,2012-11-04T15:39:02Z,,"socket.error, urllib3.exceptions.MaxRetryError","socket.error: [Errno 61] Connection refused, urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /","After applying [pull request #86](https://github.com/shazow/urllib3/pull/86), any `socket.error` raised by `ConnectionPool._make_request()` will not be caught, and will instead be raised to external code. However, if we get a ""Connection Refused"" error, we want to deal with this the same way we deal with similar errors: retry until we hit the retry limit, then (if it's still failing) raise `MaxRetryError`.

Here's how to reproduce the bug, on your current master branch:

```
>>> import urllib3
>>> urllib3.PoolManager().request('GET', 'http://127.0.0.1')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/peterscott/tmp/urllib3/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/poolmanager.py"", line 117, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 416, in urlopen
    body=body, headers=headers)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 268, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 941, in request
    self._send_request(method, url, body, headers)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 975, in _send_request
    self.endheaders(body)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 937, in endheaders
    self._send_output(message_body)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 797, in _send_output
    self.send(msg)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 759, in send
    self.connect()
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/httplib.py"", line 740, in connect
    self.timeout, self.source_address)
  File ""/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py"", line 571, in create_connection
    raise err
socket.error: [Errno 61] Connection refused
```

After merging this pull request, the behavior becomes:

```
>>> urllib3.PoolManager().request('GET', 'http://127.0.0.1')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/peterscott/tmp/urllib3/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/poolmanager.py"", line 117, in urlopen
    response = conn.urlopen(method, u.request_uri, **kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 482, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 482, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 482, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 482, in urlopen
    release_conn=release_conn, **response_kw)
  File ""/Users/peterscott/tmp/urllib3/urllib3/connectionpool.py"", line 391, in urlopen
    raise MaxRetryError(self, url)
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=80): Max retries exceeded with url: /
```

This was breaking requests, causing [requests issue #748](https://github.com/kennethreitz/requests/issues/748).
"
91,ssl problems with self signed certificate,2012-08-07T17:28:40Z,2012-08-07T21:35:05Z,,SSLError,SSLError: [Errno 336265225] _ssl.c:351: error:140B0009:SSL routines:SSL_CTX_use_In PrivateKey_file:PEM lib,"Hi, I'm having a problem with a self signed ssl certificate:

```
In [1]: import urllib3                                                                                                                                                                

In [2]: conn = urllib3.connection_from_url('https://myserver.tld')

In [3]: conn.cert_file = '/home/cg/cacert.pem'

In [4]: r = conn.request('GET', 'https://myserver.tld/', )
---------------------------------------------------------------------------
SSLError                                  Traceback (most recent call last)
/home/cg/<ipython-input-4-e1ce8d0ab014> in <module>()
----> 1 r = conn.request('GET', 'https://myserver.tld/', )

/usr/local/lib/python2.7/dist-packages/urllib3/request.pyc in request(self, method, url, fields, headers, **urlopen_kw)
     65             return self.request_encode_url(method, url, fields=fields,
     66                                             headers=headers,
---> 67                                             **urlopen_kw)
     68         else:
     69             return self.request_encode_body(method, url, fields=fields,

/usr/local/lib/python2.7/dist-packages/urllib3/request.pyc in request_encode_url(self, method, url, fields, **urlopen_kw)
     78         if fields:
     79             url += '?' + urlencode(fields)
---> 80         return self.urlopen(method, url, **urlopen_kw)
     81
     82     def request_encode_body(self, method, url, fields=None, headers=None,

/usr/local/lib/python2.7/dist-packages/urllib3/connectionpool.pyc in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
    444         except BaseSSLError as e:
    445             # SSL certificate error

--> 446             raise SSLError(e)
    447
    448         except CertificateError as e:

SSLError: [Errno 336265225] _ssl.c:351: error:140B0009:SSL routines:SSL_CTX_use_In PrivateKey_file:PEM lib
```

but this example from the python documentation works:

```
In [1]: import socket, ssl, pprint

In [2]: s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

In [3]: ssl_sock = ssl.wrap_socket(s,
ca_certs=""/home/cg/cacert.pem"",
cert_reqs=ssl.CERT_REQUIRED)

In [4]: ssl_sock.connect(('myserver.tld', 443))
```

Is this a general problem with self signed certificates or am I doing something wrong?
"
90,use PyOpenSSL as SSL backend,2012-08-05T07:48:35Z,2013-03-25T16:15:16Z,Someday,NotImplementedError,NotImplementedError: Cannot make file object of SSL.Connection,"PyOpenSSL would make it possible use SNI on versions of python:

Problem:
The connection made by PyOpenSSL does _not_ implement the `makefile()`-Method.
This is used by httplib. Stacktrace:

``` python
Traceback (most recent call last):
  File ""/home/t-8ch/Projekte/urllib3/test/with_dummyserver/test_https.py"", line 24, in test_simple
    fields={'method': 'GET'})
  File ""/home/t-8ch/Projekte/urllib3/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/home/t-8ch/Projekte/urllib3/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/home/t-8ch/Projekte/urllib3/urllib3/connectionpool.py"", line 417, in urlopen
    body=body, headers=headers)
  File ""/home/t-8ch/Projekte/urllib3/urllib3/connectionpool.py"", line 279, in _make_request
    httplib_response = conn.getresponse()
  File ""/usr/lib/python3.2/http/client.py"", line 1047, in getresponse
    response = self.response_class(self.sock, method=self._method)
  File ""/usr/lib/python3.2/http/client.py"", line 281, in __init__
    self.fp = sock.makefile(""rb"")
NotImplementedError: Cannot make file object of SSL.Connection
```

Ideas to cope with this:
- Solve #58
- Use [ndg-httpsclient](http://packages.python.org/ndg-httpsclient/), which is a wrapper around PyOpenSSL and provides a [`SSLSocket`-class](http://packages.python.org/ndg-httpsclient/ndg.httpsclient.ssl_socket.SSLSocket-class.html) which implements `makefile()`
- Just take the needed parts (`SSLSocket`) from ndg-httpsclient. ndg-httpsclient is BSD-licensed. One would also need a minor update to get python3 compability

I am in favour of the last point.
"
89,add support for SNI,2012-07-28T16:58:59Z,2012-12-16T20:08:10Z,Someday,,,"this makes urrlib3 use SNI if available.

testing can be done using https://sni.velox.ch/
I didn't include an automatic test, as this would fail on old versions of python or (open)ssl.
(SNI was added in 3.2)
I could however add a test which tests the version of python.
"
88,Close disposed connections,2012-07-22T19:21:21Z,2012-07-22T22:18:52Z,"Ready, Soon",,,"This is a cleanup of Pull Request #87.

Still need to increase test coverage up to +99%.
"
87,Refactor to support eager release of pooled connections,2012-07-18T03:37:08Z,2012-07-22T19:22:12Z,,,,"This is a rewrite of the eager release patch from kennethreitz/requests#616 against the current urllib3 master.

The ""XXX: this isn't eager enough"" issue has been corrected. However, I didn't move the disposal feature to a subclass of `RecentlyUsedContainer`, instead I ended up rewriting the original class against `OrderedDict` and adding the 2.6 backport of it to packages :-\

I got the unit tests here passing, then installed this branch in the current Requests development master and ran the tests there, but this probably needs more testing. I'm not entirely sure about the best way to do that, though.

Tell me what you think of the design? Thanks for your help.
"
86,Merge #76,2012-07-09T14:56:30Z,2012-07-22T01:35:25Z,,,,"I don't see the code from #76 merged in, so let's try a real pull request.  Not sure why I can't just do one from an issue...
"
85,"New request logic for PoolManager: strips scheme/host from request, better redirect",2012-07-01T01:00:32Z,2012-07-18T15:29:56Z,,,,"Using the new parse_url, yay.

Fixes #8 and #11.
"
84,Redirects are broken,2012-06-26T16:14:47Z,2012-07-18T15:28:49Z,,urllib3.exceptions.MaxRetryError,"urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.google.de', port=443): Max retries exceeded with url: https://www.google.de/","```
>>> m = PoolManager()
>>> m.request('GET', 'https://google.com')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 108, in urlopen
    return self.urlopen(method, e.url, **kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 108, in urlopen
    return self.urlopen(method, e.url, **kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 104, in urlopen
    return conn.urlopen(method, url, **kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 361, in urlopen
    raise MaxRetryError(self, url)
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.google.de', port=443): Max retries exceeded with url: https://www.google.de/
```

It is not an HTTP**S** error:

```
 >>> m.request('GET', 'http://google.com')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/request.py"", line 67, in request
    **urlopen_kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/request.py"", line 80, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 108, in urlopen
    return self.urlopen(method, e.url, **kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 108, in urlopen
    return self.urlopen(method, e.url, **kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 104, in urlopen
    return conn.urlopen(method, url, **kw)
  File ""/usr/local/Cellar/python/2.7.3/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 361, in urlopen
    raise MaxRetryError(self, url)
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.google.de', port=80): Max retries exceeded with url: http://www.google.de/
```
"
83,Full native URL parsing,2012-06-23T15:42:26Z,2012-06-30T23:34:23Z,,,,"This should be feature-complete against urlparse, but better and more efficient. Needs more testing. Deprecates `urllib3.util.get_host`. This will also allow us to fix #8 since we'll be able to parse out the path properly.

Might also tackle URL composition for funsies, we'll see.
"
82,301 problems,2012-06-21T05:03:32Z,2012-06-23T02:50:36Z,,HostChangedError,"HostChangedError: HTTPConnectionPool(host='www.opda.com.cn', port=80): Tried to open a foreign host with url: forum.php","Why the 301 sites will be wrong?

http = urllib3.PoolManager() 
conn = http.connection_from_url(""http://www.opda.com.cn"")
r = response = conn.request(""GET"", ""/"", retries = 5)

Error log:
HostChangedError: HTTPConnectionPool(host='www.opda.com.cn', port=80): Tried to open a foreign host with url: forum.php
"
81,Custom 'Accept' header being overwritten,2012-06-19T14:29:12Z,2012-06-19T15:34:34Z,,,,"Hi, I realized that custom 'Accept' headers are getting overwritten by the ProxyManager._set_proxy_headers function in poolmanager.py (line 124)

I did a bit of googling but couldn't come up with a reason why this should be done; if this modification of the 'Accept' header is not needed, would it be possible to get it removed somehow?
"
80,add studer to contributors,2012-06-19T14:24:42Z,2012-07-22T01:40:20Z,,,,"IPv6 url support and test coverage
"
79,ingnore,2012-06-19T14:06:20Z,2012-06-19T15:30:50Z,,,,"////
"
78,Use urlparse instead of native url parsing implementation,2012-06-16T19:34:17Z,2012-06-30T23:42:24Z,Won't fix?,,,"Trying this out for fun.
"
77,Web page 404,2012-06-16T07:39:00Z,2012-06-16T18:56:47Z,,,,"urllib3 can not access to web pages

import urllib3
http = urllib3.PoolManager()
url = 'http://waptt.com/'
r = http.request('GET', url,  retries = 5)
print r.status
404

But I use curl to get to 200 status
curl -I http://waptt.com
HTTP/1.1 200 OK
"
76,Pool Depletion / Leaking Connections,2012-06-15T16:07:54Z,2012-07-09T02:46:58Z,"Ready, Soon",FetchError,"FetchError: FetchError: msg = ""No retries left, giving up"", original exception = ""EmptyPoolError(""HTTPConnectionPool(host='SUPER_SECRET', port=80): Pool reached maximum size and no more connections are allowed."",)"", url = ""http://SUPER_SECRET_URL/"", retries = 2","In our production environment, we've noticed the following errors in our logs, pool size set to 500:

```
FetchError: FetchError: msg = ""No retries left, giving up"", original exception = ""EmptyPoolError(""HTTPConnectionPool(host='SUPER_SECRET', port=80): Pool reached maximum size and no more connections are allowed."",)"", url = ""http://SUPER_SECRET_URL/"", retries = 2
ERROR - base.py:fetch:166 - FetchError: HTTPConnectionPool(host='SUPER_SECRET', port=80): Pool reached maximum size and no more connections are allowed., retries left = 2
```

After a bit of investigation, it looks like the number of connections in the pool is slow decreasing over time as we encounter other errors, forced timeouts, etc.  On [urllib3/connectionpool.py:424](https://github.com/shazow/urllib3/blob/1c22dec48bc57199d71b86476dbed8f027877824/urllib3/connectionpool.py#L424), if httplib/socket raises an error, the connection will be dropped from the pool because it was acquired on [382](https://github.com/shazow/urllib3/blob/1c22dec48bc57199d71b86476dbed8f027877824/urllib3/connectionpool.py#L382) and never put back on line [431](https://github.com/shazow/urllib3/blob/1c22dec48bc57199d71b86476dbed8f027877824/urllib3/connectionpool.py#L431) since `conn == None`.

This issue can be replicated with the following gist (uses gevent.Timeout, so quasi-related to #69): https://gist.github.com/2932793

It looks like the `if conn` on line 431 was originally added to ensure that a SocketError raised from `_get_conn()` didn't add `None` to the pool since a connection would never be gotten, but in the current incarnation, it has the effect of causing the pool size to shrink over time if a connection is acquired and any error is raised from httplib/socket.
"
75,Gzip CRC check failed,2012-06-14T14:21:18Z,2012-06-16T19:06:50Z,,"HTTPError, except","HTTPError: Received response with content-encoding: gzip, but failed to decode it., except:","# Problem

I use the code:

headers = {  
  'User-Agent':'Baiduspider+(+http://www.baidu.com/search/spider.htm)',
   'Accept-Encoding':'gzip,deflate'  
}
r = http.request('GET', 'http://www.heroone.com', headers=headers)

Error log:
HTTPError: Received response with content-encoding: gzip, but failed to decode it.
# Analysis

Web server, add some ""tail"" behind the data of gzip compression, data.
Some data-extracting modules (such as Python's GzipFile module) in this case will handle exceptions. The browser will automatically discard the extra ""tail"" normal extracting and processing of page data.
# Solve

The python GzipFile module undisclosed attributes: extrabuf responsible for the preservation has been successfully extracting data. Therefore, the following code for better compatibility:

files :urllib3/response.py

Old code:
def decode_gzip(data):
    gzipper = gzip.GzipFile(fileobj=BytesIO(data))

Fix code:

try:
    gf = GzipFile(fileobj=StringIO(html_data), mode=""r"")
    html_data = gf.read()
except:
    html_data = gf.extrabuf
"
74,compatibility with IPv6 addresses,2012-06-12T18:10:38Z,2012-06-16T20:30:47Z,,,,"Add compatibility with IPv6 addresses for `util.get_host` and therefore for `PoolManager.request` :

```
import urllib3
http = urllib3.PoolManager()
r = http.request('GET', 'http://[2a00:1450:4001:c01::67]:80/index.html', retries=10)
print r.status, r.data
```
"
73,Breaks on certain urls that use : in the query string,2012-06-07T17:50:08Z,2012-06-10T20:10:20Z,,ValueError,ValueError: invalid literal for int() with base 10: '',"import requests
requests.get('http://online.wsj.com?CALL_URL=http://online.wsj.com/article/SB10001424052702303640104577436251166644714.html%3fmod=googlenews_wsj')
Traceback (most recent call last):
File """", line 1, in 
File ""/Users/lsemel/www/virtualenvs/muckrack/lib/python2.7/site-packages/requests/api.py"", line 51, in get
return request('get', url, *_kwargs)
File ""/Users/lsemel/www/virtualenvs/muckrack/lib/python2.7/site-packages/requests/api.py"", line 39, in request
return s.request(method=method, url=url, *_kwargs)
File ""/Users/lsemel/www/virtualenvs/muckrack/lib/python2.7/site-packages/requests/sessions.py"", line 200, in request
r.send(prefetch=prefetch)
File ""/Users/lsemel/www/virtualenvs/muckrack/lib/python2.7/site-packages/requests/models.py"", line 463, in send
conn = self._poolmanager.connection_from_url(url)
File ""/Users/lsemel/www/virtualenvs/muckrack/lib/python2.7/site-packages/requests/packages/urllib3/poolmanager.py"", line 89, in connection_from_url
scheme, host, port = get_host(url)
File ""/Users/lsemel/www/virtualenvs/muckrack/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 557, in get_host
port = int(port)
ValueError: invalid literal for int() with base 10: ''

https://github.com/kennethreitz/requests/blob/develop/requests/packages/urllib3/util.py#L75 should probably chop off the query string before checking for a :
"
72,IPv6 support,2012-06-06T11:31:09Z,2012-06-16T20:25:20Z,Soon,,,"With the recent World IPv6 Launch ( http://www.worldipv6launch.org/ ) it would be really nice if `urllib3` could support IPv6 (in such a way, that finally `requests` can support it too).
"
71,Log to DEBUG instead of INFO when a new connection is started,2012-05-14T00:26:18Z,2012-05-14T00:49:54Z,,,,"Logging to INFO creates a lot of noise in the log files when making consecutive requests, e.g., batch requests to the Facebook API. For example:

2012-05-13T17:14:00.411 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:01.099 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:02.374 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:03.238 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:04.338 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:05.474 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:06.240 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:07.131 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:07.800 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:08.685 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:09.830 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:10.612 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:11.476 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:12.355 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:13.302 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:14.644 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:15.463 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:16.164 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:54.365 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:14:56.520 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:15:02.474 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
2012-05-13T17:15:04.426 requests.packages.urllib3.connectionpool: INFO: Starting new HTTPS connection (1): graph.facebook.com
"
70,Support Google App Engine -- POST requests,2012-05-11T22:29:14Z,2012-05-12T01:40:50Z,"Has Failing Test, Soon",CallNotFoundError,CallNotFoundError: The API package 'remote_socket' or call 'Resolve()' was not found.,"I'm unable to re-open issue #61 for some reason, so reporting this separately.

It doesn't appear to work with POST requests:

``` python
import webapp2
import urllib3

class MainPage(webapp2.RequestHandler):
  def get(self):
      http = urllib3.PoolManager()
      r = http.request('GET', 'http://google.com/')
      r2 = http.request('GET', 'http://yahoo.com/')
      r3 = http.request('POST', 'http://www.example.com/')
      r4 = http.request('POST', 'http://www.example.com/', fields={""foo"":""bar""})
      self.response.headers['Content-Type'] = 'text/plain'
      self.response.out.write(r.status)
      self.response.out.write(r2.status)
      self.response.out.write(r3.status)

app = webapp2.WSGIApplication([('/', MainPage)],
                              debug=True)
```

Traceback:

<pre>
The API package 'remote_socket' or call 'Resolve()' was not found.
Traceback (most recent call last):
  File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1511, in __call__
    rv = self.handle_exception(request, response, e)
  File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1505, in __call__
    rv = self.router.dispatch(request, response)
  File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1253, in default_dispatcher
    return route.handler_adapter(request, response)
  File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 1077, in __call__
    return handler.dispatch()
  File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 547, in dispatch
    return self.handle_exception(e, self.app.debug)
  File ""/base/python27_runtime/python27_lib/versions/third_party/webapp2-2.3/webapp2.py"", line 545, in dispatch
    return method(*args, **kwargs)
  File ""/base/data/home/apps/s~megachunt/1.358837729334117339/urllib3_test.py"", line 9, in get
    r3 = http.request('POST', 'http://www.example.com/')
  File ""/base/data/home/apps/s~megachunt/1.358837729334117339/urllib3/request.py"", line 71, in request
    **urlopen_kw)
  File ""/base/data/home/apps/s~megachunt/1.358837729334117339/urllib3/request.py"", line 119, in request_encode_body
    boundary=multipart_boundary)
  File ""/base/data/home/apps/s~megachunt/1.358837729334117339/urllib3/filepost.py"", line 57, in encode_multipart_formdata
    boundary = choose_boundary()
  File ""/base/python27_runtime/python27_dist/lib/python2.7/mimetools.py"", line 140, in choose_boundary
    hostid = socket.gethostbyname(socket.gethostname())
  File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/remote_socket/_remote_socket.py"", line 299, in gethostbyname
    return _Resolve(host, [AF_INET])[2][0]
  File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/remote_socket/_remote_socket.py"", line 251, in _Resolve
    canon, aliases, addresses = _ResolveName(name, families)
  File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/remote_socket/_remote_socket.py"", line 269, in _ResolveName
    apiproxy_stub_map.MakeSyncCall('remote_socket', 'Resolve', request, reply)
  File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/apiproxy_stub_map.py"", line 94, in MakeSyncCall
    return stubmap.MakeSyncCall(service, call, request, response)
  File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/apiproxy_stub_map.py"", line 308, in MakeSyncCall
    rpc.CheckSuccess()
  File ""/base/python27_runtime/python27_lib/versions/1/google/appengine/api/apiproxy_rpc.py"", line 133, in CheckSuccess
    raise self.exception
CallNotFoundError: The API package 'remote_socket' or call 'Resolve()' was not found.
</pre>
"
69,gevent socket does not raise SocketTimeout and Timeouts fail,2012-04-30T18:53:05Z,2014-07-03T23:07:34Z,Someday,,,"The gevent monkey patching on `socket` for some reason (have yet to find out why) does not properly raise a SocketTimeout the way it should and as a consequence `urllib3` will not raise a Timeout properly if you're using gevent.

How to reproduce:
Create unresponsive socket (simulate timeout)

```
nc -l 8080
```

```
from gevent.monkey import patch_all
patch_all()
import urllib3
http = urllib3.PoolManager()
r = http.request('GET', 'http://localhost:8080/',timeout=1)
```

The Timeout is never raised.

One solution is to use `gevent.socket` when making the connection, that should properly raise a SocketTimeout. 
"
68,http/socks4/socks5 Proxy support,2012-04-27T20:08:00Z,2013-10-18T11:09:19Z,"In Progress, Soon",,,"I've finally managed to refactor the proxy support changes from requests lib into urllib3.

Test results:

```

 foxx@test01 [~/development/urllib3] > python test/test_proxy.py
TEST Proxy: http://192.168.56.1:8888
HTTPS OK
HTTP OK

---
TEST Proxy: socks4://192.168.56.1:8889
HTTPS OK
HTTP OK

---
TEST Proxy: socks5://192.168.56.1:8889
HTTPS OK
HTTP OK

---
TEST: No proxy via PoolManager
HTTPS OK
HTTP OK

---
TEST: No proxy via connection_from_url
HTTPS OK
HTTP OK
```

Could one of the core devs please review this patch and make sure you are happy with it. Once signed off, I will go ahead and do a documentation update as well (want to make sure everyone's happy first).

I've used as many of the suggestions from other tickets as possible, but please let me know if I have missed anything or if it needs to be changed.

My sincere apologies it has taken so long to get this done, spare time is like gold dust :(

This references the following discussions:

https://github.com/shazow/urllib3/pull/56
https://github.com/kennethreitz/requests/pull/478
https://github.com/kennethreitz/requests/issues/362

Others involved:
@senko @wolever @kennethreitz @shazow @Anorov 
"
67,Add support for buffering kwarg in urllib3.,2012-04-02T16:04:54Z,2012-04-08T18:16:20Z,,,,"No urllib3 buffering:
Average time: 0.00111172451973, Total time: 5.55862259865

urllib3 bufferring:
Average time: 0.000722657775879, Total time: 3.61328887939

This was from a small benchmark performing 5000 GET requests.

Fix for issue #66.
"
66,Add support for HTTPConnection.getresponse(buffering=True),2012-04-02T15:32:07Z,2012-05-15T08:17:49Z,"Ready, Soon",,,"Python 2.7 has added support for buffering when reading from HTTP connections, which leads to a significant improvement in HTTP client performance.

See: http://bugs.python.org/issue4879

It looks like the only changes required would be in HTTPConnectionPool._make_request, passing buffering=True to the getresponse() call on Python 2.7 and later. 

urllib2.py does the following:

``` python
try:
    r = h.getresponse(buffering=True)
except TypeError: #buffering kw not supported
    r = h.getresponse()
```
"
65,ImportError: cannot import name exceptions  since 1.3,2012-03-27T12:50:57Z,2012-06-16T19:13:34Z,,ImportError,ImportError: cannot import name exceptions,"from urllib3 import HTTPSConnectionPool
  File ""/usr/local/lib/python2.7/dist-packages/urllib3-1.3-py2.7.egg/urllib3/**init**.py"", line 22, in <module>
    from . import exceptions
ImportError: cannot import name exceptions

with easy_install
"
64,Added url to LocationParseError message,2012-03-24T16:37:23Z,2012-03-24T16:37:47Z,,,,"urllib3/util.py
- Added the url that couldn't be parsed to LocationParseError message.
"
63,[Errno 35] 'Resource temporarily unavailable' on Mac OS X,2012-03-20T06:02:24Z,2012-03-21T12:22:48Z,,,,"We are seeing ""error(35, 'Resource temporarily unavailable')"" thrown from urllib when running on mac.  It appears this is a known issue in python, when the caller of send must handle EAGAIN errors on BSD platforms.

This was observed with urllib3-1.1, OSX 10.6, Python 2.7.  Not surprisingly, it's particularly common over slow network connections.
"
62,Fix Sphinx complaint. Methods named 'request' are very common and cause ...,2012-03-19T01:20:43Z,2012-03-19T01:21:44Z,,,,"...cross-reference warnings.

If you want it this commit with fix those warnings.
"
61,Support Google App Engine,2012-03-16T17:42:34Z,2012-03-17T19:09:14Z,Soon,,,"[Google App Engine](http://code.google.com/appengine/) [restricts which modules can be used in their environment](http://code.google.com/appengine/docs/python/runtime.html#Pure_Python), and sadly, `select` is not on their whitelist. Is there a way to make `urllib3` work on App Engine without using this module? [`requests`](python-requests.org) depends on `urllib3`, and the `select` modules prevents users from using `requests` on App Engine.
"
60,Fix the IPv4 dummyserver/nosetests issue (issue #59),2012-03-13T10:20:50Z,2012-03-13T15:18:29Z,,,,"Just a little one line change to fix the issue (and I corrected a minor typo). See https://github.com/shazow/urllib3/issues/59 for the full info.
"
59,nosetests crashes under IPv4 (error: getsockaddrarg: bad family),2012-03-13T10:10:28Z,2012-03-13T15:19:16Z,,,,"Turns out tornado is really eager to use IPv6. Unless you expressly hand the server the address, it doesn't even check for socket IPv6 support. I'll submit a pull request for the one-line fix in dummyserver/server.py momentarily.

Source: https://groups.google.com/group/python-tornado/browse_thread/thread/3ec04536e57a2833?pli=1
"
58,Moving away from httplib,2012-03-13T08:50:36Z,,Someday,,,"Kicking off a tracking thread for some of the work started at the PyCon 2012 Requests/urllib3 sprints.

I'm not sure as to the full history of the issue, but the general driving factor is more transparency and control over behaviors that are overly opaque in the standard library's httplib, which seems to have been written without taking into account:
- Clean proxying
- HTTP parsing without establishing a connection
- Incremental HTTP parsing
- Raw socket control
- (and a few more features I can't recall)

The validity of these concerns was at least generally confirmed by Guido himself, in person, when @kennethreitz and I spoke to him and he agreed that certain batteries included with Python have started to lose their charge; libraries like urllib/urllib2/httplib are out of touch with the new momenta of web technologies.

The refactorings involved in fixing the above issues seem to point to a new httplib-like library that is more extensible and configurable. The emergence of libraries like @benoitc's [http-parser](https://github.com/benoitc/http-parser) also support this direction and urllib3 is certainly one of the best-positioned libraries for attempting this sort of enhancement.

Design discussions thus far have involved @shazow, @wolever, @atdt, @kennethreitz, @easel, @doublereedkurt, and @brandon-rhodes. More notes to follow.
## 
"
57,Fixing bugs in `PoolManager`'s docstring,2012-03-12T20:48:26Z,2012-03-12T20:53:00Z,,,,
56,Support for CONNECT tunnelling of HTTPS over HTTP proxies,2012-03-10T20:20:26Z,2012-06-16T19:12:36Z,,,,"Implemented a way to request usage of HTTPS even when using HTTP proxy, as per discussion in https://github.com/kennethreitz/requests/issues/362#issuecomment-3858000

This is more a request for comments than a request to pull, as I'm not sure if this is the best way to do it.

I've tried to make the tests as well, and hit a snag: to properly test it you need HTTP tunnelling-capable proxy and a HTTPS server that gives different results for HTTP and HTTPS (to test the proxy is really working), and I couldn't get the dummy server runner to have two tornado instances (one for the tunnel-capable proxy, other for the HTTPS server).

Example usage:

``` python

conn = proxy_from_url('http://127.0.0.1:%s/' % self.proxy_port,
    force_https=True)
conn.request('GET', 'https://localhost:%s/' % self.port)
```
"
55,Configuring proxies through environment variables,2012-03-07T23:48:40Z,2012-03-08T09:49:45Z,Someday,,,"It would be really convenient if one could configure proxies with environment variables (case in point, [urllib](http://docs.python.org/library/urllib.html#high-level-interface)).
"
54,Fix for the multiple cookies problem (Issue #3 and #53),2012-03-02T17:34:02Z,2012-03-03T21:53:32Z,,,,"In response to [issue 3](https://github.com/shazow/urllib3/issues/3) and [issue 53](https://github.com/shazow/urllib3/issues/53), headers with the same name are comma-separated under a single entry in the headers dict.
"
53,All but one Set-Cookie headers are overwritten  ,2012-03-01T18:33:46Z,2012-03-03T21:54:38Z,,,,"I have been using urllib3 via [Requests](http://docs.python-requests.org/en/v0.10.6/index.html), and I noticed that I was only getting one cookie back. After some investigation, it looks like the problem is caused by the fact that all of the HTTP headers are converted from a list of 2-tuples (from `httplib`) to a dictionary. This is normally okay, but when the server sends back multiple `Set-Cookie` headers, all but the last are overwritten.

Minimal code to reproduce. I will leave that URL up until this issue is resolved or I am smacked upside the header for something stupid I am missing.

```
import urllib3

http = urllib3.PoolManager()

r = http.request(""GET"", ""http://www.joelverhagen.com/sandbox/tests/cookies.php"")

print(r.headers)
```

I am using Python 3.2.2 and urllib3 v1.2.2.

**Edit:** It may be a good idea to open an issue on the Requests GitHub repo if this is indeed a problem. I was going to do it, but I wanted to first check on this end.
"
52,Pipelining?,2012-02-28T18:52:42Z,2014-07-03T23:09:20Z,Someday,,,"I know this is super lame to file an issue instead of emailing a mailing list or something for a feature question, but I can't find a urllib3 mailing list anywhere.

Anyway, I am trying to find any kind of support for HTTP pipelining in an existing library in Python before attempting something more drastic. Specifically, I am trying to pipeline a series of PUTs (yeah, they're idempotent), like so:

PUT
PUT
PUT
get response
get response
get response

Ideally, I'd love for this to be handled for me in some kind of threadsafe way, but I'm willing to do it myself. urllib3 seems really close! Threadsafe, connection pooling, the works! I even saw some other website that offhandedly speculated that urllib3 might even do pipelining. But I hardly think that's possible, as you have to call release_connection manually after reading the body of a response.

Anyway, do you know anything about this? It's sort of surprising to me how undersupported this HTTP feature seems to be. If urllib3 doesn't support it, any wise thoughts on what I might have to do instead?

-JT
"
51,Support for posting big files/streaming objects,2012-02-26T22:21:29Z,,Someday,,,"Currently, it is not possible to post a 4 GB file using urllib3 since that requires reading the entire file content into a buffer before sending. (unless I'm missing something?)
It would be nice if we could pass file-like objects (objects with a read() method or something) as post variables, which will be read from on demand and perform well on huge files.
The only python script capable of this that I'm aware of is http://atlee.ca/software/poster/, which is a urllib2 opener class.
## 
"
50,HTTPS requests fail through a proxy,2012-02-23T03:19:54Z,2015-01-27T14:55:59Z,Someday,,,"Requests to access a secure website (SSL/TLS) fail through a proxy.
Urllib3 does not properly implement the HTTP CONNECT method.

For example the following code should print 200.
Instead, with a burp proxy, it prints 502.

import urllib3

proxy = urllib3.proxy_from_url('http://localhost:8080/'
response = proxy.urlopen('GET', 'https://www.google.com/index.html')

print response.status
"
49,multiple values for a single key,2012-02-21T14:34:48Z,2012-02-25T23:21:20Z,,,,"... if the request is with files.

fixed: issue #48: Urllib3 doesn't allow multiple values for a single key if the request is with files. by adding list support for encode_multipart_formdata
by adding list support for encode_multipart_formdata
"
48,multiple values for a single key,2012-02-21T02:31:01Z,2012-02-25T23:20:31Z,Urgent,,,"Urllib3 doesn't allow multiple values for a single key if the request is with files.

I found this bug when I'm using Requests.  https://github.com/kennethreitz/requests/issues/285

I sent following a Pull request to Requests. But It was rejected, because it needs to modify urllib3.
https://github.com/kennethreitz/requests/pull/422#issuecomment-4058039
"
47,Please ship test-requirements.txt or avoid using it,2012-02-06T11:49:42Z,2012-02-06T16:01:06Z,,IOError,IOError: [Errno 2] No such file or directory: 'test-requirements.txt',"Hello,
test-requirements.txt is not shipped with the source so installing urllib3. However I think that listing requirements inside setup.py is a better choice.

```
$ pip install urllib3
Downloading/unpacking urllib3
  Downloading urllib3-1.2.1.tar.gz
  Running setup.py egg_info for package urllib3
    Traceback (most recent call last):
      File ""<string>"", line 14, in <module>
      File ""/home/eriol/.virtualenvs/testurllib3/build/urllib3/setup.py"", line 25, in <module>
        tests_requirements = requirements + open('test-requirements.txt').readlines()
    IOError: [Errno 2] No such file or directory: 'test-requirements.txt'
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""<string>"", line 14, in <module>

  File ""/home/eriol/.virtualenvs/testurllib3/build/urllib3/setup.py"", line 25, in <module>

    tests_requirements = requirements + open('test-requirements.txt').readlines()

IOError: [Errno 2] No such file or directory: 'test-requirements.txt'
```

Kind regards,
Daniele Tricoli
"
46,Increase unit test code coverage from 99% to 100%,2012-02-05T21:43:05Z,2012-09-04T05:01:17Z,Contributor Friendly ‚ô•,,,"There's basically 3 lines which aren't covered. Should be easy enough to cover them with a couple more tests.

IIRC something to do with timeouts and decoding.

Very contributor friendly if you're looking to dive into the codebase. :-)
"
45,request contains full url,2012-02-05T08:35:19Z,2012-02-05T17:05:11Z,,,,"The server is getting confused with the request ""GET http://blag.xkcd.com/ HTTP/1.1"":

```
>>> import urllib3
>>> url = ""http://blag.xkcd.com/""
>>> conn = urllib3.connection_from_url(url)
>>> r = conn.request(""GET"", url, redirect=False)
>>> r.status
301
>>> r.get_redirect_location()
'http://blag.xkcd.comhttp/blag.xkcd.com/'
>>> r.headers
{'content-length': '0', 'x-powered-by': 'PHP/5.2.6-1+lenny13', 'expires': 'Wed, 11 Jan 1984 05:00:00 GMT', 'vary': 'Accept-Encoding', 'server': 'Apache', 'last-modified': 'Sun, 05 Feb 2012 08:15:10 GMT', 'connection': 'close', 'location': 'http://blag.xkcd.comhttp/blag.xkcd.com/', 'pragma': 'no-cache', 'cache-control': 'no-cache, must-revalidate, max-age=0', 'date': 'Sun, 05 Feb 2012 08:15:10 GMT', 'content-type': 'text/html; charset=UTF-8', 'x-pingback': 'http://blog.xkcd.com/xmlrpc.php'}
```

It works fine if the request is ""GET / HTTP/1.1"":

```
>>> r = conn.request(""GET"", ""/"", redirect=False)
>>> r.status
200
>>> r.headers
{'x-powered-by': 'PHP/5.2.6-1+lenny13', 'transfer-encoding': 'chunked', 'vary': 'Accept-Encoding', 'server': 'Apache', 'connection': 'close', 'date': 'Sun, 05 Feb 2012 08:16:55 GMT', 'content-type': 'text/html; charset=UTF-8', 'x-pingback': 'http://blog.xkcd.com/xmlrpc.php'}
```

This behavior might be considered user's fault, but the same behavior is seen with PoolManager, where the library is expected to figure out the correct connection from the full url.
"
44,I am Interested in Adding SOCKS Proxy Support,2012-01-31T07:01:03Z,2012-06-16T20:45:33Z,,,,"Hi, I maintaining a fork of socksipy-branch, called socksipy-x, which is at https://github.com/brendoncrawford/socksipy-x. Socksipy and Socksipy-Branch have not been updated for a while, so I intend to fix some bugs and try to maintain the codebase when needed.

Before I embark on the task of adding SOCKS support to urllib3, I wanted to see if this is even something you would be interested in merging in? If you were, it could either be referenced as a dependency/submodule, or I could just copy the entire socks.py file directly into urllib3, so no external dependency would be required.

Any thoughts?
"
43,Track upload/download progress,2012-01-29T15:46:22Z,2012-01-29T19:37:34Z,,,,"I really miss this feature, because I'm developing a simple upload application.
I think this would be an interesting feature.
_Moved here from [a Requests bug](https://github.com/kennethreitz/requests/issues/396)_
"
42,Python3 fixes1,2012-01-24T23:32:34Z,2012-01-24T23:34:01Z,,,,"Just a bit of fiddling to get more tests passing.

Redirection tests were expecting status 303 (see other), but dummyserver handler explicitly specified 302 (found). For now, I've changed dummyserver to return 303, but you could equally change the tests.
"
41,urllib3 does not install if Python has no SSL support,2012-01-24T17:59:35Z,2012-01-31T18:19:15Z,Urgent,"ImportError, urllib2.URLError","ImportError: cannot import name HTTPSConnection, urllib2.URLError: <urlopen error unknown url type: https>","_( Moved over from requests: https://github.com/kennethreitz/requests/issues/306 )_

On attempting to import urllib3 in a python environment without SSL support, the following error is received:

```
File ""urllib3/connectionpool.py"", line 11, in <module>
    from httplib import HTTPConnection, HTTPSConnection, HTTPException
ImportError: cannot import name HTTPSConnection
```

For comparison / reference, urllib2 does import on the same system. Attempting to retrieve a HTTPS url via urllib2.urlopen(...) results in this error:

```
urllib2.URLError: <urlopen error unknown url type: https>
```

Standard HTTP requests, however, using urllib2 work fine in this python environment.

Perhaps urllib3 should error out in a similar fashion?
"
40,Trove classifiers for Python 2 & 3,2012-01-23T15:13:57Z,2012-01-23T16:04:42Z,,,,"This lets automatic tools recognise that the package works on Python 2 and 3.
"
39,redirect to bogus URL generates low-level exception; can do better,2012-01-23T13:38:41Z,2012-02-05T19:41:25Z,,ValueError,ValueError: invalid literal for int() with base 10: '',"This URL parses correctly, however the redirect leads to a bogus URL ---  (nice work Patch.com :-)

Can we cause this to generate a more useful exception?  Something specifically about the redirect being bogus?

Here is the URL parsing correctly:

```
>>> import urlparse
>>> urlparse.urlparse('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day')
ParseResult(scheme='http', netloc='stclairshores.patch.com', path='/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day', params='', query='', fragment='')
>>> urlparse.urlparse('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day').port
>>> 
```

Here is the redirect that it generates...  notice the http://http:// at the beginning of the new location!

```
>>> r = requests.get('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day', allow_redirects=False)
>>> r.headers
{'status': '302', 'content-length': '160', 'content-encoding': 'gzip', 'set-cookie': 'p13n=%5B%5D; path=/, _patch_session=BAh7BzoPc2Vzc2lvbl9pZCIlNmEzYzM4YzZjMWIxMTdiNDkxMmEwNmEwM2JmZDQzYTU6FnByb21wdF9mb3Jfc3VydmV5aQA%3D--547af227538bb1d039809a3d01eaadb320a9a42b; domain=patch.com; path=/', 'x-powered-by': 'Phusion Passenger (mod_rails/mod_rack) 3.0.11', 'vary': 'Accept-Encoding', 'server': 'Apache/2.2.15 (Unix) mod_ssl/2.2.15 OpenSSL/0.9.8l Phusion_Passenger/3.0.11', 'x-runtime': '15', 'location': 'http://http://www.dailytribune.com/articles/2011/11/09/news/doc4ebb336cad1c7378471368.txt?viewmode=fullstory', 'cache-control': 'no-cache', 'date': 'Mon, 23 Jan 2012 13:32:36 GMT', 'content-type': 'text/html; charset=utf-8', 'x-rack-cache': 'miss'}
>>> 
```

So, of course urllib cannot open it:

```
>>> import urllib
>>> g = urllib.urlopen('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day').read()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/urllib.py"", line 84, in urlopen
    return opener.open(url)
  File ""/usr/lib/python2.7/urllib.py"", line 205, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 356, in open_http
    return self.http_error(url, fp, errcode, errmsg, headers)
  File ""/usr/lib/python2.7/urllib.py"", line 369, in http_error
    result = method(url, fp, errcode, errmsg, headers)
  File ""/usr/lib/python2.7/urllib.py"", line 632, in http_error_302
    data)
  File ""/usr/lib/python2.7/urllib.py"", line 659, in redirect_internal
    return self.open(newurl)
  File ""/usr/lib/python2.7/urllib.py"", line 205, in open
    return getattr(self, name)(url)
  File ""/usr/lib/python2.7/urllib.py"", line 331, in open_http
    h = httplib.HTTP(host)
  File ""/usr/lib/python2.7/httplib.py"", line 1061, in __init__
    self._setup(self._connection_class(host, port, strict))
  File ""/usr/lib/python2.7/httplib.py"", line 693, in __init__
    self._set_hostport(host, port)
  File ""/usr/lib/python2.7/httplib.py"", line 718, in _set_hostport
    raise InvalidURL(""nonnumeric port: '%s'"" % host[i+1:])
httplib.InvalidURL: nonnumeric port: ''
```

urllib3 has the same error:

```
>>> http_pool = urllib3.connection_from_url('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day')>>> r = http_pool.get_url('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day')Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""urllib3/request.py"", line 136, in get_url
    **urlopen_kw)
  File ""urllib3/request.py"", line 78, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""urllib3/connectionpool.py"", line 410, in urlopen
    retries - 1, redirect, assert_same_host)
  File ""urllib3/connectionpool.py"", line 336, in urlopen
    if assert_same_host and not self.is_same_host(url):
  File ""urllib3/connectionpool.py"", line 246, in is_same_host
    scheme, host, port = get_host(url)
  File ""urllib3/connectionpool.py"", line 538, in get_host
    port = int(port)
ValueError: invalid literal for int() with base 10: ''
>>> 
```

and it propagates through to requests too:

```
>>> 
>>> r = requests.get('http://stclairshores.patch.com/articles/shores-veteran-to-receive-complimentary-wedding-on-veterans-day')Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 50, in get
    return request('get', url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 38, in request
    return s.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 200, in request
    r.send(prefetch=prefetch)
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 514, in send
    self._build_response(r)
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 253, in _build_response
    request.send()
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 430, in send
    conn = self._poolmanager.connection_from_url(url)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py"", line 94, in connection_from_url
    scheme, host, port = get_host(url)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py"", line 524, in get_host
    port = int(port)
ValueError: invalid literal for int() with base 10: ''
```
"
38,URL parsing errors,2012-01-23T13:18:41Z,2012-01-23T17:05:21Z,,"urllib3.exceptions.HostChangedError, ValueError","urllib3.exceptions.HostChangedError: Connection pool with host 'http://stats.e-go.gr' tried to open a foreign host: mailto:advertising@pegasusinteractive.gr, ValueError: invalid literal for int() with base 10: 'advertising%40pegasusinteractive.gr'","This is an example of  URL that urllib3 fails to parse correctly.  It appears that it un-encodes the URL and then gets confused about the colon symbol in the mailto: statement that is part of the path.  

Strangely enough, requests generates a different error for the same URL.  For urllib3, it makes the pool think that the URL is from a different host.  For requests, it thinks the other host is a new port number... and tries to int(string)...

Is this a known issue?

```
>>> http_pool = urllib3.connection_from_url('http://stats.e-go.gr/rx.asp?nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr')
>>> r = http_pool.get_url('http://stats.e-go.gr/rx.asp?nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr')Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""urllib3/request.py"", line 136, in get_url
    **urlopen_kw)
  File ""urllib3/request.py"", line 78, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""urllib3/connectionpool.py"", line 410, in urlopen
    retries - 1, redirect, assert_same_host)
  File ""urllib3/connectionpool.py"", line 341, in urlopen
    raise HostChangedError(host, url, retries - 1)
urllib3.exceptions.HostChangedError: Connection pool with host 'http://stats.e-go.gr' tried to open a foreign host: mailto:advertising@pegasusinteractive.gr
>>> 
>>>
>>> import requests
>>> r = requests.get('http://stats.e-go.gr/rx.asp?nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr')Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 50, in get
    return request('get', url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 38, in request
    return s.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 200, in request
    r.send(prefetch=prefetch)
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 514, in send
    self._build_response(r)
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 253, in _build_response
    request.send()
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 430, in send
    conn = self._poolmanager.connection_from_url(url)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py"", line 94, in connection_from_url
    scheme, host, port = get_host(url)
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py"", line 524, in get_host
    port = int(port)
ValueError: invalid literal for int() with base 10: 'advertising%40pegasusinteractive.gr'
>>> 
```

Just as a check, this is parsed correctly by urlparse:

```
>>> urlparse.urlparse('http://stats.e-go.gr/rx.asp?nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr')
ParseResult(scheme='http', netloc='stats.e-go.gr', path='/rx.asp', params='', query='nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr', fragment='')
>>> 
>>> urlparse.urlparse('http://stats.e-go.gr/rx.asp?nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr').port
>>> 
```

But, doh!  This is the same as issue #39, because it is a bogus redirect... nice work stats.e-go.gr....

```
>>> r = requests.get('http://stats.e-go.gr/rx.asp?nWebSrvID=100230&nCatID=23425&nLevelId=-20&target=mailto%3Aadvertising%40pegasusinteractive%2Egr', allow_redirects=False)
>>> r
<Response [302]>
>>> r.headers
{'content-length': '161', 'x-powered-by': 'ASP.NET', 'set-cookie': 'nUserID=4862144; expires=Mon, 21-Jan-2013 22:00:00 GMT; path=/, ASPSESSIONIDCSQABRTR=NOJOKALCFLCHFCALIBBMFJIJ; path=/', 'expires': 'Mon, 23 Jan 2012 13:40:53 GMT', 'server': 'Microsoft-IIS/6.0', 'location': 'mailto:advertising@pegasusinteractive.gr', 'cache-control': 'False', 'date': 'Mon, 23 Jan 2012 13:41:53 GMT', 'p3p': 'CP=""CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR""', 'content-type': 'text/html'}
>>> 
>>> 
```
"
37,Another contributor,2012-01-23T03:41:35Z,2012-01-23T03:48:35Z,,,,"Here you go, shazow ‚Äî thanks for reminding me!
"
36,Contributor list,2012-01-23T00:14:09Z,2012-01-23T00:15:27Z,,,,"Added myself to the contributors list, as suggested. I always forget that sort of thing.
"
35,Python 3 compatibility,2012-01-22T18:24:22Z,2012-01-29T21:30:41Z,"Has Failing Test, Soon",,,"Tested on Python 2.7 and 3.2 - it should work with 2.6 and 3.1 as well.

There's one failing test on Python 3 - when it tests verified HTTPS, it gets an error with the string `error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed` rather than the error it's expecting `No root certificates specified for verification of other-side certificates.` I don't know if this is a real problem or not.
"
34,"Two changes: use poll, and include absolute URL",2012-01-22T14:50:42Z,2012-01-22T22:06:41Z,,,,"Shazow and Ken,  please consider these two changes.

If there is a testing framework that you are using, let's discuss what kind of tests would make sense here.

Thanks!
John
"
33,placeholder...,2012-01-22T13:14:31Z,2012-01-22T18:04:59Z,,,,
32,Provide a way for callers to see HTTP exceptions,2012-01-21T01:59:43Z,2014-07-03T23:12:11Z,Soon,,,"Whether a `request()` or `urlopen()` call is given `retries=0` or `retries=<positive-int>`, the exception returned on most connection and network failures is a `MaxRetryError`. This prevents client code from seeing the ‚Äúreal error‚Äù that killed the connection and, therefore, clients based on `urllib3` cannot, say, adjust their behavior based on whether the problem is any of the various errors at the HTTP level, or whether the problem is some specific socket error.

At least three solutions are possible, and I defer to the maintainers to decide which is the most `urllib3`-ish.

First, it could be judged a design error to have introduced `MaxRetryError` in the first place, and the final attempt's exception could be allowed to make it through to the client (some would want to throw in a `retry_count` attribute to add a bit of information about what happened, but I am not sure that that is advisable). The `except` clause near line 350 of `connectionpool.py` would then look something like:

``` python
        except (HTTPException, SocketError), e:
            # Connection broken, discard. It will be replaced next _get_conn().
            conn = None
            # If this was the last try, let the caller see the real error.
            if not retries:
                raise
```

Other adjustments to the code might accomplish the same thing ‚Äî a larger refactoring could eliminate even making the final attempt inside of an `except:`, for example ‚Äî but this is the simplest approach I could find for this first approach.

Second, the `MaxRetryError` could be marked up with a list of failures. This would be a bit tricky, since there are finicky problems with keeping stack traces around, but at least the exception object from each failure could be kept around ‚Äî shorn of its stack trace ‚Äî and delivered back with the `MaxRetryError`. This would even make it clear whether the problem was an actual network death of the HTTP protocol, or whether something like a redirect loop had kept the request going through too many attempts ‚Äî and thus much more information would be available for the client to determine what had happened.

Finally, `retries=None` could be a special signal that the bare exception should be returned. This would maintain (so far as I can see) 100% compatibility with the current code base. The `ConnectionPool.urlopen()` function needs two or three lines adjusted so that the `None` value avoids offending the various `if` statements, but with minimal intrusion the change lets someone like me ‚Äî whose application, alas, cares quite deeply what mode of failure an HTTP request encounters ‚Äî take advantage of `urllib3` but without breaking existing code that might always expect the `MaxRetryError` exception. (Oh, and, beware ‚Äî in Python 2, `None < 0` is `True`, I just discovered, so the `retries < 0` will catch a `None` value unless an extra clause is inserted!)

Of course, other approaches might also be possible that I have not thought of.

I very much like the quality of code that I am seeing in `urllib3` and, if I can only get the actual exceptions back that it encounters, these already-written connection pools are going to save me a lot of work. Thanks!
"
31,issue with PoolManager.request() method,2012-01-19T18:32:36Z,2012-01-20T07:06:04Z,,,,"hey, 

there's two issues i noticed today:  when i do a POST like this (using the example from your github wiki):

```
import urllib3 # (Version 1.0)

http = urllib3.PoolManager()

params = ...

r = http.request('POST', 'http://localhost:1111/my/post/uri' , params)

print r.status, r.data
```

The POST directive in the request headers does not remove the host url, e.g. it will do:

```
POST http://localhost:1111/my/post/uri
```

instead of 

```
POST /my/post/uri
```

Additionally, I noticed if i do an PoolManager.request('GET', 'http://blahblah.com', 80) after creating it tries to go into a loop retrying when I don't specify the port # in the URL. that's because it sets self.port = 80 in is_host_same() even when get_host returns ""None"".  

Let me know if I am using the API wrong, or you have problems reproducing this, or there any other issues.
"
30,Roll tests into the urllib3 package,2012-01-18T17:49:48Z,2012-01-22T00:05:06Z,Won't fix?,,,"If someone today installs `urllib3` with `pip` or `easy_install` into a novel environment ‚Äî like the 64-bit version of an operating system that none of us have tested the library against ‚Äî and receives an exception, then there is no easy way for them to quickly run the test suite to see whether the problem is that their code is faulty, or that `urllib3` itself fails its own tests in the new environment.

But if we move the `test` directory, that currently sits anonymously at the top of the repository, down into the `urllib3` directory and start shipping it as a sub-package, then users who want to double-check that `urllib3` is working would simply be able to type:

`python -m unittest discover urllib3`

Or, if they were using Python <2.7, then they could install `unittest2` and use its `unit2` command line that Michael Foord developed so that older Pythons can also enjoy the new, standard way for tests to be auto-discoverable.

It is not clear to me whether the `dummyserver` top-level package that also sits in `setup.py` stands in the way of subordinating the tests beneath the package itself. (Would the dummy server need to move down inside `urllib3/tests`?)
"
29,Fixes issue #28,2012-01-17T23:09:51Z,2012-01-21T21:52:36Z,,,,"I have resolved issue #28 to my satisfaction. All tests pass, including the new one I wrote specifically for this issue. The slight change I have made to host comparison looks very safe ‚Äî it can only make the function more likely to succeed, so should not cause any failures where pools were working before! Let me know if any cleanup or style changes would make this easier to absorb. Thanks for all your work on httplib3!
"
28,"""Usage"" example at top of docs raises MaxRetryError",2012-01-16T19:33:20Z,2012-01-21T21:57:48Z,"Ready, Urgent",urllib3.exceptions.MaxRetryError,urllib3.exceptions.MaxRetryError: Max retries exceeded for url: http://www.google.com/,"The ""Getting Started"" section of the documentation suggests this simple test:

``` python
>>> import urllib3
>>> http = urllib3.PoolManager()
>>> r = http.request('GET', 'http://google.com/')
```

But actually running this code results in an exception:

```
Traceback (most recent call last):
  File ""test.py"", line 7, in <module>
    r = http.request('GET', 'http://www.google.com/')
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/request.py"", line 65, in request
    **urlopen_kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/request.py"", line 78, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 113, in urlopen
    return self.urlopen(method, e.new_url, **kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 113, in urlopen
    return self.urlopen(method, e.new_url, **kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 113, in urlopen
    return self.urlopen(method, e.new_url, **kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 113, in urlopen
    return self.urlopen(method, e.new_url, **kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/poolmanager.py"", line 109, in urlopen
    return conn.urlopen(method, url, **kw)
  File ""/home/brandon/v/local/lib/python2.7/site-packages/urllib3/connectionpool.py"", line 309, in urlopen
    raise MaxRetryError(url)
urllib3.exceptions.MaxRetryError: Max retries exceeded for url: http://www.google.com/
```

The reason is that each attempt to make a connection is dying with `HostChangedError` because the `is_same_host()` method in `connectionpool.py` is getting back the tuple `('http', 'www.google.com', None)` from `get_host(url)` but the slightly different tuple `('http', 'www.google.com', 80)` when it combines `self.scheme` with `self.host` and `self.port`.

Attempting the test with the URL `http://google.com:80/` also fails because a first successful request is made that redirects to `http://www.google.com/` which then dies as well with the `None != 80` problem.

Only a test with the URL `http://www.google.com:80/` succeeds, because only in that case is the explicit port number present to make `is_same_host()` succeed, and no subsequent redirect occurs to break things.

It looks like either `get_host()` needs to throw in the port `80` when building its tuple, or ‚Äî if that would ruin the purity and usefulness of that function ‚Äî the `is_same_host()` function needs to detect the `None` port number coming back from `get_host()` and upgrade it to 80 or 443 as appropriate.

Or: is the problem that the `self.port` that `is_same_host()` is grabbing used to have the value `None` as well, and it's the port having the value `80` so early in the process that is the problem here?

If more experienced project members could point me in the right direction here, I would be happy to contribute a patch and pull request. I could even add a test for the usage example in the documentation, so that it stays working. :) Thanks for your work on urllib3!
"
27,respect decode_content argument from urlopen,2011-12-31T07:12:47Z,2011-12-31T07:17:16Z,,,,"Make sure decode_content argument is respected when it is set from urlopen as well as from response.read
"
26,Add identity to connections and connection pools.,2011-12-22T22:19:02Z,2012-01-02T21:49:56Z,Won't fix?,,,"After this output from `test/with_dummyserver/test_connectionpool.py` looks like this

```
Finding files... done.
Importing test modules ... done.

HTTPConnectionPool, localhost:18081 (1): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (1): ""GET /encodingrequest HTTP/1.1"" 200 21
HTTPConnectionPool, localhost:18081 (1): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (2): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (2): ""GET /encodingrequest HTTP/1.1"" 200 33
HTTPConnectionPool, localhost:18081 (2): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (4): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (4): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (4): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (4): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (4): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (4): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (4): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (4): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (4): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (6): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (6): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (6): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (6): Starting new (2) HTTP connection
HTTPConnectionPool, localhost:18081 (6): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (6): Starting new (3) HTTP connection
HTTPConnectionPool, localhost:18081 (6): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (6): Connection HTTPConnection (3) returned to the pool
HTTPConnectionPool, localhost:18081 (6): Starting new (4) HTTP connection
HTTPConnectionPool, localhost:18081 (6): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (6): Connection HTTPConnection (4) returned to the pool
HTTPConnectionPool, localhost:18081 (6): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (6): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (6): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (7): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (7): ""GET /specific_method?method=GET HTTP/1.1"" 200 0
HTTPConnectionPool, localhost:18081 (7): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (9): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (9): ""GET /keepalive?close=0 HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (9): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (9): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (9): ""GET /keepalive?close=0 HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (9): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (11): ""GET /keepalive?close=1 HTTP/1.1"" 200 7
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (11): ""GET /keepalive?close=0 HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (11): ""GET /keepalive?close=1 HTTP/1.1"" 200 7
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (11): ""GET /keepalive?close=0 HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (11): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (13): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (13): ""POST /echo HTTP/1.1"" 200 2140
HTTPConnectionPool, localhost:18081 (13): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (14): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (14): ""GET /redirect?target=%2F HTTP/1.1"" 303 112
HTTPConnectionPool, localhost:18081 (14): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (14): Redirecting /redirect?target=%2F -> http://localhost:18081/
HTTPConnectionPool, localhost:18081 (16): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (16): ""GET /echo?lol=cat HTTP/1.1"" 200 7
HTTPConnectionPool, localhost:18081 (16): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (17): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (17): ""POST /specific_method HTTP/1.1"" 200 0
HTTPConnectionPool, localhost:18081 (17): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (18): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (18): ""POST /echo HTTP/1.1"" 200 289
HTTPConnectionPool, localhost:18081 (18): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (19): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (19): ""POST /echo HTTP/1.1"" 200 22
HTTPConnectionPool, localhost:18081 (19): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (20): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (20): ""GET /redirect?target=%2F HTTP/1.1"" 303 112
HTTPConnectionPool, localhost:18081 (20): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (20): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (20): ""GET /redirect?target=%2F HTTP/1.1"" 303 112
HTTPConnectionPool, localhost:18081 (20): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (20): Redirecting /redirect?target=%2F -> http://localhost:18081/
HTTPConnectionPool, localhost:18081 (20): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (20): ""GET http://localhost:18081/ HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (20): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (22): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (22): ""GET / HTTP/1.1"" 200 13
HTTPConnectionPool, localhost:18081 (24): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (24): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (25): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (25): ""POST /upload HTTP/1.1"" 200 0
HTTPConnectionPool, localhost:18081 (25): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (26): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (26): ""POST /upload HTTP/1.1"" 200 0
HTTPConnectionPool, localhost:18081 (26): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (27): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (27): ""PUT /specific_method?method=PUT HTTP/1.1"" 200 0
HTTPConnectionPool, localhost:18081 (27): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (28): Starting new (1) HTTP connection
HTTPConnectionPool, localhost:18081 (28): ""GET /specific_method?method=POST HTTP/1.1"" 400 25
HTTPConnectionPool, localhost:18081 (28): Connection HTTPConnection (1) returned to the pool
HTTPConnectionPool, localhost:18081 (28): Connection HTTPConnection (1) fetched from pool
HTTPConnectionPool, localhost:18081 (28): ""POST /specific_method HTTP/1.1"" 400 25
HTTPConnectionPool, localhost:18081 (28): Connection HTTPConnection (1) returned to the pool
----------------------------------------------------------------------
Ran 20 tests in 25.196s

OK
```
"
25,Hostname verification for VerifiedHTTPSConnection,2011-12-20T10:55:45Z,2011-12-26T20:49:30Z,Ready,,,"I added SSL certificate hostname verification for urllib3. It requires backports.ssl_match_hostname though.
Current version accepts any valid certificate, which makes VerifiedHTTPSConnection insecure.
"
24,Consider local timeout parameter when raising TimeoutError in urlopen,2011-12-18T13:20:10Z,2011-12-18T18:44:13Z,,,,"See #23
"
23,HTTPConnectionPool.urlopen does not use local timeout on SocketTimeout,2011-12-18T13:17:55Z,2011-12-18T18:44:59Z,,TypeError,"TypeError: float argument required, not NoneType","On `SocketTimeout`, a `TimeoutError` is raised with `self.timeout` as part of the exception in `urlopen()`, but when this is not set, you get a `TypeError: float argument required, not NoneType`.

When the `timeout` parameter is overriden locally in the function call, the exception should take this in to concideration as well.

To reproduce:

```
>>> from urllib3.connectionpool import connection_from_url
>>> con = connection_from_url('http://doesnotexists.com')
>>> con.urlopen('GET', 'http://doesnotexists.com', retries=0, assert_same_host=False, timeout=1.0)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""urllib3/connectionpool.py"", line 353, in urlopen
    self.timeout)
TypeError: float argument required, not NoneType
```
"
22,release_conn parameter doesn't have effect (without preload_content),2011-12-08T22:41:20Z,2014-07-03T23:11:34Z,Won't fix?,,,"After

```
pool = HTTPConnectionPool(host, port)
pool.urlopen('GET', '/', release_conn=False)
```

connection is being released.
"
21,uniform naming of http pool in test_connectionpool test module,2011-12-08T22:18:27Z,2011-12-10T02:57:04Z,,,,
20,Wrapping httplib's connection,2011-12-06T22:26:02Z,2014-07-03T23:12:54Z,Someday,,,"If we _wrap_ `httplib.HTTPConnection` into custom class we could hold some additional information about each connection. This way we could enumerate all connections and print their ids in debugging mode like curl does.
"
19,Drop support for Python 2.5,2011-12-06T17:56:44Z,2012-01-29T21:40:29Z,Someday,,,"Could we drop support for 2.5? 2.6 has already been available for 3 years.

Issues where dropping 2.5 would help:
- Fix connection pooling behavior when maxsize > 1 (issue #13)
- Ensure that the user-specified timeout is set before making a request (issue #17)
"
18,Fixed a case for _fp.read(amt) when an fp doesn't accept None,2011-12-05T04:16:30Z,2012-01-21T20:41:11Z,"Ready, Soon",,,"I came across an issue with `cStringIO` not liking `cStringIO('').read(None)`
"
17,Ensure that the user-specified timeout is set before making a request,2011-12-05T00:02:07Z,2011-12-10T22:40:46Z,"Ready, Soon","urllib3.exceptions.MaxRetryError, urllib3.exceptions.TimeoutError","urllib3.exceptions.MaxRetryError: Max retries exceeded for url: /, urllib3.exceptions.TimeoutError: Request timed out after 1.000000 seconds","Hi,

urllib3 apparently does not honor the timeout value when establishing a new connection.

In the code, one can see that the timeout value is being set on the socket, albeit only after the connection attempt. 

```
conn.request(method, url, **httplib_request_kw)
conn.sock.settimeout(timeout)
```

I believe this partly defeats the purpose of the timeout parameter.

Here's a small script that illustrates this:

``` python
import urllib3
conn = urllib3.connection_from_url('http://host.that.times.out', timeout=1.0)
r1 = conn.request('GET', '/')
```

Results

Without the change:

``` shell
$ time python urllib3_timeout.py 
Traceback (most recent call last):
 File ""urllib3_timeout.py"", line 4, in <module>
    r1 = conn.request('GET', '/')
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/request.py"", line 65, in request
    **urlopen_kw)
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/request.py"", line 78, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/connectionpool.py"", line 361, in urlopen
    redirect, assert_same_host)  # Try again
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/connectionpool.py"", line 361, in urlopen
    redirect, assert_same_host)  # Try again
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/connectionpool.py"", line 361, in urlopen
    redirect, assert_same_host)  # Try again
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/connectionpool.py"", line 361, in urlopen
    redirect, assert_same_host)  # Try again
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/connectionpool.py"", line 296, in urlopen
    raise MaxRetryError(""Max retries exceeded for url: %s"" % url)
urllib3.exceptions.MaxRetryError: Max retries exceeded for url: /

real    1m24.056s
user    0m0.044s
sys 0m0.016s
```

With the change:

``` shell
$ time python urllib3_timeout.py 
Traceback (most recent call last):
  File ""urllib3_timeout.py"", line 4, in <module>
    r1 = conn.request('GET', '/')
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/request.py"", line 65, in request
    **urlopen_kw)
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/request.py"", line 78, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File ""/usr/local/lib/python2.6/dist-packages/urllib3-1.0.2-py2.6.egg/urllib3/connectionpool.py"", line 344, in urlopen
    self.timeout)
urllib3.exceptions.TimeoutError: Request timed out after 1.000000 seconds

real    0m1.040s
user    0m0.020s
sys 0m0.020s
```

Hope you find the patch useful. 

Thank you for your work.
"
16,Python 3 compatibility,2011-12-04T22:02:10Z,2012-01-22T22:21:07Z,Someday,,,"@kennethreitz is starting some informal work on this somewhere.

Issues that are affected by this: https://github.com/shazow/urllib3/issues?milestone=1&state=open

Let's keep this thread updated as we make progress.
"
15,Small improvements to tests.,2011-12-03T20:30:01Z,2011-12-04T22:09:21Z,Urgent,,,"Stop sys.path manipulation.
Use portable null device.
Warn if run without -m option.
"
14,Stop sys.path manipulation in test modules.,2011-12-03T20:18:48Z,2011-12-03T20:31:08Z,,,,
13,Fix connection pooling behavior when maxsize > 1,2011-11-29T06:47:53Z,2012-01-29T23:54:18Z,Someday,,,"The connection pool's underlying Queue.Queue (a FIFO queue) is prefilled
with None objects; therefore, until you make maxsize HTTP requests, no
connections will be reused. I've attached a test which demonstrates this
(and now passes after switching to Queue.LifoQueue.)

The requests library defaults to a pool size of 10, which exposes this
bug.

Thanks for considering my patch! I didn't add myself to CONTRIBUTORS as this is a pretty minor fix. :)
"
12,[requests] Proxy authentication broken with 0.76 -> 0.83 upgrade.,2011-11-28T23:30:13Z,2011-12-04T23:37:48Z,,,,"Issue [293](https://github.com/kennethreitz/requests/issues/293).
I guess in the end we should use standard and tested `urlparse.urlsplit` :)
"
11,fix subsequent redirect request method type,2011-11-27T21:12:18Z,2012-08-04T06:18:57Z,Soon,,,"See issue [269 fix subsequent redirect request method type](https://github.com/kennethreitz/requests/pull/269) in Requests.
"
10,Incorrect handling of HTTP redirects,2011-11-24T10:41:57Z,2012-01-07T22:15:15Z,"Has Failing Test, Urgent",,,"Code:

manager = urllib3.PoolManager()
r = manager.request('GET', 'http://ynet.co.il')
r.data - returns incorrect page

same with paypal.com and other domains... The code takes the location value from the HTTP response and issues a GET request to http://www.ynet.co.il to hostname http://ynet.co.il
"
9,Support for specifying a source address for connections,2011-11-16T14:14:48Z,2014-04-18T05:57:29Z,Someday,,,"Since version 2.7, httplib supports specifying a source address for HTTP(S)Connection: http://docs.python.org/library/httplib.html#httplib.HTTPConnection
Would be nice if urllib3 could let me use this when creating connections.
"
8,Scheme and host erroneously passed to HTTPConnection request method,2011-11-14T11:08:46Z,2012-08-04T06:18:01Z,Soon,,,"I think there is a problem in the use of `httplib.HTTPConnection` method `request` when called at
[line 213 of urllib3/connectionpool.py](https://github.com/shazow/urllib3/blob/master/urllib3/connectionpool.py#L213) where you pass it the full URL, containing the scheme and host, instead of just the path (and query part), as show in [httplib usage examples](http://docs.python.org/library/httplib.html#examples).

This ends up in a wrong HTTP request performed to the server. To see it, you can for instance run

```
python -m SimpleHTTPServer
```

in a shell and then, in another one, run

```
python -c 'from urllib3 import PoolManager; http = PoolManager(); http.request( ""GET"", ""http://localhost:8000/this/is/an/example"" )'
```

and compare what the access log in the first shell reports as compared to what happens if you do 

```
curl  ""http://localhost:8000/this/is/an/example""
```

I can submit a patch, but I'm not an urllib3 expert so I will probably miss some other place where the same error occurs.
"
7,RecentlyUsedContainer is not threadsafe,2011-11-04T10:32:28Z,2011-11-04T16:44:50Z,,RuntimeError,RuntimeError: deque mutated during iteration,"I'm testing 1.0.1

As noted in the code, RecentlyUsedContainer is not threadsafe.

I'm getting:  RuntimeError: deque mutated during iteration

Stacktrace:

```
  File ""/home/chrsjo/src/arkenutils_omkomp/arken/hcap.py"", line 153, in _request
    return self._pool.request(*args, **kwargs)
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/request.py"", line 65, in request
    **urlopen_kw)
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/request.py"", line 78, in request_encode_url
    return self.urlopen(method, url, **urlopen_kw)
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/poolmanager.py"", line 107, in urlopen
    conn = self.connection_from_url(url)
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/poolmanager.py"", line 98, in connection_from_url
    return self.connection_from_host(host, port=port, scheme=scheme)
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/poolmanager.py"", line 73, in connection_from_host
    pool = self.pools.get(pool_key)
  File ""/home/chrsjo/.virtualenvs/arkenutils_omkomp/lib/python2.7/_abcoll.py"", line 342, in get 
    return self[key]
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/_collections.py"", line 96, in __getitem__
    self._prune_invalidated_entries()
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/_collections.py"", line 77, in _prune_invalidated_entries
    self.access_log = deque(e for e in self.access_log if e.is_valid)
  File "".../urllib3-1.0.1-py2.7.egg/urllib3/_collections.py"", line 77, in <genexpr>
    self.access_log = deque(e for e in self.access_log if e.is_valid)
RuntimeError: deque mutated during iteration

```
"
6,Fix a typo in VerifiedHTTPSConnection.,2011-10-11T10:41:09Z,2011-10-11T16:35:54Z,,,,"`ca_certs` attribute was not present on `VerifiedHTTPSConnection` objects before calling `set_cert` for some reason.
"
5,VerifiedHTTPSConnection: Write more tests,2011-10-09T20:51:13Z,2012-01-22T00:08:17Z,"Contributor Friendly ‚ô•, Someday",,,"There ~~are currently no tests~~ is one test that check SSL cert verification in `tests/with_dummyserver/test_https.py`. More tests would be great. :)

Specifically we want to reach good coverage for HTTPS-related code.
"
4,ProxyManager: Document and write tests,2011-10-09T20:44:40Z,2012-02-05T21:41:40Z,"Contributor Friendly ‚ô•, Someday",,,"Also check if it works for HTTPS proxies. It might not.
"
3,Use MultiDict for headers,2011-10-09T20:43:13Z,2014-07-03T23:14:01Z,Someday,,,"Currently httplib coerces headers into a dict, which breaks some things. Some monkeypatching might be required.

See also: [Issue 15 @ GoogleCode](http://code.google.com/p/urllib3/issues/detail?id=15) for more discussion.
"
2,BadStatusLine for twitter.com,2011-10-01T10:31:24Z,2011-10-01T18:42:24Z,,urllib3.exceptions.MaxRetryError,urllib3.exceptions.MaxRetryError: Max retries exceeded for url: https://twitter.com,"Can't figure this one out. Any ideas?

``` python
from urllib3.connectionpool import connection_from_url

url = 'https://twitter.com'

http_pool = connection_from_url(url, strict=False)
r = http_pool.urlopen('GET', url)
```

``` pyexcept
urllib3.exceptions.MaxRetryError: Max retries exceeded for url: https://twitter.com
```
"
1,Bugfixes,2011-10-01T10:15:41Z,2011-10-01T17:49:39Z,,,,"I lost hours over this :)
"
