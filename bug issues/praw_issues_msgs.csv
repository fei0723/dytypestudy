ID,Title,Created At,Closed At,Issues,Errors,Messages,Body
878,Recursion Error -- UPDATED,2017-12-23T16:33:00Z,2017-12-23T18:16:54Z,,,,"I am adding more info for #877 .

# System Info:
Praw: 5.3.0
Python: 3.6.1
OS: Windows 10 Build 15063/Version 1703

# All Code:
```
import praw, pokestarfansloggingsetup, logging, bs4, urllib.request
from logininfo import *

import subreddit_list

logger = pokestarfansloggingsetup.setup_logger('poster', loglevel = logging.DEBUG)


def login_to_reddit(us, pw, c_id, c_sec):
    reddit = praw.Reddit(username=us, password=pw, client_id=c_id, client_secret=c_sec,
                         user_agent='AutoPoliticsPoster by /u/PokestarFan')
    logging.info(f'Logged into reddit as {us}')
    return reddit


reddit = login_to_reddit(u, p, c_i, c_s)


def format_title_of_website(title):
    global newtitle
    if ':' in title:
        newtitle = title.split(':')[0]
    elif '|' in title:
        newtitle = title.split('|')[0]
    elif '-' in title:
        newtitle = title.split('-')[0]
    else:
        newtitle = title
    logging.debug(f'New title: {newtitle}')
    return newtitle


def post_to_subreddit(subreddit, link):
    global response
    soup = bs4.BeautifulSoup(urllib.request.urlopen(link).read(), ""lxml"")
    title = soup.html.head.title.string
    post_title = format_title_of_website(title)
    answer = input(f'Post with this title: {post_title}? Type y or n:')
    if answer == 'n':
        response = False
    elif answer == 'y' or len(answer) == 0:
        response = True
    if not response:
        title = input('Type in the new title:')
    try:
        logging.debug(f'''Displaying itemtypes:
        Subreddit : {str(type(subreddit))}
        Link: {str(type(link))}''')
        reddit.subreddit(subreddit).submit(title, url=link)
    except Exception as e:
        logging.warning('Submission error.', exc_info=True)
    else:
        logging.info(f'Successfully submitted link {link} to subreddit {subreddit}')


def post_to_all_subs(sub_list, link):
    global given_answer
    for num in range(len(sub_list)):
        post_to_subreddit(sub_list[num], link)
        try:
            answer = input(f'Post to subreddit {sub_list[num+1]}?')
            if answer == 'n':
                given_answer = False
            elif answer == 'y' or len(answer) == 0:
                given_answer = True
            if not given_answer:
                print('Complete')
                break
        except IndexError:
            pass


def get_link(sub_list):
    link = input('What is the link you would like to post?:')
    post_to_all_subs(sub_list, link)


def main(subreddit_list):
    while True:
        get_link(subreddit_list)

if __name__ == '__main__':
    main(subreddit_list.politics_list)
```
## Link I used
https://www.nytimes.com/2017/12/22/us/politics/fbi-director-president-trump.html


# Full Log File
[poster_12_23_17.log](https://github.com/praw-dev/praw/files/1584270/poster_12_23_17.log)
"
877,Some random recursion error,2017-12-23T04:25:13Z,2017-12-23T06:29:38Z,,RecursionError,"RecursionError: maximum recursion depth exceeded","I have this program to automatically post links to subreddits, but for some reason when it tries to submit a link it coughs up a very long recursion error.

# System Info
PRAW version: 5.3.0
Python Version: 3.6.1
System Version: Windows 10 Build 15063/Version 1703

# Code
```
Traceback (most recent call last):
  File ""C:\Users\----\Desktop\Programs\python projects\Mine\PoliticsPoster\poster.py"", line 46, in post_to_subreddit
    reddit.subreddit(subreddit).submit(title, url=link)
  File ""C:\Program Files\Python36\lib\site-packages\praw\models\reddit\subreddit.py"", line 502, in submit
    return self._reddit.post(API_PATH['submit'], data=data)
  File ""C:\Program Files\Python36\lib\site-packages\praw\reddit.py"", line 431, in post
    params=params)
  File ""C:\Program Files\Python36\lib\site-packages\praw\reddit.py"", line 472, in request
    params=params)
  File ""C:\Program Files\Python36\lib\site-packages\prawcore\sessions.py"", line 175, in request
    data = deepcopy(data)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Program Files\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Program Files\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
RecursionError: maximum recursion depth exceeded
```
"
876,Comment by url,2017-12-22T19:20:37Z,,,,,"## Feature Summary and Justification

Added:

- Optional URL argument to the Comment class
- id_from_url method that extracts the ID from a standard format comment perma-link
- The URL may be either a full perma-link or a shortened perma-link, with or without context"
875,Parse short URLs with trailing slashes,2017-12-22T03:49:31Z,2017-12-22T05:41:22Z,,,,"## Feature Summary and Justification

Currently, a line like `s = reddit.submission(url='https://redd.it/2gmzqe/')` will raise `ClientException: Invalid URL: https://redd.it/2gmzqe/`. This is due to the way URLs are parsed: if a URL doesn't have the 'comments' component, it only parses properly if there is no trailing slash. 

This PR adds a test case that should pass but fails, and then fixes the issue."
874,Reset sleep time when using positive vlaues of pause_after,2017-12-20T07:02:43Z,2017-12-20T07:09:36Z,,,,
873,Bump prawcore dependency and bump to PRAW 5.3.0,2017-12-17T02:38:21Z,2017-12-17T02:48:40Z,,,,
872,Subreddit object is missing certain attributes,2017-12-13T23:24:22Z,2017-12-13T23:43:33Z,,,,"## Issue Description

When looking at the attributes of a Subreddit object using vars() it doesn't show all available attributes unless one of the missing ones is accessed (actually I only tested accessing Subreddit.title by printing it).

`sub = reddit.subreddit('askreddit')`

`vars(sub) #Will be missing attributes like sub.title`

`print(sub.title)`

`vars(sub) #Will have all the attributes expected`


## System Information

    PRAW Version: 5.2.0
  Python Version: 3.6.3
Operating System: Windows 10
"
871,Not secure call and payload call to mothership,2017-12-08T22:26:00Z,2017-12-08T22:52:37Z,,,,"## Issue Description

Local dev this morning had my network filter alert me that PRAW was phoning home.  Via Little Snitch:

![image](https://user-images.githubusercontent.com/170310/33778249-265ffff0-dc05-11e7-8b23-2e978051184b.png)

I am concerned, can you elaborate on this non secure call and payload and how to disable?"
870,"HTTP 500 on submission.post(flair_id, flair_text) if user is a moderator of said subreddit",2017-12-06T03:46:54Z,2017-12-22T05:28:38Z,,prawcore.exceptions.ServerError,"prawcore.exceptions.ServerError: received 500 HTTP response","## HTTP 500 on submission.post(flair_id, flair_text) if user is a moderator of said subreddit

Posting a submission on a subreddit you moderate on with `flair_id` and `flair_text` specifies leads to a HTTP 500 error. Using a `SubmissionModeration` object avoids the error, but is annoying nonetheless and the error message is non-descriptive. 

[External Report](https://www.reddit.com/r/redditdev/comments/6w0yn0/500_error_while_setting_flair_on_post/)

### Traceback

```
Traceback (most recent call last):
  File ""/var/lib/discussion_thread/discussion_service.py"", line 28, in <module>
    main()
  File ""/var/lib/discussion_thread/discussion_service.py"", line 23, in main
    thread.check()
  File ""/var/lib/discussion_thread/discussion_thread.py"", line 57, in check
    self.schedule.run_pending()
  File ""/usr/local/lib/python3.6/dist-packages/schedule/__init__.py"", line 78, in run_pending
    self._run_job(job)
  File ""/usr/local/lib/python3.6/dist-packages/schedule/__init__.py"", line 131, in _run_job
    ret = job.run()
  File ""/usr/local/lib/python3.6/dist-packages/schedule/__init__.py"", line 411, in run
    ret = self.job_func()
  File ""/var/lib/discussion_thread/discussion_thread.py"", line 85, in post
    send_replies=False
  File ""/usr/local/lib/python3.6/dist-packages/praw/models/reddit/subreddit.py"", line 502, in submit
    return self._reddit.post(API_PATH['submit'], data=data)
  File ""/usr/local/lib/python3.6/dist-packages/praw/reddit.py"", line 431, in post
    params=params)
  File ""/usr/local/lib/python3.6/dist-packages/praw/reddit.py"", line 472, in request
    params=params)
  File ""/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py"", line 179, in request
    params=params, url=url)
  File ""/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py"", line 122, in _request_with_retries
    retries, saved_exception, url)
  File ""/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py"", line 88, in _do_retry
    params=params, url=url, retries=retries - 1)
  File ""/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py"", line 122, in _request_with_retries
    retries, saved_exception, url)
  File ""/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py"", line 88, in _do_retry
    params=params, url=url, retries=retries - 1)
  File ""/usr/local/lib/python3.6/dist-packages/prawcore/sessions.py"", line 124, in _request_with_retries
    raise self.STATUS_EXCEPTIONS[response.status_code](response)
prawcore.exceptions.ServerError: received 500 HTTP response
```

## System Information

PRAW Version: 5.2.0
Python Version: 3.6.3
Operating System: Linux
"
869,Clarify usage of `limit` in the `replace_more` tutorial.,2017-11-30T05:09:12Z,2017-11-30T05:40:28Z,,,,"Hi! Long time no commit.

## Feature Summary and Justification

The last few code snippets on the ""Comment Extraction and Parsing"" page use a `limit` of 0, but I believe they were supposed to say `None` because it sounds like they're supposed to fetch all comments.

I clarified the meaning of `limit=0` vs `limit=None`, and changed the limits in the last snippets to `None` especially because these are the most frequently copy-pasted.

Admittedly the one that only prints toplevel comments is kind of wasteful to fetch all the Mores, but since we don't have a root-only version I feel it's the better of the two and, again, people are going to be copying this.

There's a new section header just for this `replace_more` lesson because I think the topic is big enough to deserve it.



## References

* https://www.reddit.com/r/redditdev/comments/7gazwg/large_discrepancy_between_submissionnum_comments/
"
868,Add Redditor.block(),2017-11-26T20:30:28Z,2017-11-26T21:17:08Z,,,,"## Feature Summary and Justification

This feature provides support for the API endpoint /api/block_user, which I believe is fairly new. Specifically, it adds the `Redditor.block()` method, which accepts no params and blocks the Redditor it belongs to.

## References

* https://www.reddit.com/dev/api/#POST_api_block_user
"
867,Add note about dynamically provided attributes to all Models,2017-11-26T12:01:50Z,2017-12-19T16:49:45Z,,,,"This new pull request is about the first point on my list in #862 which seems to have been overlooked:

> * For now, this pull request consists only of changes made to `comment.rst`. However, if you like my suggestion, of course the note should be appended to each file in `docs/code_overview/models/`.

I'm sorry, I only now found time to have a look at my previous pull request again and saw that it's been merged already. What did I do wrong? Maybe I should have put a certain label on the pull request? I'm sorry for the inconvenience.

So this is a new pull request to add the note about dynamically provided attributes to all Models - not only to `comment.rst`. My commits for this pull request do

* add a substitution definition (which contains the content of the previously already added note from #862) to `docs/code_overview/praw_models.rst` **(⚠ This seems to be a wrong approach - see below for more details)**
* remove the note with text from `comment.rst` because we can use the new substitution definition there as well
* use the new substitution definition in all files from `docs/code_overview/models/`

---

**Edit:** The build failed with the following error:

> ```
> Warning, treated as error:
> 
> /home/travis/build/praw-dev/praw/docs/code_overview/models/comment.rst:7:Undefined substitution referenced: ""note-dynamically-provided-attributes"".```

~Where do I have to put the substitution definition of `note-dynamically-provided-attributes` so it will be properly defined when it's being used within the files from `docs/code_overview/models/`?~
**(See possible solutions below)**

---

**Edit 2:**

Huh, substitution definitions don't seem to be remembered from file to file but can only be used inside the file they're defined in. Otherwise one will have to do something like this:

> If you want to use some substitutions for all documents, put them into rst_prolog or put them into a separate file and include it into all documents you want to use them in, using the include directive. (Be sure to give the include file a file name extension differing from that of other source files, to avoid Sphinx finding it as a standalone document.)

(from [http://www.sphinx-doc.org/en/stable/rest.html](http://www.sphinx-doc.org/en/stable/rest.html))

Which of the following options 1, 2 or 3 do you want me to do to fix this? I guess, it depends on personal preference.

1. Add `rst_prolog` with the substitution definition to `docs/conf.py`
2. Create an extra file in `docs/code_overview/models/` which will be included in the other files
3. Use the full text note without substitution in all files which would the violate Don't-repeat-yourself principle
"
866,"submission.comments.list() does not contains all comments, even after replace_more()",2017-11-16T16:10:23Z,2017-11-16T16:44:49Z,,,,"## Issue Description

In some cases, I get a limited list of comments.
I'm unable to get a full list of comments without inspecting the replies of every comment.

### Sample code

   
    reddit = praw.Reddit()
    submission = reddit.submission(id='7d3oap')
    submission.comments.replace_more(limit=0)
    len(submission.comments.list()) # 10

My expectation was to have 15 comments in the list, like in the [web interface](https://www.reddit.com/r/DimmiOuija/comments/7d3oap/spiriti_quale_parola_sussurrata_allorecchio_di/).

## System Information

PRAW Version: 5.2.0
Python Version: 3.5.4
Operating System: Windows

## Note

Maybe the issue is in Reddit, in the official app I'm unable to see the full comments tree."
865,Add documentation for 2FA,2017-11-15T21:05:15Z,2017-11-16T06:12:06Z,,,,"This adds documentation concerning how to use/why not to use 2FA with script auth.

Fixes #863.
"
864,Updating subreddit settings resets allow_post_crossposts to default ( True ),2017-11-14T13:20:39Z,,,,,"## Issue Description
Updating a sub's settings (`.mod.update()`), and not supplying allow_post_crossposts resets allow_post_crossposts.

## System Information

    PRAW Version: Version: 5.2.0
    Python Version: Python 3.4.2 
    Operating System: Linux version 3.16.0-4-amd64 (debian-kernel@lists.debian.org) (gcc version 4.8.4 (Debian 4.8.4-1) ) #1 SMP Debian 3.16.43-2 (2017-04-30)
"
863,Support two-factor authentication,2017-11-07T21:08:01Z,2017-11-16T06:12:06Z,,,,"Reddit now has 2FA for moderators, rolling out to all users in the Near Future™. So PRAW support would also be nice.

According to [this comment](https://www.reddit.com/r/modnews/comments/7bfoqg/twofactor_authentication_now_available_for/dphm3vq/), the passcode can simply be joined to the password with a colon in between."
862,Point out dynamically provided attributes,2017-11-05T00:06:05Z,2017-11-05T18:44:02Z,,,,"While getting to know PRAW, I was wondering about existing attributes of a `Submission` object which were not listed in the docs under [Working with PRAW’s Models » Submission](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html). So I posted [a question](https://stackoverflow.com/questions/46840319/why-is-submission-permalink-with-praw-working-even-if-i-cant-find-anything) about it on Stack Overflow. Turns out, while intensely studying the Model's page, I simply missed [this section](https://praw.readthedocs.io/en/latest/getting_started/quick_start.html#determine-available-attributes-of-an-object) from the Quick Start docs about dynamically provided attributes. Therefore, I think it might be a good idea to add a note at the bottom of each Model's list of attributes to remind the reader of dynamically provided attributes.

* For now, this pull request consists only of changes made to `comment.rst`. However, if you like my suggestion, of course the note should be appended to each file in `docs/code_overview/models/`.
* The added words are more or less taken directly from @bboe 's well written answer on Stack Overflow. I hope that's fine with you?
* ~~From looking at other links in the docs, I couldn't find out how to add or look up the correct `:ref:` identifier, so I just called it ``:ref:`determine-available-attributes-of-an-object` `` for now. It is supposed to link to https://praw.readthedocs.io/en/latest/getting_started/quick_start.html#determine-available-attributes-of-an-object~~ (This should be correct now, thanks to @lambdaman121 )
* I don't know about Sphinx docs syntax, so the indentation might very well be wrong.

Looking forward to hearing from you. :+1: "
861,[Errno 21] Is a directory: '/',2017-11-02T22:06:20Z,2017-11-03T15:29:42Z,,,,"## Issue Description

```
reddit = praw.Reddit(user_agent=userAgent)
reddit.login(username, password, disable_warning=True)
submission = reddit.get_submission(submission_id=id) 
```

raises Exception ""[Errno 21] Is a directory: '/'"" on Android only. Windows seems to be unaffected and working as normal.

## System Information

    PRAW Version: 3.6.2
    Python Version: 2.7
    Operating System: Android 5.1
"
860,Remove permalink for PRAW3.,2017-10-28T23:33:16Z,2017-10-29T03:11:26Z,,,,
859,Add stream to Multireddit,2017-10-26T18:01:39Z,2017-11-05T18:46:36Z,,**Note**,"**Note**: I did not write tests.","## Feature Summary and Justification

This feature provides support for streams from Multireddit objects. Subreddit and Redditor both support streams, and some use cases would benefit from supporting a stream of Multireddit posts/comments as well.

**Note**: I did not write tests.

I chose to return an instance of `SubredditStream` because I see no benefit from subclassing or duplicating to create a `MultiredditStream` class.

## References

* https://www.reddit.com/r/redditdev/comments/78qvot/is_there_anyway_to_make_a_multireddit_stream/
"
858,Remove Comment.permalink(),2017-10-24T16:15:07Z,2017-10-24T17:22:33Z,,,,"Fixes #856.

## Feature Summary and Justification

This fix just removes the `Comment.permalink()` method. Tests may fail, but I did my best to remove the now-unnecessary tests.

This PR and #857 are two different ways of fixing PRAW. Both should not be merged."
857,Fix #856 and deprecate Comment.permalink(),2017-10-24T06:38:29Z,2017-10-24T17:11:22Z,,,,"Fixes #856.

## Feature Summary and Justification

I'm not positive that I did this entirely correctly, so I will try to justify what I changed.

First, there now exists a naming conflict where if a comment object is fetched, it has the attribute `my_comment.permalink`, which is a string provided by Reddit's JSON response. However, if the comment is not fetched, `my_comment.permalink` is a method. I decided the best approach would be to restore functionality as it was before Reddit started returning the `permalink` attribute.

To do this, I first modified the `Comment` class's `__setattr__` method to rename any attribute named `'permalink'` to `'_permalink'` so as to avoid a naming conflict. [You can see that in these two lines.](https://github.com/jarhill0/praw/blob/93ccd79f61268ffb9b971f4e633e908c99b44d72/praw/models/reddit/comment.py#L77-L78)

As indicated in the comment, when PRAW 6 rolls around, these two lines of code should be removed. This is for reasons explained below.

Next I modified the method `permalink` so that it would continue to function in the expected way. It fetches the comment if it is not yet fetched (the old way still required a fetch to get `comment.submission`), and then returns `comment._permalink`, a new attribute created by my above-described changes. The function still accepts the parameter `fast` for compatibility, but it is unused, because the full permalink is contained in Reddit's JSON representation of the comment.

Finally, I made the choice to deprecate `comment.permalink` as a method so that by PRAW 6, `comment.permalink` can be an attribute like any other attribute returned by Reddit. This is a backwards-incompatible change, as it would break any scripts that expect `comment.permalink` to be a method. For this to work properly, the two lines discussed above will need to be removed, so that a call to `comment.__setattr__('permalink', foo)` will set an attribute named `permalink`, not `_permalink`.

I updated the docstring of `comment.permalink` to reflect these changes.

# On testing

This will likely break tests, and I am not competent enough in that area to fix those. Any guidance is appreciated."
856,Name shadowing due to permalink becoming a Reddit-provided JSON attribute. ,2017-10-24T03:10:44Z,2017-10-24T17:22:33Z,,,,"See https://www.reddit.com/r/redditdev/comments/78bgvn/praw_has_reddit_changed_the_api_in_such_a_way/.

[Reddit-provided JSON comment objects](https://gist.github.com/jarhill0/9aa047651d4289ff2ea5241465ebb967) now have a `permalink` attribute. This wasn't present previously, and now that it is, there ends up being a naming conflict with the `Comment.permalink()` method:

https://github.com/praw-dev/praw/blob/fb836c46a381a937df8659fb9e8ef959f7e734f9/praw/models/reddit/comment.py#L149-L168

Code such as the following worked in the recent past but will no longer work due to name shadowing:

```
my_comment = reddit.comment(id='dosiyca')
print(my_comment.permalink())  # raises TypeError: 'str' object is not callable
```

I believe there are two ways to resolve this issue. The first would be to remove the `Comment.permalink()` method and instead tell PRAW users to use the `Comment.permalink` attribute provided by Reddit. This change would be backwards-incompatible.

Another way to fix this would be to somehow fix the naming conflict (make `Comment.permalink` a method again, rather than a string attribute), and update `Comment.permalink()` to just return the `permalink` attribute Reddit provides. This fix would be backwards-compatible, as `Comment.permalink()` would still return exactly what PRAW users expect.

## System Information

PRAW Version: 5.1.1
Python Version: any
Operating System: any
"
855,Make Liveupdate attributes lazy,2017-10-19T15:52:11Z,2017-10-20T19:10:47Z,,,,"## Feature Summary and Justification

This feature provides lazy attribute population on LiveUpdate instance. E.g.,

```
>>> from praw.models import LiveUpdate
>>> import praw
>>> reddit = praw.Reddit(...)
>>> update = LiveUpdate(reddit, 'ukaeu1ik4sw5', '7827987a-c998-11e4-a0b9-22000b6a88d2')
>>> update.author
Redditor(name='umbrae')
```

Until now, a LiveUpdate instance hadn't been able to get details from reddit
when a attribute is first accessed because of the reddit API's limitation.
But [the limitation is recently removed](https://github.com/reddit/reddit-plugin-liveupdate/commit/88a490f733bf9c889005b39f9569ce6cd094f445), so this commit reflects the change."
854,Deprecate subreddit.comments.gilded,2017-10-09T22:58:53Z,2017-10-09T23:08:58Z,,,,
853,Is it possible to do a search with a specific date,2017-10-09T08:45:44Z,2017-10-09T17:19:15Z,,,,"I'm searching in subreddits:
```python
subreddit = 'MachineLearning'
search_lib = 'tensorflow'
print(""Searching titles with the word {}..."".format(search_lib))
for submission in reddit.subreddit(subreddit).search(search_lib, sort='relevance', time_filter='year', syntax=None, limit=None):
    print(submission.title)
    print(submission.score)
    print(submission.num_comments)
    print()
```
Is there an option to filter per specific date?, for example I want the results from March 2016 to April 2016.


#### System Information
OS:  linux
Python version:  3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
Python Reddit API Wrapper version:  5.1.0"
852,How would one go about citing this package? ,2017-09-29T18:22:05Z,2017-10-09T15:44:11Z,,,,"## Issue Description

Is this okay?

> Boe  B. PRAW: The Python Reddit Api Wrapper 2014 [cited <date>]. Available from:
https://praw.readthedocs.io/en/stable/.
"
851,"Document that ""contributor"" is another term for ""approved submitter""",2017-09-28T17:23:35Z,2017-09-28T19:27:06Z,,,,"## Feature Summary and Justification

This adds an example and a note saying that ""contributor"" is another term for ""approved submitter.""

Reddit's API and associated documentation use the term ""contributor"" where the website uses the term ""approved submitter."" PRAW currently follows the API documentation only, which makes it hard to find things by searching for ""approved submitter.""

## References

* https://www.reddit.com/r/redditdev/comments/72v97e/praw_invite_user_to_inviteonly_subreddit/
"
850,Please stop breaking API for useless changes,2017-09-21T07:25:29Z,2017-09-21T15:56:16Z,,,,"I have never had so many issues when upgrading a software before. You completely broke your API from 3.X to 5.X. `text=` becomes `selftext=` in `submit`, `get_*` becomes the second half, `.permalink` is not a string anymore - or is it? - I don't even know, seems to be a method for `Comment` and a string for `Submission`, or maybe it depends on whether the object has been fetched or not... Why? Just... Why? Oh my god..."
849,Clarify subreddit filtering,2017-09-07T01:01:12Z,2017-09-07T03:55:28Z,,,,"Fixes #848

## Feature Summary and Justification

This feature provides clarification in the documentation that subreddit filtering (`reddit.subreddit('all-the_donald')`) does not work for some methods, including methods for returning comments.

## References

* [/r/redditdev thread](https://www.reddit.com/r/redditdev/comments/6yhalr/cant_block_subreddits_with_praw/)"
848,Clarify that subreddit filtering does not work for comment listings,2017-09-07T00:27:41Z,2017-09-07T03:55:28Z,,,,"[Original post in r/redditdev](https://www.reddit.com/r/redditdev/comments/6yhalr/cant_block_subreddits_with_praw/) and [response](https://www.reddit.com/r/redditdev/comments/6yhalr/cant_block_subreddits_with_praw/dmnfdl3/) from @bboe.

Subreddit filtering does not have any effect on comment listings, and this should be clarified in the PRAW documentation. [The current documentation for the Subreddit class](http://praw.readthedocs.io/en/v5.1.0/code_overview/models/subreddit.html) only states ""Subreddits can be filtered from combined listings,"" followed by a code example. This should be clarified to reflect that they can only be filtered for certain purposes, like `hot()` and `new()`.

See the following code for an example of the issue.

```
for comment in reddit.subreddit(""all-the_donald"").comments(limit=250):
  if comment.subreddit == 'the_donald':
    print(comment.id)
```

This code snippet ends up printing comment IDs from r/the_donald despite its exclusion in the declaration of the for loop."
847,Fix a few Sphinx typos,2017-09-05T16:16:36Z,2017-09-05T16:29:12Z,,,,"Re #846

## Feature Summary and Justification

* `.. note:` -> `.. note::` to prevent the `note` from being interpreted as a comment, which wouldn't show up when the docs are rendered.
* Double backticks for the code bits.
* Correct typo (""atribute"" -> ""attribute"").
* Sphinx doesn't like characters immediately after the backticks, so add a hyphen in to prevent it from being rendered incorrectly.

## References

N/A
"
846,Escape API Exception message with `unicode_escape` encoding,2017-09-05T14:54:20Z,2017-09-05T15:05:34Z,,,,"Fixes #845

## Feature Summary and Justification

Now APIException's error message string is escaped with `unicode_escape`.

```
$ python2
Python 2.7.13 (default, Jul 18 2017, 09:16:53)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from praw.exceptions import APIException
>>> ex = APIException('RATELIMIT', u'実行回数が多すぎます', u フィールド')
>>> str(ex)
""RATELIMIT: '\\u5b9f\\u884c\\u56de\\u6570\\u304c\\u591a\\u3059\\u304e\\u307e\\u3059' on field '\\u30d5\\u30a3\\u30fc\\u30eb\\u30c9'""
>>> ex.message
u'\u5b9f\u884c\u56de\u6570\u304c\u591a\u3059\u304e\u307e\u3059'

$ python3
Python 3.6.2 (default, Jul 17 2017, 16:44:47)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> from praw.exceptions import APIException
>>> ex = APIException('RATELIMIT', '実行回数が多すぎます',  フィールド')
>>> str(ex)
""RATELIMIT: '\\u5b9f\\u884c\\u56de\\u6570\\u304c\\u591a\\u3059\\u304e\\u307e\\u3059' on field '\\u30d5\\u30a3\\u30fc\\u30eb\\u30c9'""
>>> ex.message
'実行回数が多すぎます'
>>> str(APIException('RATELIMIT', 'Too many requests', 'some field'))
""RATELIMIT: 'Too many requests' on field 'some field'""
```

I had thought that changing APIException's error message from str to unicode will suffice, but [the unittest](https://github.com/praw-dev/praw/blob/master/tests/unit/test_exceptions.py) assumes the exception can be passed to str(), so to encode the messge from unicode to ASCII is inevitable.


## References

For `unicode_escape` encoding:

* https://docs.python.org/2/library/codecs.html#python-specific-encodings
* https://docs.python.org/3/library/codecs.html#text-encodings"
845,UnicodeEncodeError is raised if reddit returns localized error message,2017-09-02T00:45:45Z,2017-09-05T15:05:34Z,,UnicodeEncodeError,"UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-10: ordinal not in range(128)","## Issue Description

Context: [[PRAW] UnicodeEncodeError when submitting non-unicode text : redditdev](https://www.reddit.com/r/redditdev/comments/6xf600/praw_unicodeencodeerror_when_submitting/)

Reddit may return localized error messages depends on the user's preference settings. Since
localized error messages may contain non-ascii characters (and underlying requests library
converts the errror message to unicode type), running this code in Python2 may raise UnicodeEncodeError:

https://github.com/praw-dev/praw/blob/efbe90f8c01a8afcda1fa09a59d1d89ed0da0f6b/praw/exceptions.py#L25

Here is an example of the localized message:

```
  File ""/usr/local/lib/python2.7/site-packages/praw/exceptions.py"", line 25, in __init__
    error_str = '{}: \'{}\''.format(error_type, message)
UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-10: ordinal not in range(128)
Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program
> /usr/local/lib/python2.7/site-packages/praw/exceptions.py(25)__init__()
-> error_str = '{}: \'{}\''.format(error_type, message)
(Pdb) p error_type
u'RATELIMIT'
(Pdb) print message
実行回数が多すぎます。9分経ってから再試行してください。
```

I think this issue is only affect to Python2 users because Python3's str type is unicode string. 

## System Information

    PRAW Version: 5.0.0
    Python Version: Python 2.7.13
    Operating System: OS X El Capitan 10.11.6"
844,Fix issue when refreshing a removed comment,2017-08-31T14:26:10Z,2017-08-31T14:35:24Z,,,,Resolves #841 
843,Dates and Times are not datetime.datetime,2017-08-31T04:53:43Z,2017-08-31T05:21:45Z,,,,"## Issue Description

for things like `Submission.created_utc` and `ModmailConversation.last_updated` and similar, it takes a bit of an effort to convert the provided items into something usable

```python
# for Submission et al.
time = datetime.datetime.fromtimestamp(Submission.created_utc, tz=datetime.timezone.utc)
# for ModmailConversation et al.
# '2015-12-23T11:11:11.111111+00:00'
time = ModmailConversation.last_updated
time = datetime.datetime.strptime(time[:-3]+time[-2:], '%Y-%m-%dT%H:%M:%S.%f%z')
```

Both incantations are a bit unweildly if used inline, and would take up >8 extra lines if moved into functions (assuming proper coding style) or result in creation of yet another dreaded `utils.py` file!

It would be nice if this work is moved into the praw (or prawcore) library, instead of re-created by each user of the praw library (at least when those attributes are used)

perhaps under `Submission.created_dt` and `ModmailConversation.last_updated_dt` to avoid backwards compatibility issues?

## System Information

PRAW Version: 5.0.1
Python Version: 3.6.X
Operating System: Windows for Workgroups 3.11
"
842,Remove broken and accidentally added gilded method on Submission,2017-08-31T04:44:50Z,2017-08-31T04:54:38Z,,,,
841,comment.refresh() causes praw to crash,2017-08-29T14:22:52Z,2017-08-31T14:35:24Z,Bug,,,"## Issue Description

This issue arises when calling refresh on a comment. I have been able to reproduce the bug multiple times by running this simplified script:

    #!/usr/bin/python3
    import praw

    reddit = praw.Reddit('bot1', user_agent='python')
    askreddit = reddit.subreddit('askreddit')

    for comment in askreddit.stream.comments():
        print('REFRESHING:', comment.id, comment.body)
        try:
           comment.refresh()
        except praw.exceptions.ClientException:
            pass

This script can run for up to half an hour before it crashes. Sometimes it crashes immediately, and sometimes it will run fine for a while before crashing. I also noticed that it always crashes on the same incriminating comments.

This is an example of a comment that that will cause it to crash - dma3mi5. Strangely enough when following the permalink in my browser I cannot see the comment in question, but rather the other comments in the thread. Perhaps this has something to do with the user being shadowbanned?

    c = reddit.comment('dma3mi5')
    c.refresh()

ERROR OUTPUT

    Traceback (most recent call last):
      File ""./test.py"", line 12, in <module>
        comment.refresh()
      File ""/usr/local/lib/python3.4/dist-packages/praw/models/reddit/comment.py"", line 196, in refresh
        queue.extend(comment._replies)
    AttributeError: 'MoreComments' object has no attribute '_replies'



## System Information

PRAW Version: 5.0.1
Python Version: 3.4
Operating System: Debian"
840,Small change in writing,2017-08-29T06:10:19Z,2017-08-29T14:11:11Z,,,,Just correcting a sentence.
839,Raise ClientException on Comment.refresh if comment is not in tree,2017-08-27T22:14:05Z,2017-08-27T23:01:34Z,,,,"Removes the the bug listed in #838, but now results in ClientException instead
of unexpected behavior."
838,Comment ID changing on comment.refresh(),2017-08-27T13:48:57Z,2017-08-27T23:04:42Z,Bug,,,"## Issue Description

I do believe this is a bug:

```
        print(ancestor.id)
        ancestor.refresh()
        print(ancestor.id)
```
does not always print the same ID.

For example, I have a log entry which has:

```
item id BEFORE already_responded: dm6wi3q  # (code which is outside the scope)
dm6wi3q
dm6vozp
item id AFTER already_responded:  dm6vozp  # (code which is outside the scope)
```

I have not yet figured out the exact reproducible steps.

## System Information

PRAW Version: 5.0.1
Python Version: 3.6
Operating System: Debian Testing"
837,Lucene (default) search does not return correct results for phrase searching (with double quotes),2017-08-25T19:49:08Z,2017-08-26T18:49:36Z,,,,"## Issue Description

The following search produces zero results:

```
query = ""NOT (flair:expired OR flair:meta) \""yunnan sourcing\"" OR yunnansourcing OR yunnansourcing.com""
reddit.subreddit(""teasales"").search(query, sort=""new"", time_filter=""month"")
```
Despite [the web-based search page returning 1 result](https://www.reddit.com/r/teasales/search?q=NOT+%28flair%3Aexpired+OR+flair%3Ameta%29+%22yunnan+sourcing%22+OR+yunnansourcing+OR+yunnansourcing.com&restrict_sr=on&sort=new&t=month).

It would appear that the praw `search` or the Reddit API is not supporting the phrase search quotation marks correctly.

## System Information

   PRAW Version: 5.0.1
  Python Version: 3.6
Operating System: Debian Testing
"
836,Fixes URL-escape password placeholder for Betamax,2017-08-25T19:39:51Z,2017-08-26T22:46:51Z,,,,"Fixes issues with #788 

This fixes the password problems I had in my previous pull request (#835).

My password contains two symbols: a `*` and a `!`, however my password was showing up in the cassettes JSON files with the two symbols encoded as `%252A` and `%2521` respectively rather than `%2A` and `%21`. This would mean that the password is getting URL-encoded twice.

So my guess is that Betamax is already automatically doing URL-encoding, so I removed the `placeholders['password'] = quote_plus(placeholders['password'])` line. And then I set it so that `password` is URL-encoded only when being passed into `define_cassette_placeholder`.

This worked for me. Can anyone else confirm?

Mentioning @leviroth "
835,Add crosspost support,2017-08-25T05:16:04Z,2017-08-29T14:16:02Z,,,,"Fixes n/a

## Feature Summary and Justification

This feature provides support for creating a crosspost. Today (4/24/2017) Reddit [launched the beta for crossposting](https://www.reddit.com/r/modnews/comments/6vths0/beta_crossposting_better_attribution_for_cat/). As the beta launched just today, things could still change so I've marked this pull request as WIP in case of Reddit crosspost API changes between now and when the beta ends.

I have the crosspost method take a thing `fullname` instead of an `id` because Reddit plans to add support for crossposting comments some time in the future.

Maybe it would be better to have the `crosspost` method be on the Submission class and take the subreddit to crosspost to as the parameter?

## Example usage

I can confirm this works:

```
$ py
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 07:18:10) [MSC v.1900 32 bit (Intel)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import praw
>>> reddit = praw.Reddit(<REDACTED>)
>>> subreddit.crosspost
<bound method Subreddit.crosspost of Subreddit(display_name='LPT_test')>
>>> subreddit.crosspost('t3_6vqtvb', title='another crosspost test')
Submission(id='6vvund')
>>> quit()
```

Post I crossposted: https://www.reddit.com/r/onionhate/comments/6vqtvb/yes_they_are/
New post created: https://www.reddit.com/r/LPT_test/comments/6vvund/another_crosspost_test/

As for integration tests, this is my first contribution to PRAW so I'm unsure of how to do those yet.

## References

* Modnews post: https://www.reddit.com/r/modnews/comments/6vths0/beta_crossposting_better_attribution_for_cat/
* Admin comment on crossposting comments as submissions: https://www.reddit.com/r/modnews/comments/6vths0/beta_crossposting_better_attribution_for_cat/dm35fb9/?context=2 
"
834,Please add some clarification about client_id and client_sercret,2017-08-22T07:19:22Z,2017-08-31T14:37:56Z,,,,"I am here for the first time and it isn't clear enough where I could find that parameter.

My guess that I should register my app somehow, but please add this information to the description and docs. It will be very useful if we could have a direct link to getting this parameters

```
 |      Required settings are:
 |      
 |      * client_id
 |      * client_secret (for installed applications set this value to ``None``)
 |      * user_agent

````"
833,Opaque assertions in CommentForest._insert_comment - exception better?,2017-08-17T22:43:36Z,2017-08-27T02:58:35Z,Bug,,,"## Issue Description

[This method](https://github.com/praw-dev/praw/blob/master/praw/models/comment_forest.py#L68) contains a couple assertions, one of which I recently tripped. I was thinking it would be better to raise an exception. That way PRAW could make it clear what's causing the problem (I'm not sure what it is, actually), and clients would be able to catch that specific exception instead of a general `AssertionError`.

## System Information

PRAW Version: 5.0.1
Python Version: 3.5.2
Operating System: Ubuntu 16.04.1
"
832,Enables collapsing and uncollapsing messages.,2017-08-15T23:20:51Z,2017-08-17T14:48:18Z,,,,"Fixes # n/a

## Feature Summary and Justification

This change enables the ability to collapse and uncollapse messages.

## References

* Reddit PR 1804 (reddit/reddit#1804)
"
831,Add `store_json` property to keep json response from Reddit,2017-08-14T19:53:09Z,,,,,"Fixes #830 

## Add `store_json` property to keep json response from Reddit

This adds a new configuration option, `store_json` to store the Reddit response on a class after it's downloaded, accessible by the `.json_dict` property.  It's disabled by default, because of aforementioned memory and serialization problems (see #830). 
"
830,Access to raw json response from reddit,2017-08-12T21:57:59Z,,,,,"## Access to raw json response from reddit

Is there a way to access the raw json response from the reddit API in praw5?  I want to save the json to a database, but I can't find anywhere in the documentation that gives access to the original reddit response. 

I see issue #701  was closed, but I'd like access as well.  I'd like to use praw for authentication simplicity and post comment loading, but I want to save json responses to my database.  I've also saved BigQuery results to my database for comments/posts that the API can't access easily, so really they should be the same format.   

I don't really see any way to serialize the object to json and save it, even walking through `vars()`, some aren't json serializable.  Considering I want to write queries off of this data, `pickle` isn't an acceptable solution.  

I guess I could ditch praw or make `get()` requests myself, but that seems like I'm reinventing the wheel (or at least praw). 

## System Information

PRAW Version: 5.0.0
Python Version: Python 2.7.12
Operating System: windows
"
829,Handle BAD_CSS_NAME when uploading stylesheet images,2017-07-23T05:25:55Z,2017-07-23T05:59:59Z,,,,Source of bug: https://www.reddit.com/r/redditdev/comments/6nnj1s/praw_bug_uploading_images_to_stylesheet/
828,Cache parent comments when refreshing a comment,2017-07-22T07:52:24Z,2017-07-22T07:58:46Z,,,,"This change makes it possible to reduce the time to discover the top-most
ancestor comment of a given comment."
827,Type hints for classes?,2017-07-21T22:58:01Z,2017-07-22T12:08:36Z,,,,"## Issue Description

Is it possible to use type-hinting on PRAW classes? For example it would great if I could do something like this:

```python
subreddit: praw.Subreddit = reddit.subreddit('all')
```"
826,Mention (Comment object) has no attribute 'submission',2017-07-21T13:32:10Z,2017-07-25T07:56:39Z,,,,"## Issue Description
Mentions retrieved using the mention stream from inbox do not contain the submission attribute.

StackTrace:

    Traceback (most recent call last): 
    File ""/home/stan/reddit-recommendation-bot/recommendationbot/bot.py"", line 154, in check_mentions 
         submission = mention.submission 
    File ""/home/stan/reddit-recommendation-bot/lib/python3.4/site-packages/praw/models/reddit/base.py"",      line 34, in __getattr__ .format(self.__class__.__name__, attribute)) 
    AttributeError: 'Comment' object has no attribute 'submission'

Relevant section:

```Python
for mention in self.reddit.inbox.mentions():
    submission = mention.submission
```

Unfortunately, I did not log any info about the particular mention. But this account only received mentions and no other messages in its inbox.

This [issue](https://github.com/praw-dev/praw/issues/794) was resolved in PRAW [4.5.1](https://github.com/praw-dev/praw/pull/793), but seems to be back again in 5.0.1.

## System Information

PRAW Version: 5.0.1
Python Version: 3.4.2
Operating System: Debian

```
$ uname -a
Linux flockbots 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt25-2 (2016-04-08) x86_64 GNU/Linux
```
"
825,Fix mistake in changelog,2017-07-19T05:38:04Z,2017-07-19T14:49:01Z,,,,Fixes #824 
824,Incorrect changelog,2017-07-19T05:19:37Z,2017-07-19T14:49:01Z,,,,"## Issue Description

The [changelog](https://github.com/praw-dev/praw/blob/master/CHANGES.rst) says in the second bulletpoint of added features under the Unreleased heading that the class [`Redditor`](http://praw.readthedocs.io/en/latest/code_overview/models/redditor.html#praw.models.Redditor) has been added. This should refer to the class [`RedditorStream`](http://praw.readthedocs.io/en/latest/code_overview/other/redditorstream.html#praw.models.reddit.redditor.RedditorStream) instead.



---

Geez, whoever wrote that changelog should be seriously re-evaluating their life…"
823,Add Redditor.stream.comments() and Redditor.stream.submissions(),2017-07-18T06:36:55Z,2017-07-19T04:50:12Z,,,,"## Feature Summary and Justification

This feature provides a `Redditor.stream` attribute that can be used as `Redditor.stream.comments()` and `Redditor.stream.submissions()`. This is a feature that one would expect already exists in PRAW, because user contributions, just like subreddit posts and comments, can be sorted chronologically, and sometimes must be handled ASAP.

---

This is my first ever contribution to a real project. I did my best to follow [both](http://praw.readthedocs.io/en/latest/package_info/contributing.html) sets of [guidelines](https://github.com/praw-dev/praw/blob/master/.github/CONTRIBUTING.md) for contribution.

The code itself seemed simple enough, so I hope I did that all correctly.

Let me know if there are any issues with my PR so that we can fix them together. I hope this can be added to PRAW so that I can use it."
822,Allow betamax-matchers 0.4,2017-07-13T08:22:15Z,2017-07-19T01:34:53Z,,,,Test suite runs fine here.
821,Hide and unhide submissions in batches of 50,2017-07-12T04:48:44Z,2017-07-12T04:55:24Z,,,,
820,Bug with retrieving permalink,2017-07-02T23:22:36Z,2017-07-03T00:27:16Z,,,,"## Issue Description

Retrieving a comment's permalink works only with using `permalink` as a function, and retrieving a submission's permalink only works with using `permalink` as an attribute.

    >>> comment = reddit.comment(id='djnhv6f')
    >>> comment.permalink
    <bound method Comment.permalink of Comment(id='djnhv6f')>
    >>> comment.permalink()
    '/r/pokemon/comments/6kmw8f/rage_thread_01_july_2017/djnhv6f'
    >>> submission = reddit.submission(id=""6kmw8f"")
    >>> submission.permalink()
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
    TypeError: 'str' object is not callable
    >>> submission.permalink
    '/r/pokemon/comments/6kmw8f/rage_thread_01_july_2017/'

This is the only code I ran after importing praw and logging in.

## System Information

    PRAW Version: 4.5.1
    Python Version: 3.6.0
    Operating System: Windows 10
"
819,Various small corrections to documentation,2017-06-28T18:14:43Z,2017-07-02T14:47:19Z,,,,This PR collects various quibbles with the documentation that I've come across. Some are just little grammar tweaks. I also fixed a broken example and changed a docstring that referred to the wrong class.
818,"Depend on prawcore >= 0.11.0, <0.12.0",2017-06-20T20:40:12Z,2017-06-20T20:54:53Z,,,,This will enable prawdditions (and any other PRAW clients) to use the new `prawcore.exceptions.Conflict`.
817,"Replace deprecated Sphinx option ""html_use_smartypants""",2017-06-19T17:02:58Z,2017-06-19T17:38:59Z,,,,"Sphinx has deprecated the `html_use_smartypants` option, resulting in [a bunch of noise](https://travis-ci.org/praw-dev/praw/jobs/244599349#L697) when running the pre-push scripts. This PR moves this option  to docutils config as described at rtfd/readthedocs.org#2940."
816,Add a WikiPage helper method for safely updating pages,2017-06-19T02:37:53Z,2017-06-19T16:22:56Z,,,,"## Feature Summary and Justification

This feature provides a new method, `WikiPage.update`, for safe concurrent update of wiki pages. It takes a callback that computes the updated page content from its current content (i.e., `str -> str`). If Reddit returns an HTTP 409 Conflict response, the method takes the new server-side content from the response and re-computes the content for the edit. The process is repeated until the edit succeeds.

Some notes:

* This bumps the prawcore dependency from 0.10 to 0.11.

* Unfortunately, no single API endpoint gives us both the original page content and its revision ID, so it's necessary to make two API calls to put together the page content for the first edit attempt. Perhaps the Reddit admins could be talked into including the revision ID in the wiki page endpoint.

* Currently there's no backoff strategy."
815,Support link_flair endpoint,2017-06-18T00:08:09Z,2017-06-20T15:34:01Z,,,,"Fixes #812.

I tried a couple different approaches to getting rid of the `is_link` parameters. I wanted to keep it around since existing code might do something like:

    subreddit.flair.templates.clear(is_link=True)

The latest commit removes a lot of code duplication, and it seemed to me like the best approach."
814,Add warning about unreliability of reddit's search,2017-06-17T01:47:19Z,2017-06-19T08:20:07Z,,,,"## Feature Summary and Justification

Since `Subreddit.submission` uses reddit's search, its results are only as reliable as reddit's search. Submissions are sometimes missing from the results. (see the reference for an example and some investigation)

## References

* #813"
813,Subreddit.submissions returns empty and unusually quickly semi-randomly,2017-06-16T18:39:03Z,2017-06-20T14:58:35Z,,,,"It seems that, semi-randomly, [Subreddit.submissions](http://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html#praw.models.Subreddit.submissions) does not yield anything, and returns unusually quickly.

A script to which I used to test this.
```python
import time

# `post` is just any non-deleted non-removed post

results = {}
for i in range(100):
    before = time.time()
    l = len(set(post.subreddit.submissions(start=post.created_utc, end=post.created_utc)))
    total_time = time.time() - before
    try:
        results[l].append(total_time)
    except KeyError:
        results[l] = [total_time]
    time.sleep(1)

for k, v, in results.items():
    print(k, sum(v) / len(v), len(v))
```

For me the results were

    0 0.20122485160827636 15
    1 1.4370657444000243 85

Which means of the 100 test runs I did, it returned empty and also returned significantly quicker 15 out of 100 times.

It's possible that this is a reddit bug, but I've yet to be able to duplicate it with a browser and the F5 key.

## System Information

PRAW Version: 4.5.1
Python Version: 3.5.3
Operating System: Fedora 26
"
812,Implement the link_flair API endpoint,2017-06-15T21:48:54Z,2017-06-20T15:34:01Z,,,,"Currently, the only way to get the link flair templates for a subreddit is to already have a submission from that subreddit and then call `submission.flair.choices`. However, Reddit has since added [an endpoint](https://www.reddit.com/dev/api/#GET_api_link_flair) allowing templates to be retrieved using only the subreddit name.

It's not immediately clear to me where this would best be implemented. There's already `subreddit.flair.templates`, but it iterates through user flair. Arguably, the most natural way of organizing the different functionality would involve some breaking changes, so that ambiguous descriptions like `subreddit.flair.templates` don't default to one or the other type of flair."
811,Add flair parameters to Subreddit.submit,2017-06-15T21:29:48Z,2017-06-16T17:10:07Z,,,,Fixes #804.
810,http 500 response on api/me script test,2017-06-12T01:04:53Z,2017-06-16T17:10:30Z,,prawcore.exceptions.ServerError,"prawcore.exceptions.ServerError: received 500 HTTP response","## 
running the script below gets 500 error

```
import praw

reddit = praw.Reddit(client_id='<removed Script client_id>',
                     client_secret='<removed Script client_secret>',
                     password='<removed>',
                     user_agent='testscript 312',
                     username='<removed>')

print(reddit.user.me())
```
traceback:
```
/Users/4357/kode2/venv/bin/python /Users/4357/kode2/reddit-c/run.py
Traceback (most recent call last):
  File ""/Users/4357/kode2/reddit-c/run.py"", line 16, in <module>
    print(reddit.user.me())
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/praw/models/user.py"", line 60, in me
    user_data = self._reddit.get(API_PATH['me'])
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/praw/reddit.py"", line 367, in get
    data = self.request('GET', path, params=params)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/praw/reddit.py"", line 477, in request
    params=params)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/prawcore/sessions.py"", line 174, in request
    params=params, url=url)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/prawcore/sessions.py"", line 120, in _request_with_retries
    retries, saved_exception, url)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/prawcore/sessions.py"", line 86, in _do_retry
    params=params, url=url, retries=retries - 1)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/prawcore/sessions.py"", line 120, in _request_with_retries
    retries, saved_exception, url)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/prawcore/sessions.py"", line 86, in _do_retry
    params=params, url=url, retries=retries - 1)
  File ""/Users/4357/kode2/venv/lib/python2.7/site-packages/prawcore/sessions.py"", line 122, in _request_with_retries
    raise self.STATUS_EXCEPTIONS[response.status_code](response)
prawcore.exceptions.ServerError: received 500 HTTP response

Process finished with exit code 1
```
"
809,Add test for reddit.live.now() in the presence of featured live thread exists,2017-06-08T16:10:08Z,2017-06-08T18:08:28Z,,,,"Quote from #733 which I wrote:

> This commit doesn't contain test for 200 OK (the endpoint returns 200 if there is a featured live thread, 204 no content if there is no featured). So the test for latter should be added in the future.

Here is the test for the 200 (featured live thread exists)."
808,Fix accessing LiveUpdate.contrib raises AttributeError,2017-06-08T15:24:49Z,2017-06-08T15:41:15Z,,,,Fixes #807
807,`LiveUpdate` objects have no `contrib` attribute,2017-06-07T21:15:22Z,2017-06-08T15:41:15Z,,AttributeError,"AttributeError: 'LiveUpdate' object has no attribute 'contrib'","## Issue Description

`LiveUpdate` objects have no [`contrib` attribute](https://github.com/praw-dev/praw/blob/32e1f2eb57d3c7fa15c59537406f49b18998eb45/praw/models/reddit/live.py#L527) when those objects are initialized with responses from reddit:

```
$ cat lm.py
import praw
from praw.models import LiveUpdate

thread_id = 'z20sk2xfmvry'
update_id = '11e8e596-4ab5-11e7-97c4-0ed64337f69e'

reddit = praw.Reddit(""nmtake"")
update = reddit.live(thread_id).updates().next()
print(update.created)
print(update.contrib)
```

```
$ python lm.py
1496781520.0
Traceback (most recent call last):
  File ""lm.py"", line 10, in <module>
    print(update.contrib)
  File ""/usr/local/lib/python2.7/site-packages/praw/models/reddit/base.py"", line 34, in __getattr__
    .format(self.__class__.__name__, attribute))
AttributeError: 'LiveUpdate' object has no attribute 'contrib'
```

This bug is due to [this improper initialization of LiveUpdate instance](https://github.com/praw-dev/praw/blob/32e1f2eb57d3c7fa15c59537406f49b18998eb45/praw/models/reddit/live.py#L573). (`_contrib` attribute isn't set in the first clause). I'll submit a fix today.

## System Information

```
    PRAW Version: 4.5.1
  Python Version: 2.5.1
Operating System: OS X 10.11.6
```"
806,Adding URL parameter to Reddit.info,2017-06-07T02:27:34Z,2017-06-07T21:11:27Z,,,,"Fixes #798 

## Feature Summary and Justification

This feature allows for a url to be passed instead of a list of fullnames to Reddit.info in order to collect a list of all submissions to the given url string.
"
805,"Inconsistent log-in behavior, using wrong user",2017-06-01T02:50:43Z,2017-06-02T01:51:21Z,,,,"## Issue Description
If I load comments to reply to through: `comments = subreddit.stream.comments() `
replies come through my bot account, but if I load comments to reply to through:
```
submission = reddit.submission(id='xxxxxxx')
all_comments = submission.comments.list()
```

then replies come through my personal account.

I don't even understand how this is possible, as the bot shouldn't have my personal log-in credentials.

Any advice on how to fix this? I want to execute it such that it only loads comments from a specific submission, but from the bot account. I also want to ensure it never posts from my personal account.

Regardless, this is inconsistent behavior that points to the need for a fix.

## System Information

PRAW Version: 4.4
Python Version: 3.5
Operating System: OSX
"
804,Support flair in Subreddit.submit,2017-05-25T17:49:42Z,2017-06-16T17:10:07Z,⭐️ New Contributor Friendly ⭐️,,,"## Issue Description

Reddit's public API now allows link flair to be set [as part of the ""submit"" endpoint](https://www.reddit.com/dev/api/#POST_api_submit), rather than after the fact. This is not currently supported in PRAW.

I'm not especially familiar with flair, and I haven't looked into how the new parameters behave, but presumably the implementation of this would look a lot like the code + docstring of [`SubmissionFlair.select`](https://github.com/praw-dev/praw/blob/32e1f2eb57d3c7fa15c59537406f49b18998eb45/praw/models/reddit/submission.py#L223)."
803,Clarify when inboxable methods can be used,2017-05-24T23:28:43Z,2017-05-24T23:41:56Z,,,,
802,Add examples for flair templates,2017-05-23T05:23:54Z,2017-05-23T05:41:06Z,,,,
801,Enable toggling inbox replies on comments and submissions,2017-05-22T02:15:07Z,2017-05-22T16:03:34Z,,,,"Fixes #800.

I added an `InboxToggleableMixin` for this, but that's a horrible name and I'd welcome a better suggestion."
800,Add ability to enable/disable inbox replies,2017-05-22T01:24:07Z,2017-05-22T16:03:34Z,,,,There is [an API endpoint](https://www.reddit.com/dev/api/#POST_api_sendreplies) that makes it possible to enable or disable inbox replies on an existing comment or submission. It has not yet been added to PRAW (as of 4.5.1).
799,Add examples for new modmail,2017-05-17T16:17:26Z,2017-05-17T16:29:05Z,,,,"This should cover all the things on the present checklist at #797. If there are other things that should be included, perhaps I can add them to this PR and close that issue."
798,Add url parameter to Reddit.info,2017-05-16T15:41:23Z,2017-06-07T21:11:27Z,,,,"Per [Reddit's API documentation](https://www.reddit.com/dev/api/#GET_api_info), the `/api/info` endpoint can take a `url` parameter. The result is a listing of link submissions to that URL. For example, the following yields a listing of links to https://www.youtube.com:

https://www.reddit.com/api/info?url=https://www.youtube.com/

Currently, `Reddit.info` provides support for the `/api/info` endpoint, but it only supports the `id` parameter. It would be nice to have the `url` parameter as an option as well."
797,Improve new modmail documentation,2017-05-11T22:40:40Z,2017-05-17T21:36:06Z,,,,"There have been a couple posts on /r/redditdev looking for information that should be in the documentation, but isn't. Here is a partial checklist of info that should (probably) be included:

- [x] How to get individual messages from a conversation
- [x] An example showing how to extract the body of a conversation's messages
- [x] How to get the author's info
- [x] [The meaning of 'all' in the context of message states](https://gist.github.com/leviroth/dafcf1331737e2b55dd6fb86257dcb8d#general)

Related Reddit threads:
https://www.reddit.com/r/redditdev/comments/6aihxq/praw_bug_modmailconversationsstateall_not/
https://www.reddit.com/r/redditdev/comments/694xji/praw_how_the_hell_do_i_read_a_new_modmail_message/"
796,config.py no longer reads in praw.ini proxy settings,2017-05-10T18:27:47Z,2017-05-12T17:28:38Z,,,,
795,re-add support for https_proxy as a praw.ini entry,2017-05-10T06:44:32Z,2017-05-10T07:00:57Z,,,,"It looks like this function was added in #317, but has been removed somewhere along the way. As `https_proxy` is still a supported attribute, I figured this functionality was still desired. praw-dev/prawcore#58 has the changes necessary in `prawcore`."
794,'Comment' objects from Inbox have no attribute 'submission',2017-05-09T12:14:53Z,2017-05-09T15:24:06Z,,AttributeError,"AttributeError: 'Comment' object has no attribute 'submission'","## Issue Description
This is still/again broken. I found #684 which has been closed and merged into 4.5.0 in 994a817.  
I'm still able to reproduce this bug. Or this isn't a bug and my hack is the solution: d-schmidt/hearthscan-bot@5bade7dd8897e27669d69f0c3a856a48bfada7fd
```
i:\git\bot>python
Python 3.4.0 (v3.4.0:04f714765c13, Mar 16 2014, 19:25:23) [MSC v.1600 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pkg_resources
>>> pkg_resources.get_distribution(""praw"").version
'4.5.0'
>>> import praw
>>> r = praw.Reddit('bot')
>>> import logging as log
>>> log.basicConfig(format='%(asctime)s %(levelname)s %(name)s %(message)s', level=log.DEBUG)
>>> mention = r.inbox.mentions(limit=1).next()
2017-05-06 11:10:59,452 DEBUG prawcore Fetching: GET https://oauth.reddit.com/message/mentions
2017-05-06 11:10:59,453 DEBUG prawcore Data: None
2017-05-06 11:10:59,453 DEBUG prawcore Params: {'limit': 1, 'raw_json': 1}
2017-05-06 11:10:59,455 DEBUG requests.packages.urllib3.connectionpool Starting new HTTPS connection (1): www.reddit.com
2017-05-06 11:11:00,244 DEBUG requests.packages.urllib3.connectionpool https://www.reddit.com:443 ""POST /api/v1/access_token HTTP/1.1"" 200 161
2017-05-06 11:11:00,259 DEBUG requests.packages.urllib3.connectionpool Starting new HTTPS connection (1): oauth.reddit.com
2017-05-06 11:11:00,594 DEBUG requests.packages.urllib3.connectionpool https://oauth.reddit.com:443 ""GET /message/mentions?limit=1&raw_json=1 HTTP/1.1"" 200 510
2017-05-06 11:11:00,597 DEBUG prawcore Response: 200 (510 bytes)
>>> mention.refresh()
2017-05-06 11:11:08,323 DEBUG prawcore Fetching: GET https://oauth.reddit.com/r/subsub/comments/aaaaaa/titletitle/bbbbbb/
2017-05-06 11:11:08,325 DEBUG prawcore Data: None
2017-05-06 11:11:08,325 DEBUG prawcore Params: {'raw_json': 1}
2017-05-06 11:11:08,551 DEBUG requests.packages.urllib3.connectionpool https://oauth.reddit.com:443 ""GET /r/subsub/comments/aaaaaa/titletitle/bbbbbb/?raw_json=1 HTTP/1.1"" 200 1871
2017-05-06 11:11:08,555 DEBUG prawcore Response: 200 (1871 bytes)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Python34\lib\site-packages\praw\models\reddit\comment.py"", line 157, in refresh
    reply.submission = self.submission
  File ""C:\Python34\lib\site-packages\praw\models\reddit\base.py"", line 34, in __getattr__
    .format(self.__class__.__name__, attribute))
AttributeError: 'Comment' object has no attribute 'submission'
>>>
```
"
793,Comment.parent works on comments via comment_replies,2017-05-08T06:50:15Z,2017-05-08T06:56:49Z,,,,
792,Error while initializing PRAW object in a thread,2017-05-05T20:06:00Z,2017-05-05T20:16:15Z,,TypeError,"TypeError: child watchers are only available on the default loop","Hey,

I'm trying to use praw within a thread I created using python `threading.Thread(target=RedditCollector.collector_main).start()`

I'm receiving the error below on initialization, but it happens just when I'm running it in a different thread (when I'm calling the same method from the main thread it works).

I'd appreciate you help.
Rani

```
  File ""/scripts/Collector.py"", line 57, in get_api_connector
    return Reddit(client_id='XXXX', client_secret='XXXX', user_agent='XXX')
  File ""/usr/local/lib/python3.5/site-packages/praw/reddit.py"", line 119, in __init__
    self._check_for_update()
  File ""/usr/local/lib/python3.5/site-packages/praw/reddit.py"", line 240, in _check_for_update
    update_check(__package__, __version__)
  File ""/usr/local/lib/python3.5/site-packages/update_checker.py"", line 181, in update_check
    result = checker.check(package_name, package_version, **extra_data)
  File ""/usr/local/lib/python3.5/site-packages/update_checker.py"", line 72, in wrapped
    retval = function(obj, package_name, package_version, **extra_data)
  File ""/usr/local/lib/python3.5/site-packages/update_checker.py"", line 127, in check
    data['platform'] = platform.platform(True) or 'Unspecified'
  File ""/usr/local/lib/python3.5/platform.py"", line 1328, in platform
    system, node, release, version, machine, processor = uname()
  File ""/usr/local/lib/python3.5/platform.py"", line 1002, in uname
    processor = _syscmd_uname('-p', '')
  File ""/usr/local/lib/python3.5/platform.py"", line 756, in _syscmd_uname
    f = os.popen('uname %s 2> %s' % (option, DEV_NULL))
  File ""/usr/local/lib/python3.5/os.py"", line 1035, in popen
    bufsize=buffering)
  File ""/usr/local/lib/python3.5/site-packages/gevent/subprocess.py"", line 555, in __init__
    restore_signals, start_new_session)
  File ""/usr/local/lib/python3.5/site-packages/gevent/subprocess.py"", line 1161, in _execute_child
    self.pid = fork_and_watch(self._on_child, self._loop, True, fork)
  File ""/usr/local/lib/python3.5/site-packages/gevent/os.py"", line 375, in fork_and_watch
    watcher = loop.child(pid, ref=ref)
  File ""gevent.libev.corecext.pyx"", line 518, in gevent.libev.corecext.loop.child (src/gevent/libev/gevent.corecext.c:7589)
  File ""gevent.libev.corecext.pyx"", line 1886, in gevent.libev.corecext.child.__init__ (src/gevent/libev/gevent.corecext.c:21473)
TypeError: child watchers are only available on the default loop
```

PRAW Version: 4.4.0
Python Version: 3.5
Operating System: Ubuntu 16
"
791,Can't retrieve random submission url from specific subreddit?,2017-05-01T08:41:12Z,2017-05-02T02:59:12Z,,praw.exceptions.ClientException,"praw.exceptions.ClientException: Invalid URL: https://www.reddit.com/r/Pizza/","## Issue Description

For some reason I can't receive a random post from /r/pizza.
It works on every other subreddit I have tried.
What am I doing wrong?

```python
import praw

reddit = praw.Reddit(client_id='',
                     client_secret='',
                     password='',
                     user_agent='',
                     username='')

random_post = reddit.subreddit('pizza').random().url
```


```
File ""C:\Users\perso\AppData\Local\Programs\Python\Python35\lib\site-packages\praw\models\reddit\subreddit.py"", line 318, in random
    self._reddit.config.reddit_url, path))
  File ""C:\Users\perso\AppData\Local\Programs\Python\Python35\lib\site-packages\praw\models\reddit\submission.py"", line 125, in __init__
    self.id = self.id_from_url(url)
  File ""C:\Users\perso\AppData\Local\Programs\Python\Python35\lib\site-packages\praw\models\reddit\submission.py"", line 41, in id_from_url
    raise ClientException('Invalid URL: {}'.format(url))
praw.exceptions.ClientException: Invalid URL: https://www.reddit.com/r/Pizza/
```


## System Information

```
PRAW Version: 4.5.0
Python Version: 3.5
Operating System: Windows 10
```
"
790,Resolve deprecations,2017-04-30T06:16:54Z,2017-04-30T06:24:48Z,,,,"See changelog additions for details.

After this merge, the next release of PRAW will be PRAW5 unless there is a bug fix required for PRAW4. PRAW5 release should be no sooner than June."
789,Add `**stream_options` to other stream methods,2017-04-30T05:55:46Z,2017-04-30T06:04:07Z,,,,
788,URL-escape password placeholder for Betamax,2017-04-25T17:28:12Z,2017-04-25T18:05:25Z,,,,"Fixes #787 

I was unsure how to test whether this actually works. Perhaps delete and re-record some cassette with an affected password?"
787,Passwords that contain spaces are not properly filtered from cassettes.,2017-04-22T18:57:21Z,2017-04-25T18:05:25Z,,,,"## Issue Description

Passwords that contain spaces (and other special characters) are not properly filtered from cassettes.

In the cassettes, the password shows up in the body of a POST request that fetches an access token. Because the request body is URL-encoded, passwords that contain spaces and other special characters will not show up verbatim in the cassette, and will not be filtered.

For example, suppose my username is `spez` and my password is `foo bar`, and I'm using this account to generate a new cassette. The Betamax filters look for the string `foo bar` in the cassettes, but they only ever find `foo+bar`. As a result, the following line leaks into the cassette:

    ""string"": ""grant_type=password&password=foo+bar&username=<USERNAME>""

If there aren't any other lurking issues in this area, I assume this could be fixed by slapping a `urllib.parse.quote_plus` into the placeholder setup.

Fortunately, it doesn't appear that any passwords have leaked into the existing cassettes.


## System Information

 PRAW Version: 4.4.0
  Python Version: 3.6.1
Operating System: Windows 10
"
786,Allow pausing comment streams after a number of data fetches with no new comments,2017-04-20T04:43:39Z,2017-04-20T05:49:25Z,,,,
785,Fix docstring detected as not using imperative mood,2017-04-20T04:36:07Z,2017-04-20T04:41:21Z,,,,
784,Fix some typos.,2017-04-12T18:55:45Z,2017-04-12T21:32:59Z,,,,"Found I typo I added, tried to find others. The only real code change is [`Subreddit.subscribe`](https://github.com/praw-dev/praw/compare/master...elnuno:typos?expand=1#diff-a6c46ac5ed8df35e279b7cd7973765fcL483), but I'm assuming it's a fix too (inital -> initial):

```python
        data = {'action': 'sub', 'skip_inital_defaults': True,
                'sr_name': self._subreddit_list(self, other_subreddits)}
```"
783,"Add timeout, maximum failed retries and resume to streams.",2017-04-12T16:40:01Z,2017-04-20T05:51:10Z,,,,"Fixes #778

## Feature Summary and Justification

Allows limiting the fetching loop of stream by either elapsed time since last successful fetch happened or number of failed ones. On reaching that limit, an exception is raised that stores enough data to resume the stream at a later time.

This is all experimental, so we can discuss the design."
782,Doc additions,2017-04-11T06:19:12Z,2017-04-11T06:26:40Z,,,,Closes #779.
781,Make style more consistent,2017-04-09T17:29:57Z,2017-04-09T17:40:34Z,,,,
780,PRAW5 will use lucene search syntax by default,2017-04-09T17:27:22Z,2017-04-09T17:27:46Z,,,,
779,"Explain ""PRAW is not thread safe"" in the documentation.",2017-04-08T15:37:42Z,2017-04-11T06:26:40Z,Documentation,,,"It would be great to provide an example of why it is not thread safe.

Relevant comment:

https://www.reddit.com/r/redditdev/comments/63ugl5/praw_mulitprocessing_handler_prawhandler_is_not_a/dfx9oet/?context=3"
778,Add `timeout` param to streams,2017-04-08T15:34:01Z,2017-04-30T06:04:22Z,⭐️ New Contributor Friendly ⭐️,,,"Comment streams are only interruptible when results are being returned by user-level break statements. However, if a stream receives no new data the function will essentially block indefinitely while it periodically pulls for new information.

It would be beneficial to have a `timeout` parameter which raises some sort of timeout exception if there are no new items after `timeout` seconds. With this added, client code can iterate between infrequently used streams.

Bonus points if the stream can be resumed after handling the exception, otherwise duplicates will be returned and have to be handled by the end-user."
777,Upload image with reddit.subreddit.submit,2017-04-07T08:40:55Z,2017-04-07T15:29:11Z,,,,"Hi
I can add new link with command:
reddit.subreddit('name').submit('title','url')
but I don't know how to submit image.
In this form https://www.reddit.com/submit I see ""image"" field, so I suspect that it is possible.

"
776,Remaining modmail endpoints,2017-04-06T20:42:50Z,2017-04-07T05:21:25Z,,,,"Partially implements #703 (provide issue number of applicable)

This adds support for a few more new modmail features: bulk read, unread count, and subreddit listing.

Once this is merged, all the features provided by the new modmail API should be supported by PRAW. 🎉🎉🎉"
775,Added missing examples to new modmail docstrings,2017-04-06T20:39:20Z,2017-04-06T21:41:18Z,,,,This adds examples to the new modmail methods that were missing them.
774,Add Modmail.create,2017-04-04T17:11:49Z,2017-04-06T20:12:16Z,,,,"Partially implements #703 

This adds the ability to create new modmail conversations."
773,"Provide human-readable information on conversation state, mod actions",2017-04-03T17:29:56Z,2017-08-31T14:40:40Z,,,,"## Issue Description

As documented [here](https://gist.github.com/leviroth/dafcf1331737e2b55dd6fb86257dcb8d#secret-codes), conversation states and mod actions are represented in the API responses as numerical codes. Some way or other, it would be nice to provide this information in a more readable form; perhaps by replacing the codes when the responses are parsed?"
772,Add ModmailConversation.read and unread,2017-04-03T15:14:16Z,2017-04-06T14:45:41Z,,,,"Partially implements #703.

This adds support for marking conversations as read and unread. Multiple conversations can be handled through the `other_conversations` parameter."
771,Add support for fetching modmail conversation lists,2017-04-02T01:44:20Z,2017-04-04T16:18:45Z,,,,"Partially implements #703 

Some notes:

Working on this feature has me wondering if `ModmailConversation.__call__` has the right name. Compare with `SubredditRelationship`, where `__call__` is a listing and individual relationships are handled through named methods.

There is a `limit` parameter. However, I wasn't able to find any maximum: in manual testing, I got as far as 1998 before I ran out of conversations to request. For this reason, I didn't make use of the existing `ListingGenerator` framework.

The second commit returns a list of `ModmailConversation` objects with just the `id` and no other data. The last commit adds some parsing: the idea is to just use the conversation metadata, while leaving messages and mod actions to be fetched on the individual conversation. This required a couple tweaks to `ModmailConversation.parse`. Apart from feedback on the code itself, I'm wondering what you think of this design.
"
770,update inbox.unread() docs to reflect actual behavior,2017-04-01T19:05:40Z,2017-04-02T02:34:07Z,,,,"## Feature Summary and Justification

`unread(mark_read=True)` does not mark each message, just the inbox.  
https://www.reddit.com/r/redditdev/comments/3502mg/_/cqzu3wz/"
769,Add parameter check for ModmailConversation,2017-04-01T00:20:26Z,2017-04-01T00:24:46Z,,,,Finishes #767
768,Add mark_read parameter to Modmail.__call__,2017-03-31T15:50:33Z,2017-04-01T00:07:01Z,,,,"Fixes #764 

This also tweaks `RedditBase._fetch` in the manner we discussed, in case this pattern is useful in the future."
767,Re-add check for ModmailConversation.id existing in constructor,2017-03-31T15:17:15Z,2017-04-01T00:20:51Z,,,,"This reverts commit 7040228388a7ddf5ca379a8382372698e343aa3e.

Turns out this check was doing something important: when it's missing, the call to `cls(reddit, _data=conversation` at the end of `ModmailConversation.parse` results in a `ModmailConversation` with the `id` attribute set to `None`. This ends up overriding the `id` attribute of the original object that called `_fetch`, and breaking, e.g., the `str` function.

I'm not sure this is the best way to handle the issue, but wanted to open the PR in any case."
766,"New modmail: archive, mute, highlight",2017-03-30T18:28:39Z,2017-03-31T06:08:22Z,,,,"Partially implements #703.

This adds to `ModmailConversation` the methods `archive`, `unarchive`, `mute`, `unmute`, `highlight`, and `unhighlight`.

The methods for muting use `Reddit.request` instead of `Reddit.post` because of the odd format of the response: the API returns the conversation, just as in `Modmail.conversation`, but the `'conversation'` key is named `'conversations'` instead. As a result, `objectify` goes through the `if 'user' in data` branch of the `else` clause (line 86), and raises an exception when the data isn't in the expected format.

We might be able to use `Reddit.post` if the `if 'user' in data` clause were narrowed so as not to match the response from the mute endpoints."
765,Create streams for different inbox methods,2017-03-30T16:10:18Z,2017-03-30T16:30:58Z,,,,"## Issue Description

Similar to a SubredditStream, an InboxStream would allow a bot to endlessly retrieve the newest unreads, mentions, comment_replies and possibly even sents. 

Looking at the SubredditStream implementation, it seems like a relatively easy task and I wouldn't mind taking it on myself. I just want to know for sure that PRAW is looking for such a feature.
"
764,Marking modmail conversation as read,2017-03-29T15:35:18Z,2017-04-01T00:07:01Z,,,,"## Issue Description

The new modmail API [endpoint for getting a conversation](https://www.reddit.com/dev/api/#GET_api_mod_conversations_:conversation_id) includes a `'markRead'` parameter that isn't currently implemented in PRAW. I was wondering what the best way would be to add this.

One option would be an extra argument to `Subreddit.modmail.__call__`. However, this would probably require overriding `RedditBase._fetch`, which doesn't take any parameters. Is there a cleaner way to pass extra data along with the `_info_path`?"
763,Add ModmailConversation.reply,2017-03-29T15:24:37Z,2017-03-30T16:59:03Z,,,,"Partially implements #703 

This adds a `reply` method on the `ModmailConversation` class, providing the ability to reply to a modmail conversation.

The method returns a `ModmailMessage` object for the new message. However, the API actually returns a fresh copy of the conversation metadata and all the messages (or is it just a subset?). This data could be used to do something more fancy, but for now I went with the simpler option."
762,Is `Search` still supported in praw 4.4?,2017-03-29T14:01:02Z,2017-03-29T14:32:51Z,,,,"I saw the snippet below on stackoverflow:
```

import praw
r = praw.Reddit(user_agent='Getting the data!!')
r.login(""username"",""password"",disable_warning=True)
results=r.search('whatever', subreddit=None, sort=None, syntax=None, period=None)
for x in results:
    print x
```

It seems that this snippet is outdated. Because I can't find the `search` function in praw 4.4.0.. I didn't find the search-related API in the document of praw..

Is this feature still supported?"
761,Add parsing for ModmailConversation,2017-03-27T17:06:16Z,2017-03-29T00:09:23Z,,,,"This adds parsing for the ModmailConversation class. It should convert user information into `Redditor` objects, submissions into `Submission`s, and so on, while ensuring that the data included in the modmail API does not clobber pre-existing attributes of those classes.

If you would rather break this down into smaller pieces, let me know and I can give it a shot.

I also included the minor tweak to `ModmailConversation.__init__` that you mentioned in #758."
760,Update the obtain oauth token script,2017-03-27T04:01:17Z,2017-03-29T00:07:59Z,,,,"Make it more interactive and fix a few bugs

## Feature Summary and Justification

The current script doesn't always flush the output of the url print command before entering the receive connection command. It's also slightly confusing for amateur users where to get the client id and secret and where to put them in the script."
759,Limit is not getting passed to ListGenerator constructor on call to Subreddit.top(),2017-03-24T00:05:12Z,2017-03-24T14:41:21Z,,,,"## Issue Description

I am attempting to set the limit of the records returned for the top posts in a subreddit. My code is as follows:

`for submission in reddit.subreddit(subreddit).top(limit=DEF_SUB_COUNT):`

The documentation states that any additional parameters passed to top() should be passed to the construction of the ListGenerator that is returned from this function. When the limit is set to a value below 1000 (which is the upper bound of this limit according to the documentation), then the expected behavior should be that the ListGenerator iterates until it reaches that limit. This is not the case when I run this code, it continues to iterate until the upper limit of 1000 is reached. This is not a critical issue but any help would be greatly appreciated!


## System Information

    PRAW Version: 4.4
  Python Version: 2.7.12
Operating System: OSX Sierra 10.12.3

"
758,Add barebones support for fetching a modmail conversation,2017-03-21T23:38:43Z,2017-03-24T05:20:26Z,,,,"This provides the basic features that bboe mentioned in #740. I've tried to leave out the parsing while making minimal changes to the surrounding code; let me know if I should include more or less.

I included a test, but it's really just a stub at this point as the test doesn't actually fetch anything."
757,Add request delay between no-update requests for all streams,2017-03-20T05:18:04Z,2017-03-20T05:23:25Z,,,,
756,Fix self posts with no selftext,2017-03-16T15:23:35Z,2017-03-16T16:09:16Z,,,,"Attempting to self post without any body text will throw a TypeError.
This attempts to address that. Much help and encouragement from @bboe
thanks. Obviously I have no idea what I'm doing with the test stuff,
sorry for the extra work there.

New to using GitHub so let me know if there's something I should be doing or not doing with this request. Thanks."
755,Persist comment_limit and comment_sort after fetch,2017-03-16T04:59:06Z,2017-03-16T05:11:28Z,,,,"This fixes the root problem as noted in #753, which is that the comment sort value was not being correctly passed into the call to comment replacement."
754,Add Inbox.stream,2017-03-16T04:31:05Z,2017-03-16T04:39:01Z,,,,
753,Sort Order and `replace_more`,2017-03-15T22:35:11Z,2017-03-16T11:15:37Z,,,,"## Issue Description

It is necessary to change the value of `submission.comment_sort` before calling `submission.comments.replace_more()` sorting to be effective; however, any comments added by `replace_more()` are then sorted by another method (seemingly 'best').

I've confirmed this via thread '5z8dkm'.

Here's the code I used:

```
import praw
from praw.models import MoreComments
#import pprint
from datetime import datetime
import re

subid = '5z8dkm'

# create Reddit instance here
# insert in your own credentials
reddit = praw.Reddit()


# auto sort by old
submission = reddit.submission(id=subid)
submission.comment_sort = 'old' # default is new for this sub
submission.comments.replace_more(limit = None)
comments = list(submission.comments)

# get some basic information to compare to web browser
# also get datetime stamp for manual sorting
line_list = list()
date_list = list()
for e in comments:
	line = ''
	tab = ''
	
	# get top level comment information
	date = datetime.fromtimestamp(e.created)
	line1 = tab + str(e.author) + ' [' + str(e.score) + '] ' + str(datetime.fromtimestamp(e.created)) + '\n'
	line2 = tab + e.body.encode('utf-8') + '\n'
	line = line1 + line2 + '\n'

	line_list.append(line)
	date_list.append((date, line))

# write auto sorted version to file
file = open('auto_sorted_by_old.txt', 'w')
for l in line_list:
	file.write(l)
file.close()



# manually sort by old via date
date_list.sort(key = lambda tup: tup[0])
sort_list = list()
for l in date_list:
	sort_list.append(l[1])

# write manually sorted version to file
file = open('manual_sorted_by_old.txt', 'w')
for l in sort_list:
	print l
	file.write(l)
file.close()
```

I've also attached the files generated, but in summary both files are identical (and correctly sorted) for the original comments ending with this comment:

> JadedIdealist [41] 2017-03-14 15:47:57
> Thanks for hosting.
> 
> Just a note that normally on launch threads the latest information appears at the top of the updates (ie most recent first as you read down, and earlier stuff appears later).
> 
> It just makes it a little quicker and easier to grok what's happened most recently - you might want to maybe consider doing it that way round on  Thursday (assuming it's you again). 
> 
> Cheers.

 but after the first top-level `MoreComments`, the auto-sorted version is as follows:

> ethan829 [20] 2017-03-14 19:56:49
> It's official:
> 
> >[Targeting Thursday, March 16 for @EchoStar XXIII launch; window opens at 1:35am EDT and weather is 90% favorable.](https://twitter.com/SpaceX/status/841679133084135424)
> 
> steezysteve96 [18] 2017-03-15 19:44:55
> Are we gonna get a new thread or reuse this one?  Reusing threads would fit the SpaceX spirit, after all
> 
> EDIT: never mind, found the update
> 
> > 24 hours until T-0. Weather 90% go. Falcon 9 is vertical. We'll be reusing this launch thread - see you all tomorrow!

while the manually sorted version continues to be correct:

> None [2] 2017-03-14 17:47:53
> [removed]
> 
> ByteStalker [4] 2017-03-14 18:51:14
> Aww man I'm going to be in Orlando on the 16th but Im not going to be able to see the launch :(
> 
> ethan829 [20] 2017-03-14 19:56:49
> It's official:
> 
> >[Targeting Thursday, March 16 for @EchoStar XXIII launch; window opens at 1:35am EDT and weather is 90% favorable.](https://twitter.com/SpaceX/status/841679133084135424)
> 
> BEAT_LA [4] 2017-03-14 23:13:26
> Is this the reflown booster?

Here are some screenshots to show the correct behavior in the web browser:

Last comment when sorted by 'new' and expanded:

![screen shot 2017-03-15 at 5 54 16 pm](https://cloud.githubusercontent.com/assets/10160679/23973643/640a31e8-09ad-11e7-83e5-f1175d017c9f.png)

Is the first comment when sorted by 'old':

![screen shot 2017-03-15 at 5 54 42 pm](https://cloud.githubusercontent.com/assets/10160679/23973631/4f0ed3e8-09ad-11e7-8e78-65982b6f510c.png)

The last comment when sorted by 'old' before expanding is the last comment correct auto-sorted using praw:

![screen shot 2017-03-15 at 5 54 49 pm](https://cloud.githubusercontent.com/assets/10160679/23973658/85e758fe-09ad-11e7-8b40-c130730f1eb3.png)


Here are the two output files. The problem starts at line 526.

[auto_sorted_by_old.txt](https://github.com/praw-dev/praw/files/846162/auto_sorted_by_old.txt)
[manual_sorted_by_old.txt](https://github.com/praw-dev/praw/files/846161/manual_sorted_by_old.txt)



## System Information

PRAW Version: 4.4.0
Python Version: Python 2.7.11 |Anaconda 4.0.0 (x86_64)
Operating System: macOS 10.12.3 16D32 (Sierra)"
752,"Unable to create Redditor for sending messages, throws TypeError",2017-03-15T14:03:59Z,2017-03-15T14:17:51Z,,TypeError,"TypeError: __str__ returned non-string (type Redditor)","## Issue Description

### Setup:
1. Create a Reddit instance, as in `reddit = new praw.Reddit(...)`
2. Pass instance to a function
3. Read and store all unread messages
4. For each message, add the author (from `Message.author`) to list,, and create Redditor instance via `reddit.redditor(user)`
5. Send message to redditor via `Redditor.message('subject', 'text')`.

### Expected Behaviour:
Create the user without issue, and send a message

### Actual Behavior:
Exception is thrown when attempting to create Redditor object.
```
Traceback (most recent call last):
  File ""C:/Users/krobertson/Documents/GitHub/postsubscribe/postsubscribe.py"", line 118, in <module>
    main()
  File ""C:/Users/krobertson/Documents/GitHub/postsubscribe/postsubscribe.py"", line 101, in main
    new_users = poll_messages(reddit, users)
  File ""C:/Users/krobertson/Documents/GitHub/postsubscribe/postsubscribe.py"", line 36, in poll_messages
    useracc = reddit.redditor(user)
  File ""C:\Program Files\Python35\lib\site-packages\praw\reddit.py"", line 388, in redditor
    return models.Redditor(self, name)
  File ""C:\Program Files\Python35\lib\site-packages\praw\models\reddit\redditor.py"", line 36, in __init__
    self._path = API_PATH['user'].format(user=self)
TypeError: __str__ returned non-string (type Redditor)
```

## System Information

    PRAW Version: v4.4.0 
    Python Version: 3.5.2
    Operating System: Windows 7 Professional SP1
"
751,Documents replace_more and comment_sort order,2017-03-12T23:19:26Z,2017-03-12T23:31:46Z,,,,"Fixes

Documentation was unclear that `replace_more()` called comments and thus it is necessary to make any changes to `comment_sort` beforehand.  

## Feature Summary and Justification

A sentence for clarification was added in two places: quickstart and submission.py
"
750,Add changelog entry for WikiPage revision bugfix,2017-03-12T02:38:17Z,2017-03-12T03:00:04Z,,,,"Adds changelog entry for #748 
Fixes #746 "
749,Can mentions be filtered for new only?,2017-03-11T04:12:22Z,2017-03-11T04:30:05Z,,,,"## Issue Description

Making a bot that will respond with a message every time bot's username is mentioned. Can mentions be filtered for only unread ones? Also, what's the max limit of mentions? What if new mentions surpass the limit?

Currently I'm using something like below.

```
for m in reddit.inbox.mentions(limit=50):
    if m.new:
        c = reddit.comment(m.id)
        c.reply()
        m.mark_read()
```

## System Information

    PRAW Version: 4.4.0
  Python Version: 2.7
Operating System: ubuntu 14.4
"
748,Fix crashes on wiki revisions with deleted author,2017-03-10T20:26:35Z,2017-03-11T04:17:43Z,,,,Fixes #746
747,Allow passing custom Requestor class to Reddit.,2017-03-10T06:36:59Z,2017-03-24T05:15:49Z,,,,"This feature allows customizing the Reddit instance's Requestor. It can be useful for caching, logging and other enhancements. This, along with praw-dev/prawcore/pull/54 would make PRAW's networking more customizable. E.g., in `tests.integration`:

```python
    # Before

    reddit = Reddit(...)
    http = reddit._core._requestor._http
    recorder = Betamax(http)
```

```python
    # After, untested

    def req(*args, **kwargs):
        return Requestor(session_cls=betamaxxed, *args, **kwargs)

    def betamaxxed():
        return Betamax(requests.Session())

    reddit = Reddit(..., requestor_cls=req)

```
"
746,WikiPage.revisions raises TypeError if an author of revision isn't exist,2017-03-09T14:52:24Z,2017-03-12T03:00:04Z,,TypeError,"TypeError: 'NoneType' object is not subscriptable","## Issue Description

As the title said. To reproduce:

```
$ cat rev_no_author.py
import praw

reddit = praw.Reddit('prawtest')
list(reddit.subreddit('theoryofreddit').wiki['index'].revisions())

$ python3 -mpdb rev_no_author.py
> /Users/foo/issues/rev_no_author.py(1)<module>()
-> import praw
(Pdb) c
Traceback (most recent call last):
...
  File ""/usr/local/lib/python3.6/site-packages/praw/models/reddit/wikipage.py"", line 16, in _revision_generator
    _data=revision['author']['data'])
TypeError: 'NoneType' object is not subscriptable
Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program
> /usr/local/lib/python3.6/site-packages/praw/models/reddit/wikipage.py(16)_revision_generator()
-> _data=revision['author']['data'])
(Pdb) p revision
{'timestamp': 1357210312.0, 'reason': None, 'author': None, 'page': 'index', 'id': '873933a0-5550-11e2-82f1-12313b0c1e2b'}
```

And here is the [URL](https://www.reddit.com/r/TheoryOfReddit/wiki/index?v=873933a0-5550-11e2-82f1-12313b0c1e2b) whose ID is '873933a0-5550-11e2-82f1-12313b0c1e2b'.

## System Information

PRAW Version: 4.4.0
Python Version: 3.6.0
Operating System: El Capitan 10.11.6
"
745,Add help message for NoSectionError,2017-03-06T00:23:58Z,2017-03-06T04:25:34Z,,,,"Closes #744. When the user passes a non-None `site_name` causing `praw.Reddit(site_name)` to raise `configparser.NoSectionError`, the exception has some help text appended to it, and is reraised.

    configparser.NoSectionError: No section: 'test'
    You provided the name of a praw.ini configuration which does not exist.

    For help with creating a Reddit instance, visit
    https://praw.readthedocs.io/en/latest/code_overview/reddit_instance.html

    For help on configuring PRAW, visit
    https://praw.readthedocs.io/en/latest/getting_started/configuration.html

How's it look?

&nbsp;

Edit: Sorry, I'm still a noob at code coverage. I'm assuming Coveralls highlighted those lines because it wants me to write a unittest for catching that case?

Edit: All good."
744,Users attempting to run old code are confused by configparser.NoSectionError. Should they get a different help message?,2017-03-05T07:27:28Z,2017-03-06T04:25:34Z,,,,"Over the past couple of months, I've been getting a lot of reddit PMs from people who find PRAW3 tutorials and resources online, and are confused by:


    >>> r = praw.Reddit('useragent')
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""C:\Python36\lib\site-packages\praw\reddit.py"", line 103, in __init__
        **config_settings)
      File ""C:\Python36\lib\site-packages\praw\config.py"", line 66, in __init__
        self.custom = dict(Config.CONFIG.items(site_name), **settings)
      File ""C:\Python36\lib\configparser.py"", line 846, in items
        raise NoSectionError(section)
    configparser.NoSectionError: No section: 'useragent'

When filled positionally, that argument lands in  [site_name](https://github.com/praw-dev/praw/blob/e142a508076625a994814e75d78f245c1e83d65d/praw/reddit.py#L75), so we can check for `site_name is not None and config_settings == {}`.

When `user_agent` is filled by name, the error is instead `praw.exceptions.ClientException: Required configuration setting 'client_id' missing`. However, I have not been getting any messages about this so I don't think PRAW3 authors did this very much. It's a low enough priority that I would not add the help message in this case.

Should the `Reddit` constructor try to detect this event and give some info on the v3-v4 distinction? I don't know if making direct references to prior releases is generally favored but it's frustrating for new users."
743,Fix argument name for ban reason in example,2017-03-01T08:44:40Z,2017-03-02T00:42:00Z,,,,"should be ban_reason, not reason. see:

https://www.reddit.com/dev/api/#POST_api_friend

Fixes #742"
742,Subreddit docs use wrong argument name for ban reason,2017-03-01T08:43:35Z,2017-03-02T00:42:00Z,,,,"## Issue Description

https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html

This example:

    reddit.subreddit('SUBREDDIT').banned.add('NAME', reason='a reason')

should be

    reddit.subreddit('SUBREDDIT').banned.add('NAME', ban_reason='a reason')

At least if the reddit api docs are to be trusted:

https://www.reddit.com/dev/api/#POST_api_friend

## System Information

not relevant"
741,Documented comment sorting and limits in Submission class,2017-02-28T04:15:16Z,2017-03-03T07:01:53Z,,,,"This adds documentation to `Submission.comments` showing how to use `Submission.comment_sort` and `Submission.comment_limit`. This is [already documented](http://praw.readthedocs.io/en/latest/getting_started/quick_start.html#obtain-comment-instances), but this way the information can also be found in the Code Overview as well.

I also wonder if the note in the section linked above would be helpful to include in the tutorial on [Comment Extraction and Parsing](http://praw.readthedocs.io/en/latest/tutorials/comments.html)."
740,Some very provisional work for new modmail,2017-02-27T02:27:35Z,2017-03-29T00:18:10Z,,,,"This provides very a very basic implementation for fetching modmail conversations. I wanted to check in and see if I'm following the basic patterns for object creation correctly.

This should definitely not be merged; it doesn't even have tests yet.

Edit: Tagging #703."
739,typo fix,2017-02-25T22:03:54Z,2017-02-25T22:49:02Z,,,,"Fixes # (provide issue number of applicable)

## Feature Summary and Justification

This feature provides ...

## References

* 
*
"
738,Updated a lot of documentation,2017-02-25T20:49:01Z,2017-02-26T02:37:51Z,,,,"I updated a lot of documentation mostly surrounding wiki pages and moderation queues such as /about/modqueue, /about/log, /about/reports, etc.

Also fixed code example for removing a moderator from a subreddit."
737,praw 3 requests dependency version,2017-02-25T04:10:34Z,2017-02-25T05:55:10Z,,AttributeError,"AttributeError: 'Session' object has no attribute 'merge_environment_settings'","## Issue Description

In PRAW 3, the setup.py file pins the requests version at ``'requests >=2.3.0'``. However, actually trying to use requests v2.3.0 will raise an exception. The problem is that the ``merge_environment_settings`` method wasn't added until v2.4.0.

``` 
  File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 1522, in refresh_access_information
    refresh_token=refresh_token or self.refresh_token)
  File ""<decorator-gen-11>"", line 2, in refresh_access_information
  File ""/usr/local/lib/python3.4/dist-packages/praw/decorators.py"", line 294, in require_oauth
    return function(*args, **kwargs)
  File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 745, in refresh_access_information
    retval = self._handle_oauth_request(data)
  File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 657, in _handle_oauth_request
    response = self._request(url, auth=auth, data=data, raw_response=True)
  File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 451, in _request
    response = handle_redirect()
  File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 432, in handle_redirect
    verify=self.http.validate_certs, **kwargs)
  File ""/usr/local/lib/python3.4/dist-packages/praw/handlers.py"", line 138, in wrapped
    return function(cls, **kwargs)
  File ""/usr/local/lib/python3.4/dist-packages/praw/handlers.py"", line 56, in wrapped
    return function(cls, **kwargs)
  File ""/usr/local/lib/python3.4/dist-packages/praw/handlers.py"", line 101, in request
    settings = self.http.merge_environment_settings(
AttributeError: 'Session' object has no attribute 'merge_environment_settings'
```

This bug was initially discovered here https://github.com/michael-lazar/rtv/issues/325. I've tested on requests v2.4.0 and verified that it solved the problem. I can fix it on my end by changing the package requirements, but if you're still supporting bug fixes for PRAW 3 I think it's worth updating here too.

## System Information

PRAW Version: 3.6.1
Python Version: 3.4.0
Operating System: Ubuntu 14.04
"
736,Updated documentation for messages.,2017-02-25T03:55:46Z,2017-02-25T06:38:37Z,,,,"Updated the documentation for sending messages and provided an additional example to send a message from a subreddit.

Also added documentation for getting moderator mail as well as replies to moderator mail messages.
"
735,Fix LiveContributorRelationship.__call__ return type,2017-02-20T13:38:15Z,2017-02-20T15:02:54Z,,,,"Fix `LiveContributorRelationship.__call__` return type

After https://github.com/praw-dev/praw/commit/5b743ee50f81c32d5fbc4b43ce4524c028666117, the method returns RedditorList consists of normal dicts
(not Redditor instances):

```
>>> reddit.live(""ukaeu1ik4sw5"").contributor()
<praw.models.list.redditor.RedditorList object at 0x10c217f90>
>>> _[0]
{u'id': u't2_f3lhp', u'name': u'Acidtwist', u'permissions': [u'all']}
```

This fix changes return type to RedditorList consists of Redditor
instances. Those instances have `permissions` property:

```
>>>reddit.live(""ukaeu1ik4sw5"").contributor()
<praw.models.list.redditor.RedditorList object at 0x10382ef90>
>>> _[0]
Redditor(name='Acidtwist')
>>> _.permissions
[u'all']
```"
734,Add LiveHelper.info,2017-02-18T00:16:49Z,2017-02-19T07:47:15Z,,,,"## Feature Summary

This feature provides interface to `GET /api/live/by_id/:names` which fetches information about each live thread in `names`.

I choose name `info` (not `by_id`) because this method works similar to `Reddit.info`. 

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#GET_api_live_by_id_{names}
* nmtake@e147a37 - API design discussion"
733,Add LiveHelper.now,2017-02-15T14:33:54Z,2017-02-16T16:41:02Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/happening_now` which gets currently featured live thread.

This commit doesn't contain test for 200 OK (the endpoint returns 200 if there is a featured live thread, 204 no content if there is no featured). So the test for latter should be added in the future.

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#GET_api_live_happening_now
* nmtake@e147a37 - API design discussion"
732,Add LiveThread.report,2017-02-12T00:32:01Z,2017-02-12T05:14:53Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/:thread/report` to report the thread violating the reddit rules.

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#POST_api_live_{thread}_report
* nmtake@e147a37 - API design discussion"
731,Add usages to reddit live methods,2017-02-11T00:35:21Z,2017-02-11T23:17:17Z,,,,"## Feature Summary

Add usages to reddit live methods.  Concerns: I am a bit unsure articles (a/the/no articles) are correctly used.

## References

* #676 - feature request for reddit live"
730,Updating sidebar using SubredditModeration,2017-02-10T09:59:57Z,2017-02-10T18:00:40Z,,,,"## Issue Description

I've been trying to update the subreddit through praw using a SubredditModeration, this is the code I have. 

```
  sub = r.subreddit(""dota2test"")
  mod = sub.mod
  settings = mod.settings()
  sidebar_contents = settings['description']

  new_sidebar = ""....""
  mod.update(settings={""description"": new_sidebar})
```

After running this code I see no change in the sidebar. The account I am logging into is a mod of this subreddit.

#### Edit: Ok I got it to work but I had to call _create_or_update() directly. I'm not sure what's going on with the update() function

## System Information

PRAW Version: 4.3.1
Python Version: 2.7
Operating System: Mac OSX
"
729,Add LiveThread.discussions to get submissions linking to the live thread,2017-02-08T14:16:35Z,2017-02-08T21:48:19Z,,,,"## Feature Summary

This feature provides interface to `GET /live/:thread/discussions` to get submissions linking to the live thread.

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#GET_live_{thread}_discussions
* nmtake@e147a37 - API design discussion"
728,Spelling Fix,2017-02-08T04:55:17Z,2017-02-08T06:08:04Z,,,,
727,Add LiveThreadContributorRelationship.update_invite to update contributor invite permissions,2017-02-07T14:12:36Z,2017-02-07T15:42:29Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/:thread/set_contributor_permissions` with `type=liveupdate_contributor_invite` data which updates contributor invite permissions.

The difference between this `update_invite()` and `update()` is `type` data (`type=liveupdate_contributor_invite` and `type=liveupdate_contributor`, respectively). 
`ModeratorRelationship` [has same interfaces](https://github.com/praw-dev/praw/blob/7f4863b2a2dd8643adf5336287f04b3ad11e2c0e/praw/models/reddit/subreddit.py#L1349-L1398).

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#POST_api_live_{thread}_set_contributor_permissions
* nmtake@e147a37 - API design discussion"
726,Documentation for models should include all fields,2017-02-07T05:45:12Z,2017-02-24T04:52:43Z,Documentation,,,"The documentation for a [submission](https://praw.readthedocs.io/en/latest/code_overview/models/submission.html) seems to be missing a number of fields that are present on the object. A quick comparison between that page and a vars() call shows these missing fields.

- is_self
- link_flair_text
- selftext_html
- gilded
- url
- saved
- over_18
- suggested_sort
- visited
- downs
- author
- spoiler
- edited
- media_embed
- domain
- clicked
- secure_media_embed
- thumbnail
- contest_mode
- num_comments
- report_reasons
- archived
- secure_media
- mod_reports
- post_hint
- created
- link_flair_css_class
- approved_by
- quarantine
- hidden
- preview
- id
- hide_score
- permalink
- author_flair_css_class
- created_utc
- subreddit_id
- likes
- name
- stickied
- banned_by
- author_flair_text
- title
- ups
- subreddit
- user_reports
- selftext
- media
- locked
- distinguished
- removal_reason
- score
- comment_limit
- num_reports

A few of them, author_flair_css_class and secure_media_embed, probably aren't worth mentioning, but many of the others should at least be listed. As a developer just getting into the api, I feel frustrated having to google or guess the field names that I'm sure must exist."
725,Add LiveContributorRelationship.update to update permissions for a live thread contributor,2017-02-04T22:25:21Z,2017-02-04T23:56:51Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/:thread/set_contributor_permissions`
which updates permissions for a live thread contributor. 

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#POST_api_live_{thread}_set_contributor_permissions
* nmtake@e147a37 - API design discussion"
724,paginate using praw4,2017-01-30T09:56:52Z,2017-01-31T15:25:02Z,,,,"## Issue Description

<REMOVE>
I am trying to get data for a search term over a period of 5 days. Earlier versions of Praw used to have the  **after** parameter which made life easy. I couldn't find anything similar to this in Praw4. Please point me in the right direction.
Thanks.

<REMOVE>

## System Information

    PRAW Version:4
  Python Version:2.7
Operating System:Ubuntu 14.04
"
723,Add LiveUpdateContribution.strike to strike a content of a live thread.,2017-01-26T12:42:59Z,2017-02-03T07:07:42Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/:thread/strike_update` which strikes a content of live update ([example](https://www.reddit.com/live/xyu8kmjvfrww/updates/cb5fe532-dbee-11e6-9a91-0e6d74fabcc4)).

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#POST_api_live_{thread}_strike_update
* nmtake@e147a37 - API design discussion"
722,Add LiveThreadContribution.update to edit a live thread settings,2017-01-21T11:13:53Z,2017-01-25T16:40:29Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/thread/edit` which updates a live thread's settings.

## References

* #676 - feature request for reddit live
* https://www.reddit.com/dev/api#POST_api_live_{thread}_edit
* nmtake@e147a37 - API design discussion"
721,Update comments.rst,2017-01-20T17:53:06Z,2017-01-20T18:03:13Z,,,,"Fixes # (provide issue number of applicable)

## Feature Summary and Justification

Just fixed a typo
## References

* 
*
"
720,Add LiveContributorRelationship.accept_invite,2017-01-18T21:26:12Z,2017-01-18T21:35:34Z,,,,"## Feature Summary

This feature provides intaface to `POST /api/live/thread/accept_contributor_invite` which accepts an invite to contribute the live thread.

## References
* https://www.reddit.com/dev/api#POST_api_live_{thread}_accept_contributor_invite
* #676 - feature request for reddit live
* nmtake@e147a37 - API design discussion"
719,Add LiveUpdateContribution.remove,2017-01-17T12:31:50Z,2017-01-18T15:43:46Z,,,,"## Feature Summary

This feature provides intaface to POST /api/live/thread/delete_udpate which removes a live update.

## References
* https://www.reddit.com/dev/api#POST_api_live_{thread}_delete_update
* #676 - feature request for reddit live
* nmtake@e147a37 - API design discussion"
718,Add `LiveThreadContribution.close`,2017-01-16T13:47:25Z,2017-01-16T15:18:39Z,,,,"## Feature Summary

This feature provides intaface to POST /api/live/thread/close_thread which closes the live thread permanently.

## References

* https://www.reddit.com/dev/api#POST_api_live_{thread}_close_thread
* #676 - feature request for reddit live
* nmtake@e147a37#commitcomment-19895733 - API design discussion"
717,Add `LiveThreadContribution.add()` and `LiveThread.contrib`,2017-01-14T22:44:03Z,2017-01-15T18:38:01Z,,,,"## Feature Summary and Justification

This feature provides intaface to POST /api/live/thread/update which adds an update to the live thread.

## References

* https://www.reddit.com/dev/api#POST_api_live_%7Bthread%7D_update
* #676 - feature request for reddit live
* https://github.com/nmtake/praw/commit/e147a37#commitcomment-19895733 - API design discussion"
716,Add LiveContributorRelationship.remove(),2017-01-12T14:23:27Z,2017-01-12T15:34:25Z,,,,"## Feature Summary

This feature provides interface to `POST /api/live/thread/rm_contributor` which removes the redditor from the live thread contributors.

## References

* https://github.com/praw-dev/praw/issues/676
* https://www.reddit.com/dev/api#POST_api_live_{thread}_rm_contributor"
715,other_submissions added to (un)hide functions,2017-01-12T03:55:33Z,2017-01-13T03:59:09Z,,,,"Included ""other_submissions"" variable and multiple options
to the (un)hide functions.

Fixes # 700

## Feature Summary and Justification

This feature provides an optional variable to `submission.hide()` and `submission.unhide()` to allow for multiple ids to be passed at once and hidden in a single call."
714,Change some LiveContributorRelationship related API_PATH,2017-01-10T13:54:39Z,2017-01-10T15:04:06Z,,,,"Fixes https://github.com/praw-dev/praw/pull/713#discussion_r95134651

`LiveContributorRelationship.invite` and `LiveContributorRelationship.remove_invite` now hit endpoints, which starts with 'api/', for consistency."
713,Add LiveContributorRelationship.leave(),2017-01-08T10:05:37Z,2017-01-09T16:53:52Z,,,,"## Feature Summary

This feature provides interface to abdicate live thread contributor position.

## References

* #676
* https://www.reddit.com/dev/api#POST_api_live_{thread}_leave_contributor"
712,Add invite() and remove_invite() to LiveContributorRelationship,2017-01-07T03:01:39Z,2017-01-07T18:48:14Z,,,,"## Feature Summary and Justification

This feature provides interfaces to send/remove live thread contributor invitation.

## References

* #676
* https://www.reddit.com/dev/api/#POST_api_live_{thread}_invite_contributor
* https://www.reddit.com/dev/api/#POST_api_live_{thread}_rm_contributor_invite

"
711,Fix LiveContributorRelationship callable return type,2017-01-05T16:10:07Z,2017-01-05T17:19:43Z,,,,"Fixes #710 

``live_thread.contributor()`` now returns `RedditorList` correctly. Information about pending invitations, which is included when caller have 'manage' permission,  are dropped."
710,LiveContributorRelationship.__call__() returns a list of RedditorList if caller have 'manage' permission,2017-01-04T16:37:54Z,2017-01-05T17:19:43Z,Bug,,,"## Issue Description

As the title said. To reproduce:

```
>>> import praw
>>> reddit = praw.Reddit(...)
>>> thread = reddit.live(...)  # thread I created 
>>> thread.contributor()
[<praw.models.list.redditor.RedditorList object at 0x101865d50>, <praw.models.list.redditor.RedditorList object at 0x100be6f90>]
```

JSON (copied from browser):

```
[
    {
        ""kind"": ""UserList"",
        ""data"": {
            ""children"": [
                {
                    ""permissions"": [
                        ""all""
                    ],
                    ""id"": ""t2_ll32z"",
                    ""name"": ""nmtake""
                }
            ]
        }
    },
    {
        ""kind"": ""UserList"",
        ""data"": {
            ""children"": [ ]
        }
    }
]
```

It seems that latter UserList reprsents invited users who didn't accept invitation yet: ([source](https://github.com/reddit/reddit-plugin-liveupdate/blob/a7cde1cf917c4118c4cb114c73a92f272e439816/reddit_liveupdate/controllers.py#L515-L540))

## System Information

    PRAW Version:  50516a
    Python Version: 2.7.13
    Operating System: OS X 10.11.6"
709,subreddit object has no method moderators(),2017-01-04T07:21:35Z,2017-01-05T05:46:16Z,,,,"## Issue Description

In the documentation [here](https://praw.readthedocs.io/en/praw4/code_overview/other/moderatorrelationship.html), I try 

    for moderator in reddit.subreddit('redditdev').moderators():
        print(moderator)

However, there doesn't appear to be any `moderators` attribute or method here, despite the description:

https://github.com/praw-dev/praw/blob/master/praw/models/reddit/subreddit.py#L1198

## System Information

    PRAW Version: 4.10
  Python Version: 2.7.12
Operating System: Ubuntu 16.0.4
"
708,Submission with automatic title,2017-01-02T19:15:44Z,2017-01-02T19:31:58Z,,,,"## Issue Description

I'm trying to submit a submission to subreddit, but I'd like Reddit to handle `title` assignment. According to the documentation it should be possible to simply invoke `subreddit.submit(url=<URL>)`, but it seems to not work and all I get is the following error

```
ipdb> sr.submit(url=link)
*** TypeError: submit() takes at least 2 arguments (2 given)
```

Is it possible in the API to send a submission without a title and let Reddit handle the title's assignment? 

## System Information

    PRAW Version: praw==4.1.0
  Python Version: 2.7.12
Operating System: Ubuntu 16.04
"
707,Getting Error 500 when trying to select flair,2016-12-31T14:07:03Z,2016-12-31T16:31:10Z,,AssertionError,"AssertionError: Unexpected status code: 500","## Issue Description

I can't seem to change submission flair using OAuth and PRAW 4.1.0. I have the following OAuth scopes: read, submit, flair, modposts, modflair. The post is created successfully but I get an exception thrown when trying to change the flair. This is the code I'm using to try to set the flair:

```
        r = reddit_login()
        post = r.subreddit(sr).submit(title, selftext=text)
        post.flair.select('announcement', 'announcement')
```

Here is the stack trace:

```
Unexpected error:
Traceback (most recent call last):
  File ""./shipit.py"", line 696, in <module>
    build_snapshot(version)
  File ""./shipit.py"", line 293, in build_snapshot
    post_snapshot_thread(version)
  File ""./shipit.py"", line 610, in post_snapshot_thread
    post.flair.select('announcement', 'announcement')
  File ""/usr/local/lib/python2.7/site-packages/praw/models/reddit/submission.py"", line 168, in select
    return self.submission._reddit.post(url, data=data)
  File ""/usr/local/lib/python2.7/site-packages/praw/reddit.py"", line 365, in post
    params=params)
  File ""/usr/local/lib/python2.7/site-packages/praw/reddit.py"", line 406, in request
    params=params)
  File ""/usr/local/lib/python2.7/site-packages/prawcore/sessions.py"", line 131, in request
    params=params,  url=url)
  File ""/usr/local/lib/python2.7/site-packages/prawcore/sessions.py"", line 86,in _request_with_retries
    'Unexpected status code: {}'.format(response.status_code)
AssertionError: Unexpected status code: 500
```

## System Information

    PRAW Version: 4.1.0
  Python Version: 2.7.13
Operating System: Mac OS 10.11
"
706,Method to return the Subreddits a user is a Moderator of.,2016-12-29T21:10:41Z,2016-12-29T21:19:05Z,,,,"## Issue Description

I see that I can get a list of moderators from a given subreddit with this template:

```
subr= rd.subreddit('redditdev')
mods = [mod.name for mod in subr.moderator]
```
But there is no method in the PRAW documentation under the Redditor class where one can get the list of subreddits that a particular redditor is a moderator of.

If I missed it elsewhere or there is another way to get this info inside PRAW4, please let me know.
Thanks.

## System Information

    PRAW Version: 4.1
    Python Version: 3.6
    Operating System: Win 10
"
705,Add LiveContributorRelationship to interact live thread's contributors,2016-12-29T17:48:45Z,2016-12-29T22:37:58Z,,,,"## Feature Summary and Justification

This feature provides interface for live thread's contributors. Usage:

```
thread = reddit.live('ukaeu1ik4sw5')
rel = thread.contributor  # LiveContributorRelationship
redditors = rel()  # RedditorList
```

LiveContributorRedditorList returns RedditorList rather than ListingGernarator because /live/:thread_id/contributors doesn't implement listing interface, e.g., https://www.reddit.com/live/ta535s1hq2je/contributors.json

## References

* #676 
* https://www.reddit.com/dev/api#GET_live_{thread}_contributors

## Concerns

I got warning when I ran `pre_push.py`

```
Warning, treated as error:
docstring of praw.models.LiveThread.contributor:1: WARNING: py:class reference target notfound: LiveContributorRelationship
```

How can I resolve this error?"
704,Add Subreddit.rules to get subreddit rules,2016-12-29T01:43:58Z,2016-12-29T06:34:51Z,,,,Fixes #699.
703,Add support for new modmail,2016-12-29T01:14:12Z,2017-04-07T15:35:51Z,Feature,,,"The API for the new modmail [is now documented](https://www.reddit.com/dev/api/#section_modmail), so that's a feature that would be pretty nice to have."
702,Turning script into exe using PyInstaller does not work,2016-12-29T00:50:13Z,2016-12-29T04:16:01Z,,AttributeError,"AttributeError: '_NotSet' object has no attribute 'lower'","## Issue Description

I'm trying to turn a script I made into an exe using PyInstaller (Which works running the .py) But I get the following error running the .exe.

Traceback (most recent call last):
  File ""myprogram.py"", line 14, in <module>
  File ""site-packages\praw\reddit.py"", line 105, in __init__
  File ""site-packages\praw\config.py"", line 70, in __init__
  File ""site-packages\praw\config.py"", line 96, in _initialize_attributes
  File ""site-packages\praw\config.py"", line 30, in _config_boolean
AttributeError: '_NotSet' object has no attribute 'lower'
Failed to execute script myprogram

I'm new to python, and don't know why this is happening

## System Information

    PRAW Version: newest
  Python Version: 3.5
Operating System: W10
"
701,Where did json_dict go?,2016-12-28T12:34:26Z,2016-12-28T21:33:59Z,,,,"I was wondering where the property `json_dict` of Reddit objects went? I can't seem to find any other way of retrieving the original JSON object returned by Reddit.

I've found many references to the property in past on [this repo](https://github.com/praw-dev/praw/search?q=json_dict&type=Issues&utf8=%E2%9C%93) and [StackOverflow](http://stackoverflow.com/a/24749142/3565450) but not what it was renamed to or why it was removed.

## System Information

PRAW Version: 4.1.0
Python Version: 2.7
Operating System: Windows 10 (unfortunately)
"
700,Add `other_submissions` parameter to `hide` and `unhide`,2016-12-28T07:33:55Z,2017-01-13T03:59:36Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"The `hide` and `unhide` endpoints support a comma-separated list of submissions to (un)hide. This support was added in PRAW3.5 after PRAW4 work began.

To support batch (un)hiding, the existing methods should take a `other_submissions` parameter, similar to how `subscribe` has an `other_subreddits` parameter:

https://github.com/praw-dev/praw/commit/4522ddfa6f550e31b9e46ba60cf4855a756fdcd6"
699,Add `Subreddit.rules`,2016-12-28T07:26:29Z,2016-12-29T06:34:51Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"See https://github.com/praw-dev/praw/pull/612 for the PRAW 3.6 addition.

@leviroth any chance you want to re-add this for PRAW4?"
698,Make CommentForest iterable,2016-12-28T04:47:05Z,2016-12-28T05:23:57Z,,,,"I'm wondering if it would be a good idea to add an `__iter__` method to `CommentForest`. This would probably deprecate the `list` method in favor of the Python built-in function of the same name, and in fact it could (if I'm not mistaken) work about the same as that method, except that it would yield comments as they're popped from the queue instead of returning a whole list."
697,(Some of) Submission moderation broken,2016-12-23T06:57:05Z,2016-12-23T15:28:04Z,,,,"## Issue Description

    print(submission.mod.sticky) # in the SubmissionModeration class
    print(submission.mod.remove) # NOT in the SubmissionModeration class

It's my understanding that overriding `ThingModerationMixin` (https://github.com/praw-dev/praw/blob/master/praw/models/reddit/mixins/__init__.py) in `SubmissionModeration` (https://github.com/praw-dev/praw/blob/master/praw/models/reddit/submission.py) seems to be broken, as the code above prints the code below:

    <bound method SubmissionModeration.sticky of <praw.models.reddit.submission.SubmissionModeration object at 0x000002506864B080>>
    'SubmissionModeration' object has no attribute 'remove'


---

If it helps, to get the submission I do

    submission = r.submission(id=thread_id)
"
696,Fix two small typos,2016-12-21T20:58:53Z,2016-12-22T00:51:31Z,,,,Fixes two small typos that I came across.
695,Make SubredditFlair instances callable,2016-12-21T20:42:52Z,2016-12-22T00:27:57Z,,,,"This makes SubredditFlair work like SubredditRelationship. Includes redditor
argument. Deprecates SubredditFlair.\_\_iter\_\_.

Fixes #693"
694,Fixes a couple docstring typos,2016-12-21T17:18:38Z,2016-12-21T18:01:17Z,,,,
693,Make Subreddit.flair callable,2016-12-19T16:21:01Z,2016-12-22T00:27:57Z,,,,"SubredditFlair instances should be callable so that single Redditors can be passed in, as well as other parameters such as limit.

This change should be similar to: https://github.com/praw-dev/praw/commit/483be01730ed3b37e2dae3fd8fb7bb137729cb10

See: https://www.reddit.com/r/redditdev/comments/5j5ypc/praw4_attributeerror_subreddit_object_has_no/dbdufv1/?context=3"
692,Add praw.models.LiveUpdate,2016-12-18T16:22:21Z,2016-12-19T17:23:18Z,,,,"## Feature Summary and Justification

This feature provides `praw.models.LiveUpdate` and `praw.models.LiveThread.updates`, ~~which is dependency of `LiveUpdate._fetch()`~~.

## Concerns

~~* LiveUpdate._fetch() is really slow due to the lack of proper API like /api/info.~~
* updated: Now accesing missing attributes throws AttributeError."
691,Fix typo in AUTHORS.rst,2016-12-17T04:04:40Z,2016-12-17T04:30:33Z,,,,I left out a bracket.
690,Add sticky parameter to SubredditModeration.distinguish,2016-12-16T21:39:23Z,2016-12-17T03:58:16Z,,,,"## Feature Summary and Justification

This provides a `sticky` parameter for `SubredditModeration.distinguish()`. This allows comments to be stickied.

If the user tries to sticky a submission or a non-root comment, the parameter is ignored and the target is just distinguished. This matches the behavior of PRAW 3."
689,Properly update subreddit exclude_banned_modqueue setting,2016-12-13T07:41:31Z,2016-12-13T15:27:09Z,,,,"## Feature Summary and Justification

The `/api/site_admin` endpoint expects `exclude_banned_modqueue` to be specified while the current PRAW code sends `exclude_modqueue_banned` instead, causing the setting to always be set to false. This fixes that.

## References

* https://www.reddit.com/dev/api/#POST_api_site_admin

"
688,Use before_record hook to filter out acceess tokens from cassettes (fixes #682),2016-12-11T17:07:23Z,2016-12-12T06:28:58Z,,,,"Fixes reverted PR #682 

## Feature Summary and Justification

This feature filters out access tokens from betamax cassettes. `before_record` hook is added in betamax 0.7.0. 

I added cassettes failed to create in #682, and some fixes (`prawtest_*` envvars are missing in test environment which is invoked from tox, ~~and pylint compliance~~ (edit: alrady solved in https://github.com/praw-dev/praw/commit/ec9b648a3668a06428f65646d7fe61408268b191).

## References

* #682 
* http://betamax.readthedocs.io/en/latest/configuring.html#filtering-sensitive-data
* https://github.com/praw-dev/praw/pull/680#discussion_r90765774
* http://tox.readthedocs.io/en/latest/example/basic.html#passing-down-environment-variables"
687,Update dependencies in tox.ini,2016-12-11T03:01:03Z,2016-12-11T07:30:06Z,,,,"## Feature Summary and Justification

This PR resolves VersionConflict Error (caused by mismatch of dependency section between setup.py and tox.ini) which is thrown when running tox."
686,Properly Locate Submission/Comment moderation functions,2016-12-10T03:42:36Z,2016-12-18T21:56:04Z,,,,"This involves deprecating existing `subreddit.mod` functions as they don't really belong there.

See:

* https://www.reddit.com/r/redditdev/comments/5h2u5t/how_do_i_sticky_and_distinguish_comments_with/
* https://www.reddit.com/r/redditdev/comments/5h2r1c/why_are_submission_moderation_tools_split_between/"
685,"Update authentication.rst, include quotes",2016-12-07T06:51:37Z,2016-12-07T06:57:17Z,,,,There was no issue (number) for this. Only includes documentation.
684,Comments from mentions cannot be refreshed,2016-12-04T20:35:56Z,2016-12-16T08:05:23Z,,AttributeError,"AttributeError: 'Comment' object has no attribute 'submission'","## Issue Description

```python
Traceback (most recent call last):
  File ""reversegif.py"", line 199, in <module>
message.refresh()
  File ""/usr/local/lib/python3.5/dist-packages/praw/models/reddit/comment.py"", line 93, in refresh
comment_path = self.submission._info_path() + '_/{}'.format(self.id)
  File ""/usr/local/lib/python3.5/dist-packages/praw/models/reddit/base.py"", line 34, in __getattr__
.format(self.__class__.__name__, attribute))
AttributeError: 'Comment' object has no attribute 'submission'
```

https://www.reddit.com/r/redditdev/comments/5ggfz7/praw4_getting_the_submission_a_comment_is_from/das7edu/?context=3

This also just affects comments in the inbox in general."
683,"Revert ""Use before_record hook to filter out acceess tokens from cassettes.""",2016-12-04T19:44:27Z,2016-12-04T19:56:09Z,,,,"Reverts praw-dev/praw#682

The PR for some reason prevents tests from successfully generating cassettes that include `DELETE` requests. Let's re-add once we have that figured out."
682,Use before_record hook to filter out acceess tokens from cassettes.,2016-12-04T02:03:13Z,2016-12-04T04:47:27Z,,,,"## Feature Summary and Justification

This feature filters out access tokens from betamax cassettes. `before_record` hook is added in betamax 0.7.0.

## References

* http://betamax.readthedocs.io/en/latest/configuring.html#filtering-sensitive-data
* https://github.com/praw-dev/praw/pull/680#discussion_r90765774"
681,"""invalid css"" when using SubredditStylesheet.update with minified CSS",2016-12-03T07:50:23Z,2016-12-13T15:30:53Z,,,,"## Issue Description

I've been trying to create a script that can automatically update the subreddit CSS after minifying it but it seems that it thinks minified CSS is invalid and won't upload it. I'm not sure what the issue is

```python
sub = reddit.subreddit('dota2')
stylesheet = dota.stylesheet

with open('stylesheet.css.mini', 'r') as m:
	newSheet = m.read()
	stylesheet.update(newSheet)
```

## System Information

    PRAW Version: 4.0.0
  Python Version: 2.7
Operating System: Windows
"
680,Add praw.models.LiveHelper.__call__,2016-12-03T05:35:35Z,2016-12-07T15:12:17Z,,,,"## Feature Summary and Justification

This feature provides explicit interface to initialize a `praw.models.LiveThread` instance, e.g.,

```python
>>> import praw
>>> from  praw.models import LiveThread
>>> reddit = praw.Reddit(...)
>>> thread = LiveThread(reddit, 'ukaeu1ik4sw5')
>>> thread.title
u'reddit updates'
```

## References

* nmtake/praw@e147a37

## Concerns

Access tokens are visible in the cassettes. I think it would be better if we could filter out those tokens with `before_record` hook.  Any thoughts?"
679,Add search_by_topic for subreddits,2016-11-29T10:48:22Z,2016-11-29T16:58:01Z,,,,"## Feature Summary and Justification

This feature provides ability to search matching subreddits by topic ([source code via reddit api](https://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py#L4730)). It's a fairly simple and straightforward query and implementation. I believe it matches well with the spirit / use cases of the library.

## References

* [Feature Request discussion found on r/redditdev](https://www.reddit.com/r/redditdev/comments/46s39o/using_praw_to_find_subreddits_related_to_a_query/)
* [Reddit API Description for subreddits_by_topic query](https://www.reddit.com/dev/api/#GET_api_subreddits_by_topic)

## Issues

I ran into trouble attempting to record my query with BetaMax. I would appreciate pointers on this, as I could not easily find how to add my local authentication via OS environ variables or a particular config file. I am receiving a 401 error for my added tests from prawcore.auth.TrustedAuthenticator. Do let me know if you can supply instructions on how to set the local authentication and I'm happy to add and update my request. Thank you!

"
678,Change Auth.authorize method to set Authorizer on Reddit,2016-11-21T02:14:19Z,2016-11-21T05:00:20Z,,,,"Currently, the `Auth.authorize` method sets the Authorizer on the Auth instance, not the Reddit instance. This prevents Reddit.read_only from being set to False, and does not initialize a session on the Reddit instance."
677,400 Bad Request on access token refresh,2016-11-18T15:19:33Z,2016-11-18T15:22:46Z,,"requests.exceptions.HTTPError, praw.errors.HTTPException","requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://api.reddit.com/api/v1/access_token/, praw.errors.HTTPException: HTTP error","## Issue Description
 If I try to refresh the access token using:

```
reddit_auth = praw.Reddit(user_agent=""Twitter X-Poster by l3d00m"")
reddit_auth.set_oauth_app_info(client_id=""eoAG6V7plEDeAA"", client_secret=api_keys.reddit_client_secret,
                               redirect_uri=""http://127.0.0.1"")
reddit_auth.refresh_access_information(api_keys.reddit_client_refresh)
````

I get an 400 Bad Request. Stacktrace:

```
Traceback (most recent call last):
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\internal.py"", line 213, in _raise_response_exceptions
    response.raise_for_status()  # These should all be directly mapped
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\requests\models.py"", line 884, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://api.reddit.com/api/v1/access_token/

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/git/pietsmiet_android/scripts/lol.py"", line 52, in <module>
    submit_to_reddit(""lol.de"")
  File ""C:/git/pietsmiet_android/scripts/lol.py"", line 48, in submit_to_reddit
    reddit_auth.refresh_access_information(api_keys.reddit_client_refresh)
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\__init__.py"", line 1476, in refresh_access_information
    refresh_token=refresh_token or self.refresh_token)
  File ""<decorator-gen-11>"", line 2, in refresh_access_information
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\decorators.py"", line 288, in require_oauth
    return function(*args, **kwargs)
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\__init__.py"", line 733, in refresh_access_information
    retval = self._handle_oauth_request(data)
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\__init__.py"", line 657, in _handle_oauth_request
    response = self._request(url, auth=auth, data=data, raw_response=True)
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\__init__.py"", line 452, in _request
    _raise_response_exceptions(response)
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\praw\internal.py"", line 215, in _raise_response_exceptions
    raise HTTPException(_raw=exc.response)
praw.errors.HTTPException: HTTP error
```

The same code worked a month ago like a charm, I have no idea why this is happening. (I only generated a new access token using [this method](https://github.com/l3d00m/TwitterToReddit/blob/master/README.md)). Thanks for your help!

## System Information

    PRAW Version: 3.6.0 (and 3.4.0)
    Python Version: 3.5.2
    Operating System: Windows 10"
676,Wishlist: Implementing reddit live access,2016-11-16T18:33:55Z,2017-02-19T21:42:11Z,Feature,,,"Hi,

it would be nice to implement a way to access content from a live thread between, e.g. all content created between two timestamps.
Getting a list of updates for live threads is documented [here](https://www.reddit.com/dev/api#GET_live_{thread}).

"
675,closed,2016-11-05T12:05:41Z,2016-11-05T12:12:33Z,,,,
674,Reddit.Subreddit.Submissions is not working,2016-11-04T05:36:05Z,2016-11-04T06:00:57Z,,,,"## Issue Description

I am unable to get Reddit.Subreddit.Submissions() to work properly

Example code:
```
import praw, time

r = praw.Reddit(credentials)
now = time.time()
t24 = now - 86400

for s in r.subreddit(""pics"").submissions(t24, now):
    print(s.title)

```
No submissions are returned. Everything functions properly if I try something like `r.subreddit(""pics"").hot()`. Perhaps I'm mistaken in assuming it acts exactly like `praw.helper.submissions_between()` ?
## System Information

PRAW Version: praw-4.0.0b21
 Python Version: 3.5.2
Operating System: OS X
"
673,PRAW4: Duplicate entries from subreddit().stream.submissions(),2016-10-18T13:10:39Z,2016-12-13T05:35:32Z,,,,"## Issue Description

When using `subreddit('all').stream.submissions()` to iterate over and wait for new submissions, I get a lot of duplicates. Sometimes after only a few seconds (<10), but always after a few minutes.

Test code:

```
#!/usr/bin/env python3

import settings

import praw
import prawcore


r = praw.Reddit(**settings.api_credentials)
submissions = r.subreddit('all').stream.submissions()

ids = set()
cnt = 0
duplicates = 0

try:
    while True:
        try: 
            for s in submissions:
                print(s.id)

                cnt += 1
                if s.id in ids:
                    duplicates += 1
                    continue
                ids.add(s.id)

        except prawcore.exceptions.RequestException:
            print()
            print('RequestException, retrying...')
            print()

except KeyboardInterrupt:
    pass

print()
print(""Got {} submissions ({} duplicate ids)."".format(cnt, duplicates))
```

Sample output is available here: http://pastebin.com/yZUAMJN1
I've tried to insert newlines whenever there was a short pause. The last block in particular contains a lot of duplicates.

Note: The try/catch block for `RequestException` is for a different problem. It occurs usually after running the above code for 70-90 minutes. I'm still investigating and might open another issue for this later.
## System Information

   PRAW Version: 4.0.0b20
  Python Version: 3.5.1
Operating System: Fedora 24
"
672,praw API downloads a limited number of images from reddit,2016-10-17T20:29:06Z,2016-10-18T05:21:21Z,,,,"I am using PRAW API for downloading images and their related comments from the submissions that have images. Can you please point as why I get very limited number of images from the following code? I know I have lots of limitations posed here but I was expecting lots of images to get downloaded. If this is something opposed the reddit, I wonder if you would suggest other methods for downloading reddit images from these categories with their comments?

```
#written by ""Mona Jalal""

import datetime
import praw
import re
import urllib2
import requests
from bs4 import BeautifulSoup
import sys
import os
from os import path
import socket


def retrieve_comments(submission, category):
    sub_file_name = 'thirty_comments/'+category+'/'+str(submission.id) + '.txt'
    submission.replace_more_comments(limit=None, threshold=0)
    all_comments = praw.helpers.flatten_tree(submission.comments)
    if len(all_comments) >= 30:
        with open(sub_file_name, 'wb') as sub_file:
            top_comments = sorted(all_comments, key=lambda comment: -comment.score)
            for comment in top_comments[:30]:
                sub_file.write(comment.body.encode('utf-8')+'\n')

r = praw.Reddit(user_agent = ""download all images from a subreddit"",
        user_site = ""lamiastella"")
already_done = []
check_words = ['jpg']
mscoco_categories = ['cars', 'motorcycles', 'airplanes', 'bus', 'trains', 'trucks', 
                     'traffic', 'light', 'fire', 'parking', 'bench', 'bird', 'birdpics', 
                     'cats', 'cat', 'dog', 'dogs','horse', 'horses', 'sheep', 'cows', 'elephants', 'bears',
                     'giraffes', 'umbrella', 'frisbee', 'ultimate', 'sports', 'baseball',
                     'tennis', 'wine', 'banana', 'sandwiches', 'sandwich', 'orange', 
                     'broccoli', 'hotdog', 'hotdogs', 'pizza', 'cake', 'cakes', 'chairs',
                     'plants', 'whatsthisplant', 'toilet', 'toilets', 
                     'laptop', 'laptops', 'keyboard', 'keyboards', 'microwave', 'sinks', 
                     'sink', 'refrigerator', 'clock', 'clocks', 'vase', 'hair']
#mscoco_categories = ['glass'] #for testing


for category in mscoco_categories:
    if not os.path.exists(os.getcwd()+'/thirty_comments_images/'+category):
        os.makedirs(os.getcwd()+'/thirty_comments_images/'+category)
    if not os.path.exists(os.getcwd()+'/thirty_comments/'+category):
        os.makedirs(os.getcwd()+'/thirty_comments/'+category) 
    print('{0}: {1}'.format(""category is"", category))
    subreddit = r.get_subreddit(category)
    print('{0}: {1}'.format(""subreddit is"", subreddit))
    for submission in subreddit.get_controversial_from_all(limit=None, comment_sort = 'top'):
    #for submission in subreddit.get_top_from_year(limit = None, comment_sort='top'):
        #for submission in subreddit.get_top_from_all(limit=10000):
    #for submission in subreddit.get_hot(limit=10000):
        print('{0}:{1}:{2}'.format('number of comments is', submission.num_comments, submission.id))
    is_image = any(string in submission.url for string in check_words)
    print '[LOG] Checking url:  ' + submission.url
    if submission.id not in already_done and is_image and submission.num_comments >= 30 and submission.num_comments < 200:
        if not submission.url.startswith(""http://i.imgur.com/PcXoIvy.jpg"") and submission.url.endswith('/') and not (('</a>' or 'PcXoIvy') in submission.url):
            modified_url = submission.url[:len(submission.url)-1]
        try:
            image_file = urllib2.urlopen(modified_url, timeout = 5)
            image_file_name = '/home/mona/computer_vision/reddit/thirty_comments_images/'+category+'/'+str(submission.id)+'_'+  modified_url[-4:]
            output_image = open(image_file_name, 'wb') 
            retrieve_comments(submission, category)
            output_image.write(image_file.read())
            print '[LOG] Done Getting ' + submission.url
            print('{0}: {1}'.format('submission id is', submission.id))
            output_image.close()
        except urllib2.URLError as e:
            print(""An URL error has happened!"")
            print(e)
            continue
        except socket.timeout as e:
            print(""Socket timed out!"")
            print(e)
            continue
        except socket.error as e:
            print(""Socket error has happened!"")
            print(e)
            continue
                already_done.append(submission.id)
            elif not submission.url.startswith(""http://i.imgur.com/PcXoIvy.jpg""):
        try:
            image_file = urllib2.urlopen(submission.url, timeout = 5)
                image_file_name = '/home/mona/computer_vision/reddit/thirty_comments_images/'+category+'/'+ str(submission.id)+'_' + submission.url[-4:]
            output_image = open(image_file_name, 'wb')
            retrieve_comments(submission, category)
            output_image.write(image_file.read())
            print '[LOG] Done Getting ' + submission.url    
            print('{0}: {1}'.format('submission id is', submission.id))
            output_image.close()
        except urllib2.URLError as e:
            print(""An URL error has happened!"")
            print(e)
            continue
            except socket.timeout as e:
            print(""Socket timed out!"")
            print(e)
            continue
        except socket.error as e:
            print(""Socket error has happened!"")
            print(e)
            continue
        already_done.append(submission.id)
        elif 'imgur.com' in submission.url and not ('gif' or 'webm' or 'mp4' or 'all' or '#' or '/a/' or 'png' or 'jpeg' or '</a>' in submission.url):
            html_source = requests.get(submission.url).text # download the image's page
        soup = BeautifulSoup(html_source, ""lxml"")
        image_url = soup.select('img')[0]['src']
        if image_url.startswith('//'):
            image_url = 'http:' + image_url
            image_id = image_url[image_url.rfind('/') + 1:image_url.rfind('.')]
            try:
                image_file = urllib2.urlopen(image_url, timeout = 5)
            image_file_name = '/home/mona/computer_vision/reddit/thirty_comments_images/'+category+'/'+ str(submission.id)+'_' + image_url[-4:]
            output_image = open(image_file_name, 'wb') 
            retrieve_comments(submission, category)
            output_image.write(image_file.read())
                print '[LOG] Done Getting ' + submission.uri
                print('{0}: {1}'.format('submission id is', submission.id))
            output_image.close()
            except urllib2.URLError as e:
            print(""An URL error has happened!"")
            print(e)
            continue
            except socket.timeout as e:
                print(""Socket timed out!"")
            print(e)
            continue
            except socket.error as e:
            print(""Socker error has happened!"")
            print(e)
            continue
        else:
            continue
paths = []
for cat in mscoco_categories:
    paths.append(os.getcwd()+'/thirty_comments_images/'+cat)
outfile = open('num_files_in_categories_thirty_comments.txt', 'wb')
for path in paths:
    outfile.write(path+"" has ""+str(len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]))+"" images"")
```

After the code is ran, I get ridiculously low number of comments/images downloaded (the other categories had less that 4 items in them so I omitted them):

```
mona@pascal:~/computer_vision/reddit/thirty_comments$ find . -type d -print0 | while read -d '' -r dir; do files=(""$dir""/*); printf ""%5d files in directory %s\n"" ""${#files[@]}"" ""$dir""; done
    7 files in directory .
    4 files in directory ./wine
   10 files in directory ./baseball
   13 files in directory ./cats
   21 files in directory ./cars
   30 files in directory ./motorcycles
   11 files in directory ./pizza
   23 files in directory ./trucks
```

Link to my SO here if you prefer to answer over there:
http://stackoverflow.com/questions/40095012/praw-api-downloads-a-limited-number-of-images-from-reddit
"
671,Added project chattR to useful_scripts.rst,2016-09-30T19:53:55Z,2016-10-04T00:33:35Z,,,,"Fixes # (provide issue number of applicable)
## Feature Summary and Justification

This feature provides ...
## References
- 
  *
"
670,Pin test dep versions for tox,2016-09-06T23:15:35Z,2016-09-07T04:07:55Z,,,,"## Feature Summary and Justification

This patch fixes an error when running tox against the project. If these versions are not pinned, then tox encounters errors when creating the virtualenv for tests.
"
669,New content for get started doc,2016-09-02T01:28:26Z,2016-09-04T01:42:08Z,,,,"Add prerequisites and the most common tasks.
"
668,PRAW4 - 403 on login with LMGTFY Bot Example,2016-09-01T16:42:34Z,2016-09-24T22:23:20Z,,prawcore.exceptions.Forbidden,prawcore.exceptions.Forbidden: received 403 HTTP response,"## Issue Description

When I attempt to run the LMGTFY example bot from [the docs](https://praw.readthedocs.io/en/praw4/pages/reply_bot.html), I'm getting a 403 Forbidden.
## Stack Trace

```
Traceback (most recent call last):
  File ""test.py"", line 37, in <module>
    main()
  File ""test.py"", line 15, in main
    for submission in subreddit.stream.submissions():
  File ""/usr/local/lib/python2.7/dist-packages/praw/models/util.py"", line 40, in stream_generator
    limit=limit, params={'before': before_fullname}))):
  File ""/usr/local/lib/python2.7/dist-packages/praw/models/listing/generator.py"", line 70, in next
    return self.__next__()
  File ""/usr/local/lib/python2.7/dist-packages/praw/models/listing/generator.py"", line 43, in __next__
    self._next_batch()
  File ""/usr/local/lib/python2.7/dist-packages/praw/models/listing/generator.py"", line 53, in _next_batch
    self._listing = self._reddit.get(self.url, params=self.params)
  File ""/usr/local/lib/python2.7/dist-packages/praw/reddit.py"", line 207, in get
    data = self.request('GET', path, params=params)
  File ""/usr/local/lib/python2.7/dist-packages/praw/reddit.py"", line 258, in request
    return self._core.request(method, path, params=params, data=data)
  File ""/usr/local/lib/python2.7/dist-packages/prawcore/sessions.py"", line 119, in request
    params)
  File ""/usr/local/lib/python2.7/dist-packages/prawcore/sessions.py"", line 73, in _request_with_retries
    raise self.STATUS_EXCEPTIONS[response.status_code](response)
prawcore.exceptions.Forbidden: received 403 HTTP response
```
## System Information

```
PRAW Version: praw-4.0.0b17
Python Version: 2.7.12
Operating System:  Linux Mint 18 Cinnamon (Ubuntu 16.04 LTS)
```
## Code

```
from urllib import quote_plus

import praw

QUESTIONS = ['what is', 'who is', 'what are']
REPLY_TEMPLATE = '[Let me google that for you](http://lmgtfy.com/?q={})'


def main():
    reddit = praw.Reddit(user_agent='This is a thing that gives you stuff by /u/Zetaphor',
                         client_id='My Client ID', client_secret=""My Client Secret"",
                         username='My Username', password='My Password')

    subreddit = reddit.subreddit('AskReddit')
    for submission in subreddit.stream.submissions():
        process_submission(submission)


def process_submission(submission):
    # Ignore titles with more than 10 words as they probably are not simple
    # questions.
    if len(submission.title.split()) > 10:
        return

    normalized_title = submission.title.lower()
    for question_phrase in QUESTIONS:
        if question_phrase in normalized_title:
            url_title = quote_plus(submission.title)
            reply_text = REPLY_TEMPLATE.format(url_title)
            print('Replying to: {}'.format(submission.title))
            submission.reply(reply_text)
            # A reply has been made so do not attempt to match other phrases.
            break


if __name__ == '__main__':
    main()
```
"
667,Update AUTHORS.rst,2016-08-30T15:04:33Z,2016-08-30T15:22:38Z,,,,"Added name to documentation section.
"
666,Add Submission lock and unlock.,2016-08-30T05:47:19Z,2016-08-30T05:53:32Z,,,,
665,Extra praw.ini config values are now available under Config.custom.,2016-08-27T22:14:30Z,2016-08-27T22:16:42Z,,,,
664,setup.py: require update_checker >=0.11,2016-08-25T21:35:18Z,2016-08-25T21:59:47Z,,,,"update_checker-0.12 was released in July, which should also be supported
"
663,make update_checker an optional dependency,2016-08-25T18:23:45Z,2016-08-25T23:48:03Z,,,,"Address #610 and the comments
for commit 209b6e506b3c3b0e33d697542d114f6bb51a5e5b.

Also move update_checker module to extras_require
"
662,Use simple Authorizer when refresh_token is provided in some cases.,2016-08-24T06:21:18Z,2016-08-24T06:33:22Z,,,,"Fixes #658
"
661,Add subreddit.flair.templates.,2016-08-23T06:35:00Z,2016-08-23T06:37:55Z,,,,
660,Add tests 3,2016-08-21T20:28:58Z,2016-09-07T04:26:09Z,,,,"## Feature Summary and Justification

This feature provides various tests and a proper fix to the auto refresh test. Additionally, some convenience methods / classes / another module is added for
- being able to avoid betamax caching too harshly by adding a dummy header value and matching against it sequentially
- tests that need the new tokens because the client ids, secrets, and refresh uris unfortunately have to match or reddit will abort the request
- Mocking a response

respectively. Sorry about being late and this being a tad longer than the previous ones.
"
659,praw4.0.0b14 released as stable on PyPI,2016-08-20T22:50:23Z,2016-08-21T00:14:50Z,,,,"It seems an incomplete/beta version of praw was accidentally published/released on PyPI. This means that people who install or upgrade praw using `pip install praw` will get that version, which doesn't seem like the right thing to do at the moment.
"
658,Reddit._core._authorizer is set to an prawcore.auth.ScriptAuthorizer instance when password is not given,2016-08-20T13:13:22Z,2016-08-24T06:33:22Z,,,,"## Issue Description

To reproduce:

```
$ python2
Python 2.7.12 (default, Jun 29 2016, 14:05:02)
[GCC 4.2.1 Compatible Apple LLVM 7.3.0 (clang-703.0.31)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import praw
>>> reddit = praw.Reddit(client_id=""ID"", client_secret=""SECRET"", user_agent=""UA"")
>>> reddit._core._authorizer
<prawcore.auth.ScriptAuthorizer object at 0x100e776d0>
>>> reddit.config.username
<object object at 0x100533130>
>>> reddit.config.password
<object object at 0x100533130>
```

Since ScriptAuthorizer manages password grant flow which requires username and password,  I think this behavior is not correct.
## System Information
- PRAW Version: latest comimit b7b03fc
- Python Version: 2.7.12
- Operating System: Mac OS X 10.11.6
"
657,Address pylint complaints.,2016-08-08T06:20:57Z,2016-08-08T06:30:53Z,,,,
656,"Support ""installed"" applications.",2016-08-07T23:04:07Z,2016-08-08T05:48:12Z,,,,
655,Add tests 2,2016-08-07T15:49:50Z,2016-08-07T22:52:13Z,,,,"## Feature Summary and Justification

This feature provides tests / updates tests as mentioned in the commit summaries, and also simplifies one import in `praw.settings` (six.moves provides a configparser MovedModule so a try/except block is not needed for the equivalent module).

Also fixed one line from the last tests (`os.environ.get` was used, but `os.getenv` is preferred). Sorry for being a bit late, this caused a cherry pick conflict and git was showing the rest of the function as included in the conflict, and that made me confused and incorrectly resolve a few times.
"
654,Add tests 1,2016-08-04T19:08:37Z,2016-08-05T03:38:01Z,,,,"## Feature Summary and Justification

This feature provides tests / updates tests as mentioned in the commit summaries, and also simplifies one import in `praw.__init__` (six provides a unichar method so a if/else block is not needed for [equivalent function](https://pythonhosted.org/six/#six.unichr))

Unfortunately, one of the tests won't pass because it isn't cached and the tokens were regenerated. I've fixed this, (and found a way for it to be cached so it won't happen again) in a later commit on my /add_tests branch; however the commit history is a bit convoluted until that point and fixing conflicts is giving me a headache, so I hope that can wait until I make a PR with those commits in sequence or I can just add those commits in sequence to this PR.
"
653,Work around annoyance when testing on OS X.,2016-08-04T07:23:49Z,2016-08-04T07:35:19Z,,,,
652,Add Subreddit.comments.gilded.,2016-08-04T07:13:49Z,2016-08-04T14:37:06Z,,,,
651,WikiPage.revision_by is a Redditor instance.,2016-08-04T06:09:39Z,2016-08-04T06:17:06Z,,,,
650,Required configuration setting 'client_id' missing,2016-08-03T01:35:39Z,2016-08-03T01:42:36Z,,"Error, praw.exceptions.ClientException","Error:, praw.exceptions.ClientException: Required configuration setting 'client_id' missing. ","## Issue Description

Hi,

I'm new to praw so I was trying to follow the documentation. 
http://praw.readthedocs.io/en/stable/pages/getting_started.html#connecting-to-reddit

I've installed everything up to this point but I cannot get past this exception. I think it has something to do with OAuth, but the walkthrough guide does not mention it.

My code:

```
import praw
import pprint


user_agent = ""python:r_nba_app:v1 (by /u/xxxx)""

r = praw.Reddit(user_agent=user_agent)

user_name = 'xxxx'
user = r.get_redditor(user_name)

thing_limit = 10
sub = user.get_submitted(thing_limit)
karma_by_subreddit = {}
for thing in sub:
    subreddit = thing.subreddit.display_name
    karma_by_subreddit[subreddit]  = (karma_by_subreddit.get(subreddit, 0) + thing.score)

pprint.pprint(karma_by_subreddit)
```

Error:

```
Traceback (most recent call last):
  File ""reddit_scraper.py"", line 13, in <module>
    r = praw.Reddit(user_agent=user_agent)
  File ""/Users/brianlin/anaconda/lib/python2.7/site-packages/praw/reddit.py"", line 92, in __init__
    raise ClientException(message.format(attribute))
praw.exceptions.ClientException: Required configuration setting 'client_id' missing. 
This setting can be provided in a praw.ini file, as a keyword argument to the `Reddit` class constructor, or as an environment variable.

```
## System Information

PRAW Version: 4.0.0b12
Python Version: 2.7
Operating System: OS X Yosemite
"
649,Depend on prawcore >=0.0.15 <0.1.,2016-08-02T15:11:30Z,2016-08-02T15:26:09Z,,,,"I want to be a bit more flexible now on what prawcore version is
allowed. semversion allows backword breaking changes in an pre 1.0
release. However, I will commit to bumping prawcore's minor version if any
backwards incompatible changes are made.
"
648,Replace pre_push.sh with pre_push.py,2016-08-02T04:30:31Z,2016-08-02T04:34:09Z,,,,
647,Replace pre_push.sh with pre_push.py,2016-07-31T09:48:07Z,2016-08-02T04:31:13Z,,,,
646,Update comment extraction and parsing document.,2016-07-31T09:29:52Z,2016-07-31T09:40:18Z,,,,
645,Enable sphinx nitpick mode and fix warnings.,2016-07-31T06:16:56Z,2016-07-31T07:08:32Z,,,,
644,Many changes pertaining to auto generated documentation.,2016-07-30T23:38:20Z,2016-07-30T23:41:01Z,,,,
643,Fix TestConfig unit tests by removing other possible environments.,2016-07-30T18:03:36Z,2016-07-30T20:46:53Z,,,,
642,Add subreddit creation and update_settings,2016-07-30T09:07:50Z,2016-07-30T23:32:48Z,,,,"### What's new
- Add class `SubredditHelper`.
- Remove method `Reddit.subreddit`.
- Add staticmethod `Subreddit._create_or_update` to access the site_admin endpoint.
- Add method `SubredditHelper.create`. The required fields are `name`, `title`, `link_type`, `subreddit_type`, `wikimode`. Everything else can be left for reddit to decide.
- Add method `SubredditModeration.settings` to fetch the current settings.
- Add method `SubredditModeration.update_settings` to overwrite specific settings. In my opinion, `set_settings` from PRAW3 should not be re-implemented, updating is where it's at.
- Add test `TestSubreddit.test_create`
- Add test `TestSubredditModeration.test_update_settings`.
- Move multireddit creation test from `test_reddit.py` to `test_multi.py`.
- Rename mispelled cassettes ""Mu**li**reddit"" -> ""Mu**lti**reddit"".
### Concerns
- There's a subreddit setting called ""domain"", and I can't find any documentation for what it does. It's not listed on the [site docs](https://www.reddit.com/dev/api/oauth#POST_api_site_admin) and I don't know what [this](https://github.com/reddit/reddit/blob/4b2a669d6fe85f85c518a06a905ef1c0cfa57478/r2/r2/controllers/api.py#L2854-L2857) or [this](https://github.com/reddit/reddit/blob/4b2a669d6fe85f85c518a06a905ef1c0cfa57478/r2/r2/models/subreddit.py#L494-L510) mean. It's included in the `_create_or_update` parameters just for completeness sake.

&nbsp;

It's been a while. Please critique!
"
641,decorator signature error using version >=3.4.0 on my Pi (Py3.2),2016-07-29T21:54:13Z,2016-07-29T22:02:53Z,,AttributeError,AttributeError: 'module' object has no attribute 'signature',"## Issue Description

I went back to 3.3.0 now as I can not find anything about problems using new versions. Might my problem be related to python 3.2?

```
pi@raspberrypi ~/zeugs/bot $ sudo pip-3.2 install --upgrade 'praw==3.4.0'
Downloading/unpacking praw==3.4.0
  Downloading praw-3.4.0.tar.gz (4.3Mb): 4.3Mb downloaded
  Running setup.py egg_info for package praw

Downloading/unpacking decorator>=4.0.9,<4.1 (from praw==3.4.0)
  Downloading decorator-4.0.10.tar.gz (68Kb): 68Kb downloaded
  Running setup.py egg_info for package decorator

Downloading/unpacking requests>=2.3.0 (from praw==3.4.0)
  Downloading requests-2.10.0.tar.gz (477Kb): 477Kb downloaded
  Running setup.py egg_info for package requests

    warning: no files found matching 'test_requests.py'
Downloading/unpacking six==1.10 (from praw==3.4.0)
  Downloading six-1.10.0.tar.gz
  Running setup.py egg_info for package six

    no previously-included directories found matching 'documentation/_build'
Downloading/unpacking update-checker==0.11 (from praw==3.4.0)
  Downloading update_checker-0.11.tar.gz
  Running setup.py egg_info for package update-checker

Installing collected packages: praw, decorator, requests, six, update-checker
  Running setup.py install for praw

    Installing praw-multiprocess script to /usr/local/bin
  Running setup.py install for decorator

  Running setup.py install for requests

    warning: no files found matching 'test_requests.py'
  Running setup.py install for six

    no previously-included directories found matching 'documentation/_build'
  Running setup.py install for update-checker

Successfully installed praw decorator requests six update-checker
Cleaning up...
```

The new error:

```
    comments = r.get_subreddit(SUBS_STRING).get_comments(limit=250)
  File ""/usr/local/lib/python3.2/dist-packages/praw/decorators.py"", line 54, in wrapped
    func_args = _make_func_args(function)
  File ""/usr/local/lib/python3.2/dist-packages/praw/decorator_helpers.py"", line 33, in _make_func_args
    func_items = inspect.signature(function).parameters.items()
AttributeError: 'module' object has no attribute 'signature'
```

lines are the same on praw 3.5.0

I uninstalled all dependencies and force installed 3.30 and it is working again.
## System Information

PRAW Version: 3.4 and 3.5
Python Version: 3.2
Operating System:
pi@raspberrypi ~ $ cat /etc/issue
Raspbian GNU/Linux 7 \n \l
pi@raspberrypi ~ $ cat /etc/debian_version
7.10
"
640,Added comment extraction and parsing,2016-07-29T15:27:57Z,2016-07-30T00:56:10Z,,,,"Added rst file for comment extraction and parsing
"
639,Issues install with pip3,2016-07-28T11:34:26Z,2016-07-28T21:07:39Z,,AttributeError,AttributeError: 'Requirement' object has no attribute 'project_name',"## Issue Description

When trying to praw with `sudo pip3 install praw` it errors out with 

``` Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pip/req/req_install.py"", line 1006, in check_if_exists
    self.satisfied_by = pkg_resources.get_distribution(str(no_marker))
  File ""/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/__init__.py"", line 535, in get_distribution
    dist = get_provider(dist)
  File ""/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/__init__.py"", line 415, in get_provider
    return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
  File ""/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl/pkg_resources/__init__.py"", line 695, in find
    raise VersionConflict(dist, req)
pkg_resources.VersionConflict: (decorator 4.0.6 (/usr/lib/python3/dist-packages), Requirement.parse('decorator<4.1,>=4.0.9'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 209, in main
    status = self.run(options, args)
  File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 317, in run
    requirement_set.prepare_files(finder)
  File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 360, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 448, in _prepare_file
    req_to_install, finder)
  File ""/usr/lib/python3/dist-packages/pip/req/req_set.py"", line 387, in _check_skip_installed
    req_to_install.check_if_exists()
  File ""/usr/lib/python3/dist-packages/pip/req/req_install.py"", line 1011, in check_if_exists
    self.req.project_name
AttributeError: 'Requirement' object has no attribute 'project_name'
```

As far as I can tell this is a setuptools error
## System Information

```
PRAW Version: NA
```

  Python Version: 3.5.1
Operating System: Ubuntu 16.04
"
638,Update reply_bot.rst,2016-07-26T12:37:46Z,2016-07-26T15:42:31Z,,,,"Fixes #N/A
## Feature Summary and Justification

Fixed a few typos.
"
637,Updated RateLimitHandler request to merge environment variables,2016-07-25T17:16:34Z,2016-07-25T19:03:00Z,,,,"Fixes #636

Updates `RateLimitHandler.request` to pull in environment variables for proxies and custom SSL certificates. Necessary for PRAW to work with custom SSL certs.
"
636,PRAW does not work with custom SSL cert bundle,2016-07-24T11:51:07Z,2016-07-28T15:08:20Z,Feature,,,"Because of the proxy I'm using, I need to use a custom SSL cert bundle. Normally using the requests library, this is achievable in one of 2 ways:

Explicitly setting `verify` to the path of the bundle:

```
requests.get('https://google.com', verify='/path/to/cacert.pem')
```

Or setting an environment variable so that all requests use it:

```
export REQUESTS_CA_BUNDLE='/path/to/cacert.pem'

requests.get('https://google.com')
```

The environment variable is preferred because this allows the requests library to work when called from other packages that I did not write.

However, this does not work with PRAW. The problem I see is severalfold:

Using `Session.request` from requests library gets the environment variable properly through the `merge_environment_settings` method:

https://github.com/kennethreitz/requests/blob/fb014560611f6ebb97e7deb03ad8336c3c8f2db1/requests/sessions.py#L461
https://github.com/kennethreitz/requests/blob/fb014560611f6ebb97e7deb03ad8336c3c8f2db1/requests/sessions.py#L617-L629

But this code is never reached since PRAW builds its own request and uses Session.send which does not pull the environment variable:

https://github.com/praw-dev/praw/blob/3902dc24b0f42e487e26481aae46352806e3e6a8/praw/handlers.py#L101-L102

PRAW does support a setting for `validate_certs` which gets passed along as the `verify` parameter to requests library. The issue here is that PRAW only allows a boolean. Setting this variable to the path of a `cacert.pem` file evaluates to False and turns off SSL verification:

https://github.com/praw-dev/praw/blob/3902dc24b0f42e487e26481aae46352806e3e6a8/praw/__init__.py#L222-L223

There are a couple ways to solve this that I can think of. I would be glad to help out with a fix if that is something that is desirable.
"
635,Update contributing related documentation.,2016-07-23T07:45:26Z,2016-07-23T07:47:09Z,,,,
634,Add reply bot documentation. Thanks @DCuddies.,2016-07-22T16:00:28Z,2016-07-22T16:01:20Z,,,,
633,Fix typo in  and clarify docs of helpers.valid_redditors,2016-07-21T04:44:04Z,2016-07-23T06:29:50Z,,,,"The parameter is ""sub"", not ""mod_sub"", and it's unclear as to what a ""valid redditor"" actually is.
"
632,PRAW timestamps returning posts outside of range,2016-07-20T12:13:25Z,2016-09-24T22:39:07Z,,,,"I'm trying to filter posts between two timestamps but I keep getting posts that have timestamps outside of the range (sometimes a year before the timeframe specified).

An example query trying to filter posts between 16 Jun,2016 and 24 Jun,2016

`search_str = '(or title:""brexit"" (or title:""referendum"") (and timestamp:1466101800..1466726400))'`
`submissions = r.search(search_str,subreddit = 'unitedkingdom',syntax='cloudsearch', limit=None)`

Is there an issue with my query or is this behaviour expected from timestamp queries?
"
631,_prepare_request: Attach session hooks to the request,2016-07-19T14:27:28Z,2016-07-21T14:19:52Z,,,,"Users can't add hooks to the request object itself, but they can to the session. However session hooks aren't merged with the request's since it isn't prepared with Session.prepare_requests.
"
630,Update README.rst with PRAW4 content.,2016-07-19T05:07:25Z,2016-07-19T06:41:53Z,,,,"And other minor documentation fixes.
"
629,Add a Gitter chat badge to README.rst.,2016-07-19T01:28:46Z,2016-07-19T01:29:44Z,,,,
628,Add a Gitter chat badge to README.rst,2016-07-18T22:15:42Z,2016-07-19T01:31:08Z,,,,"### praw-dev/praw now has a Chat Room on Gitter

@bboe has just created a chat room. You can visit it here: [https://gitter.im/praw-dev/praw](https://gitter.im/praw-dev/praw?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&content=body_link).

This pull-request adds this badge to your README.rst:

[![Gitter](https://badges.gitter.im/praw-dev/praw.svg)](https://gitter.im/praw-dev/praw?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=body_badge)

If my aim is a little off, please [let me know](https://github.com/gitterHQ/readme-badger/issues).

Happy chatting.

PS: [Click here](https://gitter.im/settings/badger/opt-out) if you would prefer not to receive automatic pull-requests from Gitter in future.
"
627,"The first request made ""is not"" cached",2016-07-17T17:34:35Z,2016-07-17T18:23:30Z,,,,"I'm unsure if this is intended or just by consequence. I found this while writing up some tests for a Betamax-like package for python's httplib, and I was comparing responses with those from praw for the sake of ease.

I'll split this up into three scenarios:
1. No authentication: The first request ""is"" cached by the handler, however reddit adds the ""__cfduid"" (I don't know what this does from a cursory look), ""loid"" (logged out id cookie for A/b tests), and ""loidcreated"" (timestamp of the LOid cookie creation so that reddit can change your variant test after a set amount of time) cookies to the requests Session cookiejar. Because of this, a request to the same url right after this does not read from the cache (as the cookies do not match, obviously).
2. OAuth2 authentication: Same as scenario 1, except it's not the first request, it's the first request after authenticating.
3. Login authentication: Same as scenario 1, except ""reddit_session"" and ""secure_session"" cookies are also added.

Sometimes a session_id cookie is also added, however this seems to be for anonymous tracking purposes and doesn't occur all the time.

Regardless the addition of these cookies make it so that the initial request, when stored in the DefaultHandler's cache, would never be retrieved from the cache (unless you manually manipulate the cookiejar)
"
626,Add faq entry on pyOpenSSL,2016-07-17T12:14:44Z,2016-07-17T13:13:42Z,,,,"Still an issue since the underlying prawcore uses requests and upon import SNI support is injected.
"
625,Resolve errors with inadequate pyOpenSSL versions,2016-07-17T03:50:58Z,2016-07-17T05:24:42Z,,,,"I finally looked into this after kline on the reddit-dev irc brought it up the other day.

This closes #548 and @bboe/reddit_irc#3.

The issue lies in the fact that requests does not _require_ pyOpenSSL. However, if it's on the system, it injects it into urllib3 [upon loading](https://github.com/kennethreitz/requests/blob/master/requests/__init__.py#L54). This allows for certificates to be validated with SNI support.

However,  some systems, such as debian from what I recall, and others, come with pyOpenSSL 0.14 or lower. Somewhere before 0.15, a regression caused data to not be accepted as strings, albeit previously allowed. This was fixed in version 0.15 and higher.

Personally, I didn't feel that praw should outright _require_ pyOpenSSL, since requests didn't. So instead, this patch requires a version >= 0.15 _only_ if a version less than that is found. In addition, upon first instantiation of the Reddit class a warning is given if applicable (some packages install versions of pyOpenSSL < 0.15).

If this method is accepted, I'll port the changes to prawcore for PRAW4 support.
"
624,Narrow down the DeprecationWarnings filter,2016-07-14T04:08:49Z,2016-07-15T04:27:59Z,,,,"By using filterwarnings instead of simplefilter, #367 will no longer occur
"
623,Minor updates in response to PRs and issue questions.,2016-07-07T22:44:41Z,2016-07-07T23:03:16Z,,,,
622,Allow the user to override img_type in upload_image,2016-07-05T04:40:41Z,2016-07-07T22:12:00Z,,,,"Whenever the image format and the img_type are both not set to png, reddit converts the image to RGBA mode, which may be useful if the user wishes to have a png automatically converted from any given color mode to RGBA. This is also useful in the case that the optimization that matches the format causes the image to lose quality / data, which in my experience has happened to some jpegs in the past.

If a value of ""jpeg"" is given it is gracefully accepted and converted to ""jpg"", because strangely, if ""jpeg"" is sent to the server, given how the validator checks the value, it will default to ""png"".

Implemented with the ""upload_as"" kwarg, because this also dictates the end file type on reddit's cdn.
"
621,Insufficient scope when getting new submissions,2016-07-04T03:40:40Z,2016-07-04T18:33:35Z,,praw.errors.OAuthInsufficientScope,praw.errors.OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/r/test/new.json?limit=1,"I'm writing a very simple script to `r.get_subreddit('test').get_new(limit=1)` and I'm getting OAuth errors.

Here's the output:

```
initializing subreddit: test
substituting https://oauth.reddit.com for https://api.reddit.com in url
GET: https://oauth.reddit.com/r/test/new.json
params: {u'limit': 1}
status: 403
Traceback (most recent call last):
  File ""tii/main.py"", line 18, in <module>
    main()
  File ""tii/main.py"", line 14, in main
    bot._get_r_submissions('test')
  File ""/src/qix-.this-image-is/tii/reddit.py"", line 68, in _get_r_submissions
    self._subreddit[r] = self.__make_subreddit(r)
  File ""/src/qix-.this-image-is/tii/reddit.py"", line 74, in __make_subreddit
    print repr(next(latest_post))  # XXX TODO
  File ""/src/qix-.this-image-is/env/lib/python2.7/site-packages/praw/__init__.py"", line 567, in get_content
    page_data = self.request_json(url, params=params)
  File ""<decorator-gen-8>"", line 2, in request_json
  File ""/src/qix-.this-image-is/env/lib/python2.7/site-packages/praw/decorators.py"", line 116, in raise_api_exceptions
    return_value = function(*args, **kwargs)
  File ""/src/qix-.this-image-is/env/lib/python2.7/site-packages/praw/__init__.py"", line 622, in request_json
    retry_on_error=retry_on_error)
  File ""/src/qix-.this-image-is/env/lib/python2.7/site-packages/praw/__init__.py"", line 454, in _request
    _raise_response_exceptions(response)
  File ""/src/qix-.this-image-is/env/lib/python2.7/site-packages/praw/internal.py"", line 201, in _raise_response_exceptions
    raise OAuthInsufficientScope('insufficient_scope', response.url)
praw.errors.OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/r/test/new.json?limit=1
```

And the code (after logging in and getting a successful `get_me()` response):

``` python
def _get_r_submissions(self, r):
    if r not in self._subreddit:
        print 'initializing subreddit: %s' % r
        self._subreddit[r] = self.__make_subreddit(r)
    # XXX TODO

def __make_subreddit(self, r):
    sub = self._reddit.get_subreddit(r)
    latest_post = sub.get_new(limit=1) # succeeds, somehow
    print repr(next(latest_post))  # fails
```

Am I missing something? This occurs with both the current Pip version as well as testing against the github master/HEAD.

The strange thing is this output:

```
substituting https://oauth.reddit.com for https://api.reddit.com in url
GET: https://oauth.reddit.com/r/test/new.json
```

It appears it's logging that it's doing something correct (the correct endpoint is indeed `https://api.reddit.com/r/test/new.json`) but not _actually_ substituting it.
"
620,Set Reddit.user on the wildcard OAuth scope,2016-07-02T20:28:41Z,2016-07-02T23:50:16Z,,,,"The '*' OAuth scope provides for all possible scopes, including methods that would require 'identity'. This scope is what is used in when using the 'password' grant_type, as well as can be set manually. If using the token based flow.
"
619,PRAW4 To Do List,2016-06-30T05:37:03Z,2016-11-20T23:52:42Z,"Documentation, ⭐️ New Contributor Friendly ⭐️",,,"This list is intended to be updated over time. Some of these items should be broken up into smaller chunks.

If you would like to work on any of the items or a subset of the items please claim it with a comment. We will try to update the associated item with your name within a 24 hour window. All pull requests should be made against the praw4 branch, and should only introduce a single addition (e.g., tests for a single method, porting of one function along with its tests, a documentation page).
- [x] 100% test coverage for existing code in praw4 branch
- [x] Add BaseList unit tests [ref](https://coveralls.io/builds/6602666/source?filename=praw%2Fmodels%2Flist%2Fbase.py)
- [x] Refactor WikiPageList to be a `wiki` attribute on `Subreddit` objects. Iterating through the wiki pages should be as easy as `for wikipage in subreddit.wiki`. Fetching a single page should be `subreddit.wiki['pagename']`. Creating a page might look like `subreddit.wiki.new(...)`.
- [x] Refactor Submission flair such that flair `submission.flair` is its own object.
- [x] Test submission hide and unhide.
- [x] Moderator actions on a submission should be accessible through `submission.mod...` for example `submission.mod.contest_mode`, `submission.mod.suggested_sort`.
- [x] Fresh documentation

Re-add support (or add changelog deleted entry) for the code pertaining to the following entry points:
- [x] api/accept_moderator_invite
- [x] api/deleteflair
- [x] api/flairconfig/
- [x] api/leavecontributor
- [x] api/leavemoderator
- [x] api/recommend/sr/{subreddits}
- [x] api/subscribe/
- [x] api/username_available/
- [x] captcha/
- [x] help/
- [x] /api/info
- [x] r/{subreddit}/about/stylesheet/
- [x] api/subreddit_stylesheet/
- [x] by_id/
- [x] domain/{domain}/
- [x] message/mentions
- [x] message/messages/{messageid}/
- [x] r/{subreddit}/about/edited/
- [x] r/{subreddit}/about/reports/
- [x] r/{subreddit}/about/spam/
- [x] r/{subreddit}/about/modqueue/
- [x] r/{subreddit}/about/unmoderated/
- [x] r/{subreddit}/about/log/
- [x] r/{subreddit}/about/sticky/
- [x] r/{subreddit}/about/traffic/
- [x] r/{subreddit}/api/delete_sr_header
- [x] r/{subreddit}/api/delete_sr_img
- [x] api/upload_sr_img
"
618,Added check for blank subreddit name.,2016-06-15T08:32:07Z,2016-06-15T16:39:47Z,,,,"Addresses #615
"
617,Add method for getting lazy instance of a comment.,2016-06-15T06:05:48Z,2016-06-15T06:13:08Z,,,,
616,Remove easy_install from README.rst.,2016-06-15T05:56:09Z,2016-06-15T06:09:47Z,,,,"`easy_install` will install the latest version regardless if it is pre-release
or not. With PRAW4 in pre-release status, it is not desirable that people
attempting to install the latest stable PRAW receive that version.
"
615,"Confusing error when calling reddit.get_subreddit("""")",2016-06-14T07:50:52Z,2016-06-15T16:39:57Z,"Bug, ⭐️ New Contributor Friendly ⭐️",TypeError,TypeError: 'NoneType' object has no attribute '__getitem__'," Calling `reddit.get_subreddit("""")` with an empty string returns a confusing stack trace. I was expecting this to raise an **InvalidSubreddit** exception, but instead I got this

```
 File ""rtv/content.py"", line 411, in from_name
    subreddit = reddit.get_subreddit(name)
  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 1097, in get_subreddit
    return objects.Subreddit(self, subreddit_name, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 1528, in __init__
    subreddit_name = json_dict['url'].split('/')[2]
TypeError: 'NoneType' object has no attribute '__getitem__'
```

Poking into the source code, it looks like the issue could be solved by explicitly checking for a `None` value here.

``` python
    def __init__(self, reddit_session, subreddit_name=None, json_dict=None,
                 fetch=False, **kwargs):
        """"""Construct an instance of the Subreddit object.""""""
        # Special case for when my_subreddits is called as no name is returned
        # so we have to extract the name from the URL. The URLs are returned
        # as: /r/reddit_name/
        if not subreddit_name:
            subreddit_name = json_dict['url'].split('/')[2]
```
"
614,How to get before and after ids from the search response,2016-06-13T15:10:44Z,2016-06-14T09:56:04Z,,,,"Hi I am trying to retrieve all the posts from a particular search query. I am using 

```
import praw

 r = praw.Reddit(user_agent='some cool user agent')
posts=r.search(search, subreddit=None,sort=None, syntax=None,period=None,limit=None)
    title=[]
    for post in posts:
        title.append(post.title)
        print post
```

I am trying to paginate but I am trying to figure out how to retrieve the before and after data and how to use these to get the next page from the data. 
"
613,"SSLError: EOF occurred in violation of protocol, when using PRAW:",2016-06-12T03:33:32Z,2016-10-07T07:23:58Z,,"ssl.SSLEOFError, requests.packages.urllib3.exceptions.SSLError, requests.exceptions.SSLError","ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:645), requests.packages.urllib3.exceptions.SSLError: EOF occurred in violation of protocol (_ssl.c:645), requests.exceptions.SSLError: EOF occurred in violation of protocol (_ssl.c:645)","Ubuntu 16.04 running Python 3.5.1, the following fails on my system:

```
ERROR:root:Fatal error
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 560, in urlopen
    body=body, headers=headers)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 346, in _make_request
    self._validate_conn(conn)
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 787, in _validate_conn
    conn.connect()
  File ""/usr/lib/python3/dist-packages/urllib3/connection.py"", line 252, in connect
    ssl_version=resolved_ssl_version)
  File ""/usr/lib/python3/dist-packages/urllib3/util/ssl_.py"", line 305, in ssl_wrap_socket
    return context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/lib/python3.5/ssl.py"", line 376, in wrap_socket
    _context=self)
  File ""/usr/lib/python3.5/ssl.py"", line 748, in __init__
    self.do_handshake()
  File ""/usr/lib/python3.5/ssl.py"", line 984, in do_handshake
    self._sslobj.do_handshake()
  File ""/usr/lib/python3.5/ssl.py"", line 629, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLEOFError: EOF occurred in violation of protocol (_ssl.c:645)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 376, in send
    timeout=timeout
  File ""/usr/lib/python3/dist-packages/urllib3/connectionpool.py"", line 589, in urlopen
    raise SSLError(e)
requests.packages.urllib3.exceptions.SSLError: EOF occurred in violation of protocol (_ssl.c:645)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/elongatedmuskrat/elongatedmuskrat.py"", line 205, in <module>
    if reddit.has_new_post():
  File ""/home/elongatedmuskrat/elongatedmuskrat.py"", line 103, in has_new_post
    submission = self.fetch_latest(1)
  File ""/home/elongatedmuskrat/elongatedmuskrat.py"", line 75, in fetch_latest
    return list(self.subreddit.get_new(limit=submissions_to_fetch))[0]
  File ""/usr/local/lib/python3.5/dist-packages/praw/__init__.py"", line 567, in get_content
    page_data = self.request_json(url, params=params)
  File ""<decorator-gen-8>"", line 2, in request_json
  File ""/usr/local/lib/python3.5/dist-packages/praw/decorators.py"", line 116, in raise_api_exceptions
    return_value = function(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/praw/__init__.py"", line 622, in request_json
    retry_on_error=retry_on_error)
  File ""/usr/local/lib/python3.5/dist-packages/praw/__init__.py"", line 453, in _request
    response = handle_redirect()
  File ""/usr/local/lib/python3.5/dist-packages/praw/__init__.py"", line 434, in handle_redirect
    verify=self.http.validate_certs, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/praw/handlers.py"", line 146, in wrapped
    result = function(cls, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/praw/handlers.py"", line 56, in wrapped
    return function(cls, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/praw/handlers.py"", line 102, in request
    allow_redirects=False, verify=verify)
  File ""/usr/lib/python3/dist-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python3/dist-packages/requests/adapters.py"", line 447, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: EOF occurred in violation of protocol (_ssl.c:645)

```

How can this be resolved?
"
612,Add method get_rules for a subreddit's rules,2016-06-11T02:33:46Z,2016-06-15T05:59:21Z,,,,"Nothing more complicated than it says in the title. Subreddits can now set official rules (not just by editing the sidebar), and this lets you retrieve them as a JSON dictionary.
"
611,KeyError: u'data' in SubmitMixin,2016-06-10T07:12:09Z,2016-06-23T05:14:53Z,,KeyError,KeyError: u'data',"```
djc@enrai pmf $ python bot.py
retrieving feed items...
retrieving reddit items (MozillaTech)...
Traceback (most recent call last):
  File ""bot.py"", line 59, in <module>
    res = submit(api, r, title, link)
  File ""bot.py"", line 47, in submit
    return api.submit(r, title, url=link)
  File ""<string>"", line 2, in submit
  File ""/usr/lib64/python2.7/site-packages/praw/decorators.py"", line 271, in wrap
    return function(*args, **kwargs)
  File ""<string>"", line 2, in submit
  File ""/usr/lib64/python2.7/site-packages/praw/decorators.py"", line 177, in require_captcha
    return function(*args, **kwargs)
  File ""/usr/lib64/python2.7/site-packages/praw/__init__.py"", line 2678, in submit
    url = result['data']['url']
KeyError: u'data'
```

This has started happening this week. The result value seems to be `{u'errors': []}`.
"
610,update_checker could be extra dependency ?,2016-06-10T00:30:17Z,2016-08-29T07:13:44Z,Feature,,,"Im maintaining a reddit addon for KODI media platform and i'd prefer to use PRAW as my API layer instead of relying on hardcoded json url's.  At the moment, im in process of providing Praw as a script package to KODI. 

Kodi does not use python packages from system installation but all dependencies need to be packages and supplied from KODI's own package repositories and all of the packages Praw depends are are already provided except update_checker. Since Kodi also uses its own update mechanism, calling update_checker.update_check is rather pointless if/when praw is used from KODI (also because logging is not visible the the user). There is a a way to disable the call but there's still a hard dependency from import.

I could ofcourse always patch the praw to eliminate this when i release a new version of praw for KODI but i'd prefer if the feature could made optional ?  Having this sort of feature would also help in scenario like #600 since update_checker hasn't gotten much love recently ? 

PS. Im willing to give a shot at a pr for this.. 
"
609,Provide analog of praw.objects.Submission.replace_more_comments for praw.objects.Comment,2016-06-06T20:08:13Z,2016-12-19T07:35:28Z,Feature,,,"Currently, it's possible to automagically traverse the comment tree of a submission and replace the MoreComment instances with the actual comments they represent. However, it isn't possible to do the same thing with the reply tree of an individual comment.
"
608,Update PRAW documentation links to praw.readthedocs.io from .org.,2016-06-04T15:49:19Z,2016-06-04T16:12:50Z,,,,"Email from readthedocs:

> Hello!
> 
> Starting today, Read the Docs will start hosting projects from subdomains on
>   the domain readthedocs.io, instead of on readthedocs.org. This change
>   addresses some security concerns around site cookies while hosting user
>   generated data on the same domain as our dashboard.
> 
> Changes to provide security against broader threats have been in place for a
>   while, however there are still a few scenarios that can only be addressed by
>   migrating to a separate domain.
> 
> We implemented session hijacking detection and took precautions to limit
>   cookie usage, but there are still a number of scenarios utilizing XSS and
>   CSRF attacks that we aren't able to protect against while hosting
>   documentation from subdomains on the readthedocs.org domain. Moving
>   documentation hosting to a separate domain will provide more complete
>   isolation between the two user interfaces.
> 
> Projects will automatically be redirected, and this redirect will remain in
>   place for the foreseeable future. Still, you should plan on updating links to
>   your documentation after the new domain goes live.
> 
> If you notice any problems with the changes, feel free to open an issue on
>   our issue tracker: http://github.com/rtfd/readthedocs.org/issues. If you do
>   notice any security issues, contact us at security@readthedocs.org with more
>   information.
> 
> Keep documenting,
> Read the Docs
"
607,Update oauth.rst,2016-06-03T04:12:52Z,2016-06-05T17:20:17Z,,,,"Parenthesis needed for python3.
"
606,Add preliminary support for password OAuth grant types to PRAW3,2016-05-22T17:51:02Z,2016-05-23T19:33:37Z,,,,"Only INI-based configuration supported as yet;

add to the relevant site section of your `~/.config/praw.ini`

```
oauth_grant_type: password
oauth_client_id: myclid
oauth_client_secret: mysecret
user: myuser
pswd: mypwd
```

in your code, skip the call to `get_authorize_url()` and go directly to `get_access_information()`:

```
    tokens = r.get_access_information( 'password' )
    r.set_access_credentials( **tokens )
```

Has tentative support (read, untested) for automatic token refresh (because password grants don't return a refresh token).
"
605,Error: Could not find a version that satisfies...,2016-05-15T11:36:49Z,2016-10-07T07:33:46Z,,,,"I'm on Windows 10 with Python 3.5 installed. I got this error,

> Could not find a version that satisfies the requirement praw (from versions: )
> No matching distribution found for praw

when trying to pip install it. Any clue?
"
604,Dependency ranges too small,2016-05-10T07:15:23Z,2016-11-14T03:55:33Z,⭐️ New Contributor Friendly ⭐️,,,"In acf871d9444c5, the dependencies were pinned. Partly with my Gentoo Linux packager hat on, I thoroughly dislike this change because the dependency ranges are so small; and pinning the dependency with `==` seems very unlikely to be justified. If I want to install this package, it needs to cooperate with the rest of what's installed, and saying ""you have to use the exact version I tested with"" is essentially unhelpful. If there actual Reasons to require a very recent version (because otherwise there are bugs, or whatever), that's fine, of course, but in those cases `>=` deps should be used.
"
603,Support large lists of fullnames for `hide` and `get_info`,2016-05-09T01:12:45Z,2016-05-10T06:51:30Z,,,,"What I did:
- New helper function `chunk_sequence` converts a long list into multiple smaller lists of a certain length.
- Changed `r.hide` and `r.get_info` to use `chunk_sequence` so that the user can input any number of fullnames and have all the requests processed automatically. This involves making ""param groups"" for each request and adding the server response into a total final list.
- `url = True  # Enable returning a list` was confusing, so I've made `return_list` its own variable.
- Added comment stickying to the changelog which was forgotten last time.

Feedback wanted:
- My primary concern at the moment is the return value of `r.hide`. Previously it was just the server response, which as far as I know is always an empty dictionary as long as the request is successful. Now, I'm returning the list of these empty dictionaries. Does this seem like the best route? This could confuse programs which expect the dictionary, but I'm kind of assuming no one uses these for anything.
- Any other methods that would benefit from this chunking?

Haven't made a pull request in a while, hopefully I still remember how to make Travis pass.

&nbsp;

Edit: If I have to make any further commits I will change my list comprehension in `test_info_by_id` to please Travis 2.7. Otherwise I'm going to let that sit for now.
"
602,Sticky top-level comments.,2016-04-15T13:50:43Z,2016-04-17T20:37:55Z,,,,"Moderatable.distinguish now accepts key-word argument `sticky`.  If object is a top-level `Comment`, the `sticky` parameter is appended to the json request.  Also added a small test to ensure basic functionality works (however, I'm not sure I'm using betamax correctly).
"
601,refresh_access_information broken,2016-04-04T19:33:14Z,2016-04-04T21:03:13Z,,,,"I'm using praw with python3.4 and no matter what, once the first access_token expires, I cannot get a refresh token. We get the same error whether we run this live on our server that is using the reddit API or whether I manually go in and try to make the call to refresh the token from an ipython session.

Here is the output from the server, any guidance would be hugely appreciated:

```
[E 160404 15:12:53 ioloop:629] Exception in callback functools.partial(<function wrap.<locals>.null_wrapper at 0x7f6a6aedda60>, <tornado.concurrent.Future object at 0x7f6a6af07e48>)
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py"", line 600, in _run_callback
        ret = callback()
      File ""/usr/local/lib/python3.4/dist-packages/tornado/stack_context.py"", line 275, in null_wrapper
        return fn(*args, **kwargs)
      File ""/usr/local/lib/python3.4/dist-packages/tornado/ioloop.py"", line 615, in <lambda>
        self.add_future(ret, lambda f: f.result())
      File ""/usr/local/lib/python3.4/dist-packages/tornado/concurrent.py"", line 232, in result
        raise_exc_info(self._exc_info)
      File ""<string>"", line 3, in raise_exc_info
      File ""/usr/local/lib/python3.4/dist-packages/tornado/gen.py"", line 1014, in run
        yielded = self.gen.throw(*exc_info)
      File ""/home/skirp/projects/gulper/lib/userscraper.py"", line 21, in start
        self.data = yield scrapers.scrape(self)
      File ""/usr/local/lib/python3.4/dist-packages/tornado/gen.py"", line 1008, in run
        value = future.result()
      File ""/usr/local/lib/python3.4/dist-packages/tornado/concurrent.py"", line 232, in result
        raise_exc_info(self._exc_info)
      File ""<string>"", line 3, in raise_exc_info
      File ""/usr/local/lib/python3.4/dist-packages/tornado/gen.py"", line 1014, in run
        yielded = self.gen.throw(*exc_info)
      File ""/home/skirp/projects/gulper/lib/scrapers/__init__.py"", line 36, in scrape
        data[scraper.name] = yield scraper.scrape(user_data)
      File ""/usr/local/lib/python3.4/dist-packages/tornado/gen.py"", line 1008, in run
        value = future.result()
      File ""/usr/local/lib/python3.4/dist-packages/tornado/concurrent.py"", line 232, in result
        raise_exc_info(self._exc_info)
      File ""<string>"", line 3, in raise_exc_info
      File ""/usr/local/lib/python3.4/dist-packages/tornado/gen.py"", line 267, in wrapper
        result = func(*args, **kwargs)
      File ""/home/skirp/projects/gulper/lib/scrapers/redditscrape.py"", line 32, in scrape
        tokens = r.refresh_access_information(refresh_token = tokens['refresh_token'])
      File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 1476, in refresh_access_information
        refresh_token=refresh_token or self.refresh_token)
      File ""<decorator-gen-11>"", line 2, in refresh_access_information
      File ""/usr/local/lib/python3.4/dist-packages/praw/decorators.py"", line 288, in require_oauth
        return function(*args, **kwargs)
      File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 733, in refresh_access_information
        retval = self._handle_oauth_request(data)
      File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 657, in _handle_oauth_request
        response = self._request(url, auth=auth, data=data, raw_response=True)
      File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 452, in _request
        _raise_response_exceptions(response)
      File ""/usr/local/lib/python3.4/dist-packages/praw/internal.py"", line 205, in _raise_response_exceptions
        raise OAuthException(msg, response.url)
    praw.errors.OAuthException: Basic realm=""reddit"" on url https://api.reddit.com/api/v1/access_token/
```
"
600,Update pip,2016-03-30T15:37:41Z,2016-06-10T00:42:37Z,,,,"I'm using python 2.7 and every time I initialise praw I receive this warning

'Version 3.4.0 of praw is outdated. Version 4.0.0b4 was released 5 days ago.'

However, when trying to update praw through pip I get
Requirement already up-to-date: praw in /usr/local/lib/python2.7/dist-packages
Could you update pip?
"
599,Add method for getting lazy instance of a comment.,2016-03-26T16:24:24Z,2016-06-15T06:06:21Z,,,,"Allow for easily getting a comment by id.
"
598,Combine sentence fragments into proper sentences.,2016-03-26T04:40:55Z,2016-03-26T05:14:09Z,,,,
597,Add AsyncIO support,2016-03-16T00:14:25Z,2016-03-16T00:25:39Z,,,,"Support for asynchronous methods introduced in [PEP 0492](https://www.python.org/dev/peps/pep-0492/) and [Python 3.5](https://docs.python.org/3/whatsnew/3.5.html) allow relatively simple asynchronous querying, especially for things like HTTP requests with `aiohttp`. It would be very useful to be able to use PRAW natively with coroutines.

[Here](http://stackabuse.com/python-async-await-tutorial/) is an example of using `aiohttp` to query the Reddit API asynchronously.
"
596,Prevent `dir` from making web requests in Python 2,2016-03-11T06:21:27Z,2016-03-11T07:11:56Z,,,,"Fixes #595.

Python2 goes looking for `__members__` and `__methods__` attributes when building the result for `dir(obj)`. These attributes are now blacklisted from RedditContentObject's `__getattr__` method which performs web requests.
"
595,dir changes attributes of praw.objects.Subreddits in place,2016-03-09T21:04:16Z,2016-03-11T07:11:56Z,,,,"Today I noticed that the built-in function [`dir()`](https://docs.python.org/2/library/functions.html#dir) seems to change the attributes of praw.objects.Subreddits in place.

![screen shot 2016-03-09 at 4 01 22 pm](https://cloud.githubusercontent.com/assets/12176060/13650376/69edcf1e-e610-11e5-9e7d-5ef2051ad2d7.png)

Could this possibly be the result of a buggy `__dir__()` implementation in a praw.objects.Subreddit parent class? I honestly have no clue what to make of this.
"
594,Add Reddit Countdown,2016-03-07T08:33:32Z,2016-03-07T17:03:30Z,,,,
593,Only perform the update check once per session,2016-02-28T14:55:54Z,2016-02-28T18:57:49Z,,,,"Currently, the update checker will run each time a `Reddit` instance is created. This is because accessing `update_checked` with `self.` treats it as an instance variable rather than a static class variable. Changing the call to `[class name].update_checked` will treat it as a static class variable.

I haven't tested this change (so make sure to test it please), but a similar patch worked in my code base.
"
592,Unhelpful exception raised from Celery task,2016-02-26T23:06:07Z,2016-11-14T03:57:13Z,,,,"When using PRAW from a celery worker I recieved an unhelpful traceback.

``` pytb
Traceback (most recent call last):
  File ""/home/tippit/venv/lib/python3.5/site-packages/celery/app/trace.py"", line 240, in trace_task
    R = retval = fun(*args, **kwargs)
  File ""/home/tippit/tippit/tippit/__init__.py"", line 129, in __call__
    return TaskBase.__call__(self, *args, **kwargs)
  File ""/home/tippit/venv/lib/python3.5/site-packages/celery/app/trace.py"", line 438, in __protected_call__
    return self.run(*args, **kwargs)
  File ""/home/tippit/tippit/tippit/tasks.py"", line 107, in reddit_onboarding
    message=message)
  File ""<decorator-gen-144>"", line 2, in send_message
  File ""/home/tippit/venv/lib/python3.5/site-packages/praw/decorators.py"", line 271, in wrap
    return function(*args, **kwargs)
  File ""<decorator-gen-143>"", line 2, in send_message
  File ""/home/tippit/venv/lib/python3.5/site-packages/praw/decorators.py"", line 177, in require_captcha
    return function(*args, **kwargs)
  File ""/home/tippit/venv/lib/python3.5/site-packages/praw/__init__.py"", line 2555, in send_message
    retry_on_error=False)
  File ""<decorator-gen-8>"", line 2, in request_json
  File ""/home/tippit/venv/lib/python3.5/site-packages/praw/decorators.py"", line 142, in raise_api_exceptions
    raise error_list[0]
praw.errors.PRAWException
```

When running this task by hand on the command line it prompted me for a Captcha, so I believe the issue is that Celery doesn't leave stdin open, or something similar. Unfortunately I haven't had the time to look into the exact cause, but I'm guessing the captcha decorator has raised an exception that was caught and improperly handled as an API exception.
"
591,Add lock and unlock for submissions,2016-02-26T22:30:10Z,2016-02-27T00:56:31Z,,,,"This adds `lock` and `unlock` to the Submission class.

Closes #557
"
590,Bump to 3.4.0.,2016-02-22T01:18:45Z,2016-02-22T01:30:22Z,,,,
589,Add support for sticky comments.,2016-02-06T22:42:28Z,2016-04-17T20:38:53Z,,,,"It is now possible for a mod to sticky his own top-level comment in a submission.
It would be great to add the functionality to PRAW.

I'm not familiar with the code yet; I might have a look at it and check whether I can implement something myself. I saw that the method to sticky a post is pretty simple.
"
588,Add support for the password grant_type when using oauth ,2016-02-03T17:27:22Z,2016-06-10T17:14:07Z,,,,"As described here https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example

Which will allow people writing scripts without user interaction and a single ore defined user to simply auth without jumping through hoops to get the needed tokens and hashes. 

Python example can be found near the bottom. 
"
587,Fix to _use_oauth not being reset,2016-02-02T05:01:11Z,2016-02-06T21:30:18Z,,,,"This addresses a problem when an exception is re-thrown while using oauth.

**Problem:**
If using oauth, and the number of retries is reached, it will not re-assign the value of `self._use_oauth` before rethrowing the exception.  

In the situation where the user catches this exception, it will then cause an assertion error in the `@restrict_access` decorator from then on, such as in [this issue](https://github.com/praw-dev/praw/issues/536).

**Solution:**
Have self._use_oauth re-set for every code path.
"
586,praw.multiprocess.ThreadingTCPServer.serve_forever can't be used in multiproessing on windows,2016-02-01T17:36:50Z,2016-06-30T05:27:23Z,⭐️ New Contributor Friendly ⭐️,_pickle.PicklingError,_pickle.PicklingError: Can't pickle <class '_thread.lock'>: attribute lookup lock on _thread failed,"On windows, the following traceback is given:

E: Whoops, it didn't copy paste. Copypasting again:

```
>>> from multiprocessing import Process
>>> from praw.multiprocess import *
>>> server = ThreadingTCPServer((""localhost"", 10103), RequestHandler)
>>> p = Process(target=server.serve_forever)
>>> p.start()
Traceback (most recent call last):
  File ""<pyshell#4>"", line 1, in <module>
    p.start()
  File ""C:\Program Files (x86)\Python\Python SDK 3.4.3\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\Program Files (x86)\Python\Python SDK 3.4.3\lib\multiprocessing\context.py"", line 212, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\Program Files (x86)\Python\Python SDK 3.4.3\lib\multiprocessing\context.py"", line 313, in _Popen
    return Popen(process_obj)
  File ""C:\Program Files (x86)\Python\Python SDK 3.4.3\lib\multiprocessing\popen_spawn_win32.py"", line 66, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Program Files (x86)\Python\Python SDK 3.4.3\lib\multiprocessing\reduction.py"", line 59, in dump
    ForkingPickler(file, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <class '_thread.lock'>: attribute lookup lock on _thread failed
>>> 
```

However on linux, it runs without a problem. No idea if it's a PRAW issue or not.
"
585,Add support for /randomrising,2016-02-01T13:42:12Z,2016-11-14T04:26:39Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"Reddit has a shuffled version of the /random controller [here](https://github.com/reddit/reddit/blob/718b7a6039a0a75ac3405b8d710c45abc1ea8bd4/r2/r2/controllers/listingcontroller.py#L695). Seems simple enough just to ""add a `ger_randomrising`"", but I don't know how a test would work for the front page (on subreddits it's simple since we'd just limit it to redditapitest or something similar, and posts don't move _that_ quickly there. However they may move far too quick to catch on the front page.
"
584,Adds support for newer options to set_settings,2016-01-25T21:47:53Z,2016-01-26T07:02:13Z,,,,"New settings added:
- ~~`modmail_email_address`: a string for modmail forwarding~~
- ~~`modmail_email_enabled`: a boolean to enable modmail forwarding~~
- `key_color`: a 6-digit hex color (I think it's currently unused, but it's always included with a subreddit's settings and triggering an ""extra settings fields"" warning)
"
583,Add revision method to get_wiki_page,2016-01-25T07:27:44Z,2016-02-04T06:39:07Z,,,,"Add methods revision (get a specific version of a wiki page) and revisions (list all versions of that wiki page) to wiki page

This resolves #566.
"
582,Forbidden and Captcha errors with using update_settings [ver 3.3.0],2016-01-25T03:42:11Z,2016-11-14T03:38:50Z,⭐️ New Contributor Friendly ⭐️,"praw.errors.InvalidCaptcha, praw.errors.Forbidden","praw.errors.InvalidCaptcha: `care to try these again?` on field `captcha`, praw.errors.Forbidden: HTTP error","I'm creating a new bot that updates the sidebar and wikipages of a subreddit.   The bot is a brand new account with no posts or karma.   It has `config` and `wiki` privileges to the subs I'm trying to update, but I was getting the same error when it originally had full permissions as well.  I can update the sidebar (both via wiki and settings) and update wiki pages with no issues via browser.  

Depending on the authentication used (cookie or oauth) and settings of the sub (private/public) the behavior and errors thrown are different.  

Environments tested:
1. Pycharm - Windows 8 x64 | python 3.4.2 | praw 3.3.0
2. Red Hat 4.4.6-4 x64 |  Python 3.3.2 | praw 3.3.0

I did manage to find a workaround, it's included at the bottom. 

---
### Error Table

| Authentication | Sub Setting | Error Raised | Edit Successful |
| --- | --- | --- | --- |
| cookie | public | Invalid Captcha | yes |
| cookie | private | Invalid Captcha | yes |
| oauth | public | Invalid Captcha | yes |
| oauth | private | Forbidden | no |

For clarity, yes the edit was successful, but it still threw an InvalidCaptcha error. 

---
### Test Code:

```
import praw
import logging

logging.basicConfig(level=logging.DEBUG)

r = praw.Reddit(USERAGENT, disable_warning=True)
# r.set_oauth_app_info(CLIENT_ID, CLIENT_SECRET, REDIRECT_URI)
# r.refresh_access_information(REFRESH_TOKEN)
r.login(USERNAME, PASSWORD, disable_warning=True)

r.update_settings(r.get_subreddit(PRIVATE_SUB), description=""test cookie - private"")
```

---
### InvalidCaptcha error:

```
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): api.reddit.com
DEBUG:requests.packages.urllib3.connectionpool:""POST /api/login/.json HTTP/1.1"" 200 None
DEBUG:requests.packages.urllib3.connectionpool:""GET /subreddits/mine/moderator/.json?limit=1024 HTTP/1.1"" 200 4654
DEBUG:requests.packages.urllib3.connectionpool:""GET /r/private_sub/about/edit/.json HTTP/1.1"" 200 2746
DEBUG:requests.packages.urllib3.connectionpool:""GET /r/private_sub/about/.json HTTP/1.1"" 200 3818
:0: UserWarning: Extra settings fields: dict_keys(['key_color', 'suggested_comment_sort'])
DEBUG:requests.packages.urllib3.connectionpool:""POST /api/site_admin/.json HTTP/1.1"" 200 None
Traceback (most recent call last):
  File ""C:/Users/MyName/PycharmProjects/MyProject/demo.py"", line 20, in <module>
    r.update_settings(r.get_subreddit(""private_sub""), description=""testing-please revert"")
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\__init__.py"", line 1707, in update_settings
    return self.set_settings(subreddit, **settings)
  File ""<decorator-gen-61>"", line 2, in set_settings
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\decorators.py"", line 268, in wrap
    return function(*args, **kwargs)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\__init__.py"", line 1633, in set_settings
    return self.request_json(self.config['site_admin'], data=data)
  File ""<decorator-gen-8>"", line 2, in request_json
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\decorators.py"", line 139, in raise_api_exceptions
    raise error_list[0]
praw.errors.InvalidCaptcha: `care to try these again?` on field `captcha`
```

---
### Forbidden Error

```
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): api.reddit.com
DEBUG:requests.packages.urllib3.connectionpool:""POST /api/v1/access_token/ HTTP/1.1"" 200 149
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): oauth.reddit.com
DEBUG:requests.packages.urllib3.connectionpool:""GET /api/v1/me.json HTTP/1.1"" 200 353
DEBUG:requests.packages.urllib3.connectionpool:""GET /r/private_sub/about/edit/.json HTTP/1.1"" 200 2746
DEBUG:requests.packages.urllib3.connectionpool:""GET /r/private_sub/about/.json HTTP/1.1"" 403 14
Traceback (most recent call last):
  File ""C:/Users/MyName/PycharmProjects/MyProject/demo.py"", line 19, in <module>
    r.update_settings(r.get_subreddit(""private_sub""), description=""test oauth - private"")
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\__init__.py"", line 1707, in update_settings
    return self.set_settings(subreddit, **settings)
  File ""<decorator-gen-61>"", line 2, in set_settings
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\decorators.py"", line 268, in wrap
    return function(*args, **kwargs)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\__init__.py"", line 1598, in set_settings
    data = {'sr': subreddit.fullname,
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\objects.py"", line 188, in fullname
    return '%s_%s' % (by_object[self.__class__], self.id)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\objects.py"", line 81, in __getattr__
    self._has_fetched = self._populate(None, True)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\objects.py"", line 157, in _populate
    json_dict = self._get_json_dict() if fetch else {}
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\objects.py"", line 150, in _get_json_dict
    self._info_url, params=params, as_objects=False)
  File ""<decorator-gen-8>"", line 2, in request_json
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\decorators.py"", line 113, in raise_api_exceptions
    return_value = function(*args, **kwargs)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\__init__.py"", line 612, in request_json
    retry_on_error=retry_on_error)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\__init__.py"", line 445, in _request
    _raise_response_exceptions(response)
  File ""C:\Users\MyName\.virtualenvs\MyProject\lib\site-packages\praw\internal.py"", line 205, in _raise_response_exceptions
    raise Forbidden(_raw=response)
praw.errors.Forbidden: HTTP error
```

---
### Workaround

I've been able to successfully edit the sidebar using `edit_wiki_page` function by pointing it to `config/sidebar`.  
"
581,[u'key_color'] warning at script execution,2016-01-21T15:37:27Z,2016-02-17T04:00:41Z,,,,"I saw a similar warning to this about a month ago with [u'suggested_comment_sort'] and ended up doing a PR to fix the issue but wasn't sure if the cause was the same here. I didn't see anything on /r/redditdev or in /r/beta about this so I'm not sure if it's new or not.

AFAIK nothing in my scripts has changed and PRAW version reports `3.3.0`

Error/Warning text: `:0: UserWarning: Extra settings fields: [u'key_color']`
"
580,Forbidden errors are silenced on SubmitMixin.submit,2016-01-21T04:50:28Z,2016-11-14T04:41:33Z,,,,"While it's good to have the error handling, it's not so good to just make it appear as if the error doesn't exist. Sure you could say that the user should throw in their own check, but for the sake of being user friendly, it would be nice if a warning could be thrown upon the forbidden error. Idea because of [this](http://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/cz65mig)
"
579,Support for Reddit Live?,2016-01-20T17:36:13Z,2016-11-14T05:50:33Z,Feature,,,"Wondering if PRAW would be looking to support Reddit Live threads.
"
578,submissions_between helper: workaround for reddit bug when sometimes submissions returned out of order,2016-01-16T19:04:55Z,2016-01-16T21:44:16Z,,,,"Found these two bugs today: 
https://www.reddit.com/r/bugs/comments/419f4s/sometimes_search_with_timestampxy_misses/

https://www.reddit.com/r/bugs/comments/4194h0/sometimes_search_with_timestampxy_returns/

This should make it work slightly better.
"
577,Stack overflow when invalid subreddit fetched with get_subreddit,2016-01-10T18:17:14Z,2016-11-14T03:33:30Z,"Bug, ⭐️ New Contributor Friendly ⭐️",,,"Specifically, it happens when there is a question mark at the end of the name.  For example:

```
r.get_subreddit('arrow?',fetch=True)
```

Some simple fixes would be to remove the question mark or raise a NotFound exception, but there may be a better solution. 

See [here](http://pastebin.com/pNTCXrxK) for the error log and [this reddit thread](https://www.reddit.com/r/redditdev/comments/3zrwso) for discussion.
"
576,Add 'modconfig' scope decorator to get_traffic,2016-01-08T19:47:53Z,2016-01-08T20:30:01Z,,,,"In #575, I could not determine what scope was meant to be used with the `/r/subreddit/about/traffic` endpoint because it was undocumented. However, [/u/Developx](https://reddit.com/u/developx) has [shown me](https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/cyp9s2o) that `modconfig` is the correct scope for this endpoint. This PR adds the right restrict_access decorator.

The test has also been updated to use a private subreddit to prove the scope is correct.
"
575,Add method get_traffic,2016-01-05T07:14:45Z,2016-01-06T17:57:13Z,,,,"This is a result of [this comment thread](https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/cylpuot).

This adds `get_traffic` to the UnauthenticatedReddit mixin and the Subreddit object. The endpoint is not listed on reddit's API doc page, and I couldn't determine if it's meant to be used with a particular oauth scope. The method works without any scope requirements as long as the traffic stats are public.

The endpoint returns traffic stats for days, hours, and months in the form:

`[[timestamp, uniques, pageviews, subscribers], ...]`

For now it's just returning the json dictionary. We could do something fancy with it, but I don't really think it's necessary.
"
574,"Clean up ""Call and Response Bot"" tutorial",2016-01-04T20:15:15Z,2016-01-04T21:44:46Z,,,,"Pull request to add the tutorial was submitted prematurely. Mostly minor copyedits, some changes to the presented code.
"
573,TypeError with add_subreddit in Python 3,2016-01-03T12:50:11Z,2016-06-15T05:37:01Z,,TypeError,"TypeError: sequence item 0: expected bytes, bytearray, or an object with the buffer interface, NoneType found","``` python
multi = r.create_multireddit(
    'mymulti',
    visibility='private',
    overwrite=True
)

multi.add_subreddit('france')
```

Using `python3`, I get this stacktrace:

```
Traceback (most recent call last):
  File ""bot.py"", line 97, in <module>
    multi.add_subreddit('france')
  File ""<decorator-gen-292>"", line 2, in add_subreddit
  File ""/usr/lib/python3.4/site-packages/praw/decorators.py"", line 268, in wrap
    return function(*args, **kwargs)
  File ""/usr/lib/python3.4/site-packages/praw/objects.py"", line 1655, in add_subreddit
    *args, **kwargs)
  File ""<decorator-gen-7>"", line 2, in request
  File ""/usr/lib/python3.4/site-packages/praw/decorators.py"", line 113, in raise_api_exceptions
    return_value = function(*args, **kwargs)
  File ""/usr/lib/python3.4/site-packages/praw/__init__.py"", line 593, in request
    retry_on_error=retry_on_error, method=method)
  File ""/usr/lib/python3.4/site-packages/praw/__init__.py"", line 444, in _request
    response = handle_redirect()
  File ""/usr/lib/python3.4/site-packages/praw/__init__.py"", line 425, in handle_redirect
    verify=self.http.validate_certs, **kwargs)
  File ""/usr/lib/python3.4/site-packages/praw/handlers.py"", line 136, in wrapped
    return function(cls, **kwargs)
  File ""/usr/lib/python3.4/site-packages/praw/handlers.py"", line 57, in wrapped
    return function(cls, **kwargs)
  File ""/usr/lib/python3.4/site-packages/praw/handlers.py"", line 103, in request
    allow_redirects=False, verify=verify)
  File ""/usr/lib/python3.4/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python3.4/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/usr/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/usr/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 349, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib64/python3.4/http/client.py"", line 1090, in request
    self._send_request(method, url, body, headers)
  File ""/usr/lib64/python3.4/http/client.py"", line 1123, in _send_request
    self.putheader(hdr, value)
  File ""/usr/lib64/python3.4/http/client.py"", line 1069, in putheader
    value = b'\r\n\t'.join(values)
TypeError: sequence item 0: expected bytes, bytearray, or an object with the buffer interface, NoneType found
```

It's working with python2.

```
$ python --version
Python 2.7.10
$ python3 --version
Python 3.4.2
```

According to [the doc](https://praw.readthedocs.org/en/stable/pages/code_overview.html?highlight=multi#praw.objects.Multireddit.add_subreddit), it does look like a bug.
"
572,Add call_and_response_bot page to doc index.,2015-12-23T06:32:08Z,2015-12-23T16:17:21Z,,,,"Also provide some documentation cleanup.
"
571,Added documentation for building simple 'call and response' comment monitoring bots.,2015-12-23T00:01:13Z,2015-12-23T06:21:41Z,,,,"Per this suggestion from /u/bboe: https://www.reddit.com/r/redditdev/comments/3xkey8/how_does_reddit_bots_monitor_new_comments_on/cy67oqi?context=3
"
570,Hide debug ouput on travis for submissions_between,2015-12-21T05:21:20Z,2015-12-21T15:40:38Z,,,,"The test on travis for submissions_between is putting out debug outputs at a level of 3 and less. While great for live changes and seeing what goes wrong when making them; on travis it just kinda spams the coverage log. So; on travis; this PR makes the debug ouputs hidden.
"
569,adding support for suggested_comment_sort,2015-12-21T04:24:05Z,2015-12-21T06:06:40Z,,,,"This adds support for suggested_comment_sort in the set_settings() method.
"
568,"""pip install"" results in nonfunctional praw",2015-12-20T16:22:34Z,2015-12-21T18:13:43Z,,Exception,"Exception: Could not find config file in any of: ['/usr/local/lib/python3.4/site-packages/praw-3.3.0-py3.4.egg/praw/praw.ini', '/Users/llimllib/.config/praw.ini', 'praw.ini']","When I ""pip install praw"", then ""import praw"", I get:

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<frozen importlib._bootstrap>"", line 2237, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 2226, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1191, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1161, in _load_backward_compatible
  File ""/usr/local/lib/python3.4/site-packages/praw-3.3.0-py3.4.egg/praw/__init__.py"", line 40, in <module>
  File ""<frozen importlib._bootstrap>"", line 2237, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 2226, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1191, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 1161, in _load_backward_compatible
  File ""/usr/local/lib/python3.4/site-packages/praw-3.3.0-py3.4.egg/praw/settings.py"", line 47, in <module>
  File ""/usr/local/lib/python3.4/site-packages/praw-3.3.0-py3.4.egg/praw/settings.py"", line 45, in _load_configuration
Exception: Could not find config file in any of: ['/usr/local/lib/python3.4/site-packages/praw-3.3.0-py3.4.egg/praw/praw.ini', '/Users/llimllib/.config/praw.ini', 'praw.ini']
```

Steps to reproduce:
1. `pip install praw`
2. `python`
3. `import praw`
4. see error

When I open up the egg that pip installed, I can see that there is in fact no praw.ini included. Here are its contents:

```
$ unzip prawtools-0.19-py3.4.egg
Archive:  prawtools-0.19-py3.4.egg
  inflating: EGG-INFO/dependency_links.txt
  inflating: EGG-INFO/entry_points.txt
  inflating: EGG-INFO/pbr.json
  inflating: EGG-INFO/PKG-INFO
  inflating: EGG-INFO/requires.txt
  inflating: EGG-INFO/SOURCES.txt
  inflating: EGG-INFO/top_level.txt
  inflating: EGG-INFO/zip-safe
  inflating: prawtools/__init__.py
  inflating: prawtools/alert.py
  inflating: prawtools/helpers.py
  inflating: prawtools/mod.py
  inflating: prawtools/stats.py
```

Surely a praw.ini should come in it so that `import praw` works?

python 3.4.3, mac
"
567,Update useful_scripts.rst,2015-12-18T03:24:53Z,2015-12-18T03:37:18Z,,,,"added my subtitle bot to the list.
"
566,Add version argument to get_wiki_page,2015-12-17T01:42:12Z,2016-11-14T07:28:50Z,Feature,,,"Since (I think) bfe90aa18a092449da504770eb1ecef3f03cbe11, it seems I can no longer get a specific version of a wiki page by passing `v=` to `get_wiki_page` since it doesn't pass kwargs along to the request as it did before.
"
565,#562 Cleanup,2015-12-16T04:56:56Z,2015-12-16T05:05:02Z,,,,"Closes #562.
"
564, [u'suggested_comment_sort'] error at the start of script execution,2015-12-15T21:56:09Z,2015-12-21T06:16:17Z,⭐️ New Contributor Friendly ⭐️,,,"I have a script that runs on a 5 minute CRON, each time the script begins to execute at the top of the log I see:

`UserWarning: Extra settings fields: [u'suggested_comment_sort']`

I did a codebase search on this code and don't see anything regarding that exact field or `suggested_comment_sort` at all. This seems to have started when PRAW was upgraded to 3.3.0. Currently the machine is running Python version 2.7.6. I didn't see any closed issues on this and nothing really turned up in the Google searches.

[This gist](https://gist.github.com/caleywoods/4325fea69998441b5f27) contains the main file of the script.
"
563,Implement __hash__ for RedditContentObject,2015-12-15T21:20:22Z,2015-12-15T23:23:00Z,,,,"Hi, I need this for tests for #554 (I'd like to be able to create sets of submissions in tests). 

Right now there is `__eq__` implemented but `__hash__` has the default implementation, which means that code like this works weird:

```
>>> s1 = list(r.get_subreddit('redditdev').get_hot())[0]
>>> s2 = list(r.get_subreddit('redditdev').get_hot())[0]
>>> s1 == s2
True
>>> hash(s1) == hash(s2)
False
```

This may cause issues for people using submissions in `set`s/`dict`s, so I think `__hash__` should be defined for consistency. 
"
562,Add MIN_PNG_SIZE and MIN_JPEG_SIZE to allow for images below 128 bytes,2015-12-15T01:27:08Z,2015-12-16T05:05:02Z,,,,"See https://github.com/praw-dev/praw/issues/561
"
561,MIN_IMAGE_SIZE causes exception with valid images,2015-12-14T22:43:22Z,2015-12-16T05:06:38Z,,,,"Not sure why the 128 byte minimum size is enforced [here](https://github.com/praw-dev/praw/blob/d0893c1d0320f5301d847fc70afe2c940fecf1b4/praw/__init__.py#L1690-L1691) - I have a valid image that's 93 bytes in size which is raising the errors.ClientException exception.

Is there a reason behind the 128 byte minimum? 
"
560,Multiprocess and Cache Problem,2015-12-01T08:53:53Z,2015-12-02T00:13:53Z,,,,"I noticed that running a pool of two or more processes using the comment stream generator (collecting all comments from /all, and massive string matching), the cache will not hit when the second process requests the same GET.  

I put a delay timer on the second process, and once it caught up to the first process (cache hit to get there), both (whichever was behind) started ignoring the cache.  This results in two identical GET requests, limiting the IPS in half. Some notes:  The second process doesn't post a GET request immediately after the first (as observed on console).  The repeat GET on the console of the praw-multi server doesn't appear until a second or two after, not sure if there is actually nano seconds gap, so it misses the cache .  

My thoughts/solutions would be to impose a delay on the second process to give it more of a gap between the first process, or implementing the yield of the stream generator as a requestable object similar to the multi-server (removing the generator call from the subprocesses), or somehow making the yield a shared object between processes using Manager() (both beyond my current knowledge, but I'll do what it takes!).

http://imgur.com/qul5YKT

Any help would be hugely appreciated!  
"
559,Update changelog and fix other documentation references.,2015-11-30T01:38:04Z,2015-11-30T01:40:22Z,,,,
558,Add SelfDestructBot,2015-11-29T17:01:38Z,2015-11-30T17:46:28Z,,,,
557,Can't lock a submission? ,2015-11-29T15:41:09Z,2016-02-27T00:56:31Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"I can only see the status while logged in on a subreddit I mod but can't lock the thread? Am I missing something?
"
556,Allow sr quarantine optins and optouts,2015-11-28T18:18:05Z,2016-12-18T23:46:21Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"Allow opting into and opting out of quarantined subreddits.

reddit/reddit#1482

I'd make a PR, but I've no idea what's the best way to handle this: It works successfully, but it raises a redirect exception because the controller redirects to the subreddit or the front page for optins and optouts respectively.

Edit: Maybe a keyword arguement to `handle_redirect` which would be a list of tuples of expected redirects, ex

```
`[('https://api.reddit.com/api/quarantine_optin.json', 'https://api.reddit.com/r/{subreddit}.json'.format(subreddit=subreddit))]`
```
"
555,"""Edit on github"" in readthedocs redirects to a 404",2015-11-28T18:02:35Z,2016-11-14T01:36:23Z,Documentation,,,"Title
"
554,Add helper for downloading all submissions in a subreddit between two timestamps (Implements: #337),2015-11-24T23:02:14Z,2015-12-18T21:31:06Z,,,,"Hi, I've used this trick quite a lot, so I took some time and implemented a helper for praw. I hope you'll find it useful! 

1) Reddit seems to use `created` attribute for ""timestamp"" queries. `created` seems to be a sort of broken timestamp. I think this is very confusing, and I always forget to convert time.time() output to ""reddit timestamp"". That's why I chose to make `highest_timestamp` and `lowest_timestamp` to be normal unix timestamp(even though it is inconsistent with `r.search`)

2) I am not a native speaker, so sorry in advance for possible grammatical errors in the docstring. 

3) The basic algorithm is fairly simple, it just slides a dynamic length window over reddit timestamps and tries to keep 30..99 results within the window. Whenever this condition breaks, it increases or decreases the window length by a factor of 2x. I never really bothered to optimize these parameters, they are good enough for me. Maybe it is a good idea to expose them as function parameters. 

4) Naming! TBH, I am not sure if `all_submissions` is a good name. 

Any feedback is appereciated!
"
553,Partially fix the praw multiprocess issues,2015-11-23T05:39:31Z,2015-12-27T05:36:19Z,,,,"Simple, retval = e instead of except Exception as retval.
"
552,Patch rare refresh exception,2015-11-23T04:18:06Z,2016-07-16T18:32:15Z,,,,"Resolves a rare refresh exception upon refreshing extremely unique comments, as well as stops the InvalidComment exception from occuring when a `Comment`'s `replies` property is called and it's replies weren't previously fetched.

~~I still want to add 3 tests (assertRaises on get_info and assertWarningsRegexp to ensure that there are keys that can not be updated, such as `was_comment`, in both the deleted and the removed case), but I thought I'd make a PR now to discuss caveats.~~

This patch _heavily_ relies on `get_info`, but `get_info` just couldn't get the job done properly in some cases because of the fact that there was no way to burst the cache via the `uniq` parameter, since there were no args or keywordarguments that could be passed.  Also, `get_info` had a bug regarding it's use. The `limit` paramter actually _can_ be used when using thing_ids, and as weird as it sounds, it does make sense (assume that the user's default preference is 25 items per listing, and they use `get_info` with 37 valid ids, to actually get all 37 they'd need to pass the limit parameter  as None or 37). With the above and the fact that it was inconsistent with _all_ other casual functions that retrieve from reddit's Listing json kind, I altered the functionality to return a `get_content` generator instead, allow for args and kwargs to be passed on to it, and stop the faulty restriction with the limit param, and altered the tests to fit (but now looking at it I could have alterred them better, so I'll do that).

Also, I am not sure if the warnings would be pushed through a second time if they are called upon a second time (I don't know how the warnings module works to that extent).

I had to update the betamax version used for tests, because sometime between 0.4.2 and 0.5.1 they had a faulty release that broke all tests. I know I could have downgraded to 0.4.2, but I thought upgrading would have been better.

Edit: I've added the tests that I wanted to add.
"
551,Use inspect.signature() for Python3 and up,2015-11-22T17:09:28Z,2015-11-30T00:21:40Z,,,,"`inspect.getargspec()` is soon to be deprecated in favor of `inspect.signature()`.

`inspect.signature` was not backported for Python2, so we will keep using `inspect.getargspec()` for that.

Resolves #541
"
550,Add support for getting default subreddits,2015-11-21T21:52:41Z,2015-11-22T22:45:50Z,,,,"Hi, [reddit has support for getting default subreddits for a few months](https://www.reddit.com/r/redditdev/comments/35w2di/new_feature_subredditsdefaultjson). I needed to retrieve the list of default subreddits today, however I noticed that praw doesn't support this. So I decide to add `get_default_subreddits()` to the `UnauthenticatedReddit`.

I also think that `get_default_subreddits()` should return the whole list of default subreddits(and not just 25) to avoid confusion, so it checks if `limit` is passed in args or kwargs(which looks kinda ugly) -- maybe there is a better way(e.g. just making limit the first argument of  `get_default_subreddits()`)
"
549,Utilize Report Message,2015-11-21T18:23:55Z,2015-12-16T05:14:56Z,,,,"When a Post/Comment is reported, I want to be able to include the report text as a message to the mods.

This seems like something fairly simple to add in?
"
548,TypeError on login: cannot make memory view,2015-11-17T19:58:59Z,2016-07-17T05:24:42Z,,TypeError,TypeError: cannot make memory view because object does not have the buffer interface,"I can no longer login using PRAW 3.3.0.

Using username/password:

```
Traceback (most recent call last):
  File ""countdown.py"", line 25, in update_countdown
    reddit.login(username, password)
  File ""<decorator-gen-49>"", line 2, in login
  File ""/usr/lib/python2.7/site-packages/praw/decorators.py"", line 75, in wrap
    return function(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 1418, in login
    self.request_json(self.config['login'], data=data)
  File ""<decorator-gen-8>"", line 2, in request_json
  File ""/usr/lib/python2.7/site-packages/praw/decorators.py"", line 113, in raise_api_exceptions
    return_value = function(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 612, in request_json
    retry_on_error=retry_on_error)
```

Using OAuth:

```
Traceback (most recent call last):
  File ""countdown.py"", line 25, in update_countdown
    reddit.refresh_access_information(access_info)
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 1311, in get_access_information
    retval = super(AuthenticatedReddit, self).get_access_information(code)
  File ""<decorator-gen-9>"", line 2, in get_access_information
  File ""/usr/lib/python2.7/site-packages/praw/decorators.py"", line 285, in require_oauth
    return function(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 675, in get_access_information
    retval = self._handle_oauth_request(data)
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 650, in _handle_oauth_request
    response = self._request(url, auth=auth, data=data, raw_response=True)
```

The rest of the traceback is common to both methods:

```
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 444, in _request
    response = handle_redirect()
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 425, in handle_redirect
    verify=self.http.validate_certs, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/handlers.py"", line 136, in wrapped
    return function(cls, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/handlers.py"", line 57, in wrapped
    return function(cls, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/handlers.py"", line 103, in request
    allow_redirects=False, verify=verify)
  File ""/usr/lib/python2.7/site-packages/requests/sessions.py"", line 576, in send
    r = adapter.send(request, **kwargs)
  File ""/usr/lib/python2.7/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/usr/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 559, in urlopen
    body=body, headers=headers)
  File ""/usr/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py"", line 353, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File ""/usr/lib64/python2.7/httplib.py"", line 1053, in request
    self._send_request(method, url, body, headers)
  File ""/usr/lib64/python2.7/httplib.py"", line 1093, in _send_request
    self.endheaders(body)
  File ""/usr/lib64/python2.7/httplib.py"", line 1049, in endheaders
    self._send_output(message_body)
  File ""/usr/lib64/python2.7/httplib.py"", line 893, in _send_output
    self.send(msg)
  File ""/usr/lib64/python2.7/httplib.py"", line 869, in send
    self.sock.sendall(data)
  File ""/usr/lib/python2.7/site-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 216, in sendall
    data = memoryview(data)
TypeError: cannot make memory view because object does not have the buffer interface
```
"
547,Fix pep257 issues.,2015-11-15T19:34:23Z,2015-11-15T19:44:26Z,,,,
546,Pretty cassettes,2015-11-15T19:18:57Z,2015-11-15T19:54:54Z,,,,"This change gives us the ability to easily compare updated cassettes to ensure that only expected modifications are contained within the cassettes.

For instance, assume a change introduces an additional unnecessary request in addition to adding a feature. The additional request previously would likely go unnoticed when the cassette is updated. With this change, we can scrutinize the cassette diffs in addition to the code.
"
545,Post.replace_more_comments() function does not work with oauth,2015-11-07T01:52:44Z,2016-11-13T19:19:20Z,,,,"This function returns 403 forbidden errors, despite oauth verification having `read` in the scope.
"
544,Voting should return a client exception if the given post / comment is archived,2015-11-04T22:23:57Z,2016-11-13T19:17:16Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"As shown by reddit/reddit#1455 , reddit does not return any form of error, and even browser side it updates as if the vote has gone through. However in reality, the vote is rejected server side, and as such, it doesn't count. For the sake of minimizing the confusion done on reddit's end, it may be good to throw in a simple `if self.archived: raise praw.errors.ClientException('stuff here')`. I'd write up the PR but if I'm recalling correctly tests will still fail because betamax has updated; so I'm just leaving this here as a reminder.
"
543,Support `*` OAuth scope (#471),2015-10-24T13:44:26Z,2015-11-22T04:58:05Z,,,,"I am unfamiliar with PRAWs testsuite, so apologies for not providing a testcase.
If anyone else could do that, that'd be welcome.

Here's a script to help with that and get a `""scope"": ""*""` response, according to the [reddit Quick Start example](https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example):

``` py
from __future__ import unicode_literals, print_function
import praw
import six 

@praw.decorators.require_oauth
def get_bearer_access(self, username, password):
    data = {'grant_type': 'password',
            'username': username,
            'password': password}
    retval = self._handle_oauth_request(data)
    # scope is '*' here
    #print(retval)
    return {'access_token': retval['access_token'],
            'scope': set(retval['scope'].split(' '))}

@property
def has_oauth_app_info(self):
    return all((self.client_id is not None,
                self.client_secret is not None))

def set_oauth_app_info(self, client_id, client_secret):
    self.client_id = client_id
    self.client_secret = client_secret

praw.OAuth2Reddit.get_bearer_access = get_bearer_access
praw.OAuth2Reddit.has_oauth_app_info = has_oauth_app_info
praw.OAuth2Reddit.set_oauth_app_info = set_oauth_app_info


def has_scope(self, scope):
    """"""Return True if OAuth2 authorized for the passed in scope(s).""""""
    if '*' in self._authentication:
        return True
    if isinstance(scope, six.string_types):
        scope = [scope]
    return self.is_oauth_session() and all(s in self._authentication
                                            for s in scope)

praw.AuthenticatedReddit.has_scope = has_scope

#
# test
#

client_id = ''
client_secret = ''
username = ''
password = ''
user_agent = 'praw testing'

session = praw.Reddit(user_agent)
session.set_oauth_app_info(client_id, client_secret)
if session.has_oauth_app_info:
    access = session.get_bearer_access(username, password)
    session.set_access_credentials(**access)


print('oauthed:', session.is_oauth_session())
print('scope check identity:', session.has_scope('identity'))
print('oauth user:', session.get_me())
```
"
542,Replace all %s with .format; break up dense lines,2015-10-15T06:10:09Z,2015-11-22T04:50:32Z,,,,"All uses of %s and %d have been replaced with `.format`. API paths always use keywords, but miscellaneous messages are just fine with the `{0}` style.

Unfortunately, this made a lot of places feel cramped, especially the mixin methods. I decided to move a lot of the URL formatting onto their own lines instead of trying to keep a hole-in-one. 79 characters is too small for most of these :frowning: 

For two of the API paths, I had to break them into separate lines and get some parentheses involved. It looks a little funky, but it was the only way to stop flake8 from complaining about my visual indent without putting the continuation line too far to the left.

Please review thoroughly if you can. After staring at the word ""format"" for so long it's hard to proofread this myself.

Thanks!

&nbsp;

Edit: Woah, what happened to Travis? All tests are passing for me locally. Is the CI suite using some new parameters that make the requests not match?
"
541,inspect.getargspec() is deprecated and will be removed in Python 3.6,2015-10-07T13:22:47Z,2015-11-30T00:21:40Z,⭐️ New Contributor Friendly ⭐️,,,"When using PRAW with the recently released Python 3.5 a warning shows:

```
DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead
```

[Python's documentation](https://docs.python.org/3/library/inspect.html#inspect.getargspec) on `inspect.getargspec()` says:

> Deprecated since version 3.0: Use signature() and Signature Object, which provide a better introspecting API for callables. This function will be removed in Python 3.6.

In the file [praw/decorators.py](https://github.com/praw-dev/praw/blob/master/praw/decorators.py) the function `inspect.getargspec()` is used twice, to get a list of a function arguments' names, both lines are identical:

```
func_args = inspect.getargspec(function).args
```

This can easily be made compatible with Python 3.6 by changing the two lines to:

```
func_args = list(inspect.signature(function).parameters.keys())
```

which, to my knowledge, produces the exact same list of arguments' names so it _should_ work with the rest of the code.

**But**, the problem is that the `inspect.signature()` function, added in Python 3.0, wasn't backported to Python 2.7 and thus isn't supported by Python 2.

That's the limit of my skills I'm afraid. I'm not a programmer experienced enough to know what to do in such a case, I don't know how PRAW can be made compatible with both python 2.7 and 3.6, so instead of a pull request I can only open a new issue.
"
540,Add support for modmail muting,2015-10-06T02:18:58Z,2015-10-06T06:01:01Z,,,,"Adds features necessary for modmail muting:
- `Subreddit.add_mute` and `Subreddit.remove_mute` for muting arbitrary redditors (uses the /friend endpoint)
- `Message.mute_modmail_author` and `Message.unmute_modmail_author` for muting the sender of a particular message (uses dedicated endpoints)
- `Subreddit.get_muted` (via ModOnlyMixin)

~~**Note!** The list returned from `Subreddit.get_muted` is of the form `[{'id': 't2_6c1xj', 'date': 1444096681.0, 'name': 'PyAPITestUser3'}]`, as opposed to the usual `[Redditor(user_name='PyAPITestUser3')]`.~~
"
539,new RateLimitHandler that takes care of OAuth,2015-10-02T09:20:49Z,2015-10-02T10:03:32Z,,,,"This handler has a builtin dispatch timer to send requests based on the API Limitations, which increases the possible request timing per OAuth sessions.

Old: max 30r / ip

New: max 30r / ip unauthed AND 60r/oauthed session.

This is done by reading the requests headers and registering the oauth refresh tokens.
"
538,Allow lists or space-delimited strings as scope,2015-09-29T18:32:41Z,2015-09-30T16:02:32Z,,,,"Allowing the use of a normal `list` or space-delimited string as the `scope` parameter to `set_access_credentials` could possibly reduce some boilerplate code in a lot of applications.

This also goes in line with the OAuth API, which responds with scopes in a space-separated string, too.
"
537,Added error to documentation of refresh(self),2015-09-26T20:53:21Z,2015-09-30T06:35:31Z,,,,"Took first step in addressing comments[0] index error with refresh method by adding to documentation.
"
536,AssertionError when using OAuth2,2015-09-26T15:55:05Z,2015-09-26T19:19:32Z,,,,"I'm not entirely sure what's happening and whether it's expected behavior here.

When I run the example program at the bottom of this message _without_ logging in, everything runs just fine.

However, when I add the OAuth2 login code, I get the following error:

``` pytb
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""c:\users\ekaterina\appdata\local\programs\python\python35\Lib\threading.py"", line 923, in _bootstrap_inner
    self.run()
  File ""C:/Users/Ekaterina/.PyCharm40/config/scratches/scratch"", line 30, in run
    list(self.r.get_comments('all'))
  File ""<decorator-gen-13>"", line 2, in get_comments
  File ""C:\Users\Ekaterina\Envs\distance\lib\site-packages\praw\decorators.py"", line 247, in wrap
    assert not obj._use_oauth  # pylint: disable=W0212
AssertionError
```

I tried removing the `assert` statement and the program seems to run fine then.

Here is the example program:

``` python
from threading import Thread
import time
import praw
from constants import *


class ChildThread(Thread):
    def __init__(self, r):
        super().__init__()
        self.r = r

    def run(self):
        while True:
            list(self.r.get_front_page())
            time.sleep(5)


class MainThread(Thread):
    def __init__(self):
        super().__init__()
        self.r = praw.Reddit(USER_AGENT)

        # problematic part
        self.r.set_oauth_app_info(CLIENT_ID, SECRET, REDIRECT_URI)
        self.r.refresh_access_information(REFRESH_TOKEN)
        # end

        self.child = ChildThread(self.r)
        self.child.daemon = True
        self.child.start()

    def run(self):
        while True:
            list(self.r.get_comments('all'))
            time.sleep(5)


if __name__ == '__main__':
    main = MainThread()
    main.start()
```
"
535,More info on #519 + inconsistency with the refresh method on comments due to reddit behavior,2015-09-26T05:05:26Z,2016-11-14T06:04:49Z,,,,"Calling @d-soni and @kviktor, I finally had the time to deduce the cause. Yes, this is an actual issue, and I somewhat give the necessary requirements to reproduce in my explanation.

First, I'd like to address the cause of #519, and @d-soni 's PR. For #519 to occur would actually be extremely rare merely due to the potential minimum of how short of a time things have to occur.

For the remainder of this issue for the sake of simplicity, when I refer to ""c"", I mean a comment made by the user ""13steinj"" on a subreddit where ""13steinj"" is not a mod. The most important part about c is that it has no replies whatsoever.

If I get c, either via the user profile or through the submission, or however I may get it really, I get the comment object as I would expect to do so. However, when refreshing a comment, praw calls upon the submission's `comments[0]` to refresh it. But, this causes the problem.

In the case that the comment was originally retrieved from a submission, in between the original retrieval and the refresh, the comment was either deleted, or removed by a mod.

In the case that I get it from somewhere else, it was either removed / deleted within that time, _or_ simply, it was removed from the start, but some endpoints still show the comment as need be (such as the listings on the user page), but the refresh method calls it from the submission , which leaves it at the possibility of either being deleted/removed or already being deleted/removed. This is reddit behavior, so not much praw can do here without modifying results based on certain factors, which would defy this behavior.

Remember when I said c has no replies? That's the big deal. When a deleted comment has no replies, or has replies but those replies are removed in a certain order, the comment is both not shown in the browser, and, is not shown in the list in the json of the submission. Whereas the usually working comments[0] refresh method would work, it would not here, since the list is empty, and this is where the index error lies. Rare, yes. Possible, definitely.

Now, here comes a separate problem, which is fixing this. If we make it raise an error, a lot of scripts would get an unexpected error. If possible, I say make it run a warning along the lines of the comment could not be refreshed, and then just return the original comment, or if in ""strict"" mode (which would need to be added), it would form a pseudo deleted / removed comment object, but it wouldn't be the same as reddits, since we can't actually tell if it was deleted or removed without another request on the user page.

That said. Here comes in the inconsistency due to reddit, that we can't really control. If we get a comment not from the submission itself but rather the user page or a different endpoint, is it being refreshed as it should?

Think about it. Since it could be completely different on the endpoint used than the submission, due to both this rare occurrence, and merely, comments that can be detected as removed, in one place, it could be different than all others, yet we refresh only from that one place. Maybe (if possible) we should detect where the comment comes from, and then refresh it from where it comes from? Though, I don't think doing this is feasible, so at a minimum the docs should be updated to reflect this.

I'm sorry if some parts of this / all of it make no sense due to my use of terminology / semantics / grammar, it's 1 am here. If anyone has any questions I'll try my best to explain further, in the morning.
"
534,Fix AssertionError when hiding with OAuth,2015-09-25T05:02:39Z,2015-09-30T06:40:33Z,,,,"Brought to my attention by /u/boib [here](https://www.reddit.com/r/redditdev/comments/3m8ya2/praw_v321_crash_on_unhide_using_oauth2/). This happens whenever a @restrict_access method points to another one.

Also, while writing `test_hide_oauth` I found an issue where PRAW tried to evict the /user/name/hidden page which doesn't work when `r.user` is None, so that's all fixed too.

&nbsp;

Bryce -- I didn't realize how big the changelog has gotten. There's a couple PRs I would like to see merged, and after that I think it's a good time to release 3.3.0, is there anything else you'd like to do first? 

Thanks
"
533,Remove unsupported Python versions from Readme,2015-09-23T08:07:41Z,2015-09-25T09:12:22Z,,,,"I missed these from my other pull request
"
532,Add python 3.5,2015-09-21T12:21:03Z,2015-09-22T23:46:48Z,,,,"Also allow failure on Python 3.2 as Coverage 4.0 no longer supports
Python 3.2
"
531,Mention token autorefresh in oauth docs,2015-09-21T06:39:22Z,2015-09-22T23:49:07Z,,,,"Brought to my attention by /u/avinassh. I think this is a good place to add it in.
"
530,coveralls no longer has Python 3.2 support,2015-09-20T20:45:46Z,2015-09-25T06:04:57Z,,,,"If you look at the PyPi page, [there is no more support for Python 3.2](https://pypi.python.org/pypi/coveralls). ~~Why, I don't know.~~ Because Coverage 4 has no support for Python 3.2. Why that is, I don't know.

So, either code can't be tested for 3.2, or travis needs to be configured to use an older version (if that still works) for Python 3.2 alone.
"
529,Updated RedditRovers description (from MassdropBot),2015-09-20T15:55:28Z,2015-09-25T05:11:10Z,,,,"My bot shifted in name, focus and usability. I wanted to reflect that, since it gets a share amount of views from PRAWs rtd.
"
528,Add `prawoauth2` and `Goodreads Bot`,2015-09-20T14:16:29Z,2015-09-21T06:41:08Z,,,,"I wrote [`prawoauth2`](http://prawoauth2.readthedocs.org) which helps people writing Reddit bots/apps using OAuth2 super easy and simple. Since it's more like a tool for praw (and not a reddit bot), I have added it next to `PRAW Tools` in list.

And I also added [Goodreads Bot](https://github.com/avinassh/Reddit-GoodReads-Bot).

:beers: 
"
527,"OAuth password for ""installed app"" type",2015-09-20T08:53:29Z,2015-09-30T06:29:01Z,,,,"""installed app"" type oauth applications don't have client secrets. For these applications, the Reddit docs specify that the secret should be passed though as an empty string.

> https://github.com/reddit/reddit/wiki/OAuth2
> The ""user"" is the client_id. The ""password"" for confidential clients is the client_secret. The ""password"" for non-confidential clients (installed apps) is an empty string. 

This pull request allows the oauth client_secret to remain empty by differentiating between an unset value (None) and a deliberate empty string. You can currently get around this limitation by setting the secret to some non-empty string like ""placeholder"", but I would rather follow the official documentation.
"
526,Add support for Subreddit-owned multireddits Multireddits,2015-09-14T17:40:14Z,2016-11-14T05:53:30Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"<!-- https://github.com/praw-dev/praw/issues/new -->

Multireddits can be created / owned by subreddits, however PRAW does not appear to support it.  They can't be created on the site, but it _can_ be made via the API, after which they behave as regular multis.  (An example of one: [/r/pokechu22/m/test](https://www.reddit.com/r/pokechu22/m/test))

Apart from [1 undocumented endpoint](https://github.com/reddit/reddit/blob/3693e17d15ed1bb514d315ffeadeeb582598898e/r2/r2/controllers/multi.py#L112-L120), everything about it works the same as the existing system, except that the multipath is `/r/subreddit/m/multi` instead of `/user/username/m/multi`.  (The undocumented endpoint is simply `/api/multi/r/subreddit` and behaves the same as `/api/multi/user/username`).

Subreddit multireddits can only be created and edited by moderators of the subreddit that have the `config` permission.  It still uses the oauth scope `subscribe` (that may be changed, however).

As a side note, I am [working on a PR for the main site](https://github.com/pokechu22/reddit/tree/fix-subreddit-multireddits) that adds a way to create subreddit multireddits visually, which is why they are on my mind.
"
525,history is needed for get_comments,2015-09-14T11:32:05Z,2015-12-15T02:12:59Z,,,,
524,Addressing issue #519,2015-09-11T20:50:56Z,2015-11-22T04:45:08Z,,,,"Somewhat temporary solution to only try to access index 0 if the list size is at least 1, avoiding the error in the Refreshable Object's if-block pertaining to comments.
"
523,"Fixed typo ""Creddit""",2015-09-11T20:26:55Z,2015-09-11T20:33:43Z,,,,"Typo ""Creddit"" used in error module and subsequent documentation seemed to need to be changed to ""Credit"" and all respective forms of the word.
"
522,Support preview images,2015-09-11T18:14:03Z,2015-09-11T20:33:58Z,,,,"Hi folks!  As far as I can tell from a _very_ quick search around the repo, there isn't currently any support for [the `preview` section](https://www.reddit.com/r/redditdev/comments/39yr53/reddit_change_new_preview_images_available_for/) for posts.
"
521,easy_install isn't giving 3.2.1,2015-09-11T04:36:25Z,2015-09-11T04:58:59Z,,,,"I am trying to install Praw 3.2.1 via Easy_install but it is only giving me 3.2.0. What could the issue be?

![image](https://cloud.githubusercontent.com/assets/116760/9807168/2413f58a-5804-11e5-8b0c-84a557cb5b9d.png)

Also, I am having issues with pip

![image](https://cloud.githubusercontent.com/assets/116760/9807177/49e9170e-5804-11e5-833d-f86f65c3fc25.png)
"
520,How does the DefaultHandler work?,2015-09-09T07:26:38Z,2015-11-22T11:51:53Z,,,,"There is little to no documentation on the handlers, but how can I keep up the rate-limit with the DefaultHandler instead of praw-multiprocess when I run a multithreaded bot in Python3+?

I can't seem to get that worked out, using a new Handler or giving all Reddit Sessions the same handler don't seem to limit the rate:

Something like this:

```
import praw

handler = praw.handler.DefaultHandler()
r_1 = Reddit('somethingsomething', handler=handler)
r_2 = Reddit('somethingdifferent', handler=handler)

def c_stream():
    for comments in praw.helpers.comment_stream(r_1, 'all')
        #something cool
        pass

def s_stream():
    for submission in praw.helpers.comment_stream(r_2, 'all')
        # something more cool
        pass
```

and this is set into two threads. Looking at the request-log (a patched DefaultHandler found here: https://github.com/DarkMio/RedditRover/blob/master/core/PRAWHandler.py#L12) reveals multiple requests per second - here's a snippet:

```
07:10:17 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/new.json?count=88&after=t3_3k7i20&limit=1000&before=t3_3k7i1x
07:10:23 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/new.json?count=89&limit=1000&before=t3_3k7i2b
07:10:23 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/comments/.json?count=6&after=t1_cuvdbzb&limit=1024&before=t1_cuvdbza
07:10:25 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/comments/.json?count=7&limit=1024&before=t1_cuvdc29
07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/new.json?count=89&after=t3_3k7i2f&limit=1000&before=t3_3k7i2b
07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/new.json?count=90&limit=1000&before=t3_3k7i2o
07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/comments/.json?count=7&after=t1_cuvdc2a&limit=1024&before=t1_cuvdc29
07:10:28 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/comments/.json?count=8&limit=1024&before=t1_cuvdc53
07:10:29 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/new.json?count=90&after=t3_3k7i2q&limit=1000&before=t3_3k7i2o
07:10:29 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/comments/.json?count=8&after=t1_cuvdc54&limit=1024&before=t1_cuvdc53
07:10:32 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://api.reddit.com/r/all/new.json?count=91&limit=1000&before=t3_3k7i2w
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/message/unread/.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/message/unread/.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/message/unread/.json
07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/message/unread/.json
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/api/v1/me.json
07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET  https://oauth.reddit.com/message/unread/.json
```

I'm requesting also a bit better documentation on handlers if possible and / or would write them and pull request them if there's more info about them than what's in the docstrings currently.
"
519,Refresh method exception,2015-09-05T11:08:59Z,2016-11-13T19:13:52Z,,IndexError,IndexError: list index out of range,"The code

``` python
for c in user.get_comments():
    if not c.is_root:
        continue
    c.refresh()
```

```
Traceback (most recent call last):
  File ""bot.py"", line 28, in <module>
    c.refresh()
  File ""praw/objects.py"", line 446, in refresh
    other = sub.comments[0]
IndexError: list index out of range
```

Praw version: 3.2.1
"
518,Update __init__.py,2015-09-05T03:18:04Z,2015-09-05T03:20:26Z,,,,"Fixed typo in a __str__ method message that I happened to get the other day.
"
517,Documentation Update,2015-09-05T02:35:27Z,2015-09-05T05:17:26Z,,,,"Update various aspects of documentation. Sorry it took this long @bboe , I wasn't able to get to my pc.

Resolves: 
- #479 
- #475 
- Should resolve #413 (read the later comments)
- Undocumented issues regarding outdated info in docs.
"
516,Wikipedia references,2015-08-31T06:21:27Z,2015-08-31T16:00:14Z,,,,"Photos e.g. a photo of an un-named mosque or other building - I am using Google images to identify by looking at similar images - and then using that to identify the wiki page and/or location. Sometimes, when identified, the specific building may not have a wikipedia page, but there may be a wikipedia page for the Governorate. The Nasr Palace in Al Hudaydah (identified through Google Images) does not have its own wiki - but I used the Al Hudaydah goverorate page as the wikipedia page. The wikipedia page does not refer to the Nasr Palace but if the wiki page were updated it would be appropriate. In this case, while there is no Wiki page for the Nasr Palace, I have provide the Al Hudaydah wikipedia page as the reference as it is located in that Governorate. Any thoughts? 
"
515,Question regarding geo-location of photo,2015-08-31T05:09:50Z,2015-08-31T16:00:00Z,,,,"Till Friday when I was tagging photos, the question ""Can you geo-localize the site and/or the photo?"" would require me to type in the location... and this would allow me to locate the place according to the spelling used by the mapping tool immediately below. (often different from how it may be spelt in other places and mapping tools). I was just tagging http://geotagx.org/project/yemenculture/task/3226 and now do not see the option for typing as a response. Realize that this is now only done using the icon in the map. However, when changes are made, it would be useful to send a note to all volunteers.
Thanks,
"
514,Use deque instead of list,2015-08-30T03:42:50Z,2015-08-30T05:29:48Z,,,,"Reason being that [deques](https://docs.python.org/2/library/collections.html#collections.deque) offer O(1) insert on _both_ ends of the stack instead of just the end. Previously, using a list and appending to the 'head' is an O(n) operation.

A simple demonstration/benchmark: https://gist.github.com/eugene-eeo/549b5b5c5b72a882bc02
"
513,ReadTheDocs defaults to v3.1.0,2015-08-29T18:56:43Z,2015-08-29T19:32:08Z,,,,"http://praw.readthedocs.org/ defaults to v3.1.0 instead of 3.2.1
"
512,PRAW - The TREES Update!,2015-08-29T00:59:55Z,2016-07-18T03:40:31Z,,,,"<p align=""center""><img src=""http://rosettacode.org/mw/images/2/26/PB_FractalTree.png"" align=""center""></p>


<h1>Abstract</h1>


<p>When using the <code>get_submission</code> method on a permalink, the root of the comment forest starts at that comment. Instead of treating this as a bug, since it is actually in the FAQ, it should be treated as a feature and expanded on.</p>


<h1>Status</h1>

Complete, but Annoyances need to be addressed.

<h1>Resolves</h1>

- ~~May fix #362  and #391 though I've not tested it.~~ Nevermind, I mistook something.
- #475 
- #479 
- #500 
- #511 

<h1>Adds</h1>

- A way to get a submission object with the root of a comment and the submission id so that the comment forest starts at that comment
- A way to get the Comment object's context, parent, improved way to get the submission, and tree.
  - More functionality when getting the tree, and that functionality is also included in the above
- Documentation regarding comments

<h1>Annoyance(s)</h1>


For the new comment object related methods, sometimes the user may not want to get the submission object with the special forest root, but instead, just that special forest itself. While certainly possible, after doing such one can not use the `replace_more_comments` function due to the fact that it is a method of the `Submission` object.

<h2>Proposals</h2>


Move `replace_more_comments` to be a helper, and allow it to work on trees instead, since there are other versions of the ""load more"" objects, such as in modmail. I would do this but I lack the comprehension as to how to do so.

If this is actually impossible, an alternate proposal would be that I could add a boolean keyword argument to the new methods on Comment objects.

Neither of the above are actually needed, but may provide some convenience for users
"
511,`comments` of the `submission` property does not give the full submission object,2015-08-28T00:39:27Z,2016-12-18T22:01:26Z,Feature,,,"The `submission` property of the `Comment` object does not return the full forest of comments when doing getting it's `comments` property. This is because the `submission` property uses the ""fast permalink"" of a comment, and when getting the Submission object of a comment permalink, the forest starts at this comment. This at a minimum does not seem intended, if it is intended, it is definitely confusing to the average user.

I am writing a PR that will both fix this issue and add more functionality, but I  wanted this to be documented in case anyone else has this problem until I am done, and I should be done later today/tomorrow morning after writing the tests and adding documentation.

Edit: For more information, this does not occur when using `replace_more_comments` beforehand, as that presets the `_submission` attribute of the comment object. However when not doing this, the `_submission` attribute is NoneType, and gets set when the `submission` property is used. However this setting of the `_submission` attribute uses the fast permalink instead of the original submission object.
"
510,Captchas are missing functionality.,2015-08-26T23:20:01Z,2015-11-22T05:23:00Z,,,,"For times when a captcha is needed, reddit  allows one to get a new captcha (presumably if the original is unreadable by the user). Unless I'm mistaken, this functionality does not exist in prawns. If this is the case, a new captcha should be triggered by entering a special string such as a simple `1` or an empty string altogether. 

Also, if I'm not mistaken, if a captcha is incorrect an error is raised and prompts the exit of the application due to this. However, the API allows one to indefinitely get a newer (potentially, somehow easier to read) captcha if the captcha is incorrect, just like in-browser functionality.
"
509,the friendly update -- greatly improve oauth support for Friends,2015-08-25T23:23:35Z,2015-08-26T07:02:04Z,,,,"<p align=""center""><img src=""https://cloud.githubusercontent.com/assets/7299570/9482118/6628e6b8-4b45-11e5-8e34-4c6ca18dd197.png""</img></p>


Resolves #507. Thanks for the tip.

Some notes I want to make:
1. Moved and deprecated the objects.LoggedInRedditor.get_friends method to AuthenticatedReddit
   
   In a praw session, there is typically only a single LIR instance -- the one assigned to `r.user`. However, `r.user` is only set if you have the identity scope while logging in. This means that PRAW was implicitly requiring the identity scope to use the method, which only requires the read scope.
   
   The only workarounds were to `get_redditor` of your own username and then forcibly set `user.__class__ = objects.LoggedInRedditor`, or to call get_friends as a static method using the Redditor instance as the first argument.
   
   Futhermore, get_friends does not benefit from being a LIR method any more than something like get_inbox would. For this reason I would consider moving get_hidden, get_saved, and get_blocked as well.
2. You will notice a small comment I made in `friend` regarding the note parameter:
   
   > # Sending an empty note string raises http 400, although the reddit
   > # source code would suggest otherwise.
   
   The code in question is [here](https://github.com/reddit/reddit/blob/master/r2/r2/controllers/apiv1/user.py#L180). To me it looks like you can send an empty string and it will figure things out but I didn't find this to be the case:
   
   ![image](https://cloud.githubusercontent.com/assets/7299570/9482069/df5ecfb2-4b44-11e5-8ee2-00cac1a559bc.png)
3. I had to add 'Content-Type: application/json' to internal.py to make the JSON data bodies work with the Betamax matcher. I spent hours trying to figure this out and that was the only way I could make it work. Please let me know if this is totally wrong.
"
508,Use `raw_json=1`,2015-08-25T06:15:42Z,2016-11-13T06:13:13Z,⭐️ New Contributor Friendly ⭐️,,,"I just stumbled upon: https://github.com/reddit/reddit/commit/96b86f3bedc409ee069b3a7da370503dc81b3c23

At some point we should use that everywhere, and do away with decoding the bodies.
"
507,Redditor.friend() raises LoginRequired through OAuth; doesn't use suggested URL,2015-08-25T01:45:37Z,2015-08-26T07:02:04Z,,,,"I decided to try playing around with reddit's Friend system today, and found that Redditor.friend() raises a LoginRequired when used through OAuth.

First, I found that internal.py [chooses a restrict_access decorator with scope=None](https://github.com/praw-dev/praw/blob/c05bab70ff4b26967ca0b114436e0edad24aa6aa/praw/internal.py#L89) causing [all these checks](https://github.com/praw-dev/praw/blob/31d4984fd66930dbc886dfedb27c835dc6609262/praw/decorators.py#L329-L342) to fail until it finally gets to `elif login` and raises the exception. I tried changing that to scope='subscribers', but this causes an OAuthException with the message _""praw.errors.OAuthException: Bearer realm=""reddit"", error=""invalid_request"" on url https://oauth.reddit.com/api/friend/.json""_

&nbsp;

At this point I looked at the [api docs](https://www.reddit.com/dev/api/oauth#POST_api_friend) and saw that they actually want you to use [PUT /api/v1/me/friends/username](https://www.reddit.com/dev/api/oauth#PUT_api_v1_me_friends_%7Busername%7D) instead (ostensibly so that /api/friend will one day control everything except adding friends). I tried this:

```
@restrict_access(scope='subscribe')
def friend(self):
    # friend_v1 = 'api/v1/me/friends/%s'
    url = self.reddit_session.config['friend_v1'] % self.name
    data = {'name': self.name, 'note': 'u'}
    method = 'PUT'
    self.reddit_session.evict(self.reddit_session.config['friends'])
    self.reddit_session.request_json(url, data=data, method=method)
```

But I'm still getting HTTPException 400 errors.

&nbsp;

I'd love to make a pull request, but I'm not sure what to try next. Any insight?
"
506,Resolve issue #501.,2015-08-23T21:56:20Z,2015-08-23T22:00:16Z,,,,"When passing a `count` parameter, it appears reddit now always returns an
`after` parameter in the data set making for duplicate requests. The `count`
parameter has now been renamed to `uniq` for streams.
"
505,Resolve issue #485.,2015-08-23T21:02:09Z,2015-08-23T21:04:03Z,,,,"Resolve issue #485.
"
504,Simplify _use_oauth block.,2015-08-23T20:09:29Z,2015-08-23T20:14:26Z,,,,
503,_request function uses undefined attributes,2015-08-23T20:01:16Z,2016-06-30T05:29:00Z,,,,"BaseReddit does not define the following which appear to be used in `_request`:

```
E1101: 439,37: Instance of 'BaseReddit' has no 'refresh_token' member (no-member)
E1101: 442,34: Instance of 'BaseReddit' has no 'is_oauth_session' member (no-member)
E1101: 456,16: Instance of 'BaseReddit' has no 'refresh_access_information' member (no-member)
E1101: 534,46: Instance of 'BaseReddit' has no 'is_oauth_session' member (no-member)
```

While it works we shouldn't have these issues. The simplest way is to push these values into parameters of the `_request` method.
"
502,Refactor UA string usage.,2015-08-23T19:39:01Z,2015-08-23T19:57:15Z,,,,
501,submission_stream and comment_stream only get new data every second request,2015-08-23T16:56:51Z,2016-11-13T19:11:25Z,Bug,,,"When using submission_stream or comment_stream to consume items continuously (specifically on /r/all but any sufficiently active subreddit should work), every second request includes the params _limit_, _count_ and _before_, and returns no new items. Every other request has the additional param _after_, and it returns items as it should. 

Having every second request be redundant seems less than ideal when the API has such strict rate limiting. 

To reproduce:

``` python
import praw
reddit = praw.Reddit(user_agent=""PRAW Requests Demo"",
                     log_requests=2)

for item in praw.helpers.submission_stream(reddit, ""all"", limit=100):
    print(item)
```

Sample output (after skipping the first 100 ""old"" items and waiting for ""live"" ones):

```
GET: https://api.reddit.com/r/all/new.json
params: {'before': 't3_3i3dca', 'limit': 100, 'count': 1}
status: 200
GET: https://api.reddit.com/r/all/new.json
params: {'before': 't3_3i3dca', 'after': 't3_3i3dcb', 'limit': 100, 'count': 1}
status: 200
1 :: What's something that you wanted to try, knew you'd just love it, but en...
1 :: [PS4] Looking for Gorgon Maze CP
1 :: (Spoilers All) Why do we not believe any of the deaths in the Season 5 f...
1 :: Win Soocoo S60 Action Cam
GET: https://api.reddit.com/r/all/new.json
params: {'before': 't3_3i3dcf', 'limit': 100, 'count': 2}
status: 200
GET: https://api.reddit.com/r/all/new.json
params: {'before': 't3_3i3dcf', 'after': 't3_3i3dcg', 'limit': 100, 'count': 2}
status: 200
1 :: Hong Kong heritage experts conclude that doomed Wan Chai pawn shop is no...
1 :: A moment to appreciate F1's YouTube channel
1 :: Ready for tonight
1 :: Offering: Spanish (native) Seeking: English (native)
0 :: Mini sumo wrestlers face a giant
1 :: How is owning a website with fake profiles like Ashley Madison not fraud?
1 :: [H] 3 Flachion Keys [W] Hella Cologne Stickers
...etc
```

Praw 3.2.1, Python 3.4.0

(I'm not sure if this is somehow intended behaviour or not. I came across this while trying to find the source of another strange delay when streaming live data using submission_stream or comment_stream. Is there something I should read to understand why helpers._stream_generator was written the way it was, or known issues with it, or something? )
"
500,"An api for getting the parent comment's id, the body and the username of the commentor.",2015-08-20T07:36:04Z,2016-12-18T21:53:29Z,Documentation,,,"Just realized PRAW doesn't have an api to access the parent comment's details, for any given comment that has a parent comment.
"
499,Patch 3,2015-08-20T06:37:57Z,2015-08-21T23:35:11Z,,,,"Fix Flake8 error
"
498,Give meaningful parameters for texinfo,2015-08-20T01:15:15Z,2015-08-20T02:10:28Z,,,,"This pull request causes any texinfo document made using Sphinx to carry meaningful metadata, such as category and short description.
"
497,Forbidden error when trying to retrieve content_md of a WikiPage object in a private subreddit,2015-08-19T04:15:57Z,2016-11-14T03:49:49Z,,,,"When I try to get `content_md` of a WikiPage object in a subreddit which is private, I get an error like this:

```
Traceback (most recent call last):
  File ""./twitch.py"", line 17, in <module>
    p = r.get_wiki_page(SUBREDDIT, ""streamers"").content_md
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/objects.py"", line 81, in __getattr__
    self._has_fetched = self._populate(None, True)
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/objects.py"", line 155, in _populate
    json_dict = self._get_json_dict() if fetch else {}
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/objects.py"", line 148, in _get_json_dict
    self._info_url, params=params, as_objects=False)
  File ""<string>"", line 2, in request_json
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/decorators.py"", line 113, in raise_api_exceptions
    return_value = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/__init__.py"", line 604, in request_json
    retry_on_error=retry_on_error)
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/__init__.py"", line 437, in _request
    _raise_response_exceptions(response)
  File ""/usr/local/lib/python2.7.10/lib/python2.7/site-packages/praw/internal.py"", line 200, in _raise_response_exceptions
    raise Forbidden(_raw=response)
praw.errors.Forbidden 
```

However, when I switch the subreddit from private to public it works fine
"
496,"Fix ""multiple values for argument"" for captcha",2015-08-19T03:08:45Z,2015-08-19T10:38:34Z,,,,"This is in response to #492. I'm assuming something in [Avoid destroying method signatures with decorators](https://github.com/praw-dev/praw/commit/60dd692f0d729860c1fe74c82dea4f8c32e35163) caused this issue, since that appears to be the only time this part has changed since 3.1.0.

I was not expecting to see the function's default arguments being passed into the decorator like this, so I handled it in a way that made sense to me. Please let me know if we can refactor something elsewhere that will fix this issue more simply.

It would be good if we could write a test for captchas to catch this stuff early, but that would make it the only part of the test suite that requires manual intervention, which I don't know if that's a good idea. In the meantime here's my [proof that it works](https://cloud.githubusercontent.com/assets/7299570/9347898/a67da868-45e4-11e5-8d8b-c9983bcbaa2e.png).

edit: For clarification, the function's default arguments were being passed in addition to the arguments the user provided. It wasn't just the defaults.
"
495,Edit Errors so all errors contain __str__(),2015-08-18T01:34:35Z,2015-08-23T20:42:10Z,,,,"All errors methods now contain a returned **str** in one way or another, so `except as e:` and friends can be used as expected. Some methods now have the ability for more customized errors, which is useful for distribution.
"
494,Adding function for getting edited posts + grammar and terminology fixes,2015-08-17T07:37:58Z,2015-08-19T19:38:08Z,,,,"Sorry about not doing this before v3.2.0.

I was speaking with @voussoir in pms a tiny bit ago and it came up as to the fact that there was no way to get edited posts and comments, so I decided to have a crack at it myself :P

I also fixed some of the docstrings that had easily missed errors.
"
493,Merge pull request #1 from praw-dev/master,2015-08-17T05:21:11Z,2015-08-17T05:22:50Z,,,,"update
"
492,TypeError when trying to submit a post that results in a captcha challenge,2015-08-15T16:14:14Z,2015-08-19T10:58:52Z,,TypeError,TypeError: submit() got multiple values for keyword argument 'captcha',"When I try to create a submission by using `submit()`, get a captcha challenge, and enter the captcha, I get this exception:

```
Traceback (most recent call last):
  File ""<pyshell#5>"", line 1, in <module>
    r.submit(subreddit, title, text)
  File ""<string>"", line 2, in submit
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 255, in wrap
    return function(*args, **kwargs)
  File ""<string>"", line 2, in submit
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 161, in require_captcha
    return function(*args, **kwargs)
TypeError: submit() got multiple values for keyword argument 'captcha'
```

This didn't happen in PRAW 3.1.0, but does in 3.2.0. 

~~Of course, I don't know; I might just be doing something wrong.~~
"
491,praw.errors.HTTPException returns blank string when printed or used in str(),2015-08-15T15:22:56Z,2015-08-23T20:45:22Z,,requests.exceptions.HTTPError,requests.exceptions.HTTPError: 503 Server Error: Service Unavailable,"As of PRAW 3.1, when I tried to print a praw.errors.HTTPException, it printed a blank string. Here is some sample code:

``` python
try:
    # some code
except Exception as e:
    print(""Some thing bad happened!"", e)
```

And the traceback caused when an HTTPError occurs, note the lack of a string after `Something bad happened!`:

```
Some thing bad happened!
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\praw\internal.py"", line 201, in _raise_response_exceptions
    response.raise_for_status()  # These should all be directly mapped
  File ""C:\Python34\lib\site-packages\requests\models.py"", line 851, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 503 Server Error: Service Unavailable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""soymilk.py"", line 19, in <module>
    for comment in test_comments:
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 524, in get_content
    page_data = self.request_json(url, params=params)
  File ""C:\Python34\lib\site-packages\praw\decorators.py"", line 173, in wrapped
    return_value = function(reddit_session, *args, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 579, in request_json
    retry_on_error=retry_on_error)
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 424, in _request
    _raise_response_exceptions(response)
  File ""C:\Python34\lib\site-packages\praw\internal.py"", line 203, in _raise_response_exceptions
    raise HTTPException(_raw=exc.response)
praw.errors.HTTPException
```
"
490,fix scope issues when replying to PMs,2015-08-07T01:10:16Z,2015-08-07T06:08:11Z,,,,"/u/gooeyblob fixed a reddit bug in which the user could not reply to PMs because the /comment endpoint required the `submit` scope no matter what. Now it checks whether the item is a message and permits the `privatemessages` scope.

https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/ctu84pk
"
489,LoginRequired error thrown when attempting to ban,2015-08-02T18:40:12Z,2015-08-03T00:31:47Z,,"Error, praw.errors.LoginRequired","Error:, praw.errors.LoginRequired: `do_relationship` requires a logged in session","Error:
File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py"", line 346, in wrapped
raise errors.LoginRequired(function.**name**)
praw.errors.LoginRequired: `do_relationship` requires a logged in session

Received when attempting Subreddit.add_ban() with the following _authorization scope:

set([u'wikiedit', u'save', u'wikiread', u'subscribe', u'edit', u'modcontributors', u'mysubreddits', u'privatemessages', u'modconfig', u'read', u'modlog', u'modposts', u'modflair', u'vote', u'modwiki', u'submit', u'identity', u'flair'])

All other moderation functions seem to be working otherwise.
"
488,get_flair_list ImportWarning,2015-08-02T13:41:21Z,2015-08-23T21:11:27Z,,,,"Just trying to play around with total statistics for our subreddit and am running into:

> lib\importlib_bootstrap.py:2150: ImportWarning: sys.meta_path is empty.

From what I read there was a similar issue for comments around a year ago and after some testing with similar parameters I start suspecting that it's actually a praw issue.

Real quick as far as my code goes. It's actually pretty straight forward.

``` python
r = init_reddit_session() #this simply establishes a connection
subreddit = r.get_subreddit(""leagueoflegends"")

for user_flair in subreddit.get_flair_list():
    pass
```
"
487,more oauth test moves; better InvalidObject exceptions,2015-07-31T06:18:24Z,2015-08-04T20:22:26Z,,,,"I would like to get #486 merged before this one, even though those changes are actually included here by necessity.

There are still more tests to do, but I'm taking it in chunks. Here are some of the moves:

| from | to |
| --- | --- |
| `test_scope_modconfig` | `subreddit.test_set_stylesheet_oauth` and `subreddit.test_set_settings_oauth` |
| `test_scope_modflair` | `flair.test_set_individuals_flair_oauth` |
| `test_scope_modwiki_modcontr` | `wiki.test_add_remove_wiki_ban_contributor_oauth` |
| `test_scope_modwiki` | `wiki.test_add_remove_editor_oauth` |
| `test_scope_wikiread_wiki_page` | `wiki.test_read_wikipage_oauth` |
| `test_scope_submit` | `submission.test_submit_oauth` |
| `test_scope_vote` | `submission.test_vote_oauth` |

new tests thanks to the added error_types:
- `submission.test_raise_invalidsubmission_oauth`
- `subreddit.test_raise_invalidsubreddit_oauth`
- `comment.test_raise_invalidcomment_oauth`

&nbsp;

Finally, I'm wondering how you wanted to port the standard tests to oauth. For 90% of them, the oauth version is basically just copy-and-paste with a single `refresh_access_information` line at the top. This will basically double the size of every test file with duplicate trash. Does that matter to you, or is it okay since eventually we'll delete the standard tests anyway?

Let me know if I missed something. 
"
486,fix get_wiki_page not sending the oauth headers,2015-07-31T02:36:24Z,2015-08-04T20:22:04Z,,,,"It turned out that the get_wiki_page method was not sending the oauth headers unless you had the `read` scope due to some logic within RedditContentObjects. Of course, they require the `wikiread` scope instead, so this PR reflects that. 

Visually I think this change ended up looking messy, but flake8 didn't like any of the alternatives. Hopefully my comment makes up for it.

Here's what was happening before I realized the issue: https://i.imgur.com/VbKqRcm.png

&nbsp;

I was planning to open a secondary PR that accompanies this one, but I think I accidentally just nuked it without a suitable backup, so I'm going to start rewriting that.
"
485,login() function returning error when given no params,2015-07-29T16:15:22Z,2015-08-23T21:04:35Z,,TypeError,"TypeError: must be char, not unicode","Whenever the .login() function was called without being passed parameters, it would prompt the user for them. When done now the code throws an exception

```
>>> r.login()
Username: The1RGood
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""<string>"", line 2, in login
  File ""C:\python27\lib\site-packages\praw-3.1.0-py2.7.egg\praw\decorators.py"", line 75, in wrap
    return function(*args, **kwargs)
   File ""C:\python27\lib\site-packages\praw-3.1.0-py2.7.egg\praw\__init__.py"", line 1390, in login
    pswd = getpass.getpass('Password for %s: ' % user)
  File ""C:\python27\lib\getpass.py"", line 95, in win_getpass
    msvcrt.putch(c)
TypeError: must be char, not unicode
```
"
484,A few docstring tweaks in __init__.py,2015-07-25T13:12:15Z,2015-07-25T16:26:46Z,,,,
483,"minor bug with the encoding PLATAFORM INFO, remove non ascii by now",2015-07-22T17:09:59Z,2015-08-23T18:14:31Z,,,,
482,21 July 2015 - fix get_contributors 403; start moving tests,2015-07-22T00:50:49Z,2015-07-25T20:21:55Z,,,,"Moved the `tempauth` variable higher up so that it makes the oauth header work for certain requests that it didn't before.

Started porting some oauth tests out of test_oauth2_reddit to their appropriate files. Not all of them, just breaking the ice.

Here's what moved. I put _oauth at the end of everything so we don't get any cassette name collisions and to be distinct:

| from | to |
| --- | --- |
| `test_scope_edit` | `submission.test_edit_oauth` |
| `test_scope_modposts` | `submission.test_remove_oauth` |
| `test_scope_read_get_submission_by_url` | `submission.test_get_submission_by_url_oauth` |
| `test_scope_read_priv_sub_comments` | `submission.test_get_priv_submission_comments_oauth` |
| `test_scope_modothers_modself` | `subreddit.test_add_remove_moderator_oauth` |
| `test_scope_modself` | `subreddit.test_join_leave_moderator_oauth` |
| `test_scope_modlog` | `subreddit.test_get_modlog_oauth` |
| `test_scope_read_get_sub_listingr` | `subreddit.test_get_priv_sr_listing_oauth` |
| `test_scope_read_priv_sr_comments` | `subreddit.test_get_priv_sr_comments_oauth` |
| `test_scope_subscribe` | `subreddit.test_subscribe_oauth` |

I started trying the `betamax_oauth_init` like we talked about, but it turned out to be unecessary, and doing oauth tests and standard tests under the same reddit_session was causing problems. Putting the oauth tests into their own class solves that, and I think it's good to keep them separate for now anyway.

Let me know if I missed something or you have questions about my reasoning.
"
481,Allow search() function to use params parameter,2015-07-21T08:37:45Z,2015-07-25T16:29:24Z,,,,"As shown in [this comment](https://www.reddit.com/r/NoStupidQuestions/comments/3dtjbs/how_do_you_stick_photos_together/) by [/u/GoldenSights](http://www.reddit.com/u/GoldenSights), this allows the params parameter within the search() function to be altered. This allows for greater use of the function, from defining pages via `params={'after' : 't3_code'}` to the option of specifically defining to use the ""new"" legacy search e.g. `params={'feature' : 'legacy_search'}`
"
480,"OAuth ""read"" not granting all permissions in get_content wrappers",2015-07-20T14:41:44Z,2015-07-26T21:13:29Z,,,,"When authenticating with OAuth, and the ""read"" permission is in scope, any calls to a subreddit's contributor list with the function `subreddit.get_contributors()` returns a 403 forbidden response.

This is a sub I have full moderator permissions to, and I can view the content in-browser, as well as retrieve otherwise-private post and comment information. I simply can't access the contributor list for some unknown reason.
"
479,Document the return value of Comment.author for deleted users/comments,2015-07-19T23:00:04Z,2015-09-05T05:18:47Z,"Documentation, ⭐️ New Contributor Friendly ⭐️",,,"Had to dig through the code to find out that `Comment.author` returns `None` instead of `'[deleted]'` for deleted users/comments, would be nice if it were mentioned somewhere in the docs.

Maybe clarify what `Comment.body` returns too? I'm kinda confused myself about how it's handled upstream. Is this right?

```
comment deleted (by user or mod) -> author == [deleted], body == [deleted]
user deleted -> author == [deleted], body != [deleted]
```

What about banned users?
"
478,/u/ mentions and inbox replies available to all,2015-07-17T23:47:43Z,2015-07-18T00:16:32Z,,,,"Not just Gold members. The docstrings for `submit()` and `get_mentions()` now reflect that.
"
477,Inability to call get_message successfully (assertionerror) when under oAuth - fine under login.,2015-07-17T02:24:28Z,2015-07-17T02:29:43Z,,,,"Found after discovering issue https://github.com/praw-dev/praw/issues/476 . - Praw 3.1.0

Praw method get_message throws an assertion error in oauth but not in traditional login mode.  Note that oAuth account has been given every permission ('creddits','edit','flair','history','identity','modconfig','modcontributors','modflair','modlog','modothers','modposts','modself','modwiki','mysubreddits','privatemessages','read','report','save','submit','subscribe','vote','wikiedit','wikiread')

The following login based code prints out the message as expected.

``` python
import praw

r = praw.Reddit(user_agent='Test Praw API by /u/Pentom')
r.login('mylogin','mypassword')

message = r.get_message('39x6ll')
print(message)
```

When same code is attempted under oauth, receive an assertion error.

> > > message = r.get_message('39x6ll')
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > >   File ""/usr/local/lib/python2.6/dist-packages/praw/decorators.py"", line 348, in wrapped
> > >     return function(cls, _args, *_kwargs)
> > >   File ""/usr/local/lib/python2.6/dist-packages/praw/**init**.py"", line 2241, in get_message
> > >     return objects.Message.from_id(self, message_id, _args, *_kwargs)
> > >   File ""/usr/local/lib/python2.6/dist-packages/praw/decorators.py"", line 327, in wrapped
> > >     assert not obj._use_oauth  # pylint: disable=W0212
> > > AssertionError

``` python
import praw
reddit_application_client_id = 'myclientid'
reddit_application_client_secret_key = 'myclientkey'
reddit_user_refresh_key_for_app = 'myuserrefreshkey'

r = praw.Reddit(user_agent='Test Praw API by /u/Pentom')
r.set_oauth_app_info(client_id=reddit_application_client_id,client_secret=reddit_application_client_secret_key,redirect_uri='http://127.0.0.1:65010/authorize_callback')
access_information = r.refresh_access_information(reddit_user_refresh_key_for_app)
r.set_access_credentials(**access_information)
print(r.get_me())

message = r.get_message('39x6ll')
print(message)
```
"
476,Inconsistency oAuth vs Login - get_content - Praw 3.1.0,2015-07-17T02:17:06Z,2016-06-30T05:30:17Z,,,,"Praw method get_content behaves differently in oauth vs login.  Note that oAuth account has been given every permission ('creddits','edit','flair','history','identity','modconfig','modcontributors','modflair','modlog','modothers','modposts','modself','modwiki','mysubreddits','privatemessages','read','report','save','submit','subscribe','vote','wikiedit','wikiread')

This performs as expected and prints out a single message in login mode.  No error.

``` python
import praw

r = praw.Reddit(user_agent='Test Praw API by /u/Pentom')
r.login('mylogin','mypassword')

message_link = r.get_content(url='http://www.reddit.com/message/messages/39x6ll')
for message in message_link:
    print(message)
```

When doing the same thing under oauth, received a redirect exception (praw.errors.RedirectException: Unexpected redirect from http://www.reddit.com/message/messages/39x6ll.json to http://www.reddit.com/login.json?dest=http%3A%2F%2Fwww.reddit.com%2Fmessage%2Fmessages%2F39x6ll.json).

``` python
import praw
reddit_application_client_id = 'myclientid'
reddit_application_client_secret_key = 'myclientkey'
reddit_user_refresh_key_for_app = 'myrefreshkey'

r = praw.Reddit(user_agent='Test Praw API by /u/Pentom')
r.set_oauth_app_info(client_id=reddit_application_client_id,client_secret=reddit_application_client_secret_key,redirect_uri='http://127.0.0.1:65010/authorize_callback')
access_information = r.refresh_access_information(reddit_user_refresh_key_for_app)
r.set_access_credentials(**access_information)
print(r.get_me())

message_link = r.get_content(url='http://www.reddit.com/message/messages/39x6ll')
for message in message_link:
    print(message)
```

Received same error when trying to call get_content with second parameter '_use_oauth=True'.

Later attempted to move on to using 'get_message' as its newly added but had my own issues with that.  Creating separate defects since I assume you don't want a monster defect - apologies if otherwise, let me know and I will adjust in the future.
"
475,"""The Configuration Files"" missing info on how to tell praw to use a specific site",2015-07-15T15:02:29Z,2015-09-05T05:19:00Z,"Documentation, ⭐️ New Contributor Friendly ⭐️",,,"See http://praw.readthedocs.org/en/latest/pages/configuration_files.html.

No mention of the `site_name` argument or the env variable.
"
474,Added /r/Astros Buttsbot!,2015-07-15T14:20:51Z,2015-07-16T07:59:42Z,,,,
473,Crashing on Non-Standard Characters When Getting a Submission,2015-07-15T04:04:25Z,2016-11-13T19:10:39Z,,"EOFError, praw.errors.ClientException, UnboundLocalError","EOFError: Ran out of input, praw.errors.ClientException: Successive failures reading from the multiprocess server., UnboundLocalError: local variable 'retval' referenced before assignment","I'm running a script to collect content from a few subreddits I moderate (which then gets emailed and dumped). Every now and then, it comes across a post containing characters that aren't normal. It's a very rare occurrence (I've just added a new sub to it hence why the example link is very old). The problem occurs with praw-multiprocess so there's not much I can do.

This is the line that crashes:

```
if r.get_submission(url=sub.perma).approved_by != None:
```

An example sub.perma is:

https://www.reddit.com/r/gallifreyan/comments/1uerl8/i_tried_doing_a_name_with_accents_b%C5%91g%C3%A9r_eszter/

(sub is a peewee database object)

It gets me the following output and error when I run my script:

```
Lost connection with multiprocess server during read. Trying again.
Lost connection with multiprocess server during read. Trying again.
Lost connection with multiprocess server during read. Trying again.
Traceback (most recent call last):
  File ""/home/pcj/py3/lib/python3.4/site-packages/praw/handlers.py"", line 209, in _relay
    retval = cPickle.load(sock_fp)
EOFError: Ran out of input
```

and this:

```
praw.errors.ClientException: Successive failures reading from the multiprocess server.
```

The praw-multiprocess output is as follows:

```
GET https://www.reddit.com/r/gallifreyan/comments/1uerl8/i_tried_doing_a_name_with_accents_b%C5%91g%C3%A9r_eszter/.json
Exception in thread Thread-69877:
Traceback (most recent call last):
  File ""/usr/lib/python3.4/threading.py"", line 920, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.4/threading.py"", line 868, in run
    self._target(*self._args, **self._kwargs)
  File ""/usr/lib/python3.4/socketserver.py"", line 612, in process_request_thread
    self.handle_error(request, client_address)
  File ""/usr/lib/python3.4/socketserver.py"", line 609, in process_request_thread
    self.finish_request(request, client_address)
  File ""/usr/lib/python3.4/socketserver.py"", line 344, in finish_request
    self.RequestHandlerClass(request, client_address, self)
  File ""/usr/lib/python3.4/socketserver.py"", line 665, in __init__
    self.handle()
  File ""/usr/local/lib/python3.4/dist-packages/praw/multiprocess.py"", line 77, in handle
    cPickle.dump(retval, self.wfile,  # pylint: disable=E1101
UnboundLocalError: local variable 'retval' referenced before assignment
```
"
472,Add code of conduct (Version 1.1 of the Contributor Covenant).,2015-07-15T02:18:58Z,2015-07-15T02:29:34Z,,,,
471,Support the '*' scope in OAuth,2015-07-14T22:52:25Z,2015-11-22T04:58:05Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"I have a script based OAuth that uses the password based grant as shown in the [reddit Quick Start example](https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example). The problem arises that the scope returned by the response, e.g.:

```
{
    ""access_token"": ""J1qK1c18UUGJFAzz9xnH56584l4"", 
    ""expires_in"": 3600, 
    ""scope"": ""*"", 
    ""token_type"": ""bearer""
}
```

Uses an asterisk and PRAW does not seem to handle this case when using `set_access_credentials`. I understand that this type of auth is seemingly unsupported by PRAW though.

The first exception I got was a `TypeError`, stating that it had to be a set. Changing the `scope` passed to `set_access_credentials` to a set just yields `LoginOrScopeRequired` exceptions upon usage of the API.
"
470,Add test for double sticky,2015-07-14T21:43:25Z,2015-07-17T15:04:04Z,,,,"Also added the num parameter to `get_sticky`
"
469,Make use of raw_json for json requests?,2015-07-14T16:54:17Z,2015-07-14T16:57:44Z,,,,"https://www.reddit.com/dev/api#response_body_encoding

I don't see any reason to require users of PRAW to have to unescape the JSON as if it's HTML.
"
468,Automatically refresh oauth2 tokens,2015-07-14T06:49:33Z,2015-07-15T20:35:04Z,,,,"Seems to work very well.

The tokens last for an hour, but I wanted to set the refresh period slightly lower than that so you don't have any freak incident where the token expires on the way over or something. Probably could get away with 3599 but I thought 3590 is a much nicer looking number. 3595 wouldn't be bad either.

The test has the possibility of being a little bit flakey, since it literally relies on wasting enough time to pass the clock, but without using `time.sleep` or anything.

Okay, I'll need some input on fixing up the tests. If the config variable is too low, Python gets stuck in a loop of always refreshing, but if it's too high then I have to use a bunch of timewasters to pass the clock. What's a good middle ground?
"
467,Added support for secondary stickies,2015-07-14T01:59:32Z,2015-07-14T20:50:09Z,,,,"Pretty sure this is going to fail some tests - maybe someone can enlighten me on what goes wrong.
"
466,Added support for secondary stickies,2015-07-14T01:36:30Z,2015-07-14T01:48:52Z,,,,
465,Feature request: Keep an internal timer and refresh oauth when needed,2015-07-13T06:55:56Z,2015-07-13T20:51:09Z,Feature,,,"I figure this could use a thread.

Having to keep an access token up-to-date is probably the most annoying part about switching from password auth to oauth. It would be great if the request wrapper could check when the last time I updated my access token was, and refresh it for me before continuing. For the most part I think it would be very trivial to add, but I don't want to go opening any PRs just yet.
- Should autorefresh be opt-in or opt-out?
- Is there any reason someone might _not_ want this?
- Should this wait until August or is it as easy as I'm expecting it to be? I think it will make the transition smoother for everybody.

What are your thoughts?
"
464,upvote_ratio bug,2015-07-12T23:15:51Z,2015-07-12T23:20:23Z,,AttributeError,AttributeError: '<class 'praw.objects.Submission'>' has no attribute 'upvote_ratio',"In the following code the commented line yields an error while the latter line works.

import praw

r = praw.Reddit(user_agent='stats')
subreddit = r.get_subreddit('news')

for submission in subreddit.get_new(limit=1):
    #print(submission.upvote_ratio)
    print(r.get_submission(submission_id=submission.id).upvote_ratio)

Traceback (most recent call last):
  File ""debug.py"", line 9, in <module>
    print(submission.upvote_ratio)
  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 86, in __getattr__attr))
AttributeError: '<class 'praw.objects.Submission'>' has no attribute 'upvote_ratio'
"
463,Added collapse and uncollapse methods to Message object,2015-07-12T21:24:01Z,2015-07-12T23:37:11Z,,,,"These endpoints are currently undocumented, although I have made a PR to have reddit add documentation to their /dev/api page:

https://github.com/reddit/reddit/pull/1365
"
462,edit_wiki_page() throws a 403 error with OAuth,2015-07-12T12:04:18Z,2015-07-12T21:13:29Z,,,,"Related to #421.

Looks like it needs this decorator?

``` python
@decorators.restrict_access(scope='wikiedit', login=True)
```

I can submit a PR later.
"
461,11 July 2015 - Initialize _unique_count to 1,2015-07-11T23:29:00Z,2015-07-11T23:50:02Z,,,,"Interestingly, only Redditors, Multireddits, and WikiPages are affected by this bug. Subreddits, Submissions and Comments were not. I'm not actually sure why.

I added the test in test_wiki_page because it doesn't really belong to any particular category, and it works well for this purpose. I could have also used the MultiReddit suite, but neither is really better.

Sorry, the commits got a little messy when I created this new branch.
"
460,Ignore fetch=True when passing a multi into Subreddit(...).,2015-07-11T22:19:21Z,2015-07-11T22:31:49Z,,,,"Resolves #372.
"
459,Fix OAuth scope over edit_wiki_page(),2015-07-11T16:11:08Z,2015-07-11T18:36:06Z,,,,
458,Fix #249 Don't destroy method signatures with decorators,2015-07-11T09:09:05Z,2015-07-11T21:56:59Z,,,,"Utilize decorator.decorator in order to prevent method signature destruction. Using this library required flattening the existing decorator methods. It also made it such that the decorators themselves couldn't (easily) modify the docstring so we're not super DRY for that but that's okay.

Finally resolves #249.
"
457,Fix typo in exceptions.rst,2015-07-09T13:31:49Z,2015-07-10T21:06:38Z,,,,
456,Some tweaks and fixes for the Code Overview doc,2015-07-09T08:52:38Z,2015-07-12T21:43:00Z,,,,
455,08 July 2015 - Fix multi-scope methods; add leave_moderator,2015-07-09T03:40:29Z,2015-07-11T21:38:40Z,,,,"Changes
- Add support for multiple-scope methods by using lists for has_scope check
- Use above to fix `add_wiki_ban` and `add_wiki_contributor`
- Add `[modcontributors, modwiki]` relationship scope for above
- Add `modcontributors` relationship scope as seen in #451
- Fix `WikiPage.remove_editor` raising an AssertionError when using OAuth
- Add refresh key and tests for `modwiki`
- Add refresh key and tests for `modwiki+wikicontributors`

Concerns
- You'll notice that `test_scope_modwiki` shows me first loading the `wikiread` scope. This is required to perform `subreddit.get_wiki_page`, else it 403s. I would consider this a bug / shortcoming, because it means we're implicitly adding a scope dependency that isn't required by reddit. I'm not sure why `modwiki` doesn't allow you to load the page in the first place.
- I'm debating whether I should add a leading underscore to `leave_status`. On one hand, the user shouldn't be calling it directly, they should use leave_moderator. On the other hand, this method has all of the docstring info, so I feel it shouldn't be privatized. I mean, privatizing doesn't stop them from reading it, it's just a formatting thing.

&nbsp;

Notes
- The leave_moderator method had to go through a few evolutions to get to where it is now. Many of the reddit forms accept `{'r': 'reddit_api_test'}`, but leavemoderator and leavecontributor insist on requiring the subreddit's fullname. I tried everything to add this method to either the relationships section or the `_methods` section, but in the end I had to make it separate.

Critique.
"
454,No method to leave as an approved submitter,2015-07-08T08:12:04Z,2015-07-11T22:26:22Z,,,,"There doesn't seem to be a method to remove yourself as an approved submitter from a subreddit. There also isn't a method to leave as a moderator, but that can be accomplished using `r.get_subreddit(""subredditnamehere"").remove_moderator(r.user.name)` or `r.get_subreddit(""automodtestsub"").remove_moderator(r.get_me().name)`, although this _does_ require the `modothers` OAuth scope instead of the `modself` scope, but doesn't require full moderator permissions.

Now, `remove_contributor()` doesn't seem to work _at all_; see PR #451.

And also, even if `remove_contributor()` did work, it wouldn't work if you weren't a mod of the subreddit (since it's made for removing other approved submitters), so a method to leave as an approved submitter would still be nice.
"
453,07 July 2015 - Remove reference to user in mark_as_read,2015-07-08T02:03:30Z,2015-07-08T04:24:44Z,,,,"Because `reddit_session.user` is None when the app does not have the Identity scope, messages could not be marked as read (NoneType has no attribute mark_as_read). This method needs to work even if privatemessages is the only scope provided.

`user.mark_as_read` doesn't provide any extra functionality when you're only using a single item anyway.

&nbsp;

This is a simple PR, so I'd like this to be the last one and then I'll release 3.1.0. The to-do list is ever increasing but I've put it off long enough.
"
452,Add the final four OAuth scopes to add to the table in oauth.rst,2015-07-07T19:50:59Z,2015-07-07T21:25:15Z,,,,"The remaining four OAuth scopes: `history`, `modcontributors`, `modself`, and `wikiedit`.

I can't seem to find any methods that need the `account` scope and live threads seem to be [not supported at all](https://github.com/praw-dev/praw/issues/313), so I've omitted those two.
"
451,Fix OAuth scopes in _modify_relationship in internal.py,2015-07-07T09:44:57Z,2015-07-09T09:00:21Z,,,,"I seem to have found some mistakes in the `_modify_relationship` function in internal.py: it doesn't pass a scope value of `modcontributor` to the `@restrict_access` decorator of the local `do_relationship` function when the relationship is set to `banned` or `contributor`, despite [the API documentation](https://www.reddit.com/dev/api/oauth#POST_api_friend) saying that the `modcontributor` relationship requires that scope. The same is true for the `wikibanned` and `wikicontributor` relationships, except they also require the `modwiki` scope on top of the `modcontributor` scope.

The first commit fixes the `banned` and `modcontributor` relationships (or at least tries to do so)~~, but, to fix the other two, I need an answer to this rookie question: How can one pass two or more scopes to the `@restrict_access` decorator?~~ Edit: apparently, that's currently impossible

~~Of course, since I'm quite inexperienced with PRAW and the Reddit API, I might be completely wrong about all of this, in which case, sorry for wasting the time of anyone that read this.~~ Edit: After testing this a bit, it looks like not having the two lines that this PR would add causes `remove_contributor` and `add_contributor` to not work and raise the following exception if logged in via OAuth:

```
Traceback (most recent call last):
  File ""<pyshell#17>"", line 1, in <module>
    r.get_subreddit(""automodtestsub"").remove_contributor(r.user.name)
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 346, in wrapped
    raise errors.LoginRequired(function.__name__)
LoginRequired: `do_relationship` requires a logged in session
```

...and the following exception if logged in via cookie authentication (the `login()` method):

```
Traceback (most recent call last):
  File ""<pyshell#23>"", line 1, in <module>
    r.get_subreddit(""automodtestsub"").remove_contributor(r.user.name)
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 342, in wrapped
    raise errors.ModeratorRequired(function.__name__)
ModeratorRequired: ``do_relationship` requires a moderator of the subreddit` requires a logged in session
```

And this happens regardless of whether you're a moderator and have access permissions or not.
"
450,Yet another OAuth scope to add to the table,2015-07-07T08:28:55Z,2015-07-07T08:52:25Z,,,,
449,Minor code optimizations,2015-07-07T04:26:11Z,2015-07-07T05:21:15Z,,,,"- Comment out non-betamaxified image tests as they're flakey.
- Add lazy_name update redditor test.
- Add a `_post_populate` function to provide the functionality when needed.
- Remove unnecessary error handling
- Remove test for private attribute
"
448,wikiread not listed in OAuth scopes in documentation,2015-07-07T01:49:29Z,2015-07-07T05:42:42Z,,,,"In the [code overview](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_wiki_page), it states that the wikiread scope is necessary for the `get_wiki_page` operation. However, in the OAuth scopes table on the  [OAuth documentation](http://praw.readthedocs.org/en/latest/pages/oauth.html#oauth-scopes), this scope is missing. My calls were failing on the get_wiki_page call, but when looking through that table (and doing a ""Find 'get_wiki_page'), it did not show up.
"
447,get_top() and get_controversial() stop at low number of posts when iterating through them with Subreddit objects,2015-07-06T03:15:52Z,2015-07-06T04:06:17Z,,,,"Note: This is only an issue for get_top() and get_controversial(), and it only applies to non-default Subreddits, which seem to behave normally.  /r/programming is very large and runs into the issue, but /r/leagueoflegends, which is of similar subscriber size (but more activity) does not run into this issue.  get_new() and get_hot() seem to work normally.

When I perform either of the following, I only get output up to low numbers around 10-30.  

```
r = praw.Reddit('...')
subreddit = r.get_subreddit('classicalmusic')#or most non-defaults
posts = subreddit.get_top(limit = 1000)
for i, post in enumerate(posts):
    pass
print i
posts = subreddit.get_controversial(limit = 1000)
for i, post in enumerate(posts):
    pass
print i
```

I am running this in Python 2.7.6 in Ubuntu 14.04 LTS.
"
446,Add more OAuth scopes to the table of OAuth scopes,2015-07-05T18:50:01Z,2015-07-05T20:40:13Z,,,,"This would add ""save"", ""report"", ""creddits"" and ""modwiki"" to the table of OAuth scopes in oauth.rst.
"
445,"Add ""save"" and ""report"" to the table of OAuth scopes",2015-07-05T18:32:51Z,2015-07-05T18:35:35Z,,,,"More OAuth scopes to add to the table.
"
444,fix mod invite scope,2015-07-03T23:07:46Z,2015-07-04T06:42:40Z,,,,"As promised in #440  and #442 

&nbsp;

Hmm, I was planning on just merging this myself, but I made the Coveralls rating decrease. What should the betamax test for the modself scope look like? The user would have to have a pending moderator invite or contributor status to act upon.
"
443,"Add ""flair"" to the table of OAuth scopes",2015-07-03T15:48:26Z,2015-07-03T15:56:33Z,,,,
442,Added missing OAuth access restriction scopes to several methods,2015-07-03T09:10:36Z,2015-07-03T09:30:57Z,,,,"This request adds the missing restrict access scopes to several methods, according to https://www.reddit.com/dev/api/oauth:

| Method | API_PATH | Reddit Path | Scope |
| --- | --- | --- | --- |
| accept_mod_invite | accept_mod_invite | api/accept_moderator_invite | None (&#x2605;) |
| select_flair | select_flair | api/selectflair/ | flair |
| get_banned | banned | r/%s/about/banned/ | read |
| get_mod_queue | modqueue | r/%s/about/modqueue/ | read |
| get_reports | reports | r/%s/about/reports/ | read |
| get_spam | spam | r/%s/about/spam/ | read |
| get_unmoderated | unmoderated | r/%s/about/unmoderated/ | read |
| get_wiki_banned | wiki_banned | r/%s/about/wikibanned/ | read |
| get_wiki_contributors | wiki_contributors | r/%s/about/wikicontributors/ | read |

 (&#x2605;) The API claims it should be `modself`, but this doesn't pass the tests.
"
441,Fix #427: pickling a Comment object is slow,2015-07-02T19:21:13Z,2015-07-03T16:10:23Z,,,,
440,Added missing access restriction scopes to several methods,2015-07-02T09:30:31Z,2015-07-03T08:49:27Z,,,,"This request adds the missing restrict access scopes to several methods, according to https://www.reddit.com/dev/api/oauth:

| Method | API_PATH | Reddit Path | Scope |
| --- | --- | --- | --- |
| accept_mod_invite | accept_mod_invite | api/accept_moderator_invite | modself |
| select_flair | select_flair | api/selectflair/ | flair |
| get_banned | banned | r/%s/about/banned/ | read |
| get_mod_queue | modqueue | r/%s/about/modqueue/ | read |
| get_reports | reports | r/%s/about/reports/ | read |
| get_spam | spam | r/%s/about/spam/ | read |
| get_unmoderated | unmoderated | r/%s/about/unmoderated/ | read |
| get_wiki_banned | wiki_banned | r/%s/about/wikibanned/ | read |
| get_wiki_contributors | wiki_contributors | r/%s/about/wikicontributors/ | read |
"
439,02 July 2015 - fix get_random_submission not raising redirect,2015-07-02T07:22:44Z,2015-07-04T06:44:35Z,,,,"get_random_submission was failing to raise the expected redirect, and was returning 200s instead. Now we handle the json that is returned.

I also added a test for the method. Couldn't think of anything creative but as long as it doesn't crash I guess that's okay.
#438
"
438,get_random_submission() fails to raise redirect exception,2015-07-02T03:16:03Z,2015-07-04T07:20:42Z,Bug,ClientException,ClientException: Expected exception not raised.,"Recently the `get_random_submission()` function has not been working, as it raises the following exception (and warning):

```
r = praw.Reddit('test scraper by [username]')
r.get_random_submission()
```

Warning (from warnings module):
  File ""/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py"", line 198
    return self.connection.send(data)
DeprecationWarning: unicode for buf is no longer accepted, use bytes

Traceback (most recent call last):
  File ""<pyshell#2>"", line 1, in <module>
    r.get_random_submission()
  File ""/usr/local/lib/python2.7/dist-packages/praw/**init**.py"", line 924, in get_random_submission
    raise errors.ClientException('Expected exception not raised.')
ClientException: Expected exception not raised.

I'm not sure what's causing this issue, although I'm guessing it has to do with Reddit's switching over to https recently.

I recently updated praw to the most recent version, as well as attempting to solve these issues by updating urllib3 as well as running `sudo pip install requests[security] --upgrade`, but this has not helped.  In fact, one previous authentication warning was removed by upgrading `requests[security`, but the deprecation warning appeared when I updated urllib 3.  From the changelog, it looks like praw handles its own exceptions instead of using `requests`, but I am not certain if this is the case.

I am running the program using Python 2.7.6 on Ubuntu 14.04 LTS.
"
437,Make the OAuth guide slightly easier to understand,2015-06-30T15:56:33Z,2015-07-01T01:38:08Z,,,,
436,fix multireddit subreddits appearing as dicts,2015-06-29T06:14:09Z,2015-06-29T15:21:20Z,,,,"Hey, concurrent pull requests!

The `subreddits` attribute of Multireddits was being returned as a list of dictionaries. They are now proper Subreddit objects. I'm fairly certain we had solved this before, so I don't know why it came back.

The `_populate` method fixes the issue when requesting a single MR, and the `from_api_response` method was necessary for requesting all of a user's multis with `redditor.get_multireddits()`. Please let me know if I implemented these methods correctly.

Personally I'm still hoping the multireddit API will start making sense.

&nbsp;

Will the changes to objects.py in this PR have any effect on the changes I'm making to Redditor in #433? I've never had two open PRs before.
"
435,Configuration options for SSL validation,2015-06-28T16:20:54Z,2015-06-29T15:07:26Z,,,,"This request stems from #318.  

I think the ""target"" moved slightly since this was opened.  It doesn't restore the ability to use HTTP instead of HTTPS, but a default reddit install using the install script is now HTTPS enabled using a self-signed certificate.  This allows you to ignore the certificate validation errors and successfully interact with an out of the box installation of reddit.

I did what testing I could, I wasn't familiar enough to generate new test cases for with and without valid certificates.
"
434,change scope of report(),2015-06-28T09:37:41Z,2015-06-28T13:54:39Z,,,,"having @restrict_access(login=True), prevents using the report function in the oauth scope.

changing the scope from None to ""report"" and removing the login requirement will allow the report function to work properly with Oauth.

see hide() for precedence of this change and http://www.reddit.com/dev/api/oauth#scope_report
"
433,27 June 2015 - Change fastname mechanics for Subreddits and Redditors,2015-06-28T04:31:48Z,2015-07-07T03:39:20Z,,,,"Now that `subreddit.display_name` is guaranteed to return the properly-cased subreddit name, the `fast_name` attribute needs to become public for use where case doesn't matter, to avoid wasteful calls.

For example:

```
[comment.subreddit.display_name for comment in r.user.get_comments()]
```

grinds the program to a halt.

Subreddit names that come from reddit seem to always be the correct case anyway. fast_name is only incorrect if the user entered the name into `r.get_subreddit()` himself.

I'm open to alternate solutions, but this seems the most straightforward.
"
432,25 June 2015 - Add methods for getting comment + post replies,2015-06-25T21:47:23Z,2015-06-25T23:31:26Z,,,,"Added get_comment_replies and get_post_replies to the PrivateMessagesMixin for fetching [/message/comments/](http://reddit.com/message/comments) and [/message/selfreply/](http://reddit.com/message/selfreply) respectively. 

Cassettes included!
"
431,add :meth:`has_unread`,2015-06-25T11:06:07Z,2015-06-26T03:06:19Z,,,,"Checks to see if user has unread messages.

---

I have no idea how to make that Travis CI build pass or if it's really important in a PR.
"
430,Reddit search API not giving all results,2015-06-23T11:40:41Z,2015-06-23T16:28:38Z,,,,"import praw

def get_data_reddit(search):
    username=""""
    password=""""
    r = praw.Reddit(user_agent='')
    r.login(username,password,disable_warning=True)
    posts=r.search(search, subreddit=None,sort=None, syntax=None,period=None,limit=None)
    title=[]
    for post in posts:
        title.append(post.title)
    print len(title)

search=""stackoverflow""
get_data_reddit(search)
Ouput=953
Why the limitation?

Documenatation mentions
We can at most get 1000 results from every listing, this is an upstream limitation by reddit. There is nothing we can do to go past this limit. But we may be able to get the results we want with the search() method instead.
Any workaround? I hoping someway to overcome in API, I wrote an scrapper for twitter data and find it to be not the most efficient solution.

 I am looking for something like get the date of last reddit related to that query. Then checking each month/year from their now or some better way to calculate intervals.  
For example: One starts from the time reddit started (23 June 2005) and then calculate the number posts in the current year if greater 1000 you further divide in months and so on. It would be a better solution I suspect.

Same Question:http://stackoverflow.com/questions/31000892/reddit-search-api-not-giving-all-results
"
429,"16 June 2015 - Replace ""liked"" with ""upvoted"" for all purposes",2015-06-17T01:23:40Z,2015-06-26T03:37:50Z,,,,"I just realized that praw never received this update. Reddit isn't closing ""liked"" and ""disliked"" yet, but it will probably happen eventually.

http://www.reddit.com/r/changelog/comments/36t36j/reddit_change_all_remaining_uses_of_liked_and/

Added methods get_upvoted and get_downvoted to objects.Redditor, keeping get_liked and get_disliked as shortcut methods with a note in the docstring. I didn't think a full-blown `deocrators.deprecated` warning was necessary, since the functionality doesn't change at all.

&nbsp;

I'm not sure what's going on with `test_oauth2_reddit.test_scope_history`. When I run `py setup.py test` I see this:

```
test_scope_creddits (tests.test_oauth2_reddit.OAuth2RedditTest) ... ERROR
test_scope_edit (tests.test_oauth2_reddit.OAuth2RedditTest) ... ERROR
test_scope_history (tests.test_oauth2_reddit.OAuth2RedditTest) ... ok
test_scope_identity (tests.test_oauth2_reddit.OAuth2RedditTest) ... ERROR
test_scope_modconfig (tests.test_oauth2_reddit.OAuth2RedditTest) ... ERROR
```

The Travis error log is inverted -- all ok except for test_scope_history. What does this mean?
"
428,PyPI tarball doesn't include tests directory but references it in setup.py,2015-06-16T04:59:01Z,2015-07-11T03:57:44Z,,,,"The tarball on PyPI doesn't include the tests directory, but it is still mentioned in setup.py. For your next release on PyPI, would you be able to either remove the lines from setup.py, or add the tests to the tarball?
"
427,Pickling Comment objects is slow,2015-06-15T19:53:41Z,2015-07-03T16:10:23Z,,,,"Test case:

``` python
import pickle, praw
r = praw.Reddit('test')
comment = r.get_info(thing_id='t1_aaaa')
pickle.dumps(comment)
```

Looking at Wireshark, it seems to be caused by an HTTP request.

Good news: implementing `def __getstate__(self): return self.__dict__` in `praw.objects.RedditContentObject` fixes it.

Bad news: I'm not sure why since `pickle` use `__dict__` by default, [as per the docs](https://docs.python.org/2/library/pickle.html#object.__getstate__).

I'm not really familiar with the intricacies of `pickle`, any ideas?
"
426,Fix #425: Can't unpickle Comment objects,2015-06-15T14:08:27Z,2015-06-15T14:48:16Z,,,,
425,Can't unpickle Comment objects (maximum recursion depth exceeded),2015-06-15T13:16:30Z,2015-06-15T14:48:16Z,,RuntimeError,RuntimeError: maximum recursion depth exceeded,"Here's the stack trace:

``` pytb
>>> import pickle
>>> import praw
>>> r = praw.Reddit('test')
>>> comment = r.get_info(thing_id='t1_cs6woop')
>>> pickled = pickle.dumps(comment)
>>> pickle.loads(pickled)
Traceback (most recent call last):
  File ""<console>"", line 1, in <module>
  File ""/usr/lib/python2.7/pickle.py"", line 1382, in loads
    return Unpickler(file).load()
  File ""/usr/lib/python2.7/pickle.py"", line 858, in load
    dispatch[key](self)
  File ""/usr/lib/python2.7/pickle.py"", line 1215, in load_build
    setstate = getattr(inst, ""__setstate__"", None)
  File ""/home/___/.virtualenvs/___/local/lib/python2.7/site-packages/praw/objects.py"", line 82, in __getattr__
    if not self.has_fetched:
  File ""/home/___/.virtualenvs/___/local/lib/python2.7/site-packages/praw/objects.py"", line 82, in __getattr__
    if not self.has_fetched:
  File ""/home/___/.virtualenvs/___/local/lib/python2.7/site-packages/praw/objects.py"", line 82, in __getattr__
    if not self.has_fetched:
  ...
RuntimeError: maximum recursion depth exceeded
```
"
424,Set documentation copyright year to 2015.,2015-06-14T03:41:19Z,2015-06-14T18:47:00Z,,,,
423,13 June 2015 - Add editor and permission mgmt for wikipages (01),2015-06-14T01:22:07Z,2015-06-14T07:10:10Z,,,,"New methods add_editor, remove_editor, get_settings, and edit_settings
in class objects.WikiPage.

Used for managing contributors and permission levels of individual wiki
pages.

Cassettes included!

I don't know if the *args, **kwargs are necessary or useful for these methods. I suppose they don't hurt anything but I'm not sure what one would use them for. I can remove them if you'd like.
"
422,Approving submissions from shadowbanned accounts doesn't work? ,2015-06-08T00:47:19Z,2016-11-13T19:10:02Z,,,,"I'm not entirely sure this is a praw issue, but a while ago I was able to approve shadowbanned accounts using praw. Now I cannot. 

I do this:

`moditem = list(r.get_subreddit('watchpeoplecode').get_mod_queue(limit=1))`

then (after doing some checks) I call `moditem[0].approve()`

A while ago this worked just fine. Now I have manually approve submissions :(
"
421,Allow wiki pages to be retreived via oauth,2015-06-03T20:18:25Z,2015-06-04T07:15:51Z,,,,"This should resolve the issue with getting Forbidden errors when retrieving wiki pages using oauth authentication.
"
420,Decoding a string using 'ascii' is dangerous,2015-06-02T19:47:47Z,2016-11-13T19:08:58Z,,,,"Hi,
I am really appreciated of such a great project. However, I find that there are lots of places decoding a string to the unicode type using the default encoding 'ascii'(calling ""six.text_type"" with the default encoding). I think this is dangerous and will induce crashes when the string contains characters that cannot be decoded by 'ascii'. Would it be better to use the 'utf-8' encoding? 

Thanks,
"
419,Update useful_scripts.rst,2015-06-02T08:40:07Z,2015-06-02T19:32:50Z,,,,"Added EVE: Online Killmail Reddit Bot (https://github.com/ArnoldM904/EK_Reddit_Bot) to the list.
"
418,Add support for hiding and unhiding multiple fullnames,2015-06-01T22:47:51Z,2015-06-02T19:36:26Z,,,,"/u/thorarakis updated hide/unhide endpoints to accept a comma-separated list of fullnames. Add method `hide` and `unhide` to the reddit session under ReportMixin.

http://www.reddit.com/r/redditdev/comments/384w9i/hide_and_unhide_endpoints_now_support_comma/

I was hesitant to create the new ReportMixin class, but since all of the other Mixins are scope-based, and hide / unhide are listed under [the report scope](http://www.reddit.com/dev/api/oauth#scope_report), I felt it was the right thing to do.

`objects.Hideable.hide` now points to `reddit_session.hide`, passing its fullname. `objects.Hideable.unhide` still point to `objects.Hideable.hide` with `unhide=True`.

If you try to give more than 50 fullnames, it returns a very generic HTTPError that doesn't explain the problem. Would you like me to do anything about this, or do you think reddit will add an explanation to the error they return?

Please critique!
"
417,Added set_suggested_sort method to Submission,2015-05-27T06:47:35Z,2015-05-27T08:50:33Z,,,,"This function was added about a month ago, sets the suggested sort if the user has 'modposts' privileges.
"
416,praw.helpers comment and submission streams are slow to start.,2015-05-26T02:09:28Z,2015-05-26T08:38:41Z,,,,"I'm using multiple subreddits (19 at the moment) and it takes over 10 minutes for the first comment or submission to be yielded.
"
415,fixes #413,2015-05-24T07:17:42Z,2015-05-25T14:42:48Z,,,,"If there are no replies in `self._replies['data']['children']` then it will be an empty list anyway.
"
414,Idea: automatically refresh the access info,2015-05-23T18:15:56Z,2015-07-15T20:36:29Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,
413,Comment.replies always gives empty list,2015-05-22T19:19:16Z,2015-09-05T05:17:26Z,,,,"For the last day or so I've noticed this. Not sure if reddit changed something?
"
412,Change disliked and liked to downvoted and upvoted,2015-05-21T22:27:10Z,2015-06-26T03:40:50Z,,,,"Reddit recently changed the URLs for /user/<name>/disliked/ and /user/<name>/liked/.
https://github.com/reddit/reddit/commit/35339beeeb35ea3570b4bc989b8febc656f3df32
"
411,Fix typo in oauth.rst,2015-05-15T20:10:25Z,2015-05-15T21:33:34Z,,,,
410,r.login() ValueError: No JSON object could be decoded,2015-05-10T23:35:38Z,2015-05-11T03:49:47Z,,,,"Hi! I'm getting the same error described in issue #354. I tried following the proposed solution on that ticket but it looks like the praw3 branch/tag doesn't exist anymore. Are there any new fixes that I should be aware of?

```
pip install git+git://github.com/praw-dev/praw.git@praw3
Collecting git+git://github.com/praw-dev/praw.git@praw3
  Cloning git://github.com/praw-dev/praw.git (to praw3) to /tmp/pip-6pvob8d1-build
  Could not find a tag or branch 'praw3', assuming commit.
error: pathspec 'praw3' did not match any file(s) known to git.
```

Here's my setup

```
Python 2.7.6 (default, Mar 22 2014, 22:59:56) 
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import praw
>>> praw.__version__
'2.1.21'
```

Interestingly it works just fine with python 3.4.0
"
409,update_settings to change post type messes up sidebar and submit text,2015-05-10T03:57:18Z,2015-05-25T18:55:12Z,,,,"2.1.21

b.update_settings(b.get_subreddit('politics'),content_options='self')

Problem is in the update_settings function (line 1607 in **init**.py)

```
def update_settings(self, subreddit, **kwargs):
    """"""Update only the given settings for the given subreddit.

    The settings to update must be given by keyword and match one of the
    parameter names in `set_settings`.

    :returns: The json response from the server.

    """"""
    settings = self.get_settings(subreddit)
    settings.update(kwargs)
    del settings['subreddit_id']
    return self.set_settings(subreddit, **settings)
```

When it does get_settings, it pulls the sidebar and submit text, but reddit presents them in an HTML format where ampersand is &amp ; and nonbreaking space is nbsp; etc. 

So instead of only changing the setting you want changed, it also updates the sidebar and submit text with messed up HTML. 
"
408,Add ability to load refresh_token from praw.ini,2015-05-08T14:47:15Z,2015-05-09T07:06:41Z,,,,"I have a bot that does not run 24/7, only uses 1 reddit account, and has no additional configuration.  With this change, I can store refresh_token along with client_id and client_secret in praw.ini and simply refresh_access_information() on bot startup to retrieve a new access_key.
"
407,Is there a way to get submissions who have not been upvoted/downvoted by users?,2015-05-05T19:37:22Z,2015-05-05T19:44:38Z,,,,"As written above, I'm looking for a way to only get submissions that have not been upvoted or downvoted by a specific reddit user yet. Is that possible with the API? Thank you!
"
406,Wrong Usage of the Praw Library or something wrong with the encodings?,2015-05-03T07:04:50Z,2015-05-26T19:55:34Z,,"AttributeError, TypeError, http.client.ResponseNotReady, requests.packages.urllib3.exceptions.ProtocolError, requests.exceptions.ConnectionError","AttributeError: 'str' object has no attribute 'get', TypeError: getresponse() got an unexpected keyword argument 'buffering', http.client.ResponseNotReady: Request-sent, requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',)), requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',))","I am trying to write a small program with PRAW, and I am searching and posting to reddit greek content.

I have a function: 

```
def find_if_posted(usr, title, url):
    for content in usr.search(title):
        return True
    for content in usr.search(url):
        return True
    return False
```

which tries to search if there is a post with the same title or url. When it starts to search with the url I am getting the error:

```
Traceback (most recent call last):
  File ""C:\Users\hargikas\Dropbox\Private\src\reddit\auto_poster\leftist.py"", line 133, in <module>
    while (key is None) or find_if_posted(usr, key, content[key]):
  File ""C:\Users\hargikas\Dropbox\Private\src\reddit\auto_poster\leftist.py"", line 86, in find_if_posted
    for content in usr.search(url):
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 1108, in search
    **kwargs):
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 515, in get_content
    root = page_data.get(root_field, page_data)
AttributeError: 'str' object has no attribute 'get'
```

Also when I am trying to post something (using the submit function) I get this error: 

```
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 372, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""C:\Python34\lib\http\client.py"", line 1162, in getresponse
    raise ResponseNotReady(self.__state)
http.client.ResponseNotReady: Request-sent

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\requests\adapters.py"", line 370, in send
    timeout=timeout
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 597, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\util\retry.py"", line 245, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\packages\six.py"", line 309, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""C:\Python34\lib\site-packages\requests\packages\urllib3\connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""C:\Python34\lib\http\client.py"", line 1162, in getresponse
    raise ResponseNotReady(self.__state)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hargikas\Dropbox\Private\src\reddit\auto_poster\leftist.py"", line 140, in <module>
    usr.submit('greekreddit' , key, url=content[key])
  File ""C:\Python34\lib\site-packages\praw\decorators.py"", line 338, in wrapped
    return function(cls, *args, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\decorators.py"", line 237, in wrapped
    return function(obj, *args, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 2218, in submit
    retry_on_error=False)
  File ""C:\Python34\lib\site-packages\praw\decorators.py"", line 163, in wrapped
    return_value = function(reddit_session, *args, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 561, in request_json
    retry_on_error=retry_on_error)
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 402, in _request
    response = handle_redirect()
  File ""C:\Python34\lib\site-packages\praw\__init__.py"", line 375, in handle_redirect
    timeout=timeout, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\handlers.py"", line 144, in wrapped
    result = function(cls, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\handlers.py"", line 54, in wrapped
    return function(cls, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\handlers.py"", line 99, in request
    allow_redirects=False)
  File ""C:\Python34\lib\site-packages\requests\sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""C:\Python34\lib\site-packages\requests\adapters.py"", line 415, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',))
```

Do you have an idea what I am doing wrong? I am using Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:43:06) [MSC v.1600 32 bit (Intel)] on win32 and PRAW Version: 2.1.21

Thank you in advance!
Harry
"
405,TypeError: getresponse() got an unexpected keyword argument 'buffering',2015-04-26T01:40:02Z,2015-04-26T16:03:48Z,,"TypeError, http.client.ResponseNotReady, requests.packages.urllib3.exceptions.ProtocolError, requests.exceptions.ConnectionError","TypeError: getresponse() got an unexpected keyword argument 'buffering', http.client.ResponseNotReady: Request-sent, requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',)), requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',))","So I've been working on an update for my bot, and I've noticed a strange issue that I haven't seen before. Basically the client seems to open up multiple HTTP connections to reddit, which is something that I don't remember seeing before (it is possible that I just forgot). I am making use of username mentions to catch requests for the bot to reply, so I am scanning my bot's inbox continually for different types of requests. For example, I sent a request to change a user's default translation for my bot, and I get this crash log:

```
INFO:versebot:Connecting to reddit...
:0: UserWarning: The keyword `bot` in your user_agent may be problematic.
INFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): ssl.reddit.com
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.reddit.com
INFO:versebot:Successfully connected to reddit!
INFO:versebot:Connecting to database...
INFO:versebot:Successfully connected to database!
INFO:versebot:Updating translation list table...
INFO:versebot:Translation list update successful!
INFO:versebot:Cleaning old user translation entries...
INFO:versebot:User translation cleaning successful!
INFO:versebot:Beginning to scan for new inbox messages...
INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (2): www.reddit.com
Traceback (most recent call last):
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 372, in _make_request
    httplib_response = conn.getresponse(buffering=True)
TypeError: getresponse() got an unexpected keyword argument 'buffering'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""/home/matthieu/.pyenv/versions/3.4.2/lib/python3.4/http/client.py"", line 1164, in getresponse
    raise ResponseNotReady(self.__state)
http.client.ResponseNotReady: Request-sent

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/adapters.py"", line 370, in send
    timeout=timeout
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 597, in urlopen
    _stacktrace=sys.exc_info()[2])
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py"", line 245, in increment
    raise six.reraise(type(error), error, _stacktrace)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py"", line 309, in reraise
    raise value.with_traceback(tb)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 544, in urlopen
    body=body, headers=headers)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py"", line 374, in _make_request
    httplib_response = conn.getresponse()
  File ""/home/matthieu/.pyenv/versions/3.4.2/lib/python3.4/http/client.py"", line 1164, in getresponse
    raise ResponseNotReady(self.__state)
requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""versebot.py"", line 248, in <module>
    bot.main_loop()
  File ""versebot.py"", line 61, in main_loop
    self.respond_to_user_translation_request(message)
  File ""versebot.py"", line 207, in respond_to_user_translation_request
    message.reply(""Your default translations have been updated successfully!"")
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/objects.py"", line 384, in reply
    response = self.reddit_session._add_comment(self.fullname, text)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/decorators.py"", line 338, in wrapped
    return function(cls, *args, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/__init__.py"", line 2173, in _add_comment
    retry_on_error=False)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/decorators.py"", line 163, in wrapped
    return_value = function(reddit_session, *args, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/__init__.py"", line 561, in request_json
    retry_on_error=retry_on_error)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/__init__.py"", line 402, in _request
    response = handle_redirect()
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/__init__.py"", line 375, in handle_redirect
    timeout=timeout, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/handlers.py"", line 144, in wrapped
    result = function(cls, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/handlers.py"", line 54, in wrapped
    return function(cls, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/praw/handlers.py"", line 99, in request
    allow_redirects=False)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/sessions.py"", line 573, in send
    r = adapter.send(request, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python3.4/site-packages/requests/adapters.py"", line 415, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',))
sys:1: ResourceWarning: unclosed <ssl.SSLSocket fd=4, family=AddressFamily.AF_INET, type=2049, proto=6, laddr=('10.0.0.30', 45140), raddr=('198.41.209.141', 443)>
/home/matthieu/.pyenv/versions/3.4.2/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty
```

I believe the most important part of this crash log is `TypeError: getresponse() got an unexpected keyword argument 'buffering'`, as stated in the title. Any idea what could be causing this?
"
404,25 April 2015 - Multireddit Support - Part 1,2015-04-25T05:25:56Z,2015-05-26T02:46:51Z,,,,"TO DO:
- Fix removing subreddits from Multireddits
- Fix deleting multireddits
- Write tests

DONE:

| Function | r | Object |
| --- | --- | --- |
| Get user's multireddits | ✓ | ✓ Redditor |
| Create multireddit | ✓ | N/A |
| Copy multireddit | ✓ | ✓ Multireddit |
| Rename multireddit | ✓ | ✓ Multireddit |
| Edit multireddit | ✓ | ✓ Multireddit |
| Add subreddit to multireddit | N/A | ✓ Multireddit |

Even though removals and deletions aren't ready yet, I wanted to make this PR to get some groundwork out, get your feedback in, and make sure my copy doesn't fall behind Master overnight or something.

Deletions aren't ready because they're throwing me 403 errors for reasons I don't understand. [Redditdev post here](http://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/). Any ideas bboe? Pull request Part 2 will happen when those are working.

Please critique! There are lots of tweaks that need to be made, I just had to get a working model first.
"
403,Adds get_sticky function to Subreddit object,2015-04-24T16:02:32Z,2015-04-27T00:47:02Z,,,,"My attempt at fixing my own issue, issue #402.

Unsure how to go about the 404 though, since this is new for praw. I figured since trying to view /r/lounge as a non-gilded user gives a 403 that I'd leave the 404 here for someone with more input on praw to decide.
"
402,Feature: get_stickied() function,2015-04-24T13:51:12Z,2015-04-27T00:51:41Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"In a browser, we can just go to `http://www.reddit.com/r/redditdev/about/sticky`, which will return an error page if there's no sticky, or, the thread if there is one.

I mean it's not a huge thing but for now we can just do

`submission = r.get_subreddit('redditdev').get_hot().next()`

...and check `if submission.stickied is True` for now.

I say `get_stickied()` instead of `get_sticky()` in the hopes that multiple stickies will some day be a reality...but there may be an issue with types and such down the line... Just a thought! Yikes, just considered what use the `/about/sticky/` page would be if there were multiple stickies...
"
401,Assertion Error while filling in a MoreComments object.,2015-04-20T23:23:46Z,2016-11-14T03:52:33Z,,,,"Sorry this post is so long. I'm trying to give all the detail I have.

I am currently helping a user collect comments on a sizeable thread (3,231 comments) using a custom method to replace MoreComments objects. One of them is producing an Assertion Error, and I can't figure out why.

Here is the thread: http://redd.it/2y8jyo

I have modified objects.py to give me some more information about the object causing the error, like this:

```
def _continue_comments(self, update):
    print(self.name, self.parent_id, self.count, self.children)
    assert len(self.children) > 0
```

And here is the output:

```
Got 1 more, 2514 so far.
Got 0 more, 2514 so far.
t1_cp7ocrw t1_cp7o7ej 0 ['cp7ocrw', 'cp7phio']
Got 2 more, 2516 so far.
t1_cp7ro20 t1_cp7qqi2 0 ['cp7ro20']
Got 1 more, 2517 so far.
t1_cp7k5gi t1_cp7g40r 0 ['cp7k5gi']
Got 2 more, 2519 so far.
t1__ t1_cp7ggng 0 []
Traceback (most recent call last):
  File ""commentaugment.py"", line 76, in a
    result = function(*args, **kwargs)
  File ""C:\Python34\lib\site-packages\praw\objects.py"", line 706, in comments
    return self._continue_comments(update)
  File ""C:\Python34\lib\site-packages\praw\objects.py"", line 687, in _continue_comments
    assert len(self.children) > 0
AssertionError
```

As you can see, the one giving the error seems to have no fullname, and two underscores. The parent_id points to [this comment](http://www.reddit.com/r/TwoXChromosomes/comments/2y8jyo/_/cp7ggng), which has 1 child by /u/dalagrath even though it printed ""0, []"". I'm trying to check each step of the reply chain and figure out where this MoreComments object comes into play, but I'm getting really lost. [This is as close as I got](http://i.imgur.com/NxDMz0E.png) to matching the console by permalinking [here](http://www.reddit.com/r/TwoXChromosomes/comments/2y8jyo/planet_fitness_cancels_womans_membership_after/cp7ct6j), but nothing looks strange in the browser.

I see that you had a different Assertion issue [in January](https://github.com/praw-dev/praw/commit/af8676fea6a1f51f357b719a73017ea980500aa7) but it looks unrelated. I'm not sure if my code is relevant, but [here it is](https://github.com/voussoir/reddit/blob/master/Prawtimestamps/commentaugment.py#L88).

Personally I'm content with try-catching the AssertionError and ignoring it, but I wanted to bring this up. Please let me know if there's any way I can help to solve it. Thank you!
"
400,Update useful_scripts.rst,2015-04-20T14:04:49Z,2015-04-24T23:57:32Z,,,,"I've added a bot which is based on PRAW.
"
399,Add method get_comment,2015-04-19T13:02:52Z,2015-05-25T22:28:51Z,,,,"A little easier to use than `get_info(thing_id=id)`
"
398,Comment.replies always returns `[]`,2015-04-19T11:42:08Z,2015-04-19T19:14:10Z,,,,"I'm using PRAW version 3.0a1 (downloaded from github for a particular method) but it seemed to have broken the `comment.replies` method. It always seems to return `[]` even when there are a lot of comments.

I found that removing the first `if` statement in the method solves the problem.

Actually, just now, that has stopped working. I can no longer get comments...

EDIT: Changed to the stable build (2.1.21) and it had the same problem so it's not version related.
"
397,http -> https leads to `praw.errors.RedirectException: Unexpected redirect`,2015-04-16T11:22:39Z,2015-04-16T14:55:12Z,,,,"Hi

I've encountered this error while using [rtv](https://github.com/michael-lazar/rtv/issues/85) to interact with reddit, using praw.

If I delete lines 163 and 164 in `internal.py`, i.e. delete

``` python
    elif not RE_RANDOM.search(response.url):
        raise RedirectException(response.url, new_url)
```

the error isn't raised anymore and I can log in.

Is it possible that in enforcing https everywhere praw is creating a redirect that it isn't handling well? The specific redirect in question here is from `http://reddit.com/user/username/about/.json` to `https://reddit.com/user/username/about/.json`.
"
396,14 April 2015 - Added method for getting a Message object by its ID or fullname,2015-04-15T01:09:39Z,2015-04-15T06:09:21Z,,,,"I don't quite understand the new format for the Test files, but judging by your recent commits I assume they're a work in progress. I added a test for this method in olds.py, I hope this isn't a problem.

Please critique!

edit: Did coverage go down because I did not directly test Message.from_id? I can add that in if you like.
"
395,Add coveralls,2015-04-12T15:04:10Z,2015-04-12T18:13:03Z,,,,"Coveralls is a great tool to find out where praw needs more test coverage. If you enable the repo [on coveralls](https://coveralls.io/repos/new) this will work.
"
394,Fix for issue #371,2015-04-12T11:53:18Z,2015-04-12T14:16:37Z,,,,
393,Fix for #392,2015-04-10T23:51:34Z,2015-04-11T04:43:12Z,,,,"get_redditor() is now lazy
"
392,get_redditor should be by default lazy,2015-04-10T23:41:38Z,2015-04-11T04:43:27Z,,,,"get_redditor() is the only get method that isn't lazy. 
"
391,MoreComments.comments() return value,2015-04-10T07:17:12Z,2016-11-14T06:08:25Z,,,,"I've noticed an inconsistency in the way that MoreComments.comments() are being returned. Not sure if this is intended behavior or not, but I've found it very difficult to work around. 

Here's an example:

``` python
reddit = praw.Reddit(user_agent='reddit terminal viewer v0.0')
submission = reddit.get_submission('http://www.reddit.com/r/CollegeBasketball/comments/31owr1/game_thread_ncaa_national_championship_wisconsin/')
for comment in submission.comments:
    if isinstance(comment, praw.objects.MoreComments):
        print('MoreComments')
    else:
        print(comment._replies)
```

returns something like this

``` python
[<praw.objects.Comment object at 0x7f63d2b1a9e8>, <praw.objects.MoreComments object at 0x7f63d2b1aac8>]
[<praw.objects.Comment object at 0x7f63d2b1ada0>, <praw.objects.MoreComments object at 0x7f63d2b1aeb8>]
[<praw.objects.Comment object at 0x7f63d2b25080>, <praw.objects.MoreComments object at 0x7f63d2b252b0>]
[<praw.objects.MoreComments object at 0x7f63d2b25438>]
[<praw.objects.Comment object at 0x7f63d2b25668>, <praw.objects.MoreComments object at 0x7f63d2b25828>]
[]
[<praw.objects.Comment object at 0x7f63d2b25ac8>, <praw.objects.MoreComments object at 0x7f63d2b25dd8>]
[<praw.objects.Comment object at 0x7f63d227a048>]
[<praw.objects.MoreComments object at 0x7f63d227a588>]
[]
[<praw.objects.MoreComments object at 0x7f63d227a8d0>]
```

However, doing the same loop over `for comment in more_comments_object.comments(update=True)` returns

``` python
None
MoreComments
[]
None
MoreComments
None
MoreComments
None
MoreComments
[]
[]
[]
```

It looks like in the second case the MoreComments aren't getting attached the their parent's replies. All of the **None** values should actually be **[MoreComments]**. E.g.

``` python
[MoreComments]
[]
[MoreComments]
[MoreComments]
[MoreComments]
[]
[]
[]
```

As a result, trying to build a comment tree by flattening each `comment.replies` inserts duplicate replies into the tree and makes a bunch of unnecessary calls to _api/comments/id_ instead of _/api/morechildren_.
"
390,Test suite responses should be saved/cached,2015-04-09T20:13:14Z,2015-07-11T22:03:44Z,,,,"The test suite should be able to run in a cache-only mode where it needn't hit reddit's servers to run tests. Something like https://github.com/sigmavirus24/betamax might do the trick to enable hitting the servers periodically to ensure the cached responses are still in-sync, but to not require end-to-end communication most of the time.

Update:
I began the process of refactoring the test suite. All old tests now live in `tests/old.py`. They should be moved into a corresponding `test_<class>.py` file and modified to work with betamax. When `old.py` is removed and there is no longer a need for `travis_suite` because all tests will run on travis, this issue is considered complete.
"
389,Remove --use-mirrors flag,2015-04-09T14:18:33Z,2015-04-09T19:51:17Z,,,,"This flag has been deprecated in pip for a long time now
https://github.com/pypa/pip/pull/1098
"
388,Add pypy builds,2015-04-09T11:51:20Z,2015-04-09T19:49:57Z,,,,
387,Switch to Travis Docker build system,2015-04-09T09:21:43Z,2015-04-09T20:01:47Z,,,,"This commit switches praw to the Travis Docker build system. [This blog post](http://blog.travis-ci.com/2014-12-17-faster-builds-with-container-based-infrastructure/) highlights the benefits, but the main ones are:
- Builds start in seconds
- Faster builds
- More available resources in a build container
- Better network capacity, availability and throughput
- Caching available for open source projects
- Easier to scale
"
386,Add python nightly testing,2015-04-09T08:51:00Z,2015-04-09T19:44:40Z,,,,"[PEP478](http://legacy.python.org/dev/peps/pep-0478/) shows Python 3.5 is scheduled for beta release  on May 24, 2015 and final release on September 13, 2015. I think it's worth starting testing on Python 3.5, but allowing failures until it is at least in beta.
"
385,Test for #380,2015-04-08T19:48:30Z,2015-04-08T23:51:28Z,,,,"Wrote a test for self posts with no body
"
384,Fix for issue #375,2015-04-06T19:51:19Z,2015-04-26T23:00:57Z,,,,
383,Update useful_scripts.rst,2015-04-06T03:15:47Z,2015-04-06T14:54:23Z,,,,"Tweaked submission format
"
382,Update useful_scripts.rst,2015-04-06T00:17:46Z,2015-04-06T03:06:59Z,,,,"Adding own script to file.
"
381,Fix for issue #380,2015-04-05T00:47:26Z,2015-04-05T06:12:01Z,,,,
380,Cannot create a self post without including text for the body,2015-04-05T00:36:08Z,2015-04-05T06:12:21Z,,,,"This was asked about on [StackOverflow](http://stackoverflow.com/questions/29413324/submit-post-with-praw).
"
379,Using https in the url for get_submission() returns a 403 error.,2015-03-28T08:45:56Z,2015-04-26T16:06:16Z,,,,"I have a script that submits a link to reddit, and because I'm using OAuth it returns a string with the https url(when I use cookie based login it returns an object and the permalink is http via object.permalink)

Anyway, when I submit this link I want to add a comment and that is my problem. I have resorted to link[:4]+link[5:] in order to remove the s from https.

Just wanted to point this issue out as I am currently in the phase of switching to OAuth, if anyone knows a more efficient solution to adding a comment to a link you just submitted, I would love to hear it, thanks.
"
378,All exceptions should be in the PRAW namespace,2015-03-26T17:34:00Z,2015-05-25T18:53:56Z,,,,"PRAW users should only need to handle a single exception class in any of their code, save for exceptions like `KeyboardInterrupt`.
"
377,Adding 'hide_ads' to the input params of 'set_settings',2015-03-25T04:14:05Z,2015-03-26T15:21:28Z,,,,"This is an attempted fix for [this issue I opened a little while ago](https://github.com/praw-dev/praw/issues/370).

I'm pretty inexperienced in open source and was unable to figure out how to run the test suite or how to use this version of praw instead of the default installed by pip, any help in the right direction and I'd be glad to verify that this fixes my issue, but this is a pretty simple change.
"
376,add_moderator() doesn't support non-full permissions,2015-03-24T22:18:38Z,2016-11-13T19:05:57Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"The function add_moderator() allows you to add a mod to a subreddit with full permissions, but it would be great to have a way to add a mod with partial permissions.

Thanks!
"
375,"set_flair_csv returns ""improperly formatted row, ignoring"" if flair_text contains a comma.",2015-03-19T15:39:04Z,2016-07-18T03:56:09Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"Test code:

```
import praw
r = praw.Reddit(...)
r.login(username, password)
giveflair = [{'flair_css_class': 'icedice', 'user': username, 'flair_text': 'ayy, lmao'}]
print r.get_subreddit(subreddit).set_flair_csv(giveflair)
```

Returns
`[{u'status': u'skipped', u'errors': {u'row': u'improperly formatted row, ignoring'}, u'ok': False, u'warnings': {}}]`

While changing giveflair to...
`giveflair = [{'flair_css_class': 'icedice', 'user': username, 'flair_text': 'ayy lmao'}]`

Goes through just fine. Escaping doesn't seem to work either.

EDIT: Only tested with commas. Unsure about other symbols...
"
374,Fix submission refresh parameters.,2015-03-18T07:24:07Z,2015-03-26T15:32:16Z,,,,"Fix for #373 
"
373,submission.refresh() removes comment_sort order,2015-03-18T05:53:24Z,2015-03-26T16:06:50Z,,,,"See the example below. `comment_sort` is initially defined as `new`, but regresses to the default `hot` after being refreshed.

``` python
reddit = praw.Reddit()
URL = 'http://www.reddit.com/r/gifs/comments/2zexjb'
submission = reddit.get_submission(url=URL, comment_sort='new')

print(submission.comments[0].score)
submission.refresh()
print(submission.comments[0].score)
```

```
> 1
> 2165
```

It looks like the issue is that `submission._comment_sort` is being ignored by `Refreshable.refresh()`. I've got a fix for this, let me know if you would like me to submit a pull request.
"
372,Non-Lazy Loading of Multireddits Fails,2015-03-04T00:24:21Z,2015-07-11T22:31:49Z,⭐️ New Contributor Friendly ⭐️,HTTPError,HTTPError: 404 Client Error: Not Found,"Example Code:

``` python
In [1]: import praw

In [2]: reddit = praw.Reddit(user_agent='example')

In [3]: reddit.get_subreddit('linux+opensource', fetch=True)
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-4-ebfe79b7397d> in <module>()
----> 1 reddit.get_subreddit('linux+opensource', fetch=True)

/usr/local/lib/python2.7/dist-packages/praw/__init__.pyc in get_subreddit(self, subreddit_name, *args, **kwargs)
   1015         elif sr_name_lower == 'randnsfw':
   1016             return self.get_random_subreddit(nsfw=True)
-> 1017         return objects.Subreddit(self, subreddit_name, *args, **kwargs)
   1018
   1019     def get_subreddit_recommendations(self, subreddits, omit=None):

/usr/local/lib/python2.7/dist-packages/praw/objects.pyc in __init__(self, reddit_session, subreddit_name, json_dict, fetch)
   1348         info_url = reddit_session.config['subreddit_about'] % subreddit_name
   1349         super(Subreddit, self).__init__(reddit_session, json_dict, fetch,
-> 1350                                         info_url)
   1351         self.display_name = subreddit_name
   1352         self._url = reddit_session.config['subreddit'] % subreddit_name

/usr/local/lib/python2.7/dist-packages/praw/objects.pyc in __init__(self, reddit_session, json_dict, fetch, info_url, underscore_names)
     71         self.reddit_session = reddit_session
     72         self._underscore_names = underscore_names
---> 73         self.has_fetched = self._populate(json_dict, fetch)
     74
     75     def __eq__(self, other):

/usr/local/lib/python2.7/dist-packages/praw/objects.pyc in _populate(self, json_dict, fetch)
    131     def _populate(self, json_dict, fetch):
    132         if json_dict is None:
--> 133             json_dict = self._get_json_dict() if fetch else {}
    134         self.json_dict = (json_dict if
    135                           self.reddit_session.config.store_json_result

/usr/local/lib/python2.7/dist-packages/praw/objects.pyc in _get_json_dict(self)
    124         try:
    125             response = self.reddit_session.request_json(self._info_url,
--> 126                                                         as_objects=False)
    127         finally:
    128             self.reddit_session._use_oauth = prev_use_oauth

/usr/local/lib/python2.7/dist-packages/praw/decorators.pyc in wrapped(reddit_session, *args, **kwargs)
    161     def wrapped(reddit_session, *args, **kwargs):
    162         try:
--> 163             return_value = function(reddit_session, *args, **kwargs)
    164         except HTTPError as exc:
    165             if exc.response.status_code != 400:

/usr/local/lib/python2.7/dist-packages/praw/__init__.pyc in request_json(self, url, params, data, as_objects, retry_on_error)
    555             url += '.json'
    556         response = self._request(url, params, data,
--> 557                                  retry_on_error=retry_on_error)
    558         hook = self._json_reddit_objecter if as_objects else None
    559         # Request url just needs to be available for the objecter to use

/usr/local/lib/python2.7/dist-packages/praw/__init__.pyc in _request(self, url, params, data, files, auth, timeout, raw_response, retry_on_error)
    397             try:
    398                 response = handle_redirect()
--> 399                 _raise_response_exceptions(response)
    400                 self.http.cookies.update(response.cookies)
    401                 if raw_response:

/usr/local/lib/python2.7/dist-packages/praw/internal.pyc in _raise_response_exceptions(response)
    176         else:
    177             raise OAuthException(msg, response.url)
--> 178     response.raise_for_status()
    179
    180

/usr/lib/python2.7/dist-packages/requests/models.pyc in raise_for_status(self)
    771
    772         if http_error_msg:
--> 773             raise HTTPError(http_error_msg, response=self)
    774
    775     def close(self):

HTTPError: 404 Client Error: Not Found

In [4]: reddit.get_subreddit('linux+opensource', fetch=False)
Out[4]: Subreddit(subreddit_name='linux+opensource')
```

Originally discovered over [here](https://github.com/michael-lazar/rtv/issues/15)
"
371,AttributeError on import,2015-03-03T08:15:49Z,2015-04-12T14:17:01Z,,AttributeError,AttributeError: 'NoneType' object has no attribute 'split',"``` python
Traceback (most recent call last):
  File ""/home/anqxyr/code/test.py"", line 9, in <module>
    import praw
  File ""/usr/local/lib/python3.4/dist-packages/praw/__init__.py"", line 2278, in <module>
    from praw import objects
  File ""/usr/local/lib/python3.4/dist-packages/praw/objects.py"", line 1560, in <module>
    _add_aliases()
  File ""/usr/local/lib/python3.4/dist-packages/praw/objects.py"", line 1559, in _add_aliases
    mixin.__name__))
  File ""/usr/local/lib/python3.4/dist-packages/praw/decorators.py"", line 61, in alias_function
    wrapped.__doc__ = wrapped.__doc__.split('\n', 1)[0]
AttributeError: 'NoneType' object has no attribute 'split'
```

Not sure what the issue is. Both praw and all the dependencies seem up-to-date.
"
370,UserWarning: [u'hide_ads'] thrown on an update_settings call,2015-02-28T02:52:06Z,2015-03-26T16:25:05Z,,,,"I'm not sure if this is a new reddit feature, but this field isn't included in the set_settings method.  I've replaced my update_settings call with basically the body of set_settings but with an extra 

```
del settings['hide_ads'] 
```

call, but this field should be included in the settings parameter.
"
369,Update captcha requirements,2015-02-18T21:23:45Z,2015-03-26T16:27:25Z,,,,"Registering no longer requires a captcha, and creating a subreddit now
can require one.
"
368,Cookie based authentication is being deprecated for OAuth2.   The current flow for OAuth is not what is recommended.,2015-02-13T21:24:57Z,2016-06-15T05:15:23Z,,,,"I'm sure you're aware that OAuth2 is the way reddit is heading considering the contributors to the project.  However since OAuth2 is the future, I wanted to bring to your attention one of the comments made regarding the current flow implemented by praw. 

https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/cocegz2
"
367,Limit warning logging changes to praw,2015-02-11T19:04:17Z,2016-06-15T05:29:23Z,⭐️ New Contributor Friendly ⭐️,,,"When importing praw, i get this warning:

```
/home/user/dev/venvs/main3/lib/python3.4/imp.py:32: PendingDeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  PendingDeprecationWarning)
```
"
366,AssertionError when executing replace_more_comments(),2015-02-08T22:49:30Z,2015-02-09T02:38:55Z,,,,"Remember the `word_freqs` program that we (@bboe and I) coauthored ages ago? Someone reported an AssertionError when running it on certain subreddits: http://www.reddit.com/r/MUWs/comments/2u3stx/help_assertion_error/

It seems to happen in the middle of an analysis. Any idea what could be causing it?
"
365,Can't log in using PRAW,2015-02-07T15:33:32Z,2015-02-07T18:54:19Z,,HTTPError,HTTPError: 403 Client Error: Forbidden,"I used to be able to, but now with the latest version of PRAW I can't log in any more. Code sample:

```
import praw

r = praw.Reddit(user_agent=""/u/rhiever test"")
r.login(""rhiever-test"", ""rhiever-test"") # this is an actual login
```

It comes back with:

```
HTTPError: 403 Client Error: Forbidden
```

I thought that maybe my accounts were banned, but even a completely new account receives the same error. I've tried this from home and from other networks and it gives the same error every time. I've also tried completely uninstalling and reinstalling PRAW. Is there a setting I'm missing?
"
364,Error in praw-multipython when running comment and submission streams in different threads.,2015-02-03T17:10:53Z,2016-06-15T05:27:24Z,,UnboundLocalError,UnboundLocalError: local variable 'retval' referenced before assignment,"I'm on windows 8.1. I have a multi-threaded reddit bot that runs three scans, one is using helpers.comment_stream, another that runs helpers.submission_stream. The third thread scans my inbox looking for messages every 60 secs to tell the bot to update configuration from a wiki.

Most of the time all three run without a problem. But intermittently my praw-multiprocess gives the following error twice (I'm assuming one for each stream thread):

Traceback (most recent call last):
File ""C:\Python34\lib\threading.py"", line 921, in bootstrap_inner
self.run()
File ""C:\Python34\lib\threading.py"", line 869, in run
self.target(self._args, *self._kwargs)
File ""C:\Python34\lib\socketserver.py"", line 616, in process_request_thread
self.handle_error(request, client_address)
File ""C:\Python34\lib\socketserver.py"", line 613, in process_request_thread
self.finish_request(request, client_address)
File ""C:\Python34\lib\socketserver.py"", line 344, in finish_request
self.RequestHandlerClass(request, client_address, self)
File ""C:\Python34\lib\socketserver.py"", line 669, in __init
self.handle()
File ""C:\Python34\lib\site-packages\praw\multiprocess.py"", line 80, in handle
cPickle.dump(retval, self.wfile, # pylint: disable-msg=E1101
UnboundLocalError: local variable 'retval' referenced before assignment

Afterwards, I stop seeing requests for either of my stream threads. The mail requests still show up every 60 seconds. I have to restart the bot to recover.

Each thread creates it's own praw.Reddit object for connecting to reddit for a total of 3.
"
363,login() returns 403 Client Error: Forbidden,2015-01-27T03:52:27Z,2015-01-27T04:28:45Z,,requests.exceptions.HTTPError,requests.exceptions.HTTPError: 403 Client Error: Forbidden,"When I use the code:

```
import praw, time

r = praw.Reddit(user_agent=""Bot experiment by /u/username"")
r.login('redacted', 'redacted')
```

I get the traceback:

```
Traceback (most recent call last):
File ""redacted"", line 5, in <module>
r.login('redacted', 'redacted')
File  ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-    packages/praw/__init__.py"",   line 1263, in login
self.request_json(self.config['login'], data=data)
File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site- packages/praw/decorators.py"", line 161, in wrapped
return_value = function(reddit_session, *args, **kwargs)
File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-  packages/praw/__init__.py"", line 519, in request_json
response = self._request(url, params, data)
File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site- packages/praw/__init__.py"", line 383, in _request
_raise_response_exceptions(response)
File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site- packages/praw/internal.py"", line 172, in _raise_response_exceptions
response.raise_for_status()
File ""/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-    packages/requests/models.py"", line 831, in raise_for_status
raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden
```

I have tried this from both mac and pc, with python 2.7 & 3.4, with https turned on and off, and I am sure praw is up to date as well as my python versions.  I have tried this from more than one account and double and triple checked my name and passwords.  I also tried omitting the username and password and logging in from the terminal.  I also tried making sure I was logged out of reddit too.  I've asked this on a few other forums and nobody was able to help and then somebody suggested submitting an issue here.
"
362,praw.helpers.flatten_tree seems to be broken in version v2.1.20,2015-01-25T01:56:23Z,2015-12-19T22:12:53Z,Bug,,,"It may be the replace_more_comments method but either way on the latest version its not retrieving all the comments any more
"
361,Fix #360. Interpret scopes as space-separated,2015-01-22T18:55:12Z,2015-01-22T18:59:46Z,,,,"In [my efforts](https://github.com/reddit/reddit/commit/a82bc65192015e37276e8fe4e676f8042fd5fcbe) to make reddit's OAuth2 more [spec compliant](http://tools.ietf.org/html/rfc6749#section-3.3), I broke PRAW.
"
360,Unable to use multiple OAuth scopes,2015-01-21T17:35:46Z,2015-01-22T18:59:47Z,,,,"If I authorize a user with the 'identity' scope, it works just fine and I can use the get_me() function. If I authorize the user with the ['identity', 'submit'] scope, the user is authorized, but when I call get_me(), it I am getting the following error:

> OAuthScopeRequired: get_me requires the OAuth2 scope identity

When I print the access_information object, the following is contained in the scope variable:

```
set([u'identity submit'])
```

Is it possible that multiple scopes is not being parsed properly by PRAW?

This started happening in both of my applications that run similar authorizations. My guess is that reddit changed the way it sends back multiple OAuth scopes and PRAW isn't handling that correctly anymore.
"
359,Run tests locally using tox,2015-01-19T23:15:05Z,2015-01-27T06:35:01Z,,,,"Provides a `tox.ini` file that will allow tests to be run using tox. This should make it easier for developers to run tests against a spectrum of Python interpreters when developing locally.

The tox config is set up to run tests in a manner similar to how Travis runs tests. In particular, it only runs tests against a subset of test suites. (It seems that several test cases don't pass, but perhaps eventually they can be fixed up and tox can run the full suite of tests.)
"
358,r.login() throws 'Expecting value: line 1 column 1 (char 0)',2015-01-06T02:19:33Z,2015-01-06T18:38:27Z,,ValueError,ValueError: Expecting value: line 1 column 1 (char 0),"Since yesterday praw throws an error when trying to login - I'm using username/password authentication, so I don't know how it is with oauth2. I tried to login with my personal account as well with three different bots, all throw the same error, so I guess it is not an account problem.

The same issue was mentioned [here](https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/).

Is this a known problem or is it just happening right now, that reddit sends back invalid json-data (what would surprise me, since AutoModerator runs well)?

Thanks in advance.

Here is the traceback output, `conf` is a dict containing various information needed for the bot, other than that its just importing and setting the useragent:

``` python
In [9]: r.login(conf['username'], conf['password'])
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-9-fdbc721749e9> in <module>()
----> 1 r.login(conf['username'], conf['password'])

/usr/lib/python3.4/site-packages/praw/__init__.py in login(self, username, password)
   1264         # Update authentication settings
   1265         self._authentication = True
-> 1266         self.user = self.get_redditor(user)
   1267         self.user.__class__ = objects.LoggedInRedditor
   1268

/usr/lib/python3.4/site-packages/praw/__init__.py in get_redditor(self, user_name, *args, **kwargs)
    888
    889         """"""
--> 890         return objects.Redditor(self, user_name, *args, **kwargs)
    891
    892     @decorators.restrict_access(scope='read')

/usr/lib/python3.4/site-packages/praw/objects.py in __init__(self, reddit_session, user_name, json_dict, fetch)
    661         self.name = user_name
    662         super(Redditor, self).__init__(reddit_session, json_dict,
--> 663                                        fetch, info_url)
    664         self._url = reddit_session.config['user'] % self.name
    665         self._mod_subs = None

/usr/lib/python3.4/site-packages/praw/objects.py in __init__(self, reddit_session, json_dict, fetch, info_url, underscore_names)
     70         self.reddit_session = reddit_session
     71         self._underscore_names = underscore_names
---> 72         self.has_fetched = self._populate(json_dict, fetch)
     73
     74     def __eq__(self, other):

/usr/lib/python3.4/site-packages/praw/objects.py in _populate(self, json_dict, fetch)
    125     def _populate(self, json_dict, fetch):
    126         if json_dict is None:
--> 127             json_dict = self._get_json_dict() if fetch else {}
    128         self.json_dict = (json_dict if
    129                           self.reddit_session.config.store_json_result

/usr/lib/python3.4/site-packages/praw/objects.py in _get_json_dict(self)
    118         try:
    119             response = self.reddit_session.request_json(self._info_url,
--> 120                                                         as_objects=False)
    121         finally:
    122             self.reddit_session._use_oauth = prev_use_oauth

/usr/lib/python3.4/site-packages/praw/decorators.py in wrapped(reddit_session, *args, **kwargs)
    159     @wraps(function)
    160     def wrapped(reddit_session, *args, **kwargs):
--> 161         return_value = function(reddit_session, *args, **kwargs)
    162         if isinstance(return_value, dict):
    163             if return_value.get('error') == 304:  # Not modified exception

/usr/lib/python3.4/site-packages/praw/__init__.py in request_json(self, url, params, data, as_objects, retry_on_error)
    524         # Request url just needs to be available for the objecter to use
    525         self._request_url = url  # pylint: disable-msg=W0201
--> 526         data = json.loads(response, object_hook=hook)
    527         delattr(self, '_request_url')
    528         # Update the modhash

/usr/lib64/python3.4/json/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    316             parse_int is None and parse_float is None and
    317             parse_constant is None and object_pairs_hook is None and not kw):
--> 318         return _default_decoder.decode(s)
    319     if cls is None:
    320         cls = JSONDecoder

/usr/lib64/python3.4/json/decoder.py in decode(self, s, _w)
    341
    342         """"""
--> 343         obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    344         end = _w(s, end).end()
    345         if end != len(s):

/usr/lib64/python3.4/json/decoder.py in raw_decode(self, s, idx)
    359             obj, end = self.scan_once(s, idx)
    360         except StopIteration as err:
--> 361             raise ValueError(errmsg(""Expecting value"", s, err.value)) from None
    362         return obj, end

ValueError: Expecting value: line 1 column 1 (char 0)
```
"
357,Add parameter values to docs for _listing method,2015-01-04T18:15:22Z,2015-01-04T18:42:04Z,,,,"Updated the documentation with valid values for `sort` and `time` parameters to the `_listing` method (generated by `internal._get_redditor_listing` for e.g. `Redditor.get_comment`), per [Reddit API docs](https://www.reddit.com/dev/api#GET_user_{username}_{where}), in response to [this StackOverflow question](http://stackoverflow.com/q/27768132/3001761).
"
356,Make comments savable,2014-12-31T04:06:36Z,2014-12-31T05:38:51Z,,,,"And add necessary tests.

http://www.redditblog.com/2014/03/staying-gold.html
"
355,Added more spaces to praw/helpers.py#L153,2014-12-30T18:55:32Z,2014-12-30T19:31:27Z,,,,"In order to completely overwrite any text from L172, I added 2 spaces. For example, `Items: 20372 (24.92 ips)` has exactly 12 characters after `Items {0}`. This character count may increase if 1 request/second is reached, so perhaps even 13 spaces is needed.
"
354,r.login() fails on certain usernames. ,2014-12-25T18:50:48Z,2014-12-26T06:19:48Z,,ValueError,ValueError: No JSON object could be decoded,"For certain usernames r.login, returns an error as shown below.

```
r.login('username','password')
```

Traceback : 

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 1266, in login
    self.user = self.get_redditor(user)
  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 890, in get_redditor
    return objects.Redditor(self, user_name, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 663, in __init__
    fetch, info_url)
  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 72, in __init__
    self.has_fetched = self._populate(json_dict, fetch)
  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 127, in _populate
    json_dict = self._get_json_dict() if fetch else {}
  File ""/usr/local/lib/python2.7/dist-packages/praw/objects.py"", line 120, in _get_json_dict
    as_objects=False)
  File ""/usr/local/lib/python2.7/dist-packages/praw/decorators.py"", line 161, in wrapped
    return_value = function(reddit_session, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/praw/__init__.py"", line 526, in request_json
    data = json.loads(response, object_hook=hook)
  File ""/usr/lib/python2.7/json/__init__.py"", line 326, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python2.7/json/decoder.py"", line 366, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python2.7/json/decoder.py"", line 384, in raw_decode
    raise ValueError(""No JSON object could be decoded"")
ValueError: No JSON object could be decoded
```

This has been experienced by some other users also as posted [here](https://www.reddit.com/r/learnpython/comments/2jm2e2/praw_rlogin_fails_on_certain_usernames/).
"
353,"praw.errors.RedirectException: Unexpected redirect from ""http://..."" to ""https://...""",2014-12-18T21:28:14Z,2014-12-19T06:52:39Z,,,,"Hi,
I'm simply doing the following on a Reddit account with `force-HTTPS` setting enabled on the account. As expected, Reddit is redirecting to HTTPS from all their HTTP endpoints.

``` python
r = praw.Reddit(""My custom user-agent"")
r.login(""username"", ""password"")
```

This causes `praw.errors.RedirectException` to be thrown. Am I missing some `secure=True` option somewhere or is this an issue that went unnoticed?

Thanks,
Can Selcik
"
352,Unable to search for certain links on reddit,2014-11-30T20:37:04Z,2015-04-24T14:25:41Z,,AttributeError,AttributeError: 'unicode' object has no attribute 'get',"I had a bug in my reddit bot. After isolating data and the code responsible for it, the following code seems to expose the problem: 

```
import praw
r = praw.Reddit('A school project bot to study distribution of links amongst subreddits')
r.set_oauth_app_info(
    client_id='CHANGE ME',
    client_secret='CHANGE ME',
    redirect_uri='http://127.0.0.1:5000/')
r.login(username=""change me"", password=""change me"")

link = ""http://www.facebook.com/suren.jack""
print ""type: "", type(link)
for item in r.search(link):
    print item
```

Output:

```
/usr/bin/python /Users/sn/dev/reddit-scraper/test.py
type:  <type 'str'>
Traceback (most recent call last):
  File ""/Users/sn/dev/reddit-scraper/test.py"", line 12, in <module>
    for item in r.search(link):
  File ""/Library/Python/2.7/site-packages/praw/__init__.py"", line 1046, in search
    **kwargs):
  File ""/Library/Python/2.7/site-packages/praw/__init__.py"", line 487, in get_content
    root = page_data.get(root_field, page_data)
AttributeError: 'unicode' object has no attribute 'get'

Process finished with exit code 1
```

Is this a bug or I should be catching this error?
"
351,Useragent leaks too much information,2014-11-27T18:37:44Z,2015-01-05T02:22:39Z,,,,"Regardless of what a user sets as their user-agent, praw appends praw, python and platform versions.

Here is an example of what I am talking about:
Expected: private/0.1
Result: private/0.1 PRAW/2.1.19 Python/3.4.2 Linux-3.17.3-1-ARCH-x86_64-with-arch

The communication is also not encrypted, so anyone sitting on the wire or wireless gets to know these sensitive details. I do not see a good reason to broadcast all this data.

My recommendation is to only append the PRAW version, and only if there is a good reason to do so.

(A kernel version can be used to make reasonably accurate assumptions about the state of other package updates like firefox, which is good to know for a hacker)
"
350,Add support for /api/collapse_message and /api/uncollapse_message,2014-11-25T02:02:23Z,2015-04-11T23:51:38Z,Feature,,,"Should be easy enough but I don't really understand how PRAW is structured.
"
349,rename get_my_multis to get_my_multireddits,2014-11-24T18:26:36Z,2014-11-24T18:52:31Z,,,,"and reflect in changes.rst

As requested by bboe in the Multireddit issue thread.
"
348,Unable to get_me with login authentication,2014-11-24T16:19:53Z,2014-11-24T16:27:06Z,,,,"Hi, 
The PRAW docs say get_me ""Requires the identity oauth scope or user/password authentication,"" but it is not actually possible to use the function with login authentication. 

The offending line seems to be lines 1258-1259 in `__init__.py`

```
@decorators.restrict_access(scope='identity', oauth_only=True)
def get_me(self):
```

But, I don't know enough about the internals: is this a mistake, and the function should be available with login auth, or should the docs be updated to indicate it is only available with OAuth?
"
347,the 2.1.19 version coming via pip differs from the 2.1.19 source; all mentions of from_sr (send_message()) are missing,2014-11-23T05:05:14Z,2014-11-23T06:25:33Z,,,,"Please forgive me if I'm missing something or in error.

_From the source:_

**$ grep __version__  __init__.py**
__version__ = '2.1.19'

**$ grep from_sr __init__.py**
    def send_message(self, recipient, subject, message, from_sr=None,
        :param from_sr: A Subreddit instance or string to send the message
        if from_sr:
            data['from_sr'] = six.text_type(from_sr)

---

_Provided by pip:_

**$ pwd**
/usr/local/lib/python2.7/dist-packages/praw

**$ grep __version__ __init__.py**
__version__ = '2.1.19'

**$ grep from_sr \***
(_nada, chief_)
"
346,"The new ""restrict_access"" on get/set_stylesheet (""Fix #174"") broke set_stylesheet",2014-11-22T13:25:19Z,2014-11-23T15:49:02Z,,TypeError,TypeError: 'NoneType' object has no attribute '__getitem__',"Error on this line:

```
r.set_stylesheet(subreddit=sub, stylesheet=css_stuff) # edited out the hardcoded subreddit string for github :P
```

...I'm VERY sure a recent update of my own didn't cause this, because copying and pasting the css in manually works fine, and this worked right before I upgraded praw. That, and this is a very easy line to not mess up.

Not even sure how that commit (""Fix #174"") broke anything, but...

Traceback is as follows:

```
File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 295, in wrapped
    six.get_function_defaults(function)[0])
TypeError: 'NoneType' object has no attribute '__getitem__'
```
"
345,Praw 19 November -- Early support for Multireddits,2014-11-19T19:12:12Z,2014-11-23T01:58:16Z,,,,"New :class:`Multireddit` for multireddit objects. Proper endpoints and kind added to init config

New :meth:`get_multireddit` to get a single mr object. Standard usage: `r.get_multireddit(multi_author, multi_name)`

New :meth:`get_my_multis` which requires logged in scope. Returns a list of all multireddit objects you own.

New :class:`MultiredditTest` in tests which will test the get_my_multis function. As of writing, :class:`BasicTest` will test get_multireddit since that does not require logged in.

Added proper material to CHANGES.rst

Edit: Travis build failed but I see the error and will re-commit.

Sorry for the commit-flood :( I will learn to squash before next time.

Edit: [Build 498 failed on account of test_scope_submit timing out](https://travis-ci.org/praw-dev/praw/jobs/41518483#L415) (Line 415). Can a new build be initiated without a commit?
"
344,Insufficient support for Multireddits,2014-11-15T08:22:01Z,2015-05-26T08:40:35Z,Feature,,,"I've done several searches throughout the repo, and as far as I can tell there aren't any resources for handling Multireddits. I'd love to help develop this if a project manager can get some groundwork started. Any plans? (Is there a to-do file somewhere?)

[Multireddit API](http://www.reddit.com/dev/api#section_multis)
"
343,Praw 09 November 2014 -- Added `from_sr` param to `send_message()`,2014-11-09T22:23:18Z,2014-11-11T08:07:38Z,,,,"Added from_sr param to send_message with default value None. Raises 403 if you are not a moderator of the subreddit you send from
"
342,praw 06 November 2014 - New function get_randnsfw_subreddit,2014-11-07T02:35:24Z,2014-11-08T05:41:59Z,,,,"Created function Get randnsfw subreddit
"
341,Use a monotonic timer for rate limiting,2014-11-02T05:47:29Z,2014-11-04T06:29:32Z,,,,"This patch changes every occurrence of `time.time()` to `timeit.default_timer()`.

Currently, the rate limiter uses `time.time()` for getting the current time. Unfortunately, [`time.time()` is not monotonic](https://docs.python.org/3/library/time.html#time.time). If the system clock is adjusted while `praw` is running, it can lead to the rate limiter waiting too long or not at all.

A better alternative is [`timeit.default_timer()`](https://docs.python.org/3/library/timeit.html#timeit.default_timer), which is monotonic since Python 3.3. Older versions alias to `time.time()`, which is no different to what we have now. It's been in the standard library since Python 2.3 (albeit not documented) so there should be no issues with compatibility.
"
340,Flask example should use multiple instances of praw.Reddit?,2014-10-22T21:06:53Z,2014-10-23T15:04:19Z,,,,"Am I correct in thinking that when running on a production web server that the workflow outlined in the Flask example on this page: http://praw.readthedocs.org/en/v2.1.16/pages/oauth.html is potentially unsafe?  It seems like there's a race condition between the following two lines:

```
info = r.get_access_information(code)
user = r.get_me()
```

I'm working on a site that uses reddit for authentication, and I recently had a user tell me he was logged in as someone else.  We don't use our praw.Reddit object any after verifying the user is logged in and pulling out their reddit username, so I'm fairly certain the problem is literally in those two lines above.

If I'm misguided, please let me know!
"
339,IndexError in comment.replies,2014-10-20T15:24:52Z,2014-11-12T08:28:23Z,Bug,IndexError,IndexError: list index out of range,"I am currently using `comment_stream` to get the latest comments. To check potential replies to the comment I am iterating over the replies.

Basically, the code looks like this:

``` python
for comment in comment_stream(session, subreddits):
    for reply in comment.replies:
          # do stuff
```

However, from time to time I get an `IndexError` here in `comment.replies`.

Is this a user error or something in praw?

The shortened stacktrace:

``` python
Traceback (most recent call last):
  ...
  File ""filename"", line 69, in foo
    for reply in comment.replies:
  File ""/path/to/praw/site-packages/praw-2.1.18-py2.7.egg/praw/objects.py"", line 564, in replies
    self._replies = response[1]['data']['children'][0]._replies
IndexError: list index out of range
```

Cheers,
Stephan
"
338,Praw3,2014-10-16T03:11:49Z,2014-10-16T04:42:12Z,,,,
337,"Feature: Cloudsearching with timestamps, get all posts",2014-10-11T03:46:33Z,2015-12-19T08:53:29Z,Feature,,,"This issue thread is for discussing the addition of timestamp searching to PRAW. Bboe told me do this.

My attempt at creating this function is [here](https://github.com/voussoir/reddit/blob/master/Prawtimestamps/timesearch.py)
"
336,Add get_messages() for retrieving messages only,2014-10-08T22:05:29Z,2014-10-09T04:21:14Z,,,,"Currently the only way to get your messages is through `.get_inbox()`, which also includes all comment replies, post replies, mentions, etc. This adds `.get_messages()` which will allow fetching exclusively messages.
"
335,is_root causes the submisson to be fetched,2014-10-07T19:51:56Z,2014-10-09T05:02:28Z,,,,"I figured that if the submission of a comment hasn't been fetched yet and you try to access `is_root` the submission is gonna be fetched first.
Looking at the implementation of [`is_root`](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L550) it becomes clear why. 

However, I am wondering whether this is needed. Couldn't we just compare the type of `link_id` to `t3` to check whether the comment is root or not?

Cheers,
Stephan
"
334,Issue accessing private subreddit,2014-10-05T20:25:05Z,2014-10-05T23:20:00Z,,,,"Previously I could access a private subreddit I'm subscribed to (in my case, to access comments) by authenticating as myself, but that no longer seems to be the case. My code works if I put in a public subreddit, though.

Was this a change to reddit's API or is there a per-subreddit setting that could affect it?
"
333,UserWarning: Extra settings fields: [u'collapse_deleted_comments'],2014-09-29T01:43:27Z,2014-10-09T05:29:33Z,,,,"When running `praw.update_settings` I am getting the error:

```
:0: UserWarning: Extra settings fields: [u'collapse_deleted_comments']
```

This has been happening for roughly 5 days without any system changes (likely a change on the reddit api side)
"
332,Would it be possible to get multiple more comment objects per request?,2014-09-28T20:42:10Z,2014-09-29T05:14:00Z,,,,"In the Reddit API it is now possible to send one request for multiple objects. Would it be possible for replace_more_comments to batch together available MoreComment objects and send them in one request?

http://www.reddit.com/r/redditdev/comments/2eur0l/api_update_apiinfo_supports_lists_of_fullnames/
"
331,520 Server Error: Origin Error,2014-09-18T15:48:44Z,2014-10-09T05:31:32Z,,,,"It seems to happen some of the time, some of the time not. This thread offers an interesting idea on why this happens http://www.reddit.com/r/redditdev/comments/2cewks/praw_520_server_error_origin_error_521_server/
"
330,Pull request for issue #307,2014-09-16T21:21:05Z,2014-09-16T22:55:14Z,,,,"""Failing tests that rely on prev_description_id and prev_public_description_id in subreddit settings""

I removed the args from ModConfigMixin.set_settings (they never get returned) and changed the two tests to simply change the description and then check that the old description and the new description are different.

Cheers.
"
329,ResourceWarning and ImportWarning when running example module,2014-09-10T13:30:52Z,2014-11-16T20:59:26Z,,,,"I was trying to run the example module:

```
import praw

USERAGENT = ""Python Script using praw""
r = praw.Reddit(USERAGENT)
submissions = r.get_subreddit('opensource').get_hot(limit=5)

for x in submissions:
    print(x)
```

But them I get:

```
(venv) PS D:\Documents\RedditBot> python.exe .\bot.py
79 :: Liberating myself from Google and others
4 :: First release of OpenDoorLogistics. An Open Source application for Fleet...
9 :: 10 ways to contribute to an open source project without writing code
20 :: GitHub's new CEO: We're serious about the enterprise
2 :: OSBConf 2014 | opensourceupdates.com
sys:1: ResourceWarning: unclosed <socket object at 0x02A0DD68>
C:\Development\Languages\Python34\lib\importlib\_bootstrap.py:2150: ImportWarning: sys.meta_path is empty
(venv) PS D:\Documents\RedditBot>
```

What's wrong?
"
328,Add report reason & param,2014-09-05T21:29:47Z,2014-09-06T06:56:32Z,,,,"Added a report reason to comply with reddit's new feature of supplying a reason when reporting.
Defaults to a blank reason.
"
327,Updated with Reddit Keyword Tracking Bot,2014-09-05T03:13:27Z,2014-09-06T06:38:14Z,,,,"Added new praw project
"
326,Add reporting reasons,2014-09-05T01:51:00Z,2014-09-05T21:30:39Z,,,,"https://www.reddit.com/r/AutoModerator/comments/2fia1l/automoderator_update_you_can_now_specify_a_report/ck9hrkk
"
325,Would it be possible to add new methods for gilding users and posts?,2014-09-04T02:26:00Z,2014-11-24T16:41:34Z,Feature,,,"As the reddit api now includes a way to gild users, http://www.reddit.com/dev/api#POST_api_v1_gold_gild_{fullname}, would someone be able to include them into PRAW?
"
324,Updating the send message line,2014-08-23T16:04:29Z,2014-08-24T14:25:59Z,,,,"Changing r.user.send_message to r.send_message. r.user.send_message doesn't give an error, but it doesn't actually send a message.
Adding a title to the message in the send_message parameters to fix: TypeError: send_message() takes at least 4 arguments (3 given)
"
323,json_dict gets overwritten when using fetch = True in RedditContentObject.__init__,2014-08-23T11:32:41Z,2014-08-24T15:14:25Z,,,,"When using Reddit.get_subreddit() with fetch = True, json_dict = None and store_json_result = True, json_dict gets hosed by the last line of RedditContentObject.__init__

``` python
self.json_dict = (json_dict if reddit_session.config.store_json_result is True else None)
```

You can test this by placing a print(self.json_dict) just before and after the last line of __init__

Perhaps a change to RedditContentObject._populate and removal of the last line in RedditContentObject.__init__ would be more appropriate:

``` python
def _populate(self, json_dict, fetch)
    if json_dict is None:
        json_dict = self._get_json_dict() if fetch else {}
    self.json_dict = (json_dict if self.reddit_session_config.store_json_result is True else None)
```

That way we don't clobber self.json_dict if we don't provide a json_dict keyword argument with RedditContentObject.__init__ and the remainder of the _populate function works as per normal.
"
322,praw.helpers.comment_stream never stops consuming memory,2014-08-23T10:49:51Z,2016-06-15T05:18:26Z,,,,"``` python
import praw

reddit = praw.Reddit(user_agent='mybot')
reddit.login(username='mybot', password='mypassword')

for comment in praw.helpers.comment_stream(reddit, 'all', verbosity=0):
    print comment.submission.id
```

When I use this on a Windows 7 system, memory consumption never stops and within only minutes, the python.exe process uses over 100 MB of memory. Is this to be expected? Is there any way to get around this problem?
"
321,Deep comments not found by replace_more_comments,2014-08-20T15:28:52Z,2015-01-11T23:28:43Z,Bug,,,"Observe the following testcase:

```
import praw

def allComments(comments):
    for c in comments:
        yield c
        yield from allComments(c.replies)

r = praw.Reddit('test_stackoverflow')
sub_url = 'http://www.reddit.com/r/funny/comments/2e211k/asshole_titanic/'
sub = praw.objects.Submission.from_url(r, sub_url)
sub.replace_more_comments(limit=None, threshold=0)

assert any(c.id == 'cjvffoz' for c in allComments(sub.comments))
assert any(c.id == 'cjvfjra' for c in allComments(sub.comments))
```

Note that both comment IDs [are](http://www.reddit.com/r/funny/comments/2e211k/asshole_titanic/cjvffoz) [present](http://www.reddit.com/r/funny/comments/2e211k/asshole_titanic/cjvfjra) on the webpage, but only the former is included in the answer.

This seems to be a mistake on reddit's site, because the following fails:

```
r = praw.Reddit('test_stackoverflow')
assert json.loads(r._request(r.config['morechildren'], None, {
    'children': 'cjvfjra',
    'link_id': 't3_2e211k'
}))['json']['data']['things']
```

Instead of leaving out the comments, they should be requested as a whole.

The situation can be detected with a `MoreComments` object with a `count` property of `0`, but non-empty `children`.
"
320,Fixes a socket error.,2014-08-16T17:11:24Z,2014-08-17T19:36:59Z,,,,
319,Support Increased OAuth Rate-Limits,2014-08-14T17:46:18Z,2016-06-15T05:16:09Z,Feature,,,"The rate-limit portion of PRAW needs to be adjusted to take into consideration increased speeds for OAuth requests.

I'm not yet sure how a hybrid of requests needs to be handled (maybe they're in separate buckets?) In either case, this will probably require some significant refactoring.

What would also be nice is a configurable bursty-mode so many requests can occur in the first part of a minute, and then gradually slowing down after that. This mode should be enabled by default when using the repl. However, it probably should not be enabled by default for most scripts because some people prefer to write short-lived scripts which they run multiple times per minute.

See: https://github.com/reddit/reddit/wiki/API
"
318,HTTPS Everywhere,2014-08-14T17:10:32Z,2014-11-09T16:54:30Z,,,,"Now that HTTPS is supported, HTTP should never be used on www.reddit.com, though it should be possible for someone to specify HTTP only which would be useful for local subreddits.
"
317,"Support an HTTPS proxy, including the environment variable.",2014-08-11T18:29:48Z,2014-08-12T07:24:21Z,,,,"As per issue 316.
"
316,Proxy settings don't work for HTTPS,2014-08-11T11:27:13Z,2014-11-09T16:51:46Z,,,,"Praw can't make requests over HTTPS from behind a proxy.  The `requests` library uses the environment variables `http_proxy` and `https_proxy` unless you override them using the `proxies`.  Praw overrides `proxies` with a dictionary containing just `http`, extracted either from its config or from `http_proxy` in the environment, which means that HTTP proxying works, but not HTTPS.

A simple fix would be to check the `https_proxy` environment variable too and put it into the dictionary if it's present -- but perhaps that would have knock-on effects elsewhere?  I'm happy to put together a pull request if people think it's an OK solution, though.
"
315,Added Massdrop-Bot to the documentation,2014-07-30T09:29:13Z,2014-07-30T14:50:40Z,,,,"Simply adding the github and description of massdropbot to the collection of bots.
"
314,Encoding mutation problem,2014-07-27T19:13:10Z,2014-07-28T00:06:27Z,,,,"This code is mutating subreddit data that shouldn't be touched.  When updating /config/sidebar using this:

``` python
prawLogin.update_settings(subreddit, description = newSidebar)
```

The /config/description gets mutated from

```
AAA&nbsp;ZZZ
```

to

```
AAA&amp;nbsp;ZZZ
```

and then the next time....

```
AAA&amp;amp;nbsp;ZZZ
```
"
313,Support creating Reddit live threads,2014-07-24T18:32:23Z,2016-07-18T04:59:40Z,"Feature, ⭐️ New Contributor Friendly ⭐️",,,"Reckon we have this in praw?
"
312,Is it possible to a comment from a comment_id?,2014-07-23T11:45:05Z,2014-07-23T15:26:41Z,,,,"It's possible to get a `praw.objects.Submission` from a submission id. Shouldn't it be possible to get a `praw.objects.Comment` from a submission and comment id? For example, from [this permalink](http://www.reddit.com/r/pics/comments/2bgjsv/there_she_was_long_legs_long_neck_and_all_she/cj54lbz):
- submission id appears to be `2bgjsv`
- comment id appears to be `cj54lbz`

From the [current documentation](https://praw.readthedocs.org/en/v2.1.16/), it doesn't appear that it's possible. Currently, you have to go through the comment forest of a submission to find the comment that matches the id that you have.

Is it possible to access the comment directly?
"
311,func_defaults doesn't exist in python3,2014-07-18T04:43:57Z,2014-07-18T04:52:23Z,,,,"http://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/
"
310,Unable to set Link Flair for my own post,2014-07-04T00:48:55Z,2014-08-06T19:10:48Z,,,,"On Reddit, I am able to set the flair of a post that I made, however when using PRAW, it says that I need to be an authenticated moderator of the Subreddit to set the link flair. Am I doing something wrong, or is this not possible in this API? Thanks, Jake.
"
309,import praw?,2014-07-03T22:08:46Z,2014-07-04T16:57:12Z,,ImportError,ImportError: cannot import name deprecated,"I need to use the latest praw version with redditpostarchiver but I don't want to install praw.. I but the praw folder inside redditpostarchiver dir and tried importing it manually without success.

import sys
sys.path.append('/home/jadedgnome/bin/redditPostArchiver/praw/praw')
from objects.py import praw

$ python archive.py 287dtv

Traceback (most recent call last):
  File ""/home/jadedgnome/bin/redditPostArchiver/archiver.py"", line 7, in <module>
    from objects.py import praw
  File ""/home/jadedgnome/bin/redditPostArchiver/praw/praw/objects.py"", line 33, in <module>
    from praw.decorators import (alias_function, deprecated, limit_chars,
ImportError: cannot import name deprecated

 how can i import praw manually?
"
308,Added RemindMeBot,2014-07-03T16:03:04Z,2014-07-04T19:33:58Z,,,,"Added my bot RemindMeBot
"
307,Failing tests that rely on prev_description_id and prev_public_description_id in subreddit settings,2014-06-28T11:33:19Z,2014-09-16T22:55:24Z,,,,"I ran the test suite for PRAW and noticed a few failures, so I started investigating them. 

Currently the tests SettingsTest.test_update_settings_description and SettingsTest.test_update_settings_description are failing with a KeyError when the code tries to pull the key prev_description_id and the prev_public_description_id from the returned settings dict. It looks like Reddit is no longer returning these.

Is this known? Would you like a pull request to update those tests and remove those settings from ModConfigMixin.set_settings()?
"
306,Potential conflicts between PRAW properties and new attrs in the reddit API,2014-06-26T22:27:14Z,2014-06-27T14:43:53Z,,,,"Just wanted to register an issue for this problem, since it's what caused PRAW to break with the API update last week. Any properties that PRAW is adding via `@property` will cause `AttributeErrors` if we add any attributes with the same name to that type of item in the API.

Since the properties that you're adding on your end are (naturally) using obvious/common terms related to reddit, there's actually a pretty decent potential of this sort of thing happening again. For example, adding a `permalink` attr to comments is definitely a potential future API change, and would currently cause the same issue in PRAW.

I'm not sure offhand what would be the best way to handle this possibility, I'd probably recommend having your property definition ""overrule"" the one coming from the API, to prevent any possible behavior change in PRAW if they're not exactly the same.

I apologize again for causing the issue, thanks for responding so quickly to merge in the fix and deploy a new version.
"
305,update_settings() updates fields which include html characters incorrectly,2014-06-23T07:16:51Z,2014-06-23T14:42:27Z,,,,"Hey, I stumbled upon a problem using [update_settings()](https://github.com/praw-dev/praw/blob/dffa47294c4a3f02dd0c37473c7a0a5676a6b06a/praw/__init__.py#L1491).

The problem is that it doesn't replace html encoded characters in the set_update() function. 

Let's take the following test case. A subreddit has a submission text of `test & test`. get_settings() returns correctly:

```
{..., u'submit_text': u'test &amp; test', ... }
```

Now we update, whatever we want – which also works fine. But then update_settings() just takes the return text of get_settings() and sets it again. Because submit_text isn't unescaped it will write the escaped sequence. 

If we now call get_settings() we will get:

```
{..., u'submit_text': u'test &amp;amp; test', ...}
```

A simple HTML unescape does the trick in my cases. However, adding test cases is probably a good idea. If you have any further questions please ask.

Thanks
Tim
"
304,Integrating /r/Bottiquette blacklists into PRAW,2014-06-22T05:51:25Z,2014-11-09T17:01:34Z,,,,"For starting background on /r/Bottiquette and the blacklists: refer to [the /r/TheoryOfReddit discussion](http://np.reddit.com/r/TheoryOfReddit/comments/25yx8g/rbottiquette_provides_centralized_lists_of_bot/), the [subreddit sticky](http://np.reddit.com/r/Bottiquette/comments/24htvm/new_to_bottiquette_read_this/), and the [wiki](http://www.reddit.com/r/Bottiquette/wiki/index).

This issue exists to discuss the prospect of integrating the live bottiquette blacklists into PRAW for easy use. As a newcomer (relatively) to PRAW I figured I should get some input before I go submitting pull requests willy-nilly. So I'll start by discussing a few concept options.

---

1) **Dead-simple option**: PRAW parses the blacklist JSON on module import and holds the blacklists as a dict, updates every 24 hours if we get fancy.

A conscientious bot-maker could do the following under this scenario.

```
import praw
...
#Don't make comments where bots are not allowed
for comment in comment_actions:
    if comment.subreddit in praw.blacklists['disallowed']:
        continue
...
```

This saves the developer from explicitly grabbing the wikipage and parsing the json file in every project and little else (and the updates maybe).

2) **Integrate with stream/get_content**: A bit more gnarly but potentially more useful. The basic idea is that we implement the filtering at the api call level to save the developer from having to implement this on their own. Consequently, the bot-maker would have to explicitly assign blacklist exceptions to their reddit session (some subreddits allow selected bots).

```
import praw
r = praw.Reddit('my user agent')
r.blacklist_excepts['permission'].append('ihavepermissionhere')
...
#I'll never see stuff from where I'm not allowed, so I don't worry
for stuff in stuff_actions:
    do_stuff(stuff)
...
```

This could prevent bot makers from having to write in the permissions algebra for each project and avoid potential bottiquette abuses due to mistakes. This could save a fair amount of time and worry, but does this deny needed flexibility to the bot-maker? Could also lead to weirdness (like getting 98 comments instead of 100 as two were filtered)

But wait, this screws a lot of PRAW users over! PRAW is used for a lot of things that aren't bots which make posts or comments, and this could make things more complicated for very specialized moderation tools. This is obviously bad, so we should move on to

3) **Alternative stream/get_content implementing permissions**: Like above, but you opt-in for the filtering. A bot-maker decides that they want to make a bot that posts and/or comments and since they don't want to violate bottiquette, they decide to use the api calls that implement the filtering for them.

---

These are some of my ideas, what do others think? I would be happy to discuss this and contribute my time to implementing this in congruence with PRAW. The hope is to make it simpler for bottiquette to be widely utilized on reddit, even among beginning users.
"
303,reddit update may have broken PRAW objects,2014-06-18T21:44:01Z,2014-06-18T21:48:28Z,,AttributeError,AttributeError: can't set attribute,"Hi!

So I was working on my bot like usual, and all of a sudden I started receiving AttributeErrors in code that was previously working. The code being used can be found on [my repo](https://github.com/matthieugrieger/versebot).

The bot seems to fail almost instantly. Here is what the terminal outputs:

```
Traceback (most recent call last):
  File ""versebot.py"", line 90, in <module>
    main()
  File ""versebot.py"", line 44, in main
    for comment in comments:
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/helpers.py"", line 118, in _stream_generator
    for i, item in gen:
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/__init__.py"", line 471, in get_content
    page_data = self.request_json(url, params=params)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/decorators.py"", line 161, in wrapped
    return_value = function(reddit_session, *args, **kwargs)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/__init__.py"", line 516, in request_json
    data = json.loads(response, object_hook=hook)
  File ""/home/matthieu/.pyenv/versions/2.7.6/lib/python2.7/json/__init__.py"", line 351, in loads
    return cls(encoding=encoding, **kw).decode(s)
  File ""/home/matthieu/.pyenv/versions/2.7.6/lib/python2.7/json/decoder.py"", line 365, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/matthieu/.pyenv/versions/2.7.6/lib/python2.7/json/decoder.py"", line 381, in raw_decode
    obj, end = self.scan_once(s, idx)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/__init__.py"", line 403, in _json_reddit_objecter
    return object_class.from_api_response(self, json_data['data'])
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/objects.py"", line 58, in from_api_response
    return cls(reddit_session, json_dict=json_dict)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/objects.py"", line 514, in __init__
    underscore_names=['replies'])
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/objects.py"", line 72, in __init__
    self.has_fetched = self._populate(json_dict, fetch)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/objects.py"", line 141, in _populate
    setattr(self, name, value)
  File ""/home/matthieu/.pyenv/versions/versebot/lib/python2.7/site-packages/praw/objects.py"", line 102, in __setattr__
    object.__setattr__(self, name, value)
AttributeError: can't set attribute
```

I'm guessing it has something to do with the changes to upvotes/downvotes in the latest [reddit update](http://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/).

Someone else on StackOverflow seems to be [having the same issue as well](http://stackoverflow.com/questions/24295281/praw-attributeerror-cant-set-attribute).
"
302,Remove score property from Comment,2014-06-18T20:50:14Z,2014-06-18T21:35:17Z,,,,"reddit has this property natively now, which is causing an AttributeError.
"
301,replace_more_comments() giving 403 error on a specific thread,2014-06-10T13:26:43Z,2015-07-11T21:59:08Z,,,,"When using replace_more_comments(limit=none, submission_id = 24tmfe) on the stickied thread in /r/worldnews about Ukraine, there is a 403 error. Attempts at using the comments() method in MoreComments to get all comments also results in a 403. I haven't been able to get a 403 from any other submission. 
"
300,Added NetflixBot,2014-05-29T19:11:15Z,2014-05-29T19:20:40Z,,,,"Added my new bot NetflixBot to the list of scripts
"
299,Captcha error,2014-05-19T11:11:45Z,2014-05-19T15:54:13Z,,TypeError,TypeError: submit() got multiple values for keyword argument 'captcha',"Entering captcha doesn't appear to work correctly:

```
Captcha URL: http://www.reddit.com/captcha/GImm1LxMsMwhQfMTVdA4ZOEW8tnn697k.png
Captcha: FCOXFD
Traceback (most recent call last):
  File ""./reddit-bot.py"", line 18, in <module>
    r.submit('neuromute', like.title, None, like.short_link, None)
  File ""/usr/local/lib/python2.7/dist-packages/praw/decorators.py"", line 323, in wrapped
    return function(cls, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/praw/decorators.py"", line 225, in wrapped
    return function(obj, *args, **kwargs)
TypeError: submit() got multiple values for keyword argument 'captcha'
```
"
298,get_my_subreddits returns a selection of a user's subreddits and not all of them,2014-05-17T10:42:29Z,2014-05-17T13:59:35Z,,,,"Is there a reason why, after login, running get_my_subreddits() only returns a select few of a user's subreddits?

I would expect it to return all of them.
"
297,Update useful_scripts.rst with new script,2014-05-07T11:02:00Z,2014-05-07T17:06:49Z,,,,"Added Reddit-to-Diigo-Copier script
"
296,WikiPage.revision_by now grabs proper username (issue #258),2014-05-05T21:27:27Z,2014-05-05T22:46:26Z,,,,"This hack seems to resolve the issue, and as far as I can tell doesn't break anything. You guys know the codebase better than I do at this point, so let me know if you think this is a good solution. Some manual tests:

```
>>> r = praw.Reddit('sdf', site_name='local')
>>> w = r.get_wiki_page('reddit_test0', 'wiki1')
>>> w.revision_by
Redditor(user_name='reddit')
```

...

And on the /r/webdev wiki index, where we'd expect the latest revision to have been made by user 'jaredcheeda':

```
>>> import praw
>>> r = praw.Reddit('PRAW-dev testing wiki revision grab')
>>> w = r.get_wiki_page('webdev', 'index')
>>> w.revision_by
Redditor(user_name='jaredcheeda')
```

For unit testing there'd have to be a static wiki page for which we always know which user we expect to have revised. Or we could run it against a private reddit.com sub where no changes are planned (I think I remember seeing @bboe using this method in the code already). 
"
295,Add delete_redditor functionality for AuthenticatedReddit class (#240),2014-05-04T18:54:35Z,2014-05-05T20:26:12Z,Feature,"{u'errors', praw.errors.InvalidUserPass, praw.errors.APIException","{u'errors': [], u'data': {u'modhash': u'd_schrute', u'cookie':u'1748,2014-05-04T14:28:05,81e7d58598850870c330fd35eb2b0a34d3102dfa'}}, praw.errors.InvalidUserPass: `invalid password` on field `passwd`, praw.errors.APIException: (USERNAME_TAKEN_DEL) `that username is taken by a deleted  account` on field `user`","AuthenticatedReddit now has a method delete_redditor which deletes the currently logged-in redditor. 

Works when tested manually:

```
>>> import praw
>>> r = praw.Reddit('dsf', site_name='local')
>>> r.create_redditor('d_schrute', 'beets')
{u'errors': [], u'data': {u'modhash': u'd_schrute', u'cookie':u'1748,2014-05-04T14:28:05,81e7d58598850870c330fd35eb2b0a34d3102dfa'}}
>>> r.login('d_schrute', 'beets')
>>> r.user.name
'd_schrute'
>>> r.delete_redditor('beets')
{u'errors': []}
>>> r.clear_authentication()
>>> r.login('d_schrute', 'beets')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""praw/__init__.py"", line 1212, in login
    self.request_json(self.config['login'], data=data)
  File ""praw/decorators.py"", line 177, in wrapped
    raise error_list[0]
praw.errors.InvalidUserPass: `invalid password` on field `passwd`
>>> r.create_redditor('d_schrute', 'beets')
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""praw/decorators.py"", line 225, in wrapped
    return function(obj, *args, **kwargs)
  File ""praw/__init__.py"", line 664, in create_redditor
    return self.request_json(self.config['register'], data=data)
  File ""praw/decorators.py"", line 177, in wrapped
    raise error_list[0]
praw.errors.APIException: (USERNAME_TAKEN_DEL) `that username is taken by a deleted  account` on field `user`
```

And the test suites seem to work, if I'm doing it right:

```
>>> suite = unittest.TestSuite()
>>> suite.addTest(praw.tests.LocalOnlyTest('test_delete_existing_redditor'))
>>> suite.addTest(praw.tests.LocalOnlyTest('test_create_deleted_redditor'))
>>> suite.addTest(praw.tests.LocalOnlyTest('test_deleted_redditor_login'))
>>> unittest.TextTestRunner().run(suite)
Passing local only test: LocalOnlyTest.test_delete_existing_redditor
.Passing local only test: LocalOnlyTest.test_create_deleted_redditor
.Passing local only test: LocalOnlyTest.test_deleted_redditor_login
.
----------------------------------------------------------------------
Ran 3 tests in 12.830s

OK
<unittest.runner.TextTestResult run=3 errors=0 failures=0>
```
"
294,Spruced up docs with some minor spelling/grammar corrections.,2014-05-03T21:06:26Z,2014-05-03T22:33:44Z,,,,"Fixed some spelling and grammar errors as well as removed some duplicate words from the docs. PR looks a lot bigger than it actually is because of some necessary line formatting changes to keep it < 80 characters/line.
"
293,retrieve 'note' field in prodcedure get_banned,2014-05-02T00:54:29Z,2014-05-06T01:12:16Z,,,,"if possible i would love to request the feature of 'reason' be added to the get_banned() function in praw. right now some users are on a temporary ban and some users are not. if i could pull in the reason field i would be able to determine if my scripts should delete the user from our subreddits tools/web apps database.

currently get_banned() only returns a list of users and not the associated note.  pulling in the note field would allow bots/scripts add 'triggers' to tell the bot what to do with certain users.
"
292,Add duplicate submissions functionality (issue #290),2014-05-01T23:15:20Z,2014-05-03T08:56:47Z,,,,"Submissions objects now have an attribute .duplicates which returns either a list of Submission objects (the duplicate submissions) or an empty list (if no duplicates). Address issue #290.

Would need to write tests against static (thus private) Submission objects as far as I can tell. 

Super hacky and also my first ever contribution to a project, so forgive any messiness.
"
291,importing PRAW gives warning,2014-04-25T15:44:25Z,2014-04-26T13:20:03Z,,,,"Im using python2.7.6 with the most recent praw version.

When I import praw, this warning appears:

/usr/lib/python2.7/pkgutil.py:186: ImportWarning: Not importing directory '/usr/lib/python2.7/site-packages/mpl_toolkits': missing **init**.py
  file, filename, etc = imp.find_module(subname, path)
"
290,No way to access duplicates of a submission via praw,2014-04-22T06:31:50Z,2014-05-06T01:18:38Z,Bug,,,"For example you cannot access this page: http://www.reddit.com/r/dogecoin/duplicates/1w3yla/all_cryptocoins_in_a_nutshell/ through praw without a work around.
"
289,fix spelling it's -> its,2014-03-25T20:43:11Z,2014-03-31T22:52:32Z,,,,"Just fixing some minor errors. ""It's"" means ""it is"" or ""it has"" whereas ""its"" is the possessive form
"
288,documentation: s/depreciated/deprecated,2014-03-06T09:40:55Z,2014-04-20T13:03:55Z,,,,"Minor change in docs/pages/writing_a_bot.rst.
"
287,Better support for raw API calls,2014-03-05T01:48:52Z,2014-11-09T16:58:29Z,Feature,,,"Hello,

I found this library and it seemed pretty neat, however, it took me _way_ to long to figure out how to actually just...make a raw API call. Because this library doesn't support the entire reddit api (it really doesn't need to!) but to make a raw api call, you have to call `redditObj.get_content(url, _use_oauth=True)`

However, that `_use_oauth` parameter is very...buried (its the last parameter in a method that takes 8 positional parameters), and not to mention it has an underscore! 

I am just confused i guess to why `get_content()` and `request_json()` are not overridden in `AuthenticatedReddit` to automatically use the http credentials or oauth token (depending on what you logged in with), so that way one can just call get_content() without wondering why you are getting 403 forbidden errors even though you are logged in, or trying to find the hard-to-find _use_oauth=True parameter. 
"
286,"""thumbnail"" attribute appears to be blank even though thumbnail in JSON is not",2014-03-04T00:06:56Z,2014-03-04T00:32:33Z,,,,"Hey there!

Not sure if I'm missing something, thought I'd shoot a quick report before diving in any further. Looks like the thumbnail attribute occasionally doesn't seem to be pulling properly. For example, see here:

``` python
Python 2.7.6 (default, Feb  7 2014, 18:07:52)
[GCC 4.2.1 Compatible Apple LLVM 5.0 (clang-500.2.79)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import praw
>>> praw.__version__
'2.1.14'
>>> # This is master actually
...
>>> r = praw.Reddit(user_agent=""/u/umbrae test"")
>>> sub = r.get_submission(submission_id='1zfyy9')
sub.>>> sub.thumbnail
u''
```

However, [the raw json](http://www.reddit.com/r/comics/comments/1zfyy9/how_to_explain_gay_marriage_to_kids/.json) shows that the thumbnail actually is set.

Any ideas?
"
285,Increased compatability with GAE,2014-02-27T04:02:06Z,2014-03-24T00:13:22Z,,,,"This pull request only has a single commit, and should not have of the... problems with its commit history as did the last one. 

Also, if you havent closed my last PR already (https://github.com/praw-dev/praw/pull/284) please do so. 
"
284,[Please Close] Google App Engine compatability,2014-02-26T23:17:30Z,2014-03-25T16:42:42Z,,,,"After being stung by @Damgaard's claim that I had disappeared from github (I bear him no ill will, I have no excuse for never replying to his requests for updates), I pieced together another branch of praw which should fix the problems with my last pull request on the same topic (https://github.com/praw-dev/praw/pull/239)

I'd like to point out that in the last pull request, @bboe said that the errors I was getting from importing update_checker resulted from a bug in the module itself, which he proceeded to fix. When checking praw for errors today I realized that the bug in question had never actually been fixed. I _think_ that this is because of bboe catching ""NotImplemented"" in line 49 of his bugfix commit (https://github.com/bboe/update_checker/commit/9e6c211210205c1df1e2bcb5dd2be66e6bbd8407), rather than ""NotImplementedError"" as raised when running the app on GAE. If that's fixed then I think that commits #8973461 and #723c499 can safely be removed from the pull request (although I'm new to git, so I'm not sure how I could do that).

I'd like to also clarify that I have not given this version of praw extensive testing with GAE. I've only tried running it with my bot that only performs a few simple functions, and therefore there could be more GAE compatibility bugs that I have not found.

Other than that, I think this PR addresses the problems with my last one.
"
283,Saveable.save() requires the 'save' scope,2014-02-25T07:55:23Z,2014-02-25T08:21:40Z,,,,"The `save()` and `unsave()` calls require that the `save` scope be enabled for an access token.
"
282,store_json_result: True in praw.ini ineffective,2014-02-22T23:14:07Z,2014-02-23T09:15:25Z,,,,"Setting store_json_result to True in praw.ini does not work. This is because the check in the code is ""if reddit_session.config.store_json_result is True"", whereas when setting it in praw.ini, the value is 'True' (or whatever else you put in praw.ini).

Looks like there's code to deal with this (config_boolean), it's just not applied to store_json_result.
"
281,Added a submissions helper function,2014-02-07T22:59:00Z,2014-02-25T21:35:47Z,,,,"I don't know if the 100 numbers are correct in the comment, but since the idea and everything else is exactly the same, I just extracted the main generator function portion and added a submissions generator. I'm looking to work on a moderator bot that watches incoming new submissions to make sure they follow various rules, so a submissions generator is useful for that.
"
280,[Future] Add support for changes to UserLists on reddit,2014-02-07T18:47:05Z,2014-02-21T08:47:55Z,,,,"A few UserLists are getting paginated.  (http://www.reddit.com/r/redditdev/comments/1x83az/heads_up_important_api_changes/)

Changes will be coming out sometime within the coming week.  Putting this here for when the change gets released. 
"
279,Support before/after params in search,2014-02-05T14:51:57Z,2014-11-08T06:34:22Z,,,,
278,Allow extracting and logging in with cookie,2014-01-28T19:00:54Z,2014-02-26T00:55:40Z,,,,"In addition to providing a password in combination with the username, PRAW should support passing in `cookie=` to set a session from a known cookie. For full compatibility the cookie should be able to be stored in the config file.

If both a cookie and a password are given, that should mean -- attempt to use the cookie (verify that authentication works), if it does not, then use the provided password.

In the event the cookie does not work and no password is provided, an appropriate exception should be raised.

Finally, there should be a method to fetch the cookie-string from a logged-in Redditor.
"
277,Is there no logout functionality?,2014-01-26T00:54:29Z,2014-01-26T02:05:23Z,,,,"I searched through the docs and found no reference to ""log out"" or ""sign out"". Is there just no way to do it once signed in currently?
"
276,Fixed a bug in RateLimitExceeded with incorrect key to response dict,2014-01-16T04:27:49Z,2014-05-09T20:50:07Z,Bug,,,"RateLimitExceeded.**init** was setting sleep_time to response['ratelimit'] which was throwing a KeyError exception every time I hit it. Swtiched to 'delay' which seemed to fix the functionality.

If this isn't correct, ignore this, but it fixed my problem and I thought I'd share it.

If this isn't the correct branch to send the pull request to but the fix should still be pulled, please let me know.
"
275,fixed store_json_result config bug,2014-01-14T04:57:21Z,2014-01-15T00:13:58Z,,,,"The config setting `store_json_result` does not work correctly when specified in `praw.ini` due to it not being parsed by `config_boolean`. This was not caught by the unit tests as they manually cast `True` onto the config object.
"
274,BoundedSet,2014-01-12T06:45:53Z,2014-01-13T01:30:07Z,,,,"_fifo can contain items no longer in _set.

``` python
s = BoundedSet(5)
s.add('repeated')
s.add('repeated')
s.add('a')
s.add('b')
s.add('c')
s.add('d')
s.add('e')
s.add('f') # KeyError: 'repeated'
```

So, I guess we should keep _fifo unique?
"
273,Specifying a proxy programatically,2014-01-03T14:26:01Z,2014-01-03T18:08:32Z,,,,"Is there any way you could support specifying a proxy programatically, like so:

``` python
r = praw.Reddit('Example bot', proxy='http://0.0.0.0/')
r.login()
```

I would much rather have this than having to deal with config files, and I'm sure others feel the same. If not, would it be pulled if I did this myself?
"
272,Add support for storing json data on lazy loaded praw objects,2013-12-21T21:12:45Z,2013-12-23T19:26:25Z,,,,"See issue: https://github.com/praw-dev/praw/issues/271
"
271,Store JSON from lazy loaded praw object,2013-12-21T18:29:19Z,2013-12-23T18:52:03Z,,,,"I'm excited to see support for storing JSON data in  1f7bf343aa3dfb6a9b91f8141c2e690ae4244d2d, could make storing data so much easier!

However, have a look at the following example:

``` python2
import praw

r = praw.Reddit(user_agent=""praw json storing test"")
r.config.log_requests = 1
r.config.store_json_result = True

# example from tests/__init__.py
sub_url = ('http://www.reddit.com/r/reddit_api_test/comments/1f7ojw/oauth_submit/')
sub = r.get_submission(url=sub_url)
print sub.json_dict
# prints the expected dict


subreddit = r.get_subreddit('python')
print subreddit.title  # shows Python
print subreddit.json_dict 
# prints None
```

I would expect the second to also print the JSON data. From looking at the code, it looks like the only place the json_data attribute gets set is in the __init__ function on  RedditContentObject.

I think it would make sense to also have json_data get set in _populate function. Thoughts?

I'd be happy to make the  change, but I'd like to check in first, make sure I'm not misunderstanding something.
"
270,Helper function for testing if rate limit is exceeded,2013-12-10T21:56:31Z,2013-12-10T22:37:03Z,,,,"Might be handy to have a helper which tells you if your rate limit has been exceeded, by making a call to a known working URL and testing the response. This could be implemented easily by the user with maybe 15 lines of code, but could make a useful helper. I can send a PR if accepted.

```
praw.helpers.is_rate_limit_exceeded() # true/false
```
"
269,"Custom exceptions, instead of raising httplib/urllib3",2013-12-10T21:53:13Z,2014-03-04T17:53:38Z,,,,"I'm wondering if there would be much call for custom exceptions to be raised from Praw, instead of raising whatever comes out of httplib/urllib3.

For example, if I wanted to detect rate limit being exceeded, I would have to repeat this everywherel

```
except requests.exceptions.HTTPError, e:
    # catch rate limits
    if e.response.status_code == 429:
        raise
```

It might be easier to read if we had something like;

```
except praw.exceptions.RateLimitExceeded, e:
    # original exception can be accessed here;
    e.exc
    # original response here
    e.exc.response
```

However this does add another layer of abstraction and would not be backwards compatible. Any thoughts?
"
268,Slightly misleading documentation of praw.objects.Message,2013-12-10T15:15:33Z,2013-12-14T16:09:54Z,,,,"The description is ""A class for reddit messages (orangereds)."" But the term ""orangered"" is used for anything inboxable, not just messages. It would probably make sense not just to remove the word ""orangered"" here but also to specify that the class is for private messages only.
"
267,PR for #263,2013-12-06T01:12:38Z,2013-12-15T01:02:36Z,,,,"As per #263, and discussions in #264
"
266,PR for #265,2013-12-06T01:04:04Z,2013-12-23T19:49:55Z,,,,"As per #265
"
265,Helpers for converting between numeric id and id36,2013-12-06T00:54:04Z,2013-12-23T18:52:03Z,,,,"Reddit converts their numeric (int) ids to string values using `to36` as seen here;
https://github.com/reddit/reddit/blob/master/r2/r2/lib/utils/_utils.pyx

Converting between these values is useful when you want to increment through a historic set of names which you cannot use after/before with. For example, if you want to extract all threads between two IDs, you'd use `range(start,end)`, convert each id to id36, then use `get_submissions` in chunks of 100.

Pull request coming shortly.
"
264,Fixes #263 ( https://github.com/praw-dev/praw/issues/263 ),2013-12-06T00:43:32Z,2013-12-06T01:05:58Z,,,,"As discussed in #263
"
263,Cannot extract original JSON response for praw objects,2013-12-06T00:42:13Z,2013-12-15T01:00:31Z,,,,"Praw does not offer the ability to extract the JSON response for any object (such as submissions). 

There is an undocumented `_get_json_dict` method, this is flawed because it requires the API call to be repeated, and also will not work unless `self._info_url` has been populated correctly (for example, `get_submissions` which uses `/by_id/` results in each submission being populated with a generic _info_url of `/api/help`).

Therefore I have created a small patch which stores the original JSON response for any `RedditContentObject()`, along with the instance type and timestamp. The instance type is stored so you know what object the JSON is referencing without interrogating it for hints, and timestamp to keep track of when the request was made. These two fields were added because most users will need this, and having it added by default reduces lines of code needed by the user. However, if you disagree then type/timestamp can be removed for sake if needed.

Pull request coming shortly.
"
262,Added reddit-cloud.,2013-12-02T21:44:29Z,2013-12-03T22:55:56Z,,,,
261,Fixed r/test link (issue #259) and removed extra dot (issue #260).,2013-11-30T20:44:26Z,2013-12-03T22:55:28Z,,,,
260,One dot too much,2013-11-30T18:27:12Z,2013-12-03T22:56:34Z,,,,"In https://praw.readthedocs.org/en/latest/pages/lazy-loading.html#lazy-objects-and-errors there is one dot that shouldn't be there AFAIK.

`>> private_subreddit.subscribers.`
"
259,Broken link in praw documentation,2013-11-30T16:47:33Z,2013-12-03T22:56:43Z,,,,"There is a link in praw documentation that is broken (returns ""404 Not Found"").

Link is located on https://praw.readthedocs.org/en/latest/pages/comment_parsing.html#submission-comments
You can find the broken link in the last paragraph on the link I provided.

""So we test the bot in r/test before we let it loose on a “real” subreddit.""
Link to r/test is broken.
"
258,WikiPage.revision_by broken?,2013-11-29T20:30:54Z,2014-05-05T22:44:11Z,Bug,,,"I keep getting `None` as a return value, even though I know the wiki page has been edited by a valid user. The JSON dictionary gives the correct info though.

```
w.revision_by
Out[62]: Redditor(user_name='None')

w._get_json_dict()['revision_by']['data']['name']
Out[63]: u'test_user'
```
"
257,Remove depreciated method from documentation,2013-11-27T20:34:30Z,2013-11-29T08:11:20Z,,,,"I was going though the documentation for parsing comments and noticed that the example under ""[Getting all recent comments to a subreddit or everywhere](https://praw.readthedocs.org/en/latest/pages/comment_parsing.html#getting-all-recent-comments-to-a-subreddit-or-everywhere)"" wasn't working. I did some research and found out via issue #254 the method `get_all_comments()` has been depreciated in the latest version and that using `get_comments('all')` is preferred.

I went ahead and changed the docs in order to accurately represent this so people won't have trouble in the future.
"
256,Add RedditAgain,2013-11-06T07:46:23Z,2013-11-06T09:22:59Z,,,,
255,Handling undocumented errors,2013-11-05T20:37:20Z,2013-12-27T09:18:09Z,,requests.exceptions.HTTPError,requests.exceptions.HTTPError: 504 Server Error: Gateway Time-out,"I ran into my first 

```
requests.exceptions.HTTPError: 504 Server Error: Gateway Time-out
```

today. I was surprised that it crashed my application, since I thought I was catching all possible PRAW exceptions as shown in the docs (https://praw.readthedocs.org/en/nature/pages/code_overview.html#module-praw.errors). The only similar post I could find on the subject was here: http://www.reddit.com/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/, which explains that this is expected behaviour when there are network errors.

That's fine! There's no problem with bubbling up network errors when they occur. But it is completely undocumented that PRAW library calls may throw these exceptions, and there's no way for me to know that I'm supposed to catch them until they crash my app. There's no mention of HTTPError in the PRAW docs (https://praw.readthedocs.org/en/latest/search.html?q=HTTPError).

Possible solutions:
1. Add to the docs that PRAW library calls can raise requests.exceptions.HTTPErrors as well as praw.errors.
2. Catch these errors internally, only raising praw.errors.
"
254,get_all_comments() not working,2013-11-05T19:32:37Z,2014-01-07T21:29:32Z,,AttributeError,AttributeError: 'tuple' object has no attribute 'get_comments',"The get_all_comments() function is creating this error:

Traceback (most recent call last):
  File ""<pyshell#10>"", line 1, in <module>
    r.get_all_comments()
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 77, in wrapped
    return function(args, kwargs)
  File ""C:\Python27\lib\site-packages\praw__init__.py"", line 639, in get_all_comments
    return self.get_comments('all', _args, *_kwargs)
AttributeError: 'tuple' object has no attribute 'get_comments'

Python 2.7.5
"
253,RTD documentation moved to new theme,2013-11-05T17:09:17Z,2013-11-06T07:03:40Z,,,,"- New theme looks way more clean and neat
- Also mobile supported
- Refer to
  http://ericholscher.com/blog/2013/nov/4/new-theme-read-the-docs/ for
  more information.
"
252,Update the RTD documentation with the latest theme.,2013-11-05T16:32:11Z,2013-11-06T08:44:05Z,,,,"Would be cool if we could update the Read the Docs page to the latest theme.

More info: http://ericholscher.com/blog/2013/nov/4/new-theme-read-the-docs/
"
251,No method get_flair on a submission,2013-10-27T01:01:47Z,2013-10-28T11:38:56Z,,,,"It might be convenient to have a get_flair method on an individual submission.
"
250,Multiple (8) test failures in 2.1.11 ,2013-10-26T08:11:56Z,2013-12-26T11:44:32Z,,"RedirectException, AssertionError","RedirectException: Unexpected redirect from http://www.reddit.com/prefs/friends/.json to https://ssl.reddit.com/prefs/friends/.json, AssertionError: False is not true","Python 2.7.5 (default, Sep 18 2013, 22:52:14)
[GCC 4.2.1 Compatible FreeBSD Clang 3.3 (tags/RELEASE_33/final 183502)] on freebsd9

praw 2.1.11
requests-1.2.3

```
======================================================================
ERROR: test_add_remove_friends (praw.tests.RedditorTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1408, in test_add_remove_friends
    if self.other_user in self.r.user.get_friends():
  File ""/work/praw-dev-praw-a36c3f1/praw/objects.py"", line 769, in get_friends
    return self.reddit_session.request_json(url)[0]
  File ""/work/praw-dev-praw-a36c3f1/praw/decorators.py"", line 158, in wrapped
    return_value = function(reddit_session, *args, **kwargs)
  File ""/work/praw-dev-praw-a36c3f1/praw/__init__.py"", line 476, in request_json
    response = self._request(url, params, data)
  File ""/work/praw-dev-praw-a36c3f1/praw/__init__.py"", line 348, in _request
    response = handle_redirect()
  File ""/work/praw-dev-praw-a36c3f1/praw/__init__.py"", line 322, in handle_redirect
    url = _raise_redirect_exceptions(response)
  File ""/work/praw-dev-praw-a36c3f1/praw/internal.py"", line 159, in _raise_redirect_exceptions
    raise RedirectException(response.url, new_url)
RedirectException: Unexpected redirect from http://www.reddit.com/prefs/friends/.json to https://ssl.reddit.com/prefs/friends/.json
```

```
======================================================================
FAIL: test_add_reply_and_verify (praw.tests.CommentReplyTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 620, in test_add_reply_and_verify
    submission = self.first(self.subreddit.get_new(), predicate)
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 149, in first
    self.assertTrue(first_hit is not None)
AssertionError: False is not true
```

```
======================================================================
FAIL: test_flair_csv_and_flair_list (praw.tests.FlairTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 723, in test_flair_csv_and_flair_list
    list(self.subreddit.get_flair_list())))
AssertionError: Lists differ: [] != [(u'reddit', u'dev', u''), (u'...

Second list contains 3 additional elements.
First extra element 0:
(u'reddit', u'dev', u'')

- []
+ [(u'reddit', u'dev', u''),
+  (u'pyapitestuser2', u'', u'xx'),
+  (u'pyapitestuser3', u'awesome', u'css')]
```

```
======================================================================
FAIL: test_approve (praw.tests.ModeratorSubmissionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1079, in test_approve
    self.first(self.subreddit.get_new(), predicate)
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 149, in first
    self.assertTrue(first_hit is not None)
AssertionError: False is not true
```

```
======================================================================
FAIL: test_wiki_ban (praw.tests.ModeratorUserTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1206, in test_wiki_ban
    self.subreddit.get_wiki_banned)
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1170, in add_remove
    test_add()
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1159, in test_add
    self.assertTrue(self.other in listing())
AssertionError: False is not true
```

```
======================================================================
FAIL: test_clear_vote (praw.tests.SubmissionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1607, in test_clear_vote
    submission = self.first(self.r.user.get_submitted(), predicate)
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 149, in first
    self.assertTrue(first_hit is not None)
AssertionError: False is not true
```

```
======================================================================
FAIL: test_report (praw.tests.SubmissionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1647, in test_report
    self.first(self.r.get_subreddit(self.sr).get_reports(), predicate)
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 149, in first
    self.assertTrue(first_hit is not None)
AssertionError: False is not true
```

```
======================================================================
FAIL: test_upvote (praw.tests.SubmissionTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1707, in test_upvote
    submission = self.first(self.r.user.get_submitted(), predicate)
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 149, in first
    self.assertTrue(first_hit is not None)
AssertionError: False is not true
```

```
======================================================================
FAIL: test_unsubscribe_and_verify (praw.tests.SubredditTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/work/praw-dev-praw-a36c3f1/praw/tests/__init__.py"", line 1765, in test_unsubscribe_and_verify
    self.assertTrue(all(pred(sub) for sub in self.r.get_my_subreddits()))
AssertionError: False is not true

----------------------------------------------------------------------
Ran 218 tests in 934.789s

FAILED (failures=8, errors=1)
```
"
249,Don't destroy signatures with decorators.,2013-10-11T10:55:12Z,2015-07-11T21:56:59Z,Documentation,,,"The usage of decorators destroys the signatures of methods, making it harder to understand how to use them. In bdf5dfc4ae84b005cfd82a4f399673dfb3f43dfb a fix was added that prevented the destruction while generating the documentation for Sphinx. But the signature is still destroyed when introspecting methods with standard python introspection tools such as `help`. As we explicitly encourage people to use standard introspection to better understand the library the above mentioned fix is insufficient and the solution must be expanded to cover signature destruction everywhere.
"
248,add an author_name field?,2013-10-08T20:04:17Z,2013-10-08T20:15:45Z,,,,"Submission items return a formatted api call for author as in:

'author': Redditor(user_name='etianen')

To put that in a database, you need to process that (trivially) or you are making another call to Reddit to get myRedditor.name.

Adding an author_name field would save the 20 seconds required to write the string processing to pull the username out of there and keep from hitting reddit again if you don't need anything but the name. 
"
247,mark_as_nsfw appears to require oauth even when logging in with user/pass,2013-10-03T00:29:07Z,2013-10-07T12:42:59Z,,praw.errors.ModeratorOrScopeRequired,praw.errors.ModeratorOrScopeRequired: ```mark_as_nsfw` requires a moderator of the subreddit or the OAuth2 scope `modposts`` requires a moderator of the subreddit` requires a logged in session,"I'm not one hundred percent certain this is a bug, but it seems like it to me, and it seems to have newly started occurring.

If I login without oauth - just with raw user/pass, and attempt to mark a submission that I have mod permissions on as nsfw, I get a ModeratorOrScope error. See below, and note that the subreddit of the submission I'm looking at is /r/serendipity, and that the user is /u/serendipitybot, who is a mod of that subreddit.

`````` python
>>> import praw
>>> import settings
>>> r = praw.Reddit(user_agent=settings.UA)
>>> r.login(settings.REDDIT_LOGIN, settings.REDDIT_PASSWORD)
>>> r.config.decode_html_entities = True
>>> submission = r.get_submission(submission_id='1nftbb')
>>> submission.mark_as_nsfw()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/cdary/.virtualenvs/serendipity/lib/python2.7/site-packages/praw/decorators.py"", line 296, in wrapped
    function.__name__, scope)
praw.errors.ModeratorOrScopeRequired: ```mark_as_nsfw` requires a moderator of the subreddit or the OAuth2 scope `modposts`` requires a moderator of the subreddit` requires a logged in session
>>> settings.REDDIT_LOGIN
'serendipitybot'
>>> submission.subreddit
Subreddit(display_name='Serendipity')
>>> praw.__version__
'2.1.8'
``````

Thanks for any thoughts, and thanks for the great library!
"
246,get_contributors is only mod_only for public subreddits.,2013-09-18T21:51:47Z,2013-10-07T12:43:40Z,,,,"See [this r/redditdev thread](http://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/).

If the subreddit is public, then `get_contributors` is restricted to moderators. If it's either a private or a protected subreddit, then `get_contributors` only requires access. So the current access_restrict prevents the retrieval of contributors from such subreddits as r/vancouver. The access_restriction for this method need to be handled differently to be prevent restricting access to something that isn't actually restricted.
"
245,adding get_liked and get_disliked to Redditor. Redditor's have the optio...,2013-09-15T16:54:20Z,2013-09-16T09:32:53Z,,,,"redditors have the option to make their likes and dislikes public. I feel the praw should have the ability to access this public information.

If you attempt to access a redditor's likes, and they are not public, an exception is raised (requests.exceptions.HTTPError).
"
244,MoreComments object in collection when using flatten_tree helper,2013-09-15T16:18:07Z,2013-09-15T19:08:09Z,,,,"For submissions with a large amount of comments, when I use the flatten_tree helper, it returns a collection that includes instances of MoreComments.  I then have to do an isinstance test to ignore these to extract the body of each comment.

I have three questions:
-What is the purpose of the MoreComments object?
-Is it safe to skip over these as they contain no comment information?
-Can we alter the flatten_tree helper to exclude this type?
"
243,Have submissions also include date and time of submission.,2013-09-11T16:19:39Z,2013-09-11T19:15:23Z,,,,"When making a call to, for example, get_new(subreddit), the list returned includes the score and title of each submission, but not the date and time of submission, which would be useful information for some code I'm trying to write. Would it be possible to add this functionality?

Thanks,

Tevyn

P.S: I'm really liking this wrapper so far; it's the best one out there.
"
242,Using get_subreddit() on a non-existent subreddit should throw an error,2013-09-05T19:13:28Z,2013-09-05T19:43:08Z,,,,"Let's say we have

```
r = praw.Reddit(user_agent='agent')
r.get_subreddit('afeiofnaierfn')
```

get_subreddit() creates a praw.objects.Subreddit object when in fact it should throw an InvalidSubreddit error.

This matters when a program using this module is trying to traverse subreddits. Say, if a program asks a user to pick a subreddit, and the user inputs the name of a non-existent subreddit, then a Subreddit object is created even though the subreddit in question does not exist. The InvalidSubreddit error only pops up when the program attempts to access the contents of the non-existent subreddit in question.
"
241,403 Error on add_comment,2013-08-31T03:40:18Z,2013-08-31T03:47:54Z,,,,"This error has been resolved by the author.
"
240,Delete a user.,2013-08-17T09:39:52Z,2014-05-05T20:26:47Z,Feature,,,"I could not find a method to delete the current user. Is there one?
"
239,Google App Engine Compatibility ,2013-08-16T00:17:59Z,2014-02-25T21:05:56Z,,,,"While writing a bot I ran into some errors in praw that required me to edit the module itself. These changes should allow praw to be out-of-the-box compatible with GAE.

Pull if you will, I just thought I'd submit it.
"
238,Added a method to allow subreddit mods to (un)sticky posts in their subreddits,2013-08-14T17:31:00Z,2013-08-15T19:13:54Z,,,,"I came across the problem of not being able to sticky posts through praw while creating a bot for /r/spelunky, so I added support for it.
"
237,Not using proxy correctly?,2013-08-11T05:34:26Z,2013-08-16T14:23:04Z,,requests.exceptions.ConnectionError,"requests.exceptions.ConnectionError: HTTPConnectionPool(host='proxy.server', port=3128): Max retri","I'm trying to run a Reddit bot on https://www.pythonanywhere.com. When I try to connect to Reddit, I get the following:

```
Traceback (most recent call last):
  File ""sns.py"", line 154, in <module>
    r.login(username, password)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/__init__.py"", line 906, in login
    self.request_json(self.config['login'], data=data)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/decorators.py"", line 223, in error_check
ed_function
    return_value = function(cls, *args, **kwargs)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/__init__.py"", line 407, in request_json
    response = self._request(url, params, data)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/__init__.py"", line 294, in _request
    timeout=timeout)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/decorators.py"", line 64, in __call__
    result = self.function(reddit_session, url, *args, **kwargs)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/decorators.py"", line 167, in __call__
    return self.function(*args, **kwargs)
  File ""/home/sns/.local/lib/python2.7/site-packages/praw/helpers.py"", line 137, in _request
    allow_redirects=False, auth=auth)
  File ""/home/sns/.local/lib/python2.7/site-packages/requests/sessions.py"", line 377, in post
    return self.request('POST', url, data=data, **kwargs)
  File ""/home/sns/.local/lib/python2.7/site-packages/requests/sessions.py"", line 335, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/sns/.local/lib/python2.7/site-packages/requests/sessions.py"", line 438, in send
    r = adapter.send(request, **kwargs)
  File ""/home/sns/.local/lib/python2.7/site-packages/requests/adapters.py"", line 327, in send
    raise ConnectionError(e)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='proxy.server', port=3128): Max retri
es exceeded with url: http://www.reddit.com/api/login/.json (Caused by <class 'socket.error'>: [Er
rno 111] Connection refused)
```

This issue is raised perennially on the Pythonanywhere forum, because it tends to occur and resolve sporadically without apparent reason, but the folks over there keep saying it's a bug with PRAW (https://www.pythonanywhere.com/forums/topic/665/#id_post_5802). I don't know if that's true, but maybe you'd know.
"
236,Accessing Subreddit.display_name does not trigger populating ,2013-08-06T05:41:31Z,2014-11-23T07:29:03Z,,,,"Hello,

I ran into an issue with PRAW 2.1.4 when trying to use `Subreddit.display_name`. When calling `get_subreddit()`, `display_name` is set from the argument given to `get_subreddit()`, and the object is subsequently not populated from Reddit when `display_name` is accessed. For example:

```
>>> sub = r.get_subreddit('aSkReddIT')
>>> sub.display_name
'aSkReddIT'
>>> sub.fullname
u't5_2qh1i'
>>> sub.display_name
u'AskReddit'
```

Note that `display_name` is populated along with everything else if we access anything else.

This also leads to another side effect, wherein asking for `display_name` on a non-existent subreddit does not raise `InvalidSubreddit`, while attempts to access something like `fullname` will raise it. 
"
235,Unexpected RedirectException on single-item search result,2013-07-29T18:46:52Z,2014-09-17T00:32:12Z,Bug,,,"More details:

http://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/

The exception should not occur there.
"
234,praw resets configuration setting that allows for public viewing of statistics on subreddit.update_settings(),2013-07-17T00:54:03Z,2013-07-18T01:34:54Z,,,,"when update_settings is run on a subreddit, it clears the option for allowing public viewing of the subreddit statistics page.

```
sidebar = re.sub(r'(\[\]\(#BOT_STREAMS\)).*(\[\]\(/BOT_STREAMS\))',
    '\\1\\2',
    sidebar)
subreddit.update_settings(description=sidebar)
# Kills 'make the traffic stats page available to everyone' setting if on.
```
"
233,"Configuration initialization should also look for the http_proxy environment variable, if available",2013-07-04T16:48:57Z,2013-07-18T10:50:06Z,,,,"It is a convention in Python and Unix-based systems to set the http_proxy env. variable when defining a proxy. This change allows that to be picked up automatically, if no proxy is set in the config file.
"
232,Search,2013-07-03T12:42:57Z,2013-07-18T10:19:49Z,,,,"Two commits adding functionality to `search` and fixing the bug in the `Subreddit` convenience method `search`. More info can be found in the commit messages.
"
231,Submission Search Timestamps,2013-07-01T07:15:18Z,2013-07-18T11:08:39Z,,,,"This is in regards to [a request I made](http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/) in /r/redditdev.

Currently, the only way I know of accessing this feature is manually constructing a URL in the form: 

> http://www.reddit.com/search/.json?q=(and+timestamp%3A1354348800..1354521600)&syntax=cloudsearch

The query must use Amazon Cloud Search syntax and the timestamp values are the beginning and end times converted to unix timestamps.
"
230,Select_flair API method,2013-06-25T00:06:50Z,2013-08-23T06:01:40Z,,,,"I added a method select_flair that calls the api /api/selectflair instead of /api/flair.
/api/select flair allows changing your own flair and your own submission's flair on subreddits that allow it.

Here is an example:

``` python
r = praw.Reddit(user_agent=USER_AGENT)
r.login('SakuraiBot', PASSWORD)

id_SSB4 ='d31a17da-d4ad-11e2-a21c-12313d2c1c24'
reddit_link = '1gpsez'
submission = r.get_submission(submission_id=reddit_link)
r.select_flair(submission, id_SSB4)

id_ROB = '05208d36-05b6-11e1-b212-12313b078c81'
redditor = r.get_redditor('SakuraiBot')
r.select_flair(redditor, id_ROB, 'smashbros')
```

I tested changing my own submission flair and changing my own user flair.
I tested it on failing cases (such as another's submission, or a wrong flair_id, or a subreddit that doesn't allow self-assignment). Nothing happens of course, but Reddit doesn't throw back any error message, so we can't know it failed.
I wasn't sure what self.exert was for, so I kept it intact.

Do not hesitate to point out my mistakes, as I only had an overview of the general structure.
"
229,Changing own flair without moderator access,2013-06-21T23:11:58Z,2013-07-04T20:22:41Z,,,,"Hi,

I searched for a while, but there doesn't seem to be a way to change your own user's flair, or your own submission's flair, when you are not moderator of the subreddit.

Here's what I'm trying to do:

``` python
submission = r.submit(subreddit, title, url=post_details.picture)
submission.set_flair(flair_text='SSB4', flair_css_class='ssb4')
```

I keep getting:

```
`set_flair` requires a moderator of the subreddit or the OAuth2 scope `modflair`
```

It seems the entire function is restricted.

```
@decorators.restrict_access(scope='modflair')
```

When doing it manually on Reddit by clicking the ""flair"" button under my submission title, I don't need to be moderator.

Sorry if I am missing something.

Thanks.
"
228,Adding ALTcointip bot to useful_scripts.rst,2013-06-19T21:14:28Z,2013-06-19T21:19:45Z,,,,"ALTcointip bot is a newly open-sourced bot that allows Redditors to gift (tip) various cryptocoins to each other as a way of saying thanks.
"
227,Added support for getting username mentions (Reddit gold feature),2013-06-07T20:10:22Z,2013-06-13T04:32:55Z,,,,"Title is pretty self explanatory. It might be good to add a decorator for methods that should only be run if the user logged in has gold though.
"
226,Mod mail,2013-06-05T14:22:35Z,2013-06-13T04:24:05Z,,,,"Make `get_mod_mail` available to `Subreddit` objects, just like `get_mod_log`. Additionally fix a bug that affected methods in `Reddit` which were restricted to moderators and had the subreddit as a default argument, which were called from `Reddit` without giving the subreddit as an explicit argument or giving it as a named argument. This bug would have affected `mod_mail` in the new form.
"
225,PRAW OAuth docs have `redirect_url` instead of `redirect_uri`,2013-06-01T20:33:25Z,2013-06-02T21:59:14Z,,,,"Pretty self-explanatory, hopefully it'll also help a few other people who got a little bit confused by the documentation.

Cheers!
"
224,moderators.json does not have permission information,2013-05-27T03:31:25Z,2013-05-27T03:49:45Z,,,,"This would enable libraries and tools to access this information.  I had praw specifically in mind, I tried to ban a user via a subreddit that the account I was on did not have that permission.
"
223,r.login not working,2013-05-23T06:49:11Z,2013-05-28T17:22:35Z,,requests.exceptions.SSLError,requests.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available,"When trying to use r.login, this error is raised: raise SSLError(e)
requests.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available

Any packages I should install to fix this problem?
"
222,Test scripts shouldn't have actual password hardcoded,2013-05-21T22:32:22Z,2013-05-21T23:06:38Z,,,,"[`praw/scripts/init_test_environment.py`](https://github.com/praw-dev/praw/blob/master/scripts/init_test_environment.py) has a `PASSWORD` constant hard-coded. This password is the actual password for the associated accounts. I understand that there's no real motivation for people to abuse it, but I think it's bad regardless to expose credentials like that.
"
221,Add support for reddit API random functionality,2013-05-12T22:40:39Z,2013-05-14T17:48:22Z,,,,"When requesting a submission from a subreddit there are listing selectors for hot, top, etc. There is no support  for getting random submission from a subreddit with the url of format reddit.com/r/anysubreddit/random.json.
"
220,& to &amp;,2013-05-09T19:29:58Z,2013-05-11T16:25:05Z,,,,"Each time I request the sidebar code, any '&' is converted to '&amp;' even when 'amp;' is already there. The result is '&amp;' being turned into '&amp;amp;'
"
219,Support /domain listings,2013-05-03T18:09:47Z,2013-05-07T18:09:32Z,,,,"Prompted from this tweet: https://twitter.com/Redback93/status/329894773412462592
"
218,place_holder in get_content: id36 or fullname?,2013-04-28T18:23:08Z,2013-04-28T20:09:18Z,,,,"The documentation suggests that `place_holder` should be a fullname, but it's actually compared against each thing's id36. I would suggest changing the code (praw/**init**.py line 443) rather than the documentation.
"
217,Split helper.py into helper.py and internal.py,2013-04-25T23:36:20Z,2013-04-26T19:23:31Z,,,,"From now on helper.py will contain helper functions intended
to be used by client programs and internal will contain
exclusively internal helper functions. If a function is used
both internally and by client programs then it should be placed
in helper.py.
"
216,"Add documentation page ""Lazy loading""",2013-04-22T18:49:49Z,2013-09-22T11:51:24Z,,,,"There should be a page in the documentation explaining PRAW's lazy loading and how to to work around it.

For instance calling `get_subreddit` or `get_redditor` will create objects without actually performing an HTTP request. That means there is no validation that the subreddit, or redditor actually exist until a later action is performed on one of these objects. We should note that passing `fetch=True` into either will actually query the object for it's available info. Having this part of the document will be satisfactory to handle the issues described in #47.

The other big section is all the `get_content` generator functions which don't make any HTTP requests until they are iterated over.
"
215,Don't prompt when stdin is closed.,2013-04-21T23:48:06Z,2013-04-22T00:17:36Z,,,,"As indicated in #207, a closed input stream will result in an infinite loop of attempted submissions lacking a necessary captcha.

As such any function in PRAW that requires input should fail if the input stream is closed by raising an appropriate exception.
"
214,Fix bug in upload_image for py3.3 see #211,2013-04-20T06:42:40Z,2013-04-20T06:45:06Z,,,,
213,Failure for certain kinds of network errors?,2013-04-19T12:23:01Z,2013-04-20T03:30:30Z,,AttributeError,AttributeError: 'HTTPError' object has no attribute 'code'```,"`````` python
Traceback (most recent call last):
File ""bot.py"", line 41, in <module>
    res = submit(api, title, link)
  File ""bot.py"", line 32, in submit
    return api.submit('MozillaTech', title, url=link)
  File ""/usr/lib/python2.7/site-packages/praw/decorators.py"", line 341, in wrapped
    return function(cls, *args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/decorators.py"", line 131, in __call__
    return self.function(*args, **kwargs)
  File ""/usr/lib/python2.7/site-packages/praw/__init__.py"", line 1531, in submit
    if error.code == 403:
AttributeError: 'HTTPError' object has no attribute 'code'```
``````
"
212,Wiki pages that are not in the top level namespace cannot be edited,2013-04-19T09:36:24Z,2013-04-21T23:42:47Z,,HTTPError,HTTPError: 403 Client Error: Forbidden,"/wiki/config/sidebar for example.  `.may_revise` is `True` on all the pages that I tested.  I am using the latest PRAW on python 3.2.

Traceback:

In [92]: sb = sub.get_wiki_page('testfolder/testpage')

```
In [93]: sb.may_revise
Out[93]: True

In [94]: sb.edit(content=t)
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
<ipython-input-94-dd79faa31d61> in <module>()
----> 1 sb.edit(content=t)

/home/zach/.local/lib/python3.2/site-packages/praw/objects.py in edit(self, *args, **kwargs)
   1124 
   1125         """"""
-> 1126         self.subreddit.edit_wiki_page(self.page, *args, **kwargs)
   1127 
   1128 

/home/zach/.local/lib/python3.2/site-packages/praw/decorators.py in wrapped(self, *args, **kwargs)
    179     @wraps(function)
    180     def wrapped(self, *args, **kwargs):
--> 181         return function(self.reddit_session, self, *args, **kwargs)
    182     return wrapped
    183 

/home/zach/.local/lib/python3.2/site-packages/praw/__init__.py in edit_wiki_page(self, subreddit, page, content, reason)
    832                 'r':  six.text_type(subreddit),
    833                 'reason': reason}
--> 834         return self.request_json(self.config['wiki_edit'], data=data)
    835 
    836     def get_access_information(self, code,  # pylint: disable-msg=W0221

/home/zach/.local/lib/python3.2/site-packages/praw/decorators.py in error_checked_function(cls, *args, **kwargs)
    221     @wraps(function)
    222     def error_checked_function(cls, *args, **kwargs):
--> 223         return_value = function(cls, *args, **kwargs)
    224         if isinstance(return_value, dict):
    225             if return_value.get('error') == 304:  # Not modified exception

/home/zach/.local/lib/python3.2/site-packages/praw/__init__.py in request_json(self, url, params, data, as_objects)
    402         """"""
    403         url += '.json'
--> 404         response = self._request(url, params, data)
    405         if as_objects:
    406             hook = self._json_reddit_objecter

/home/zach/.local/lib/python3.2/site-packages/praw/__init__.py in _request(self, url, params, data, files, timeout, raw_response, auth)
    289                 retval = helpers._request(self, url, params, data, files=files,
    290                                           auth=auth, raw_response=raw_response,
--> 291                                           timeout=timeout)
    292                 if not raw_response and self.config.decode_html_entities:
    293                     retval = re.sub('&([^;]+);', decode, retval)

/home/zach/.local/lib/python3.2/site-packages/praw/decorators.py in __call__(self, reddit_session, url, *args, **kwargs)
     62         if key in self._cache:
     63             return self._cache[key]
---> 64         result = self.function(reddit_session, url, *args, **kwargs)
     65         if kwargs.get('raw_response'):
     66             return result

/home/zach/.local/lib/python3.2/site-packages/praw/decorators.py in __call__(self, *args, **kwargs)
    165             time.sleep(delay)
    166         self.last_call[config.domain] = now
--> 167         return self.function(*args, **kwargs)
    168 
    169 

/home/zach/.local/lib/python3.2/site-packages/praw/helpers.py in _request(reddit_session, url, params, data, timeout, raw_response, auth, files)
    164         else:
    165             raise OAuthException(msg, url)
--> 166     response.raise_for_status()
    167     if raw_response:
    168         return response

/home/zach/.local/lib/python3.2/site-packages/requests/models.py in raise_for_status(self)
    636             http_error = HTTPError(http_error_msg)
    637             http_error.response = self
--> 638             raise http_error
    639 
    640     def close(self):

HTTPError: 403 Client Error: Forbidden
```
"
211,Upload image fails on Python 3.3,2013-04-18T22:40:56Z,2013-04-20T06:46:20Z,,TypeError,"TypeError: startswith first arg must be bytes or a tuple of bytes, not str","Running the test suite fails everytime `upload_image` is used while running python 3.3. The problem is also present after my update of the test suite.

``` text
======================================================================
ERROR: test_upload_uerinvalid_image (praw.tests.ImageTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""c:\users\andreas\dropbox\egen\kode\python\internet\praw\praw\tests\__init
__.py"", line 683, in test_upload_uerinvalid_image
    image)
  File ""c:\Python33\Lib\unittest\case.py"", line 571, in assertRaises
    return context.handle('assertRaises', callableObj, args, kwargs)
  File ""c:\Python33\Lib\unittest\case.py"", line 135, in handle
    callable_obj(*args, **kwargs)
  File ""c:\users\andreas\dropbox\egen\kode\python\internet\praw\praw\decorators.p
y"", line 181, in wrapped
    return function(self.reddit_session, self, *args, **kwargs)
  File ""c:\users\andreas\dropbox\egen\kode\python\internet\praw\praw\decorators.p
y"", line 341, in wrapped
    return function(cls, *args, **kwargs)
  File ""c:\users\andreas\dropbox\egen\kode\python\internet\praw\praw\__init__.py""
, line 1101, in upload_image
    if first_bytes.startswith(JPEG_HEADER):
TypeError: startswith first arg must be bytes or a tuple of bytes, not str
```
"
210,Parameter docstrings,2013-04-18T22:10:09Z,2013-04-20T03:03:27Z,,,,"The decorators hide what parameters a function actually takes. Every public function should:
- [x] explicitly define via `:param XXX:` the parameters it takes
- [x] link to any functions that have `*args, **kwargs` passed to in order to reference additional arguments that may be used

For the functions on RedditContentObjects that directly map to other functions in `__init__` the parameters that are not needed (usually the first object corresponding to `self`) should be indicated as not necessary in that situation. The language here should be consistent for all these functions.
"
209,Pytest,2013-04-18T03:13:20Z,2013-06-27T10:02:39Z,,,,"The reorganization of the test suite from one big file into many smaller and a switch from unittest to pytest.

Alongside these two big changes, a number of smaller bugs in the test suite has been fixed and a few optimizations has been added. But overall, the files look a lot like they did when they were classes in the big test file.

Running setup.py test will run the test suite as per usual and this has been tested with both 2.7 and 3.3.

Surprisingly, these changes increased the line count. This comes from having to add the license to 26 files instead of 1 and pylint needing 2 lines between base level objects compared to only needing 1 line between a class methods.

I think it would be a good idea if some of the files were combined, some of the files only have a single test. One solution that wouldn't require too much refactoring is to combine all the comments classes into one, all the moderator classes into one and finally all the submission classes into one. This would eliminate 7 files.

I've left the docstring unchanged in the helper file for now. I'm not sure whether to remove it or add docstrings to all test files, with a small description of what to put into the test file.

Overall I want to say that having worked with this new test structure lately, it seems much more efficient and comparing groups of tests is much simpler.
"
208,Getting current vote status of submission,2013-04-18T01:45:45Z,2013-04-22T20:09:56Z,,,,"I'm making a little script that interacts with the api and I need to be able to see what the currently logged-in user has currently voted on a submission. (To avoid repeat requests) Is there any way to do this?
"
207,Writing a reddit Bot documentation issues:,2013-04-14T20:01:52Z,2013-08-17T12:45:59Z,,,,"Greetings, I loved the docs but noticed two things here: https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html

First: Underneath the header ""Finding what we need"" there is a typo in a function call.

> > > r = praw.reddit('Submission variables testing by /u/_daimon')
> > > vs
> > > r = praw.Reddit(""Submission variables testing by/u/_daimon')

Second: The documentation doesn't mention in order for a bot to use the send_msg function, such as depicted underneath the header ""The 3 Bot Problems"" or the full code at the bottom, they will need at least two link karma otherwise it will be asked to fill out a captcha. If they aren't paying close attention it will simply look like their code is in a loop.

--> r.user.send_message('_Daimon_', msg)
"
206,Adding proxy support to praw.,2013-04-05T21:19:04Z,2013-04-06T21:06:45Z,,,,"Sites sections of praw.ini now handle the optional directive
'http_proxy'.
Example
[reddit.com]
http_proxy:127.0.0.1:8000

Available URI are the ones handled by the Requests python module.
Proxy authentication would work with:
http_proxy:user:password@host:port

All tests passing with and without proxy (regular tests as well as python setup.py test -s
praw.tests.OAuth2Test
"
205,Use version of next that works in python 3x,2013-04-01T01:17:48Z,2013-04-01T19:12:18Z,,,,"See http://www.reddit.com/r/learnpython/comments/1afg3z/
looking_for_tutorial_on_writing_reddit_bot/c8x2pq2?context=3
"
204,References,2013-03-20T16:16:22Z,2013-03-20T17:33:42Z,,,,"This should finish #195. I gave it a look and it looked pretty quick to implement, So I did. I probably should still have asked you before hand if you were working on it, but I assume you aren't since you said something about sending ti to someone else 2 weeks ago.
"
203,Can't login...  need help,2013-03-19T20:54:57Z,2013-03-20T18:50:49Z,,requests.exceptions.SSLError,requests.exceptions.SSLError: _ssl.c:489: The handshake operation timed out,"Hi,

I am a first time user and I am following the tutorial on the praw documentation website.

Right after r.login() executes, it prompts me for username and password.
It does not login but instead after a log wait, it dumps out this trace:

Traceback (most recent call last):
  File ""C:\dev\py\reddit\test.py"", line 18, in <module>
    r.login()
  File ""C:\Python27\lib\site-packages\praw__init__.py"", line 903, in login
    self.request_json(self.config['login'], data=data)
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 223, in error_checked_function
    return_value = function(cls, _args, *_kwargs)
  File ""C:\Python27\lib\site-packages\praw__init__.py"", line 404, in request_json
    response = self._request(url, params, data)
  File ""C:\Python27\lib\site-packages\praw__init__.py"", line 291, in _request
    timeout=timeout)
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 64, in __call__
    result = self.function(reddit_session, url, _args, *_kwargs)
  File ""C:\Python27\lib\site-packages\praw\decorators.py"", line 167, in **call**
    return self.function(_args, *_kwargs)
  File ""C:\Python27\lib\site-packages\praw\helpers.py"", line 137, in _request
    allow_redirects=False, auth=auth)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 340, in post
    return self.request('POST', url, data=data, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 279, in request
    resp = self.send(prep, stream=stream, timeout=timeout, verify=verify, cert=cert, proxies=proxies)
  File ""C:\Python27\lib\site-packages\requests\sessions.py"", line 374, in send
    r = adapter.send(request, *_kwargs)
  File ""C:\Python27\lib\site-packages\requests\adapters.py"", line 213, in send
    raise SSLError(e)
requests.exceptions.SSLError: _ssl.c:489: The handshake operation timed out

I think this might be related to the fact that I am behind a corporate proxy, but I have all of these env variables set properly: HTTP_PROXY, HTTPS_PROXY and ALL_PROXY.

And help would be appreciated. Thanks.
"
202,praw SSLError during login from shared hosting,2013-03-18T15:39:36Z,2013-04-04T15:39:37Z,,,,"I have tested a particular scrip I've written using praw no problem on my local machine and I wanted to automate it on one of my shared hosting servers on Dreamhost. I got python configured and went to run my script and it errors out at the reddit login portion with an SSL error.

This is probably my lack of understanding of the reddit api and what hosts they allow to connect or it may not be. I've done some googling around and it seems the error is because of a mismatched security certificate? I thought I'd post this here in the hopes that someone else would have a better idea.

This is the traceback https://gist.github.com/dgautsch/5188044
"
201,get_wiki_page requires exact capitalization,2013-03-18T06:12:45Z,2013-03-18T22:08:16Z,,praw.errors.ClientException,praw.errors.ClientException: Unexpected redirect from http://www.reddit.com/r/gamestest/wiki/AutoModerator.json to http://www.reddit.com/r/gamestest/wiki/automoderator.json,"Passing the wrong capitalization of a wiki page name to `get_wiki_page` results in an error. For example, if a wiki page exists named ""automoderator"", but you do `sub.get_wiki_page('AutoModerator')`:

```
praw.errors.ClientException: Unexpected redirect from http://www.reddit.com/r/gamestest/wiki/AutoModerator.json to http://www.reddit.com/r/gamestest/wiki/automoderator.json
```
"
200,Multiprocess rate limiting?,2013-03-10T19:16:09Z,2013-05-11T16:25:58Z,,,,"How difficult would it be to implement multiprocess rate limiting in PRAW? e.g., instead of keeping track of request rate on a per-process basis, keep track of it at a central location and only allow 30 requests/minute for all processes running PRAW.
"
199,Update useful_scripts.rst,2013-03-08T21:29:52Z,2013-03-08T21:36:21Z,,,,"Adding my project AlienFeed to the list of useful scripts from Damgaard's request.
"
198,Add AlienFeed to the list of useful scripts.,2013-03-08T19:47:17Z,2013-03-08T21:35:23Z,,,,
197,Minor documentation fixes.,2013-03-07T22:04:31Z,2013-03-08T00:16:28Z,,,,"Updating the explicit links to the wiki as mentioned in #195 and reformatted the 3 examples in index.rst to be within 80 chars. I missed them in #194
"
196,Test reorganisation,2013-03-05T21:50:41Z,2013-08-16T15:00:59Z,,,,"I've recently been recommended the [py.test](http://pytest.org/latest/getting-started.html#our-first-test-run) python testing script, which looks a lot simpler and cleaner than `unittest`, so I'd like to convert our testing suite to it. At the same time, I'd like to separate the test's into multiple different files. Doing both things at the same time should save some work. I'm thinking of putting each of the tests/__init__ testing classes into their own file with the current class name first followed by _test. This means that it becomes trivial to test a easily defined subset of the testing suite by using a command such as `py.test redditor_test submission_test new_funcs_test`..

I'm thinking the future testing suite will continue to be located in the /tests folder, where there will be a number of test files and 1 helper file. This helper file will create and configure the reddit instance `r`, which is the default reddit object used in most functions. It will have the configuration details, any functions that might be shared across files such as `AuthenticatedHelper` and anything else that might be generally useful. 

Did you know the word ´self´ is in the test suite more than 1000 times? Doing it this way should get rid of all of them.

Thoughts?
"
195,Documentation consistency,2013-03-05T17:56:54Z,2013-03-20T17:52:37Z,,,,"reddit should appear as ""reddit"" in the documentation except for where it corresponds to the `Reddit` class in which case it should be linked to the documentation for the class.

Likewise, other classname references should link to their respective classes, and other function name references should link to the documentation for that function.

If you plan on working on this, please indicate via a reply so no two people are working on the same thing.
"
194,Useful_scripts.rst formatting fix,2013-03-05T11:43:08Z,2013-03-05T17:54:26Z,,,,"Fixes syntax and formatting in useful_scripts.rst
- Remove whitespace at the end of lines
- Remove 2 accidentally added _
- Add `` around Submission.all_comments
- Reformat to keep within 80 chars pr line

Also added another small commit that adds a direct link to the github page, so it becomes that much easier for people to add their own scripts to the list.
"
193,Threw the Example Apps/Scripts onto a new page.,2013-03-02T12:23:32Z,2013-03-03T09:14:14Z,,,,"I linked it in the Content Pages on the index so it would show up on read the docs. Also I plan on formatting the links so each submission is consistent and easier to read. I'm about 95% sure this pull request shouldn't cause any problems for you.
"
192,Reddit-Analysis to Example Applications/Scripts,2013-03-02T10:00:37Z,2013-03-02T10:59:36Z,,,,"Added the reddit-analysis script to the list of example scripts with a  brief description and link to the author's reddit account.
"
191,Test decorator arguments,2013-03-01T01:16:17Z,2013-09-14T20:26:42Z,,,,"Calling `r.get_mod_log()` without the subreddit argument (and all similarly decorated functions) provides an IndexError rather than the expected invalid number of arguments to `get_mod_log`. It would be nice if we could automatically verify the arguments are correct for the decorated function before doing pre-processing. Technically this should be possible, but I'm not certain how to do it at this time.
"
190,Make restrict_access flatter and more readable.,2013-02-28T21:05:18Z,2013-02-28T23:49:50Z,,,,"What I meant in my mangled sentence in #187 is that `@restrict_access`  is complicated and I would like to get it simplified. So here is a commit to do that :)

Calling `is_mod_of_all` with arguments is strictly not necessary, but it makes it more readable. Plus it sounds like the function might be useful somewhere else and this way we can move the function to `helpers` or elsewhere without any problems.
"
189,Add testing for (un)distinguish.,2013-02-28T20:01:41Z,2013-03-01T23:45:10Z,,,,"After http://redd.it/19ak1b distinguished is now an attribute,
and (un)distinguish can be tested,
"
188,Remove obsolete test case of users sharing cache.,2013-02-27T16:01:54Z,2013-02-28T03:05:46Z,,,,"With 1252908, the cache is now cleared upon logging in, which
means the test case tests for something which is no longer true.
"
187,Add @restrict_access to (un)friend.,2013-02-23T23:57:28Z,2013-02-26T05:16:54Z,,,,"We must be logged in with user/pswd to run (un)friend methods.
"
186,Links are returned with HTML-escaped ampersands,2013-02-23T10:12:52Z,2013-02-24T21:51:07Z,,,,"Example code:

```
subreddit = api.get_subreddit('programming')
return {item.url for item in subreddit.get_new(limit=100)}
```

The URL's in the returned set contain `&amp;` instead of `&`. Since we're not in a HTML-related context, that seems somewhat inappropriate.
"
185,No way of catching captcha requests?,2013-02-22T22:47:00Z,2013-02-23T03:10:58Z,,,,"I can't seem to find a way to catch captcha requests.

I want to store ID/URL somewhere, so I can solve them manually when needed. Currently it's not possible to write a bot/daemon without intervention when sending a PM
"
184,Evict OAuth me when setting credentials.,2013-02-22T11:48:15Z,2013-02-23T02:40:02Z,,,,"This fixes a bug with multiple `set_access_credentials` calls
within `cache_timeout` with the 'identity' scope and
`update_user` not set to `False`. Because config['me'] is
cached, the subsequent calls will return the cached and
inaccurate result.

Bug found because of [this r/redditdev thread](http://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/)
"
183,accept_mod_invite() doesn't update cached list of moderated subs,2013-02-19T00:08:19Z,2013-02-23T03:59:00Z,,,,"Accepting a mod invite won't allow you to perform mod-restricted actions on that subreddit, since the new sub won't be added to the cached list automatically.
"
182,Migration2,2013-02-18T11:22:00Z,2013-02-18T11:31:49Z,,,,"If you could pull this, so we can look at on readthedocs, then that would be great :)

It is my assumption that the edit button and image scaling is handled by RTD.

I gave up on the idea of keeping everything within 80 chars. I've tried to keep most within that limit, but accepted that code lines will extend beyond that. But I still want the actual code part i.e. without indentation and whatever to be below 80 chars, so it follows the style guides when copy pasted. 

Since I was looking over the entire documentation, I also found some phrasing that I thought could be made simpler & clearer. So the scope of this has been expanded a bit.

What needs to be done now is
- Ensure those two assumptions listed above are true.
- Implement any changes coming from our discussions of these commits.
- Making sure updates in version work correctly.
- Merge the last two squashable commits. The latter is being annoying with git and the first I wanted to emphazise to you. Hmm, maybe it should not be squashed and just be on it's own.
"
181,Automatic update checks should be off by default,2013-02-17T14:42:23Z,2013-02-17T19:54:13Z,,,,"I woke up to an inbox full of messages from various cron jobs telling me that an update was released 10 minutes ago... 20 minutes ago... 30 minutes ago...  I had no idea that praw was phoning home each time it ran.  Kinda disturbing.  This should not be the default behavior.   At the very least, you should document this highly unusual behavior in the configuration wiki.
"
180,Reddit wiki support,2013-02-15T18:26:47Z,2013-03-17T16:58:29Z,,,,"Would be great if praw supported editing reddit's new wiki.
"
179,"request_json with ""after""",2013-02-14T21:19:19Z,2013-02-14T22:10:57Z,,,,"I am trying to get this json:

http://www.reddit.com/r/all/new/.json?count=1&after=t3_5xpl2/

With this code:

```
url = ""http://www.reddit.com/r/all/new/""
data = {'count': '1','after': ""t3_""+ ""5xpl2""}
response = r.request_json(url,data=data)
```

But it returns a 502 error.  I have tried a few tweaks to this code, and tried using r._request, but nothing has worked so far.  Some tweaks (which I assume are incorrect), such as using params instead of data, return a .json that has no children. 
"
178,Migrating documentation from wiki to readthedocs,2013-02-13T23:03:49Z,2013-03-02T01:36:07Z,,,,"The work is not completed. But I wanted to show you what I've got so far, so we can avoid duplication of effort and to ensure we are in agreement about the new documentation. I don't want to make a patch bomb, that you disagree with.

The main changes in this is converting the wiki from markdown to restructured text. They've been upload as they are in their current state, so any changes to them will be easy to see in separate commits. And changes will be necessary, to ensure that when the new documentation is uploaded it is actually accurate.

Having documentation being versioned alongside the code sound awesome. I had thought it was bad we had the documentation split, but I had never realised that benefit to moving everything to readthedocs.

This is PR for issue  #173 

TODO List
- [x] Make enumerated numbers increment numbers
- [x] Add relative links. Ideally so it doesn't break if a file's title is changed.
- [x] Fix tables. Find or write a method to create the tables, because writing by hand seems both highly timeconsuming and brittle.
- [x] Make headings consistent. Multiple main titles in a file will both be shown/linked to when doing toctree.
- [x] Make changes so everything is within 80 chars
- [x] Make images scaled, in oauth one image goes off the left side.
- [x] Add content page links to sidebar
- [x] Update changelog to latest version.
- [x] Reuse code in README.rst and index.rst to prevent duplicating work and mismatching.
- [x] Import CHANGES.rst into documentation.
- [x] Update comment parsing to latest version.
- [x] Update the Configuration files to latest version
- [x] Update contributors guide
- [x] The old praw.rst is not linked to in any toctree. Figure out what to do with it
- [x] Upload new branch as a seperate package on readthedocs. Test it looks good.
- [x] Ensure there is a edit on github page link
- [x] Update project. Test readthedocs update everything correctly.
"
177,praw installation problems,2013-02-12T20:53:47Z,2013-02-12T22:50:09Z,,ImportError,ImportError: No module named 'update_checker',"Hello, this is my first attempt at any type of API interaction, or python for that matter.

I get the error:

import praw
Traceback (most recent call last):
  File ""<pyshell#2>"", line 1, in <module>
    import praw
  File ""C:\Python33\lib\praw__init__.py"", line 35, in <module>
    from update_checker import update_check
ImportError: No module named 'update_checker'

Any ideas?  I've googled extensively to try and find a solution but nothing is appearing.  Thanks for your time.
"
176,Add read scopes to a bunch of functions,2013-02-10T08:49:01Z,2013-02-16T02:07:03Z,,,,"I've not added tests for `get_new`, `get_top`, ´get_controversial`in the base Reddit object. They would  be equivalent to`get_front_page`.

The `read scope` on `get_submission` has been removed, it's need is covered in `from_url`.
"
175,"Cannot pull friends list for a user, unexpected redirect",2013-02-05T09:12:29Z,2014-09-16T23:03:03Z,Bug,praw.errors.ClientException,praw.errors.ClientException: Unexpected redirect from http://www.reddit.com/prefs/friends/.json to https://ssl.reddit.com/prefs/friends/.json,"So I tried the following with Python3 and the latest HEAD of praw

import praw
service = praw.Reddit(user_agent='some_experiment.py - penguindreams.org')
service.login('usnermae','****')
service.user.get_friends()

and I get this error:

praw.errors.ClientException: Unexpected redirect from http://www.reddit.com/prefs/friends/.json to https://ssl.reddit.com/prefs/friends/.json

sherlock on IRC (#reddit-dev) said bboe was aware of it and is working on it. I just wanted to add it to the issue list. 
"
174,Support modconfig scope on get_stylesheet,2013-02-04T20:07:31Z,2014-10-24T14:48:07Z,Feature,,,"I have the necessary code along with a test, however, this requires upstream support in GET_editreddit.
"
173,Move all wiki documentation into /docs (readthedocs),2013-02-04T18:33:33Z,2013-03-02T01:52:50Z,,,,"Documentation should be versioned along with the source. While the wiki works fairly well, the entire point of starting the read-the-docs page was to eventually have all the up-to-date documentation for PRAW versioned along with the source.

Moving the existing documentation should just mean converting all the pages from markdown to rdoc (I think) format, updating the index and then verifying that readthedocs updates properly.

A great example to follow is requests: https://github.com/kennethreitz/requests/tree/master/docs
"
172,Provide interface to a user's friend list,2013-02-03T02:44:43Z,2013-02-03T10:13:21Z,,,,"User Inv1s1ble on IRC would like this functionality.

Use the json interface here: https://ssl.reddit.com/prefs/friends/.json
"
171,Remove unneded auth code.,2013-02-03T01:27:34Z,2013-02-03T10:41:58Z,,,,"With 71c678cd1eee0a76a both login and set_access_credentials
run clear_authentication. This means some of the code lines
in those two methods are now unneded. 

I removed `update_user`. I don't think there is a need for it anymore since `clear_authentication` is run before it sets `r.user = None`.
"
170,Update logged-in-user's has_mail with get_unread.,2013-02-03T00:15:44Z,2013-02-03T10:31:18Z,,,,
169,Create link from Redditor PrivateMessageMixin,2013-01-30T01:15:32Z,2013-01-30T01:49:41Z,,,,"Add _methods to `LoggedInRedditor` so `get_unread`, `get_inbox`, `get_mod_mail` and `get_sent` can be can be used from the `LoggedInredditor` object as well as from Base Reddit. Just like in version 1.0.16 and earlier. 
"
168,get_flair_list doesn't require any special access,2013-01-29T07:29:23Z,2013-01-29T08:04:14Z,,,,
167,"Add support for ""unmoderated"" listing",2013-01-26T00:20:55Z,2013-01-26T02:55:00Z,,,,
166,Gilded comments,2013-01-22T09:08:10Z,2013-01-22T18:12:14Z,,,,"Two commits about comments. The first make `get_comments` part of `_methods` so it can be used by the Reddit object, similair to get_flair or get_banned. This change means helpers._get_section is now only used to generate non-subreddit listings as per it's docstring.

I also made `get_all_comments` simply give 'all' as the subreddit parameter in a call to `get_comments`. /comments and r/all/comments are identical and both pull from unsubscribed subreddits. But maybe we can't be sure this will always be this way and have to essentially duplicate `get_comments`?

The second adds `gilded_only` to both `get_all_comments` and `get_comments`.
"
165,Fix for Python 3.x in replace_more_comments(),2013-01-18T06:52:25Z,2013-01-18T20:38:57Z,,,,"There was a bug that made replace_more_comments() unusable for Python 3.x. In the`_extract_more_comments()` function a heap requires the ability to make a comparison. You did add the `__cmp__` function to the `MoreComments` class, but unfortunately `cmp()` has been gotten rid of in Python 3 and been replaced by `__lt__`. Anyway, there it is. You will probably understand it more if you just look at the code, I'm bad at explaining stuff. Anyway, enjoy!
"
164,Fix limit=None bug in replace_more_comments,2013-01-18T00:42:57Z,2013-01-18T00:44:49Z,,TypeError,TypeError: unsupported operand type(s) for -=: 'NoneType' and 'int',"This fixes the following error, which came from trying to decrement `remaining` when it was `None`.

``` python
>>> import praw
>>> r = praw.Reddit('bug example for limit=None by u/_Daimon_')
>>> s = r.get_submission('http://www.reddit.com/r/AskReddit/comments/16r8tc/what_
little_tricks_do_you_play_on_your_so/')
>>> s.replace_more_comments(limit=None, threshold=20)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""c:\Python27\lib\site-packages\praw\objects.py"", line 849, in replace_more
_comments
    remaining -= 1
TypeError: unsupported operand type(s) for -=: 'NoneType' and 'int'
>>>
```
"
163,Improve & bugfix modaction,2013-01-15T12:50:39Z,2013-01-17T01:22:29Z,,,,"This adds the parameters `mod` and `type` to `get_mod_log`. Tested with both username and `Redditor` object.

equality didn't work for `ModAction` because it doesn't have the variable `fullname`. But it has `mod_id36` instead which can be used to compare between two `ModAction`s.

For this testing I created and modded a new account named 'PyAPITestuser4' , the pswd for the acct is the same as usual.
"
162,Name and parameter consistency,2013-01-14T20:08:36Z,2013-01-15T08:52:24Z,,,,"- Prefix any functions that _should_ be prefixed with `get_` (E.g., flairlist)
- Separate single-word terms that should be separated (downvote and upvote are okay) (E.g., modqueue)
- All functions that end up using `get_content` should have their limit set to 0 by default (not None).
- Fix any other consistency issues.
"
161,Remove OAuth examples. OAuth docu is now in wiki.,2013-01-11T19:23:50Z,2013-01-11T19:44:12Z,,,,"I've added the [OAuth](https://github.com/praw-dev/praw/wiki/OAuth) page to the wiki. Which is more up to date with the current code base, must simpler and alongside all the other documentation. Since it is now there, we can remove the old examples. #158
"
160,Rename limit to content_limit. Add limit.,2013-01-10T22:20:08Z,2013-01-11T07:29:53Z,,,,"`limit` is a reddit parameter of how many results are returned with every requests. PRAW also uses `limit` as a parameter for a lot of methods. So it gives confusing function calls like ´subreddit.get_hot(limit=None, params={'limit': 100})`and for people familiar with the reddit api. So I renamed`limit`to`content_limit`and added a limit parameter that works like the upstream`limit` parameter. 
"
159,Update changelog for 2.0,2013-01-10T02:30:07Z,2013-01-15T08:31:29Z,,,,"I don't expect anything significant to change at this point assuming I don't discover issues when testing so will you please update the changelog with everything for version 1.1?
"
158,Fix OAuth2 example in examples directory,2013-01-10T02:25:04Z,2013-01-11T19:45:09Z,,,,
157,Add non-interactive OAuth2 tests,2013-01-10T02:22:30Z,2013-01-14T20:05:54Z,,,,"Create an obtain refresh_tokens for various scopes and use those to create non-interactive OAuth2 tests.
"
156,Depreciated gh-pages,2013-01-08T22:10:15Z,2013-01-09T10:20:29Z,,,,"It hasn't been updated in 9 months and was more misleading than helpful. I stripped out anything but the bare essentials to make the page simple and very clear it's been depreciated. I saw someone just a month or two link to the old mellort gh-page, so I'd like to keep this for 6-12 months before removing the gh-pages entirely. So anyone visiting the gh-pages get a helpful message and not simple a big HTTP error.
"
155,Refactor BaseReddit Mixins,2013-01-08T10:44:57Z,2013-01-10T02:23:40Z,,,,"Reddit object should have the following mix-ins it inherits from (note rename from Extension to Mixin)
### AuthenticationMixin

Primarily this mixin should include two functions: login and set_access_credentials. When either is called it switches the state between using a session-cookie or using a scoped OAuth2 session.
### OAuth2Mixin

Provides the functionality for obtaining access tokens
### LoggedInOnlyMixin

Provides functions that require the user to be logged in and do not have any OAuth2 scope.

Additionally there should be a mixin for each OAuth2 scope that groups all of its functions. Some of the functions are only available on the RedditContent object, that's okay. The decorators for each function will enforce the scoping rules.

Finally, all functions that do not require any form of authentication should be part of the BaseReddit class.
"
154,Fix OS-dependent error in upload_image.,2013-01-08T09:27:47Z,2013-01-08T10:24:00Z,,,,"Some operation systems, such as windows, treat binary and text files
differently. For them opening an image with 'r' and not 'rb' would
cause the cryptic ClientException '`image` is not a valid image'. 
The same would also be true if the mode was some varient of 'w'
irrespective of OS.

Also, the old `white-square` file was a copy of the `invalid` file not `white-square.jpg`, which caused `test_jpg_no_extension` to fail for the minimum size requirement.
"
153,Oauth login.,2013-01-07T22:34:52Z,2013-01-08T12:02:56Z,,,,"- Add set_access_credentials to explicitly set access_token and
  refresh_token.
  - Add set_oauth_app_info to explictly set app info / retrieve
    it from the configuration file.
  - Stop retrieving the app info from the configuration file to make
    these variables changeable.

This will make oauth login similair to regular login, but mutually exclusive. It allows app info and credentials to be changed more easily and sets `r.user` when setting access credentials. But at the same time also recover gracefully if the scope of oauth doesn't include `identity`. A 403 error will only be given if we are authenticated as the user, but lack `identity` scope. For bad access token it will give a 401.
"
152,Fix OAuth user warnings.,2013-01-07T22:22:10Z,2013-01-09T02:27:11Z,,,,
151,Moderator,2013-01-06T02:45:55Z,2013-01-08T01:52:58Z,,,,"I know you prefer separate pull requests, but I think the StopIteration error fix is small and uncontroversial enough to be here as well.
"
150,Fix all pep257 errors,2013-01-05T23:17:09Z,2013-01-07T10:31:36Z,,,,"As reported by `lint.sh`.
"
149,Cannot get *all* user comments,2013-01-03T03:04:11Z,2013-01-03T03:07:21Z,,,,"When trying to get all user comments, it seems to be limited to 1,000 comments.

``` python
r = praw.Reddit(user_agent='Mozilla/5.0')
user = r.get_redditor(username)

for i, c in enumerate(user.get_comments(limit=None)):
    # whatevs...
```

The account I'm using has a lot more than 1,000 comments.

Is this a limitation imposed by praw? Or by reddit? Is there anyway around this limitation?
"
148,Documentation and style fixes/improvements.,2013-01-01T19:19:52Z,2013-01-05T21:30:01Z,,,,"Stuff changed, from commit msg
- param as_objects in request_json has been rephrased to keep it within
  one line as the automated documentation creater has problems if the
  final param (in the generated order) streches over multiple lines.
  - Change **init** docstring to reflect that it is the docstring for the
    PRAW package, eg what people get when they help(praw).
  - Add link to Reddit.com's feedback page to 'send_feedback'.
  - Put the summary part of the docstring on it's own line for consistency.
  - Modify README so the first three sentences doesn't start wiht 'PRAW',
    use the word 'designed' instead of 'coded' to show following API rules
    is a design decision, i.e something which won't change.
  - Added that downloading from github or checking out the code may
    return unstable development code.
"
147,Oauth,2013-01-01T16:55:32Z,2013-01-03T23:29:11Z,,,,"Like I said in the reddit thread, I wanted to get in some documentation changes before the big 1.1, that is mainly what the first commit is about as well as modifying the new oauth code to match the documentation standard in the other parts of the codebase.

I refactored oauth away from objects and into an extension of Reddit. This way we don't have to pass around  `requst_fn`, we avoid duplication, maintain that `objects` only contain Things, a subset of Things or stuff affecting things. It also makes Oauth implemented more like regular login. Having Oauth be a subset of Reddit rather than an independent class should not be a problem, as there won't be a need for more than 1 instance of the class.

As it is now, it's impossible to change Oauth client. So I would like to add a method `set_oauth_client` that sets `client_id`, `client_secret` and `redirect_url` much likes `login`. Another method `set_oauth_credentials` that takes an access token and/or a refresh token, which would allow for changing who you're authenticated as in programs rather than having to initialize a new Reddit instance just to log in or asking the user to re-authenticate. Then `access_token` and `refresh_token` can be removed from the Reddit initialization.

I would also like to abstract away refreshing access tokens. I think this can be done by creating a callable class, say `OAuth_access_token`, that would decorator `helpers._request`. It would be placed between `Memoize` and `Sleepafter`. It would check if the access token was still valid, and if not either refresh it or raise an `APIException`. It would work much like `Memoize`. 

I also think `_handle_request` can be refactored into `helpers._request`, but it's not that important and can wait. Stuff like testing for Oauth is more importment.

Before going ahead with those changes I wanted to hear your thought on both them and these two commits.  I'll try and catch you on IRC later, that might be faster.

And a happy New Year to you :)
"
146,Making post list requests fetch 100 posts at once instead of 25.,2012-12-20T03:22:38Z,2012-12-20T03:24:13Z,,,,"It is my experience that the reddit api allows you to fetch up to 100 links in a single api request, but PRAW appears to only support 25 at a time. Is there a way too configure this, or is there a possibility of configuration for this becoming a feature? I've checked praw.ini docs and can't find anything there.
"
145,Add configure_flair,2012-12-09T22:45:54Z,2012-12-27T00:11:49Z,,,,"The big change.

c/p'ing some arguments from last time

> configure_flair currently doesn't come with any tests, because there doesn't seem to be any way of testing it via the api. I've tested it manually and everything seems to work as intended. Potentially we could test it by scraping Reddit, but I would prefer to keep all testing within the API.
> 
> I added the variable link_flair_enabled to configure_flair to make it identical to change link_flair and user_flair. I also stripped the trailing '_enabled', from the self_assign variables because they were a bit lengthy and link_flair_self_assign = True is perfectly readable.
"
144,Minor add,2012-12-09T22:44:23Z,2012-12-27T00:12:22Z,,,,"Two semi-large new functionality.
"
143,Minor changes,2012-12-09T22:42:59Z,2012-12-27T00:13:05Z,,,,"Minor documentation and code fixes. The major code change is in require_moderator, which now supports wrapped RedditContentObjects and not just Reddit objects.
"
142,Add configure_flair and some minor changes,2012-11-23T22:42:21Z,2012-12-09T21:01:25Z,,,,"`configure_flair` currently doesn't come with any tests, because there doesn't seem to be any way of testing it via the api. I've tested it manually and everything seems to work as intended. Potentially we could test it by scraping Reddit, but I would prefer to keep all testing within the API.

I added the variable `link_flair_enabled` to `configure_flair` to make it identical to change link_flair and user_flair. I also stripped the trailing '_enabled', from the self_assign variables because they were a bit lengthy and `link_flair_self_assign = True` is perfectly readable. 
"
141,Small docstring fix,2012-11-18T16:59:27Z,2012-11-19T19:55:27Z,,,,
140,Add /r/random support.,2012-11-15T23:57:41Z,2012-11-16T01:21:41Z,,,,"Suggested by /u/powerlanguage: http://www.reddit.com/r/learnpython/comments/139fxw/get_a_random_subreddit_using_praw/
"
139,Update readme regarding installation instructions,2012-11-13T14:52:19Z,2012-11-13T16:47:50Z,,,,"Fedora (17 and 18) users can now install praw by typing `sudo yum install python-praw`, because I packaged it for Fedora. You might want to mention that in your readme file :)
"
138,Renamed and move LoggedInExtension.get_saved_links,2012-11-12T02:40:30Z,2012-11-16T00:07:39Z,,,,"I would like to have get_saved_links renamed to get_saved and moved to LoggedInRedditor before the 1.1 update. It is the logical place for it to be now that there is get_liked, get_disliked and get_hidden there.
"
137,Bugfix,2012-11-12T02:37:37Z,2012-11-16T00:07:39Z,,,,"Two small bugfixes.
"
136,Import error in python 3.3,2012-11-01T18:24:59Z,2012-11-01T20:05:36Z,,KeyError,KeyError: 'praw.compat.urljoin',"```
Python 3.3.0 (default, Sep 29 2012, 15:50:43)
[GCC 4.7.1 20120721 (prerelease)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import praw
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/user/.local/lib/python3.3/site-packages/praw/__init__.py"", line 24, in <module>
    from praw import decorators, errors, helpers, objects
  File ""/home/user/.local/lib/python3.3/site-packages/praw/decorators.py"", line 30, in <module>
    from praw.compat import urljoin  # pylint: disable-msg=E0611
  File ""<frozen importlib._bootstrap>"", line 1558, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1528, in _find_and_load_unlocked
KeyError: 'praw.compat.urljoin'
```

Source: http://www.reddit.com/r/learnpython/comments/12fuph/error_when_importing_praw_with_python_330/
"
135,praw.compat.Request error,2012-10-30T01:04:20Z,2012-10-30T01:10:11Z,,ImportError,ImportError: No module named praw.compat.Request,"Installed praw on Python 2.6.2 and when trying to do the normal ""import praw"" I get this error. To verify six is installed, I also imported it first.

```
Python 2.6.2 (r262:71600, Apr 30 2009, 13:08:22)
[GCC 4.3.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import six
>>> six
    <module 'six' from '/usr/lib/python2.6/site-packages/six.pyc'>
>>> import praw
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
      File ""/usr/lib/python2.6/site-packages/praw/__init__.py"", line 24, in <module>
    from praw import decorators, errors, helpers, objects
  File ""/usr/lib/python2.6/site-packages/praw/decorators.py"", line 224, in <module>
    from .objects import RedditContentObject
              File ""/usr/lib/python2.6/site-packages/praw/objects.py"", line 31, in <module>
        from praw.helpers import (_get_section, _get_sorter, _modify_relationship,
  File ""/usr/lib/python2.6/site-packages/praw/helpers.py"", line 20, in <module>
    from praw.compat import (Request, quote,  # pylint: disable-msg=E0611
  File ""/usr/lib/python2.6/site-packages/praw/compat.py"", line 40, in load_module
    raise ImportError('No module named {0}'.format(fullname))
ImportError: No module named praw.compat.Request
```
"
134,Add OAuth 2 support,2012-10-24T22:04:49Z,2012-12-28T10:58:58Z,,,,"This adds basic support for using praw as an OAuth 2 client. A dependency on rauth is added, so as not to worry about code for composing authorization URLs or fetching tokens.  Once a client is authorized via OAuth, all API requests are simply directed to the oauth domain, with a bearer token authorization header added to each request.

I've also added a couple of rough examples under `examples/oauth`.
"
133,(un)hide and is_username_available,2012-10-19T12:29:27Z,2012-11-05T19:44:38Z,,,,"Reddit raises an exception if a username submitted is invalid and returns false if it is already taken. This difference can also be seen on the web-end when trying to create a new account. I've folded it into one, so both taken and invalid usernames give the result False and no errors is raised. This should simplify things for any client program trying to test whether a new redditor can be created with that name.

There are more explanations on the why and the what in the commit messages.
"
132,ModeratorUser tests broken,2012-10-16T19:15:46Z,2013-01-08T01:50:56Z,,,,"Users must now confirm being a moderator. The moderator user tests can now verify that a message was sent to the desired user and then confirm that they want to in-fact be a moderator.
"
131,Better cache-handling,2012-10-16T17:54:09Z,2012-12-27T00:20:24Z,,,,"ketralnis recomments passing LM/etag ""I'd recommend that you start, especially since people seem to be using PRAW to do polling""

Look into storing/passing this information along.
"
130,Testing and some code / documentation changes,2012-09-30T03:06:57Z,2012-10-16T19:14:41Z,,,,"I threw this into a separate request to make stuff clearer. Added non-equality, because before testing for non equality would also result true unless comparing the same python object.

``` python
import praw
r = praw.Reddit('test')
a = r.get_subreddit('python')
b = r.get_subreddit('python')
a != b
# Returns True
a == b
# Also returns true
```
"
129,3 Small commits,2012-09-27T19:28:32Z,2012-09-30T03:19:04Z,,,,"I didn't notice that get_liked didn't get evicted from the cache after upvoting, because test_get_liked accidentally works because of the cache. We don't seem to use it anywhere, so would there be any problems with simply setting config.cache_timeout to 0 in BasicHelper.configure?
"
128,Update praw/settings.py,2012-09-27T00:37:52Z,2012-09-27T22:22:48Z,,,,"The ENV while running praw under inittab/upstart supervision is limited. HOME doesn't exist and causes the process to die leading to ""respawning"" messages.

This simply adds a check for HOME and if it doesn't exist use PWD, which appears to always exist. However, it should also be noted that, unless explicitly set, on my CentOS 6.3 system, PWD always equals /.

If this isn't in place, a/the proper workaround/solution is to explicitly set HOME in the init/upstart (ie /etc/init/reddit.conf) script using, for example ""env HOME=/root"".
"
127,Added mark_as_NSFW,2012-09-25T17:24:32Z,2012-09-25T20:06:15Z,,AttributeError,AttributeError: '_MovedItems' object has no attribute 'compat',"I'm btw still unable to get the module to properly test my changes. With python 2.7 the command `python setup.py test` gives me this traceback

```
Traceback (most recent call last):
  File ""setup.py"", line 43, in <module>
    test_suite='praw',
  File ""c:\Python27\lib\distutils\core.py"", line 152, in setup
    dist.run_commands()
  File ""c:\Python27\lib\distutils\dist.py"", line 953, in run_commands
    self.run_command(cmd)
  File ""c:\Python27\lib\distutils\dist.py"", line 972, in run_command
    cmd_obj.run()
  File ""c:\Python27\lib\site-packages\setuptools-0.6c11-py2.7.egg\setuptools\command\test.py"", line 121, in run

  File ""c:\Python27\lib\site-packages\setuptools-0.6c11-py2.7.egg\setuptools\command\test.py"", line 101, in with_project_on_sys_path

  File ""c:\Python27\lib\site-packages\setuptools-0.6c11-py2.7.egg\setuptools\command\test.py"", line 130, in run_tests

  File ""c:\Python27\lib\unittest\main.py"", line 94, in __init__
    self.parseArgs(argv)
  File ""c:\Python27\lib\unittest\main.py"", line 149, in parseArgs
    self.createTests()
  File ""c:\Python27\lib\unittest\main.py"", line 158, in createTests
    self.module)
  File ""c:\Python27\lib\unittest\loader.py"", line 128, in loadTestsFromNames
    suites = [self.loadTestsFromName(name, module) for name in names]
  File ""c:\Python27\lib\unittest\loader.py"", line 103, in loadTestsFromName
    return self.loadTestsFromModule(obj)
  File ""c:\Python27\lib\site-packages\setuptools-0.6c11-py2.7.egg\setuptools\command\test.py"", line 34, in loadTestsFromModule

  File ""c:\Python27\lib\unittest\loader.py"", line 103, in loadTestsFromName
    return self.loadTestsFromModule(obj)
  File ""c:\Python27\lib\site-packages\setuptools-0.6c11-py2.7.egg\setuptools\command\test.py"", line 34, in loadTestsFromModule

  File ""c:\Python27\lib\unittest\loader.py"", line 91, in loadTestsFromName
    module = __import__('.'.join(parts_copy))
  File ""----\reddit_api\praw\compat.py"", line 37, in load_module
    mod = getattr(package, module)
AttributeError: '_MovedItems' object has no attribute 'compat'
```
"
126,add prevstyle to set_stylesheet requests,2012-09-14T18:48:18Z,2012-09-23T00:24:48Z,,,,"Earlier in the week reddit rolled out a new wiki system which also included stylesheet versioning to prevent people from overwriting eachother's changes.

The attached code will perform a get_stylesheet request before the set_stylesheet request so it can pass along the prevstyle argument so the request goes through.

PS. sorry for the earlier pull request, it wanted to delete the whole file and create it again. oops.
"
125,add prevstyle to set_stylesheet requests,2012-09-14T18:01:18Z,2012-09-14T18:04:52Z,,,,"Earlier in the week reddit rolled out a new wiki system which also included stylesheet versioning to prevent people from overwriting eachother's changes.

The attached code will perform a get_stylesheet request before the set_stylesheet request so it can pass along the prevstyle argument so the request goes through.
"
124,Reworked login to work with praw.ini.,2012-09-14T14:26:18Z,2012-09-23T00:23:17Z,,,,"The Docstring said that a user could modify praw.ini to allow for
automatic login. How to do that was both unclear and had bug potential,
the login function would only function if user/pswd was either missing
or included a username/password. If it was empty, then it would accept
the empty string as the username/password rather than query user.
"
123,Code Cleanup.,2012-09-14T14:25:15Z,2012-09-23T01:45:32Z,,,,"In addition to the original, I've changed a ""== None"" to ""is None"".
- Removed bool_str
  - Added limit_chars to **unicode** in Submission.
  - Moved magic numbers regular_comments_max and gold_comments_max
    to parw.ini.
  - Removed unnecesary checks for login/moderator in get_moderator
    and get_flair.
  - Some minor code and documentation fixes.
"
122,Added Refreshable object to allow clients to requery for latest values.,2012-09-14T14:23:50Z,2012-09-23T00:51:06Z,,,,
121,"Added get_liked, get_disliked and get_hidden to LoggedInRedditor",2012-09-14T13:55:02Z,2012-09-23T00:39:16Z,,,,
120,"Refreshing, reworked login and some more stuff.",2012-09-09T15:05:37Z,2012-09-23T01:46:48Z,,ValueError,ValueError: Attempted relative import in non-package,"I added a way of refreshing objects, as per [issure 105](https://github.com/praw-dev/praw/issues/105). I removed bool_str, because it's macro that isn't more clarifying than the standard python syntax. I worked the login function to work with praw.ini, now you can fill out your username in praw.ini and only fill out password when testing things in the interpreter. That should save some mindless char typing. I added \@limit_chars to __unicode__ for submission, so it's kept within a terminal line just like comments. The reasoning behind the remaining changes should be fairly clear and uncontroversial.

I've been unable to run tests.py as I get this error.

``` python
    from . import backport
ValueError: Attempted relative import in non-package
```

Not that it matters because I don't have access to r/apitest. So the included tests are as is. I think they are correct. I tested them in an external file. But no guarantees. 

That should be that. Hope you like the changes :)
"
119,Redditor.get_comments() doesn't return all comments?,2012-08-23T09:58:47Z,2012-08-23T16:16:13Z,,,,"I'm trying to scrape and analyze Dan Harmon's most recent AMA, but I'm only getting 26 comments. Perhaps I'm doing it wrong? But from my undestanding, the default argument to time is 'all' (python 2.6.6)

```
import praw
from os import utime

r = praw.Reddit(user_agent=$BOTNAME)
dh = r.get_redditor('danharmon')
for c in dh.get_comments():
    id = c.id
    t = c.created_utc
    if c.link_id != u't3_yne9x': # Will break as soon as he comments on a different post, I know
        print 'done!'
        break
    with open(id, 'wb') as f:
        print ""Writing comment"", id
        f.write(c.body.encode(""UTF-8""))
        utime(id, (t, t))
```
"
118,Support grabbing the list of popular subreddits,2012-08-15T19:55:54Z,2012-08-19T18:38:24Z,,,,"Just what it says in the title. Needed this for something I'm writing.
"
117,Documentation update,2012-08-14T11:09:04Z,2012-08-28T03:36:42Z,,,,"Lots of changes to make PRAW comply with the [documentation guideline.](https://github.com/praw-dev/praw/issues/116).

Please don't close this pull request before we are in agreement. I would like to add some additional commits to this pull request later also related to documentation, so that the discussion about the new documentation is all in one place.

So. What do you think?

Could you come up with a way of cutting the docstring for [_json_redditor_objector](https://github.com/Damgaard/reddit_api/blob/master/praw/__init__.py#L207) into a single summary line?
"
116,PRAW Documentation Guidelines,2012-08-12T22:33:30Z,2012-08-28T03:40:24Z,,,,"# Purpose

To draft a document to establish a common documentation style in the PRAW

The PRAW has had contributions from many people and it shows. By establishing and enforcing a common documentation strategy for the PRAW, the source code will become easier to read and understand. This will lower the barrier of entry, ease maintenance and generally make it easier for everybody.
## Proposals for documentation guidelines
- Unless otherwise stated, PRAW will follow the [PEP8 style guide](http://www.python.org/dev/peps/pep-0008/).
- As well as the [PEP 257 docstring guide.](http://www.python.org/dev/peps/pep-0257/)
- All publicly available functions, classes and modules should have a docstring.
- Things should keep the same name throughout the code. *_kwargs should never be *_kw.
- Use the imperative form. Eg ""Get the subreddit."" not ""Gets the subreddit."" or ""PRAW will get the subreddit.""
- For one-liner docstrings, the entire string including opening and closing """""""" will be on the same line.
- For multi-line comments the one-liner summary will be on the line after the opening """""" and the closing """""" will be on the line after the last line of the docstring. Example
  
  """"""
  Summary line. Sum its up.
  
  Longer explanation.
  """"""
- Use correct terminology. A subreddits name is something like ' t3_xyfc7'. The correct term for a subreddits ""name"" like [python](http://www.reddit.com/r/python) is it's display name.
- Things should be stored in the same datastructure throughout the code.
- When referring to the current reddit we are working on, use ""the reddit"", when referring to reddit.com reddit use ""reddit"". 
- Do not explain common variables. There will be a page on the wiki explaining ""limit"", that it sets to default it if's value is <= 0 and unlimited with None.

I intend to look at PRAWs documentation in the coming time and make some improvements. If we agree on precisely how the documentation should look beforehand, then that would make everything easier.
"
115,Fixed docs build,2012-08-12T01:15:36Z,2012-08-12T02:00:55Z,,,,"The directory containing the source code was named reddit, but has since
been renamed to praw. I've modified conf.py to reflect this. Also a few
minor formatting fixes, keeping things <80 characters.
"
114,Documentation Fails to Build,2012-08-12T01:12:33Z,2012-08-12T02:01:28Z,,,,"It still thinks that the directory containing the source is named `reddit`, but it has been renamed to `praw`. Will send pull request with a fix.
"
113,"Hey, I made a few changes",2012-08-07T03:28:07Z,2012-08-12T02:27:54Z,,,,"In developing TuxReddit, I noticed a few limitations of your API.
1. I added a url_data argument to the get_front page method. This is useful for passing an 'after' field.
2. I added a get_new method. Just like get_front_page, it only gets new submission from your subreddits if signed in. This also implements the url_data argument.
3. I added a get_controversial method. Same pattern as get_new and get_front_page.

Thanks for your work,
Calvin
"
112,Added Python version and dependency information to README,2012-07-31T05:10:48Z,2012-08-06T15:41:12Z,,,,"The authors of praw-dev/praw#111 and a recent [r/learnpython](http://www.reddit.com/r/learnpython/comments/vxa9u/trouble_trying_reddit_api/) thread both had problems installing PRAW. This was because the user tried using PRAW with python 2.5 when it requires at least 2.6. Adding this information to the README should make it easier for people with a similar problem in the future to trouble shoot. 
"
111,Install errors (syntax errors),2012-07-31T00:06:50Z,2012-07-31T14:27:23Z,,SyntaxError,SyntaxError: invalid syntax,"I have tried installing praw on 2 different servers (1 Slackware, 1 CentOS) and both got this same error regarding HTTPError and syntax error. I didn't see any dependencies listed though. I am running Python 2.5.2, and on the CentOS one I tried using easy_install. Neither worked, here's the full install error/log:

/usr/local/lib/python2.5/distutils/dist.py:263: UserWarning: Unknown distribution option: 'install_requires'
  warnings.warn(msg)
/usr/local/lib/python2.5/distutils/dist.py:263: UserWarning: Unknown distribution option: 'test_suite'
  warnings.warn(msg)
running install
running build
running build_py
creating build
creating build/lib
creating build/lib/praw
copying praw/**init**.py -> build/lib/praw
copying praw/backport.py -> build/lib/praw
copying praw/decorators.py -> build/lib/praw
copying praw/errors.py -> build/lib/praw
copying praw/helpers.py -> build/lib/praw
copying praw/objects.py -> build/lib/praw
copying praw/settings.py -> build/lib/praw
copying praw/tests.py -> build/lib/praw
copying praw/praw.ini -> build/lib/praw
running install_lib
creating /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/**init**.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/backport.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/decorators.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/errors.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/helpers.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/objects.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/settings.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/tests.py -> /usr/local/lib/python2.5/site-packages/praw
copying build/lib/praw/praw.ini -> /usr/local/lib/python2.5/site-packages/praw
byte-compiling /usr/local/lib/python2.5/site-packages/praw/**init**.py to **init**.pyc
  File ""/usr/local/lib/python2.5/site-packages/praw/**init**.py"", line 193
    except HTTPError as error:
                      ^
SyntaxError: invalid syntax

byte-compiling /usr/local/lib/python2.5/site-packages/praw/backport.py to backport.pyc
byte-compiling /usr/local/lib/python2.5/site-packages/praw/decorators.py to decorators.pyc
  File ""/usr/local/lib/python2.5/site-packages/praw/decorators.py"", line 96
    except errors.BadCaptcha as exception:
                              ^
SyntaxError: invalid syntax

byte-compiling /usr/local/lib/python2.5/site-packages/praw/errors.py to errors.pyc
byte-compiling /usr/local/lib/python2.5/site-packages/praw/helpers.py to helpers.pyc
byte-compiling /usr/local/lib/python2.5/site-packages/praw/objects.py to objects.pyc
byte-compiling /usr/local/lib/python2.5/site-packages/praw/settings.py to settings.pyc
byte-compiling /usr/local/lib/python2.5/site-packages/praw/tests.py to tests.pyc
/usr/local/lib/python2.5/site-packages/praw/tests.py:101: Warning: 'with' will become a reserved keyword in Python 2.6
  File ""/usr/local/lib/python2.5/site-packages/praw/tests.py"", line 101
    with warnings.catch_warnings(record=True) as w:
                ^
SyntaxError: invalid syntax

running install_egg_info
Writing /usr/local/lib/python2.5/site-packages/praw-1.0.2-py2.5.egg-info
"
110,Stylesheet errors are not throwing errors.,2012-07-30T05:25:12Z,2012-11-20T20:29:34Z,,,,"You can update with invalid stylesheets and it'll just go through silently. It won't update but errors aren't reported. 
"
109,Gives the class Comment the attribute score through a property decorated function.,2012-07-29T06:22:28Z,2012-07-29T17:30:01Z,,,,"Functions doing karma calculations can now use `score` for both submissions and comments.

I closed the previous pull request as I had accidentally pulled from mellorts now depreciated fork.
"
108,Support image upload,2012-07-25T23:22:35Z,2013-01-06T22:58:15Z,,,,
107,Added an option to get_content to retrieve data as raw json data,2012-07-23T00:32:49Z,2012-07-23T04:34:25Z,,,,"See: https://github.com/praw-dev/praw/issues/106
"
106,Getting raw json data,2012-07-22T21:48:45Z,2012-07-23T04:43:43Z,,,,"I don't see a way to get the raw json data.  Is there a way?

If not I was thinking of adding a flag for it in ____init____.get_content() so that it returns the json data for the page_url given. Would this feature be desired?

Great library, Thanks!
"
105,A way to bypass the cache and pull in updated data from Reddit?,2012-07-21T16:14:06Z,2012-09-23T01:47:55Z,,,,"I may have overlooked something, but is there a straightforward way to re-query Reddit and pull in new data? 

Consider:

```
> submission.comments
Out[197]: []

> submission.add_comment('test')
Out[198]: <reddit.objects.Comment at 0x4823730>

> submission.comments
Out[199]: []
```

Note that the data in `submission.comments` is outdated.
"
104,Updated require_moderator decorator to support multi-reddits,2012-07-11T05:08:24Z,2012-07-11T05:50:59Z,,,,
103,"Added an option to log the underlying http requests being made, useful for debugging/learning",2012-07-10T13:55:43Z,2012-07-10T23:55:18Z,,,,"While learning about the API and debugging I've found it helpful to see the underlying http requests as they're made.

I would be happy to build this out to use more legitimate logging techniques, please let me know what your thoughts are.  I did it this way because it was quick & easy and the project doesn't use python's `logging` modules anywhere else.

Here's what my IPython terminal looks like with the logging:

```
In [14]: top5 = list(r.get_subreddit('funny').get_hot(limit=5))
retrieving: http://www.reddit.com/r/funny/.json
data: None

In [15]: top5
Out[15]:
[<praw.objects.Submission at 0x310ba90>,
 <praw.objects.Submission at 0x310b810>,
 <praw.objects.Submission at 0x2d20ad0>,
 <praw.objects.Submission at 0x2d200d0>,
 <praw.objects.Submission at 0x310bb90>]

In [16]: print len(top5[0].all_comments_flat)
retrieving: http://www.reddit.com/r/funny/comments/wbbaf/well_played_simpsons_ad_well_played/.json
data: None
retrieving: http://www.reddit.com/api/morechildren/.json
data: r=funny&link_id=t3_wbbaf&uh=5dq5xo1l2t1de3251a5922b4b27b9a3a5b9ce95758fa354e29&api_type=json&children=c5bwsba%2Cc5bwlpb%2Cc5bx076%2Cc5bx4yw
retrieving: http://www.reddit.com/api/morechildren/.json
data: r=funny&link_id=t3_wbbaf&uh=5dq5xo1l2t1de3251a5922b4b27b9a3a5b9ce95758fa354e29&api_type=json&children=c5bygra
retrieving: http://www.reddit.com/api/morechildren/.json
data: r=funny&link_id=t3_wbbaf&uh=5dq5xo1l2t1de3251a5922b4b27b9a3a5b9ce95758fa354e29&api_type=json&children=c5bxzep
203

In [17]:
```
"
102,"""user is a mod of /r/____"" check doesn't support multi-reddits",2012-07-09T03:26:24Z,2012-07-11T06:53:34Z,,,,"A mod of /r/sub1 and /r/sub2 should be able to call `r.get_subreddit('sub1+sub2').get_reports()` (and all of the other mod-restricted functions), but isn't currently able to. Mod-restricted functions for multi-reddits should work if the user is a mod of _all_ (not just some) of the subreddits in the multi-reddit.
"
101,Configuration path settings,2012-07-08T22:23:07Z,2012-07-08T23:22:45Z,,,,"Line 31 on settings.py reads:

```
os_config_path = os.path.join(os.environ['HOME'], '.config')
```

But in my particular configuration (mod_wsgi under apache), this will raise a KeyError exception because os.environ['HOME'] isn't set.  

Could you please change it to read something along the lines of:

```
os_config_path = os.path.join(os.environ['HOME'], '.config') if os.environ.has_key('HOME') else '.config'
```
"
100,Documentation: Add general section on object attribute discovery via `vars`.,2012-06-29T17:11:33Z,2012-09-26T15:20:54Z,,,,
99,Documentation: Make it clear that PRAW has built in rate-limiting support.,2012-06-29T17:10:39Z,2012-12-27T19:24:37Z,,,,
98,Redditor object should implement a comparison function for efficiency.,2012-06-25T04:22:28Z,2012-12-27T00:14:14Z,,,,"The comparison should compare Redditor names such that extra lookups aren't required and comparing to None returns False.
"
97,Double submitting,2012-06-15T23:11:04Z,2012-06-16T18:13:47Z,,,,"I just coded a stats bot using this API /r/userinfo but it keeps double submitting it. I only have one r.submit call but it still double submits. I start the bot and when it finished it posts. I then close the window when it asks for a CAPTCHA (even if i enter the correct one it still double posts and it posts the first one before i touch the window) yet it still double posts even after i close the window.
"
96,Checking if account is a moderator fails for new account,2012-06-15T01:01:38Z,2012-08-19T23:07:12Z,,,,"I noticed that the moderator checking functionality of the API, used by ""moderator_required_function"" decorator, fails for accounts that are very new, and have made no posts or comments, accrued no karma, etc. At least, this is the only difference between a number of accounts that I have tested which cause this issue. Any reason for this, or is it a bug?
"
95,I get ascii encoding errors running through the intro example,2012-06-03T15:00:06Z,2012-06-03T19:46:52Z,,UnicodeEncodeError,UnicodeEncodeError: 'ascii' codec can't encode character u'\u2019' in position 194: ordinal not in range(128),"Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/willcritchlow/ve/anything/local/lib/python2.7/site-packages/reddit/objects.py"", line 77, in **str**
    retval = self.**unicode**()
  File ""/home/willcritchlow/ve/anything/local/lib/python2.7/site-packages/reddit/objects.py"", line 487, in **unicode**
    return '{0} :: {1}'.format(self.score, title)
UnicodeEncodeError: 'ascii' codec can't encode character u'\u2019' in position 194: ordinal not in range(128)

This is the result of running:
[str(x) for x in submissions] in the example given here:
https://github.com/mellort/reddit_api
"
94,Added score attribute to Comment to allow cleaner code,2012-05-21T17:51:27Z,2012-07-29T06:06:32Z,,,,"Before you had to calculate the score yourself with comment.ups - comment.downs. By letting the generator calculate it, the users can have cleaner more readable code. Secondly, submission already has this attribute, so by giving it to Comment as well we allow users to reuse karma calculation functions without having to intentionally degrade the readability.
"
93,RateLimitException can't find 'ratelimit' field,2012-05-21T16:06:56Z,2012-05-22T02:38:17Z,,,,"The `RateLimitException` checks the JSON response of the action for the `ratelimit` field; it apparently does not exist in at least some cases.

```
reddit/decorators.py:117: UserWarning: Unknown return value key: ratelimit
  warnings.warn('Unknown return value key: %s' % k)
```
"
92,Make note of domain consistency,2012-05-15T19:06:14Z,2012-09-01T23:14:52Z,,,,"As per [this thread](http://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/), make a note regarding the server name in a configuration file matching that of the default server name for the reddit install.
"
91,README Fix,2012-05-06T03:11:09Z,2012-05-11T03:20:26Z,,,,"In the intro, fixed the amount of submissions grabbed (the paragraph said 10, the example code showed 5), also a capitalization fix.

Also added easy_install to the installation instructions.
"
90,Added doc strings to the functions in class Distinguishable,2012-04-29T22:20:29Z,2012-04-29T22:43:28Z,,,,
89,Small suggestion: change the default remove() to remove(spam=False),2012-04-29T19:04:08Z,2012-04-29T22:44:25Z,,,,"I think it's safer because remove(spam=True) has the side effect of training the spam filter, which is potentially harmful if you didn't expect it.
"
88,"Submission.banned_by is returning a Redditor object for ""banned_by"": ""true""",2012-04-27T19:49:51Z,2012-04-27T20:22:57Z,,,,"Posts banned by spam filter have the JSON result `""banned_by"": ""true""`, and reddit_api is interpreting `true` as a Redditor.

As an aside, the JSON `""banned_by""` and `""approved_by""` values are broken/confusing, since they're both set to the same name when someone _does_ ban a submission; and furthermore `""banned_by""` is set to `""true""` for posts that _aren't_ removed or approved.
"
87,Message.Subreddit.compose_message() doesn't seem to know I'm logged in,2012-04-25T00:30:20Z,2012-04-25T00:39:25Z,,AttributeError,AttributeError: 'function' object has no attribute '__self__',"I'm trying to send a mod message using `Message.Subreddit.compose_message()`, but Reddit or reddit_api seems to forget that I'm logged in, and calls `get_captcha()`.

Here's a debug session that shows the issue:

```
> (Pdb) msg.subreddit.reddit_session.user
<reddit.objects.LoggedInRedditor object at 0x04CACF90>

> (Pdb) msg.subreddit.reddit_session.user.is_mod
True

> (Pdb) msg.subreddit.compose_message('test', 'test')
*** AttributeError: 'function' object has no attribute '__self__'
```

Here's the full traceback:

```
  File ""C:\Program Files (x86)\Python\2.7\lib\site-packages\reddit-1.3.6-py2.7.egg\reddit\objects.py"", line 194, in compose_message
    return self.reddit_session.compose_message(self, subject, message)
  File ""C:\Program Files (x86)\Python\2.7\lib\site-packages\reddit-1.3.6-py2.7.egg\reddit\decorators.py"", line 198, in login_required_function
    return function(self, *args, **kwargs)
  File ""C:\Program Files (x86)\Python\2.7\lib\site-packages\reddit-1.3.6-py2.7.egg\reddit\decorators.py"", line 88, in __call__
    kwargs['captcha'] = self.get_captcha(captcha_id)
  File ""C:\Program Files (x86)\Python\2.7\lib\site-packages\reddit-1.3.6-py2.7.egg\reddit\decorators.py"", line 94, in get_captcha
    url = urljoin(self.function.__self__.config['captcha'],
AttributeError: 'function' object has no attribute '__self__'
```
"
86,pip installation fails,2012-04-22T23:00:14Z,2012-04-23T19:14:05Z,,ImportError,ImportError: No module named six,"OS - ubuntu 11.10

```
(venv)luis@linux:~/Dropbox/projects$ pip install reddit
Downloading/unpacking reddit
  Downloading reddit-1.3.6.tar.gz
  Running setup.py egg_info for package reddit
    Traceback (most recent call last):
      File ""<string>"", line 14, in <module>
      File ""/home/luis/venv/build/reddit/setup.py"", line 1, in <module>
        from reddit.version import VERSION
      File ""reddit/__init__.py"", line 16, in <module>
        import reddit.backport  # pylint: disable-msg=W0611
      File ""reddit/backport.py"", line 16, in <module>
        from six import MovedAttribute, add_move
    ImportError: No module named six
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""<string>"", line 14, in <module>

  File ""/home/luis/venv/build/reddit/setup.py"", line 1, in <module>

    from reddit.version import VERSION

  File ""reddit/__init__.py"", line 16, in <module>

    import reddit.backport  # pylint: disable-msg=W0611

  File ""reddit/backport.py"", line 16, in <module>

    from six import MovedAttribute, add_move

ImportError: No module named six

----------------------------------------
Command python setup.py egg_info failed with error code 1
Storing complete log in /home/luis/.pip/pip.log
```
"
85,Request: link-flair support,2012-04-16T20:11:16Z,2012-04-20T19:44:17Z,,,,"This was added to Reddit recently, but don't think reddit_api supports it yet.
"
84,Can't build docs when reddit_api is not installed,2012-04-16T14:12:13Z,2012-08-19T23:06:05Z,,ImportError,ImportError: No module named reddit,"I'm packaging reddit_api for Fedora, and I wanted to include the docs in my package, but due to the way RPM works, it is not possible to have reddit_api installed on the system that builds it.

Therefore, I get

```
  File ""/home/elad/sources/reddit_api/docs/conf.py"", line 15, in <module>
    import reddit
ImportError: No module named reddit
```

when I try to build the docs.

Ways to make it possible include adding a line to modify sys.path in docs/conf.py, and it sounds like an easy fix for me.

The sooner you can fix this, the sooner reddit_api will be an official Fedora and EPEL package :)
"
83,get_community_settings() for use with set_community_settings(),2012-04-04T23:21:14Z,2012-04-18T00:29:42Z,,,,"Added a function to make changing a subreddit's settings easier/more dynamic. The code in its current state is quite horrid, and I'm sure there's a better way to parse the options, but I am still a bit new to Python :s

objects.py now imports HTMLParser if thats ok, I couldn't find a competent standard unescape function other than the one in that package :S

Maybe use regex to get the options quicker?
"
82,clear_all_flair should chunk calls,2012-04-01T01:15:29Z,2012-04-02T23:47:32Z,,,,"The csv api call only accepts 100 lines at a time, but clear_all_flair doesn't check if flair_list returns more than 100 users.
"
81,Request: ability to send mod messages,2012-03-28T03:05:02Z,2012-04-02T22:21:08Z,,,,"Doesn't seem like it's currently possible to compose a new mod-mail message?
"
80,Cuts off post title,2012-03-26T17:32:39Z,2012-03-26T17:49:57Z,,,,"When i get a post title if it is too long it will cut it off, is there any way around this?
"
79,Request: approved_by/removed_by support,2012-03-26T16:27:09Z,2012-04-02T21:40:55Z,,,,"Looks like Reddit recently added this to the JSON.
"
78,HTTP 404 for get_submitted() call on deleted user,2012-03-25T02:15:36Z,2012-03-30T01:42:21Z,,,,"Here's an example of a post by a deleted user:

```
sub = r.get_submission(submission_id='rbw6o')
auth = sub.author 
```

Certain things seem to work okay, like `auth.name`, but `list(auth.get_submitted())` raises a 404 error.

Perhaps this should be handled a little differently, like making `auth.get_submitted()` return `None`, or implementing some other way of detecting deleted users?
"
77,backport.py causes errors when installing in Python 2.x,2012-03-24T05:31:31Z,2012-04-02T20:24:31Z,,,,"Python 2.x doesn't have the `six` module, `setup.py` fails.
"
76,Implement support for updating sub-reddit stylesheets,2012-03-19T00:29:27Z,2012-03-23T18:12:45Z,,,,"This would allow for the automated creation of things like post flair (for the X hottest posts in order to keep stylesheet size manageable)

API in reddit's codebase is here
https://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py#L1091
"
75,Enable editing of comments and self submissions.,2012-03-17T18:30:25Z,2012-03-29T23:58:04Z,,,,
74,Caching of error messages defeats posting retries,2012-03-17T07:07:47Z,2012-03-30T07:37:07Z,,,,"https://github.com/reddit/reddit/issues/382

My bot was attempting to comment on a post and hit the rate limiter. I was rate limited for 9 minutes. 9 minutes later I was rate limited for 53 seconds. 53 seconds later I was rate limited for 202 milliseconds. All of that is as expected, although there is a pull request pending elsewhere regarding eliminating the millisecond instance.

This is where it got weird. My bot waited 1 second and tried again, and was again rate limited for 202 milliseconds. This repeated 30 times, over a period of 30 seconds, before it aborted. I think that this behavior is a bug, and bboe has indicated the bug is because the result was cached undesirably.
"
73,"RateLimitExceeded.sleep_time asserts on singular ""second"" and any ""milliseconds""",2012-03-15T17:02:59Z,2012-04-03T19:32:21Z,,,,"sleep_time doesn't handle ""1 second"" or ""123 milliseconds"" correctly. it probably also fails on ""1 minute"".

my attempt at a solution is as follows...

modified and new regular expressions:

```
MINUTES_RE = re.compile('(\d+) minutes?')
SECONDS_RE = re.compile('(\d+) seconds?')
MILLISECONDS_RE = re.compile('(\d+) milliseconds?')
```

new match condition for the new regex:

```
    match = self.MILLISECONDS_RE.search(self.message)
    if match:
        return 1
```
"
72,BaseReddit.get_content() assumes url_data will be a dict,2012-03-14T08:36:04Z,2012-03-14T16:58:55Z,,,,"```
  File ""/usr/local/lib/python2.7/dist-packages/reddit/__init__.py"", line 257, in get_content
    url_data['after'] = root[after_field]
```

This will fail if url_data is a list instead of a dict. urllib.urlencode allows url_data to be a mapping object OR a list of two-element tuples.
"
71,Added new 'spam' parameter to remove,2012-03-10T17:19:27Z,2012-03-10T18:06:19Z,,,,
70,Add cross-compatibility for python3,2012-03-03T05:45:30Z,2012-03-23T18:09:48Z,,,,
69,Submission.add_comment() returns dictionary instead of Comment object,2012-03-03T00:37:56Z,2012-03-03T03:20:03Z,,,,"Not a bug, but I think `Submission.add_comment()` should return a single `Comment` instead of a JSON dictionary:

```
>>> s.add_comment('test')
{u'data': {u'things': [<reddit.objects.Comment at 0x4dee750>]}, u'errors': []}
```

Seems better to abstract away some of that stuff.
"
68,Add timeout flag to Reddit._request,2012-02-27T05:02:25Z,2012-02-27T18:10:56Z,,,,"This patch adds a timeout argument to Reddit._request. It defaults to 30, in reddit_api.cfg. This prevents client application lockups due to waiting indefinitely on an API call to finish.
"
67,Message.replies is dictionary instead of list of Message objects,2012-02-24T21:30:42Z,2012-03-03T03:19:49Z,,,,"```
In [23]: r.user.get_modmail().next().replies
Out[23]: 
{u'data': {u'after': None,
  u'before': None,
  u'children': [<reddit.objects.Message at 0x5055290>,
   <reddit.objects.Message at 0x50558b0>],
  u'modhash': u'xxxxxxxxxxxxxx'},
 u'kind': u'Listing'}
```

Not a huge problem, but this is inconsistent with `Comment.replies`.
"
66,NSFW issues,2012-02-20T19:09:18Z,2012-04-03T19:01:37Z,,,,"There are two issues:
- Non-authenticated users must provide an over18 cookie in order to access NSFW pages
- I suspect authenticated users must have the over18 field enabled in their settings in order to access NSFW pages

Ideally we will provide a method for obtaining the over18 cookie and either automatically perform this when an over18 exception is raised (if possible) or prompt the user for action.
"
65,Missing a way get subscribed subreddits?,2012-02-18T22:54:54Z,2012-02-19T00:46:47Z,,,,"I looked through the source and wiki and could not find a method to get a list of subscribed subreddits for a logged in user.

The `account cloner` app in the examples (https://github.com/mellort/reddit_account_cloner) uses Reddit.get_my_reddits() to get the list, but I could not find this method anywhere in the newest branch.
"
64,MoreComments: Child comment retrieved before its parent,2012-02-18T02:38:41Z,2012-02-28T07:32:31Z,,KeyError,KeyError: u't1_c3pv5gy',"[Reference URL](http://www.reddit.com/r/politics/comments/pinf0/a_week_ago_i_was_antigay_rights_now_im_)

The above submission results in the following:

```
File ""reddit/objects.py"", line 486, in _fetch_morecomments
    tmp = self._comments_by_id[comment.parent_id].replies
KeyError: u't1_c3pv5gy'
```

Upon initial inspection, it is in fact the case that we fetch the child before its parent, though we do eventually fetch its parent. However, this submission also results in fetching the same comment more than once which is unexpected. Additional investigation is needed to learn why this occurs.

The ideal solution will guarantee we process the parent prior to its children so that we do not need to maintain an orphaned list.
"
63,HTTP Error 429: Unknown when fetching submissions,2012-02-16T14:40:41Z,2012-02-16T17:21:56Z,,HTTPError,HTTPError: HTTP Error 429: Unknown,"Hi,

I am the developer of http://rvytpl.appspot.com/.  First of all, thank you for providing the Python library for the Reddit API.  It's very convenient and easy to use.

I'm encountering a problem relatively frequently in this code:

``` python
reddit_api = reddit.Reddit(user_agent=USER_AGENT)
subreddit = reddit_api.get_subreddit('videos').get_top(limit=100)
new_entries = { }
for rank in range(100):
    if rank and (rank % 25 == 0):
        logging.info('Fetched %d entries, sleeping' % rank)
        time.sleep(60)
    entry = subreddit.next()
    new_entries[entry.id] = (rank, entry)
```

I'm trying to fetch the top 100 entries of the videos subreddit.  I get this exception:

``` python
HTTP Error 429: Unknown
Traceback (most recent call last):
  File ""/base/python_runtime/python_lib/versions/1/google/appengine/ext/webapp/_webapp25.py"", line 703, in __call__
    handler.post(*groups)
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/admin.py"", line 62, in post
    entry = subreddit.next()
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/__init__.py"", line 231, in get_content
    page_data = self.request_json(page_url, url_data=url_data)
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/decorators.py"", line 110, in error_checked_func
    return_value = func(self, *args, **kwargs)
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/__init__.py"", line 265, in request_json
    response = self._request(page_url, params, url_data)
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/__init__.py"", line 165, in _request
    url_data)
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/util.py"", line 44, in __call__
    return self._cache.setdefault(key, self.func(*args, **kwargs))
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/decorators.py"", line 100, in __call__
    return self.func(*args, **kwargs)
  File ""/base/data/home/apps/s~rvytpl/1.356858274957999053/reddit/helpers.py"", line 97, in _request
    response = reddit_session._opener.open(request)
  File ""/base/python_runtime/python_dist/lib/python2.5/urllib2.py"", line 387, in open
    response = meth(req, response)
  File ""/base/python_runtime/python_dist/lib/python2.5/urllib2.py"", line 498, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/base/python_runtime/python_dist/lib/python2.5/urllib2.py"", line 425, in error
    return self._call_chain(*args)
  File ""/base/python_runtime/python_dist/lib/python2.5/urllib2.py"", line 360, in _call_chain
    result = func(*args)
  File ""/base/python_runtime/python_dist/lib/python2.5/urllib2.py"", line 506, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
HTTPError: HTTP Error 429: Unknown
```

This only happens after 25, 50 and 75 entries are fetched.  I've tried increasing the sleep time, to no avail.  What could be going wrong?

Cheers,
Michael
"
62,httplib.IncompleteRead exception handling,2012-02-13T03:05:25Z,2012-02-13T19:19:23Z,,,,"While crawling comments, every hour or two I would get an exception with a traceback ending with:
httplib.IncompleteRead: IncompleteRead(1555 bytes read)

This may be an problem on my end, or an issue with Reddit. Regardless, the library should probably be able to work through this hiccup without ending the whole program.

So I added exception handling for http.IncompleteRead to response.read() in helpers._request. If the connection is terminated before read is complete, the response will be read again until successful or a different error is raised.  Repeating the whole read operation does not seem to be an issue since the amount of data lost was relatively small.
"
61,httplib.IncompleteRead,2012-02-12T21:13:11Z,2012-02-13T18:48:38Z,,,,"The `return response.read()` statement in helpers._request occasionally raises httplib.IncompleteRead during extended sessions.  This may be a network error on my end, or it could be on reddit's end.

I am testing a patch, and will submit a pull request if successful.
"
60,Fix for issue #58,2012-02-04T15:26:59Z,2012-02-17T22:32:12Z,,,,
59,"Disregard that, ...",2012-02-04T15:10:11Z,2012-02-04T15:17:35Z,,,,
58,"KeyError: 'data' (MoreComments class, comments method)",2012-02-04T14:52:19Z,2012-02-17T22:29:59Z,,KeyError,KeyError: 'data',"  File ""C:\Python\Python27\lib\site-packages\reddit-1.2.5-py2.7.egg\reddit\objects.py"", line 326, in comments
`self._comments = response['data']['things']`
KeyError: 'data'

I believe this is a result of the previous line:
`response = self.reddit_session.request_json(url, params)`
returning null or some other NOT dict; for whatever reason.

Recommendation is to check for this occurrance and fail silently.
"
57,Submission.author occasionally returns unicode string instead of Redditor?,2012-02-02T18:18:02Z,2012-02-28T19:40:39Z,,AttributeError,AttributeError: 'unicode' object has no attribute 'name',"I'm occasionally getting this exception:

```
print post.author.name.encode('utf-8')
AttributeError: 'unicode' object has no attribute 'name'
```

I don't know the author or post right now since it happens after running for a long time, but I suspect the post or author may be deleted. Perhaps in this case it should raise an exception?
"
56,Feature request: interface to search.json,2012-02-02T16:14:57Z,2012-03-03T01:36:34Z,,,,"I would like an interface to reddit search via the mellort wrapper:

An example search GET query: http://www.reddit.com/search.json?q=funny

Additional accepted GET params are:

restrict_sr: on | off
sort: new | top | relevance
limit: 1 - 100
"
55,Comment replies don't show up,2012-01-31T01:16:11Z,2012-03-06T01:03:41Z,,,,"The `comment.replies` list is empty for a comment that I _know_ has replies. Code looks like this:

```
print r.get_redditor('roger_').get_comments().next().replies
```
"
54,Missing 'permalink' attribute in comments,2012-01-29T20:27:33Z,2012-01-31T22:01:56Z,,,,"I'm getting this traceback:

```
print comment.permalink
 File ""C:\Program Files (x86)\Python\2.7\lib\site-packages\reddit-1.2.4-py2.7.egg\reddit\objects.py"", line 58, in __getattr__
attr))

 AttributeError: <class 'reddit.objects.Comment'> has no attribute 'permalink'
```

(The comment ID in this case is: c30rjqb)

Not sure why the comment would have no `permalink` value. This also causes `reply()` to fail.
"
53,Getting all comments from subreddit,2012-01-22T18:19:49Z,2012-01-31T22:15:30Z,,,,"I may be missing something, but I don't see a way of getting all comments for a specific subreddit (`http://reddit.com/r/sub/comments`). Useful for much the same things as reddit.get_all_comments - I'd think you could pretty much just make that function take an optional subreddit argument (and add a call to that to the Subreddit class). In fact while writing this comment, I did just that, and it seems to be working. I'd create a pull request, but I suck at git, and it's quite trivial anyway.
"
52,Method to change subreddit settings (sidebar etc),2012-01-22T17:41:19Z,2012-01-25T10:59:28Z,,,,"Unfortunately there's no API method to _get_ the current settings, so you'll need to supply them all if you want to change something. You can get the sidebar text though, which does help a bit, depending on what you want to do.
"
51,Moderation functions,2012-01-22T00:20:03Z,2012-01-25T08:26:55Z,,,,"Added in approve/remove functions, as well as fetching from the spam and modqueue pages.
"
50,The subreddit _get_sorter functions seem to be stuck on time = hour,2012-01-13T18:42:03Z,2012-01-13T21:13:55Z,,,,"I think the culprit is this bit of code being deleted in the last commit:

https://github.com/mellort/reddit_api/commit/7fab73a645cd1f6d2f37cc858b781c6e1bf3698a#commitcomment-865769
"
49,LoggedInRedditor.my_reddits() error,2012-01-11T08:51:42Z,2012-01-13T05:03:40Z,,TypeError,TypeError: string indices must be integers,"When using `r.user.my_reddits()`, I get the error 

```
Traceback (most recent call last):
  File ""/home/john/.virtualenvs/rdoqdoq/lib/python2.7/site-packages/flask/app.py"", line 1518, in __call__
    return self.wsgi_app(environ, start_response)
  File ""/home/john/.virtualenvs/rdoqdoq/lib/python2.7/site-packages/flask/app.py"", line 1506, in wsgi_app
    response = self.make_response(self.handle_exception(e))
  File ""/home/john/.virtualenvs/rdoqdoq/lib/python2.7/site-packages/flask/app.py"", line 1504, in wsgi_app
    response = self.full_dispatch_request()
  File ""/home/john/.virtualenvs/rdoqdoq/lib/python2.7/site-packages/flask/app.py"", line 1264, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/home/john/.virtualenvs/rdoqdoq/lib/python2.7/site-packages/flask/app.py"", line 1262, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/home/john/.virtualenvs/rdoqdoq/lib/python2.7/site-packages/flask/app.py"", line 1248, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/home/john/Projects/rdoqdoq/app.py"", line 110, in login
    my_reddits.append(subreddits.next())
  File ""/home/john/.virtualenvs/rdoqdoq/src/reddit/reddit/__init__.py"", line 221, in get_content
    root = page_data[root_field]
TypeError: string indices must be integers
```

lines 215 - 220 in `__init__.py`

```
while fetch_all or content_found < limit:
        page_data = self.request_json(page_url, url_data=url_data)
        if root_field:
            root = page_data[root_field]
        else:
            root = page_data
```

Here's what I got when printing variables in the above snippet:

```
page_url => 'http://www.reddit.com/reddits/mine/'
url_data => {}
page_data => {}
root_field => 'data'
```

So... `page_data` is an empty dict but being indexed by `root_field`?

I can't figure out why `page_data = self.request_json(page_url, url_data=url_data)` is returning an empty dict.

I hope this makes sense. 
"
48,r.login() parameters in README,2011-12-27T15:13:18Z,2011-12-27T16:39:21Z,,,,"Bboe changed the login parameter names from user/password to username/password in commit b57d14887490c2d1f4711df8bf4cd16fd9ba2166, breaking the example in the README.md file. My commit simply does away with the named parameter use in the readme alltogether. It does not touch any code of the project.
"
47,Subreddit Object does not check that it exists on creation.,2011-12-14T21:50:14Z,2013-04-22T18:50:47Z,,,,"If you call get_subreddit on a nonexistent subreddit, it creates the object regardless of whether or not the subreddit exists. To check whether a subreddit exists, you have to call an equivalent of `r.get_subreddit(""_df"").get_hot(limit=1).next()`.

I can go ahead and stick that sanity check in there on creation, but first I'd like to confirm whether or not the lazy-fail version is a design decision or an oversight.
"
46,"return value of ""submit"" has changed",2011-12-14T19:41:37Z,2012-01-04T00:55:38Z,,,,"Hi,

I just noticed that the return value of the submit function changed. Example:

```
r = reddit.Reddit(user_agent=""..."")
r.login(user=cfg.USERNAME, password=cfg.PASSWORD)
res = r.submit(subreddit, title, url=url)
```

Now the return value is just this: {u'errors': []}. The previous return value was much more informative, it also included the URL of the submission on reddit. Do you know what happened?

Laszlo
"
45,Ability to report a submission,2011-12-14T14:03:33Z,2011-12-16T00:59:59Z,,,,"This adds the ability to report a particular submission to the mods of a subreddit, using the ""report"" method of a Submission object.
"
44,Ability to report a submission,2011-12-14T13:49:42Z,2011-12-14T14:03:50Z,,,,"Hello!

There is one particular feature that seems to be missing from the API: the ability to report a submission.

To report a submission, one just needs to make a request to ""api/report/"". See [here](https://github.com/reddit/reddit/wiki/API:-report) for more information.

Of course, if I just missed it and that it's indeed already possible to report a submission, you can completely ignore this issue! But I looked around in the code and couldn't find it, so I don't think it's available...

Thanks in advance!

EDIT: Just noticed how it's possible to attach your own code. Please disregard this particular issue. Sorry about that!
"
43,Issues with Python 2.7.1?,2011-12-13T21:33:03Z,2011-12-13T21:44:40Z,,,,"Getting some strange behavior on Mac OS X Lion. Perhaps something went awry during install? (I had to sudo)

```
>>> r = reddit.Reddit(user_agent='my_cool_application')
>>> submissions = r.get_subreddit('opensource').get_hot(limit=5)
>>> list(submissions)
[<reddit.objects.Submission object at 0x1070ea0d0>, <reddit.objects.Submission object at 0x1070ea310>, <reddit.objects.Submission object at 0x1070eaa50>, <reddit.objects.Submission object at 0x1070eab90>, <reddit.objects.Submission object at 0x1070eab10>]
>>> submission = submissions.next()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
StopIteration
>>> 
```
"
42,It seems not to require setuptools.,2011-12-10T22:13:39Z,2011-12-10T22:34:36Z,,,,"It seems not to require `setuptools`.  Added a common fallback for `setuptools` (`distutils` is a part of Python standard library).  This patch make the package able to be installed under system that doesn’t have `setuptools`, like:

``` console
$ python setup.py install
```
"
41,HTTPS Login,2011-12-07T19:59:22Z,2012-01-03T00:58:23Z,,,,"Need to support HTTPS login.
"
40,Tests tests tests,2011-11-30T03:40:33Z,2011-12-10T01:42:01Z,,,,"Every feature should have a test which verifies that it works as intended. Likewise, nothing should be added or modified unless a corresponding test is added that demonstrates it works properly.
"
39,Added set_flair method,2011-11-30T00:54:08Z,2011-11-30T00:56:56Z,,,,"Added support for reddit's flair api (""set_flair"" method)
"
38,"Cache submission's comments, and make tests pass.",2011-11-15T09:07:16Z,2011-11-30T00:58:36Z,,,,"In making the tests pass, I added a fix to the modhash problem. The patch that is available is actually better (I think) however, I'll leave that up to you to include.

I also added a ""get_submission"" function which returns a submission object from a given url. This is all part of a larger change I'm hoping to get through, however, it's not quite ready yet.
"
37,modhash patch,2011-11-10T22:11:34Z,2011-11-30T00:59:06Z,,,,"Hi,

Here is the patch. This is my 1st pull request, I hope I'm doing it correctly.

Laszlo
"
36,modhash solution,2011-11-10T11:38:37Z,2011-11-20T21:17:43Z,,,,"Hi,

In `reddit.py`, it is written in comments at function `_fetch_modhash`:

_# TODO: find the right modhash url, this is only temporary_

I have the following solution:

```
r = reddit.Reddit(user_agent=""my_cool_application"")
r.login(user=USERNAME, password=PASSWORD)
opener = r._opener

def get_modhash():
    url = 'http://www.reddit.com/api/me.json'
    data = json.load(opener.open(url))
    return data['data']['modhash'] if 'data' in data else False

print get_modhash()
```

It solves two things: the modhash is not in quotes (see my [previous ticket](https://github.com/mellort/reddit_api/issues/35)) and it is extractred from JSON, which is more elegant than doing regex on HTML source.

Laszlo
"
35,fetched modhash is between quotes,2011-11-10T03:26:48Z,2011-11-20T21:16:24Z,,,,"Hi,

The fetched modhash has "" characters around it that should be removed. Example:

```
r = reddit.Reddit(user_agent=""my_cool_application"")
r.login(user=USERNAME, password=PASSWORD)
modhash = r.modhash
print modhash    # ""nb8gzf3...""
```

Patch in `reddit.py` at the end of `def _fetch_modhash(self):`

```
self.modhash = self.modhash.replace('""', '')
```

Laszlo
"
34,adding comment stopped working,2011-11-10T02:57:29Z,2011-11-12T00:09:00Z,,,,"Hi,

Adding comments went well, then suddenly it stopped working. Here is my test code:

```
r = reddit.Reddit(user_agent=""my_cool_application"")
r.login(user=USERNAME, password=PASSWORD)
post = r.get_submission_by_id(POST_ID)
post.add_comment('test comment')
```

It may be some temporary problem, I don't know.

Thanks,

Laszlo
"
33,Arch Linux (feature request),2011-10-31T12:46:19Z,2012-03-23T18:10:45Z,,,,"Hi,

I just packaged reddit_api for Arch Linux. It's available as ""python2-reddit-git"". It would be great if a Python 3 version of reddit_api was also available. Python 3 is default on Arch. If tarballs were released, there could even be python2-reddit and python-reddit packages, which, if popular, could become official Arch Linux packages in the future.

Thanks.

And also thanks for the nice examples in the documentation (the .md files), it made it easy to test and verify that the API can actually do something interesting. :)

Best regards,
   Alexander Rødseth
"
32,NameError in parse_api_json_response,2011-10-18T15:50:42Z,2011-11-10T14:43:02Z,,,,
31,Login does not raise error if invalid credentials are presented.,2011-10-03T09:34:18Z,2011-12-01T01:59:50Z,,,,"calling reddit.login() does not give any indication if the credentials are incorrect. worse, it populates. the reddit.user field with a correct Redditor object, so there is no way to tell if login failed.
"
30,subreddit moderation list,2011-09-21T14:11:53Z,2011-11-10T14:41:41Z,,,,"Added ability to get the list of subreddit's the current user is a moderator for. 
"
29,Request:Get submission karma,2011-09-06T03:24:31Z,2011-11-30T03:41:41Z,,,,"is this possible?
"
28,"The url isn't working, and it doesn't look like I can call it anyways. ",2011-09-03T03:42:12Z,2011-09-03T04:49:50Z,,"""AttributeError","""AttributeError: 'Reddit' object has no attribute 'get_comments'""","""AttributeError: 'Reddit' object has no attribute 'get_comments'""
"
27,Mark Messages as Read,2011-08-26T21:22:53Z,2011-11-30T01:07:54Z,,,,"Added an option to mark Comment objects as read, as well as an Inbox method to mark all new messages as read.

Unfortunately, the orangered still shows up when browsing but clicking on it shows an empty inbox. I've [read](http://www.reddit.com/r/redditdev/comments/gp6su/apiread_message_doesnt_affect_orangered_envelope/) that you can make a request to messages/unread.json to kill the orangered, but it would take a lot more work to determine whether or not an individual comment being marked as read makes the inbox empty and I'm not sure it's worth it.
"
26,Does not get mail from inbox,2011-08-24T23:40:58Z,2011-08-26T19:58:56Z,,,,"I'm 100% positive I have mail in my inbox. The library reports that my inbox is empty.
Here's what I'm doing: 
```import reddit
r = reddit.Reddit(user_agent='whatever')
r.login(user='myusername', password='password')
r.get_inbox()

```
```
"
25,removed redundant import line,2011-08-24T21:09:17Z,2011-08-24T22:34:16Z,,,,
24,Enhancement Request: Support for new Flair APIs,2011-08-20T19:17:42Z,2011-12-02T22:59:08Z,,,,"As seen [here](https://github.com/reddit/reddit/wiki/API), Reddit has new APIs for the recently-added Flair system for subreddits.
- flairlist
- flair
- flaircsv
"
23,User karma,2011-08-14T05:59:30Z,2011-08-21T16:14:09Z,,,,"I just started playing around with this wrapper today when I noticed something strange.

I attempted to get the user karma, based on the example given in the README:

```
ketralnis = r.get_redditor(""ketralnis"")
print ketralnis.link_karma, ketralnis.comment_karma
```

However I ran into an error when I ran my test program. Apparently link_karma and comment_karma aren't defined in the Redditor class? 

EDIT: I did a search on old issues and came across [this](https://github.com/mellort/reddit_api/issues/6). I did the same test but a certain number of variables were not returned (has_mail, created, etc.)

Please correct me if I'm wrong, or doing something wrong, I was just curious as to what was happening.
"
22,added dependencies section to README.md with setuptools as a package dependency for install,2011-08-13T07:02:36Z,2011-08-21T15:51:54Z,,,,
21,Fix for issue #20 https://github.com/mellort/reddit_api/issues/20 ,2011-08-12T17:49:17Z,2011-08-21T15:51:02Z,,,,"Caused by https://github.com/mellort/reddit_api/blob/master/reddit/redditor.py#L67

```
return self._get_content(urls[""my_reddits""], limit=limit)
```

Should be `self.reddit_session._get_content()`
"
20,AttributeError: 'Redditor' object has no attribute '_get_content',2011-08-11T14:22:09Z,2011-08-21T16:14:21Z,,,,"Caused by https://github.com/mellort/reddit_api/blob/master/reddit/redditor.py#L67

```
return self._get_content(urls[""my_reddits""], limit=limit)
```

Should be `self.reddit_session._get_content()`?
"
19,Next/Previous pages for front page and subreddits,2011-08-08T19:15:52Z,2011-08-08T22:48:55Z,,,,"Is it possible to get the next/previous pages for the front page and/or subreddits? 
"
18,How do I get a Submission object given a post's title/url/id using the Python wrapper?,2011-08-08T00:00:24Z,2011-08-08T02:33:41Z,,,,"There doesn't seem to be a `.get_submission(id='t3_xxxxx')`. I'm trying to add it myself but I'm having a hard time figuring out how.  

It would b really useful, I think. Right now the only way to get submissions is by either of the `get_hot`, `get_top`, etc. methods, which doesn't offer too much control. 

Thanks in advance. :)
"
17,AttributeError: 'Subreddit' object has no attribute 'encode',2011-06-28T14:24:27Z,2011-06-30T02:26:14Z,,AttributeError,AttributeError: 'Subreddit' object has no attribute 'encode',"Hi, I was trying to write a script to delete comments for a user:

```
USER = ""foo""
PASS = ""bar""
USER_AGENT = ""baz""

r = reddit.Reddit(user_agent=USER_AGENT)
r.login(user=USER, password=PASS)

for comment in r.get_redditor(USER).get_comments():
    comment.delete()
```

I can get the comments ok, but when I try to delete them, I get a stacktrace with:

AttributeError: 'Subreddit' object has no attribute 'encode'
"
16,"Fixed use of urllib2 ""global opener"".",2011-06-23T04:15:43Z,2011-06-26T00:50:28Z,,,,"When multiple instances of a Reddit object were used (think checking messages for 2+ users at the same time), the use of urllib2.install_opener let only one set of cookies be used at a time. Instead of storing the cookie jar in self (it wasn't ever being used again), I'm storing the openerdirector that used to be ""installed"" globally and providing that with every request.
"
15,Fix to issue #9,2011-05-19T07:21:11Z,2011-06-02T18:51:43Z,,,,"Added MoreComments class with property comments to allow fetching of additional comments.

This implements one way to fix issue #9, ""Comments are Dictionaries?""

While it could be implemented such that these comments are automatically fetched, I think it should be up to the end user to perform the additional fetching. The following code is an example to fetch all the comments of a single submission:

``` python
#!/usr/bin/env python
import reddit, sys

def main():
    r = reddit.Reddit('test')
    page_url = 'http://www.reddit.com/r/UCSantaBarbara/comments/hcfq7/dae_feel_pissed_that_the_as_rep_caught_stealing/.json'

    submission_info, comment_info = r._request_json(page_url)[:2]
    submission = submission_info['data']['children'][0]
    comments = submission.comments

    ids = set()

    while len(comments) > 0:
        comment = comments.pop(0)

        if isinstance(comment, reddit.MoreComments):
            comments.extend(comment.comments)
        else:
            if comment.id in ids:
                print 'duplicate'
            ids.add(comment.id)

            comments.extend(comment.replies)

    print 'Found %d comments' % len(ids)

if __name__ == '__main__':
    sys.exit(main())
```

This solution does require the user to test the Comment instance type when iterating through comments, however, I feel that is completely acceptable.
"
14,Lots of updates,2011-05-18T19:14:30Z,2011-06-26T00:50:13Z,,,,"Added get_new_by_date function, modified get_content to work iteratively instead of recursively, and allowed the function to fetch all items.

-- See commit messages for more

Added MoreComments class with property comments to allow fetching of additional comments.

This implements one way to fix issue #9, ""Comments are Dictionaries?""

While it could be implemented such that these comments are automatically fetched, I think it should be up to the end user to perform the additional fetching. The following code is an example to fetch all the comments of a single submission:

``` python
#!/usr/bin/env python
import reddit, sys

def main():
    r = reddit.Reddit('test')
    page_url = 'http://www.reddit.com/r/UCSantaBarbara/comments/hcfq7/dae_feel_pissed_that_the_as_rep_caught_stealing/.json'

    submission_info, comment_info = r._request_json(page_url)[:2]
    submission = submission_info['data']['children'][0]
    comments = submission.comments

    ids = set()

    while len(comments) > 0:
        comment = comments.pop(0)

        if isinstance(comment, reddit.MoreComments):
            comments.extend(comment.comments)
        else:
            if comment.id in ids:
                print 'duplicate'
            ids.add(comment.id)

            comments.extend(comment.replies)

    print 'Found %d comments' % len(ids)

if __name__ == '__main__':
    sys.exit(main())
```

This solution does require the user to test the Comment instance type when iterating through comments, however, I feel that is completely acceptable.
"
13,Need new class for Messages,2011-03-27T23:52:52Z,2011-12-07T02:24:58Z,,,,"Getting messages from the Inbox object sometimes just returns json because I haven't made on object for Messages yet
"
12,About URL in base_object.py,2011-03-23T02:21:35Z,2011-03-27T23:49:52Z,,,,"This is just a recommendation, but while creating the Inbox object it took FOREVER to realize I needed to add a self.ABOUT_URL string in my __init__ or else the parent's __init__ would overflow the stack grabbing the JSON dictionary.
"
11,Reddit Inbox,2011-03-23T02:17:00Z,2011-03-27T23:49:52Z,,,,"Allows a logged in user to download and read his/her inbox.
"
10,Now a package,2011-03-02T23:25:12Z,2011-03-03T22:07:08Z,,,,"I added setup.py. Also renamed the reddit_api folder to reddit because ... ""import reddit""
"
9,Comments are Dictionaries?,2011-02-25T01:26:49Z,2011-06-26T00:53:07Z,,,,"This (or there abouts) code
import reddit
r = reddit.Reddit()
sr = r.get_subreddit(""subreddit"")
n = sr.get_hot(limit=100)
for p in n :
   for c in p.comments :
       c.body

Will sometimes throw an error that c is a dict?
"
8,Get list of subreddits?,2011-02-25T00:56:14Z,2011-02-25T08:58:52Z,,,,"Was hoping to find a method get_subreddits() which returned a list of all subreddit names, is this possible.

Side Note : Examples.md ""r = reddit.Reddit"" should be  ""r = reddit.Reddit()""
"
7,Load comment tree from a reddit,2011-02-16T00:06:40Z,2011-02-17T00:34:32Z,,,,"From each specific thread page ( e.g. http://www.reddit.com/r/programming/comments/d6r59/hey_rprog_i_made_a_python_wrapper_for_the_reddit/.json) the JSON array has 2 variables.  The first has the info about the original post (submitter, title, etc...) and this appears to be what is returned by get_hot and similar requests.  The second JSON variable is a tree of all the comments on that post.  Is there a way to access this entire variable, preferably with a single request?
"
6,How can I use this API to get karma numbers for a user?,2011-02-11T00:33:49Z,2011-02-11T00:54:39Z,,,,"It doesn't appear in the json for a user's page.  Can this API be used to retrieve the karma?  It's in the html.

Thanks
"
5,Copyright and license ambiguity,2011-01-07T23:11:00Z,2011-01-08T07:58:46Z,,,,"Code currently is ambiguous as to:
a) who the copyright holder is and
b) what (if any) license or reuse is allowed

this makes usage of this code perilous, and it probably should be fixed (see this comment I made on reddit here http://www.reddit.com/r/Python/comments/exknd/how_would_i_go_about_using_python_to_submit_links/c1buru3 , and no downvotes, it's a real problem :-)

I'd recommend picking a license and adding it to the project. If you are having difficulty picking a license, I'd be glad to offer advice and/or suggestions based upon how you'd like your code to be reused (I'm an old-hat to this stuff).
"
4,Changed WAIT_BETWEEN_CALL_TIME to two seconds,2011-01-04T05:05:00Z,2011-03-27T23:38:13Z,,,,"Just updated the default WAIT_BETWEEN_CALL_TIME and the mention in README.md. Assuming http://code.reddit.com/wiki/API is current and official.

Awesome lib, thanks!
"
3,"Support self posts, fix submitting bug with url being overwritten",2010-11-19T17:12:30Z,2010-11-20T01:16:05Z,,,,"This should be fairly simple and self-explanatory really.
"
2,Some more commits,2010-09-08T00:46:44Z,2010-11-18T02:27:43Z,,,,"Friending/Modding/Banning/Contributoring, Create a Subreddit, Some more code cleanup.
"
1,Some code to take a look at,2010-09-03T00:55:53Z,2010-09-03T15:43:54Z,,,,"Hey!

Did a pretty small amount of work on your code. I've been meaning to do a Reddit API binding myself for a while but have never found the time, so it's great that you've started it, I'd love to help if I can.

Take a look, see if what I've got interests you if you'd like. I'll be hopefully doing some more work if I get a chance. I haven't tested much other than quickly checking that I haven't broken anything you had already in an obvious way, so keep that in mind I guess as you look over the code.

Some things that I haven't fixed yet are, for example, a lot of the methods that take a parameter @limit@ will hang if given limit as a string, so we should probably be int()'ing what comes in there. The problem is that there are those parameters all over the place, so we'd need to factor a bit. There are a few other things I have here too, so I'll either get to them or you will.

Get in touch with me if you'd like, I'd love to discuss whatever with you.

Thanks!
"
